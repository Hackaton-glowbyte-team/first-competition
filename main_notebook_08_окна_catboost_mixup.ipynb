{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5606014",
   "metadata": {},
   "source": [
    "## Энергетический оракул\n",
    "Ноутбук команды #12\n",
    "\n",
    "Работа выполнена на основе модели LightGBM\n",
    "\n",
    "\n",
    "### 1. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4351135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.callback import TrainingCallback\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True )\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "random_state = 12345\n",
    "NUM_ITERATIONS = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45891200",
   "metadata": {},
   "source": [
    "#### 1.1 Функции для расшифровки прогноза погоды в колонке 'weather_pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece12617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расшифровка прогноза в колонке 'weather_pred'\n",
    "\n",
    "# функция формирует колонки 'cloudy', 'rainy', 'windy', 'clear', 'rain_probability', 'has_rain_probability'\n",
    "# в колонках число, которое 0 при отсутсвии упоминания явления в weather_pred или степень упоминания\n",
    "# функция дает в колонках номер первого списка, элемент которого есть в строке плюс 1\n",
    "# списки cloudy_list, rainy_list, windy_list, clear_list можно модифицировать\n",
    "# соответственно, можно экспериментировать с расположением значений в списках\n",
    "# например, сейчас 'дождь', 'снег', 'д+сн' - первая степень  дождя, а 'гроз', 'ливень' - вторая\n",
    "# а можно сделать снег второй, а грозу с ливнем убрать в третью\n",
    "# также сделал отдельный список для \"ясности\", чтобы выделить 'ясно' и 'солнечно'\n",
    "\n",
    "def in_what_list(weather, big_list):\n",
    "    for list_number, small_list in enumerate(big_list):\n",
    "        if any(word in weather for word in small_list):\n",
    "            return list_number+1\n",
    "    return 0\n",
    "\n",
    "def weather_split2(row):\n",
    "    weather = row['weather_pred']\n",
    "    cloudy_list = [['проясн', 'пер.об.', 'п/об'], ['пасм', 'обл']]\n",
    "    rainy_list = [['дождь', 'снег', 'д+сн'], ['гроз', 'ливень']]\n",
    "    windy_list = [['вет'],['штор']]\n",
    "    clear_list = [['проясн'], ['ясно'], ['солнеч']]\n",
    "    numbers = re.findall(r'\\d+', weather)\n",
    "    cloudy = in_what_list(weather, cloudy_list)\n",
    "    rainy = in_what_list(weather, rainy_list)\n",
    "    windy = in_what_list(weather, windy_list)\n",
    "    clear = in_what_list(weather, clear_list)\n",
    "    rain_probability = 0 if len(numbers)==0 else int(numbers[0])\n",
    "    has_rain_probability = int(len(numbers)==0)\n",
    "    return cloudy, rainy, windy, clear, rain_probability, has_rain_probability\n",
    "\n",
    "def fill_weather_columns(df):\n",
    "    df['weather_pred'] = df['weather_pred'].fillna('')\n",
    "    df['cloudy'], df['rainy'], df['windy'], df['clear'], df['rain_probability'], df['has_rain_probability'] = \\\n",
    "                zip(*df.apply(weather_split2, axis=1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f39d660",
   "metadata": {},
   "source": [
    "#### 1.2 Функции для загрузки данных о ВВП \n",
    "данные загружаются из файла 'data/VVP.csv'\n",
    "\n",
    "Некоторые научные работы указывают на прямую связь величины потребления электричества и показателя ВВП, который отражает ситуацию в экономике. Данные по экономике публикуются различными министерствами с разной периодичностью. Для использования в работе были взяты фактические данные по ВВП с сайта investing, который агрегирует публикации Минэкономразвития. Данные за месяц побликуются с месячной задержкой, поэтому модель использует для прогнозирования данные за прошлые месяцы, которые известны.   \n",
    "  \n",
    "Ссылка на данные: https://ru.investing.com/economic-calendar/russian-monthly-gdp-407\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3dd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция добавляет данные о ВВП из файла 'data/VVP.csv' в датасет\n",
    "\n",
    "def add_vvp2(data, file_source = 'data/VVP.csv'):\n",
    "    \"\"\"\n",
    "    сырой датафрем подаем на вход\n",
    "    \"\"\"\n",
    "    # обработаем файл с динамикой ВВП\n",
    "    vvp = pd.read_csv(file_source)\n",
    "    # преобразуем дату файла-источника в формат datetime64 и дропнем один столбик\n",
    "    vvp['date'] = pd.to_datetime(vvp['date'], format ='%Y-%m-%d %H:%M:%S')\n",
    "    vvp.drop('for_month',axis=1,inplace=True) \n",
    "    \n",
    "    # обработаем основной фрейм - создадим столбец для соединения, который потом удалим\n",
    "    data['date_temp'] = pd.to_datetime(data['date'], format = '%Y-%m-%d' )\n",
    "    data['date_temp'] = data['date_temp'] + pd.to_timedelta(data['time'] , 'H')\n",
    "    \n",
    "    # соединяем основной фрейм и ВВП по дате объявления показтеля ВВП\n",
    "    for idx in reversed(vvp.index):\n",
    "        data.loc[data['date_temp']>=vvp.date[idx],'VVP'] = vvp.VVP_perc[idx]\n",
    "        \n",
    "    data.drop('date_temp',axis=1,inplace=True)   \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe769599",
   "metadata": {},
   "source": [
    "#### 1.3 Функции для загрузки архива данных о фактической погоде\n",
    "данные загружаются из файла 'data/preprocessing_loaded_table.csv'\n",
    "\n",
    "Изначально данные для формирования таблицы \"preprocessing_loaded_table\" были взяты из с сайта [https://rp5.ru](https://rp5.ru/Архив_погоды_в_Храброво,_им._императрицы_Елизаветы_Петровны_(аэропорт),_METAR), где хранятся архивы погоды в аэрапорту Калининграда, за период с 31.12.2018 по 30.09.2023\n",
    "\n",
    "Описание данных в таблице:\n",
    "- Местное время в Храброво / им. императрицы Елизаветы Петровны (аэропорт) - Дата / Местное время\n",
    "- T -  Темпиратура воздуха\n",
    "- Po - Давление на уровне станции\n",
    "- P - Давление приведённое к уровню моря\n",
    "- U - Относительная влажность\n",
    "- DD - Направление ветра\n",
    "- Ff - Скорость ветра\n",
    "- ff10 - Максимальное значение порыва ветра\n",
    "- WW - Особое явление текущей погоды (осадки)\n",
    "- W'W' - Явление недавней погоды, имеющее оперативное значение\n",
    "- с - Общая облачность\n",
    "- VV - Горизонтальная дальность видимости\n",
    "- Td - Темпиратура точки росы\n",
    "\n",
    "Данные, которые были взяты из данной таблицы и загружаются из 'data/preprocessing_loaded_table.csv':\n",
    "- P - не подверглось изменению\n",
    "- U - не подверглось изменению\n",
    "- Td - не подверглась изменению\n",
    "\n",
    " WW - разделили на 4 категории:\n",
    "- Нет осадков (где были пропуски)\n",
    "- слабый дождь\n",
    "- сильный дождь\n",
    "- снег\n",
    "\n",
    "DD - создали 4 столбца, соответствующих сторонам горизонта, которые принимали значения 0; 0.5 и 1 в зависимости от силы ветра в конкретном направлении\n",
    "- N - north\n",
    "- S - south\n",
    "- W - west\n",
    "- E - east\n",
    "\n",
    "В дальнейшем эти данные использовались с лагом в сутки: в поля на завтрашний день записывались данные сегодняшнего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb3456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для работы с данными о фактической погоде из 'data/preprocessing_loaded_table.csv'\n",
    "\n",
    "# Кодировка информации об осадках из колонки WW\n",
    "def true_weather_WW_replace(ww):\n",
    "    if ww=='нет осадков':\n",
    "        return 0\n",
    "    elif ww=='слабый дождь':\n",
    "        return 1\n",
    "    elif (ww=='сильный дождь') or (ww=='снег'):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "# Вычисление Timestamp из даты и времени\n",
    "def row_plus_hours_to_index(row):\n",
    "    return row['date'] + pd.to_timedelta(row['time'] , 'H')\n",
    "\n",
    "# Функция для сдвига на сутки (в скачанном датасете разбивка по 30 мин, поэтому timeshift=48)\n",
    "def shift_features_fact(df, timeshift=48):\n",
    "    list_fact_columns=list(df.columns)\n",
    "    list_fact_columns.remove('date_tw')\n",
    "    new_df = df.copy()\n",
    "    for column in list_fact_columns:\n",
    "        new_df[column] = new_df[column].shift(timeshift)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5eb669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для вычисления метрики mae по дням из почасовых массивов данных\n",
    "\n",
    "def mae_day(y_true, y_pred):\n",
    "    y_true_copy = pd.DataFrame(y_true).reset_index(drop=True)\n",
    "    y_true_copy['day'] = y_true_copy.index // 24\n",
    "    y_true_grouped = y_true_copy.groupby(by='day').sum()   \n",
    "    y_pred_copy = pd.DataFrame(y_pred).reset_index(drop=True)\n",
    "    y_pred_copy['day'] = y_pred_copy.index // 24\n",
    "    y_pred_grouped = y_pred_copy.groupby(by='day').sum()\n",
    "    \n",
    "    return mean_absolute_error(y_true_grouped, y_pred_grouped)\n",
    "# Функция для вычисления метрик по дням из почасовых массивов данных\n",
    "\n",
    "def metrics_day(y_true, y_pred):\n",
    "    y_true_copy = pd.DataFrame(y_true).reset_index(drop=True)\n",
    "    y_true_copy['day'] = y_true_copy.index // 24\n",
    "    y_true_grouped = y_true_copy.groupby(by='day').sum()   \n",
    "    y_pred_copy = pd.DataFrame(y_pred).reset_index(drop=True)\n",
    "    y_pred_copy['day'] = y_pred_copy.index // 24\n",
    "    y_pred_grouped = y_pred_copy.groupby(by='day').sum()\n",
    "    \n",
    "    mae = mean_absolute_error(y_true_grouped, y_pred_grouped)\n",
    "    mape = mean_absolute_percentage_error(y_true_grouped, y_pred_grouped)\n",
    "    r2 = r2_score(y_true_grouped, y_pred_grouped)\n",
    "    return mae, mape, r2\n",
    "\n",
    "def metrics_hour(y_true, y_pred): \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mape, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808c109",
   "metadata": {},
   "source": [
    "#### 1.5 Чтение файлов с данными\n",
    "Данные объединяются в один датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a4ce10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "начало открытого теста: 2023-04-01 00:00:00     конец открытого теста: 2023-08-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# читаем исходные датасеты и складываем в один\n",
    "train_ds = pd.read_csv('data/train_dataset.csv')\n",
    "test_ds = pd.read_csv('data/test_dataset.csv')\n",
    "train_ds = pd.concat([train_ds, test_ds])\n",
    "\n",
    "# запоминаем дату начала тестовых данных, потом также поступим и с закрытым датасетом\n",
    "open_test_begin = pd.to_datetime(test_ds['date']).min()\n",
    "open_test_end = pd.to_datetime(test_ds['date']).max() + pd.to_timedelta(1,'d')\n",
    "print('начало открытого теста:', open_test_begin, '    конец открытого теста:', open_test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237ce32",
   "metadata": {},
   "source": [
    "#### 1.6 Формирование колонок с производными от даты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16090ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразуем дату и делаем из нее колонки\n",
    "train_ds['date'] = pd.to_datetime(train_ds['date'])\n",
    "train_ds['year'] = train_ds['date'].dt.year\n",
    "train_ds['month'] = train_ds['date'].dt.month\n",
    "train_ds['day_of_week'] = train_ds['date'].dt.dayofweek\n",
    "train_ds['day'] = train_ds['date'].dt.day\n",
    "train_ds['day_of_year'] = train_ds['date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98567815",
   "metadata": {},
   "source": [
    "#### 1.7 Подгрузка данных о праздниках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03029389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавление данных о праздниках из файла 'data/holidays.csv'\n",
    "\n",
    "df_holidays = pd.read_csv('data/holidays.csv')\n",
    "df_holidays['date'] = pd.to_datetime(df_holidays['date'])\n",
    "\n",
    "# Assuming df_holidays and train_ds are your dataframes\n",
    "train_ds = pd.merge(train_ds, df_holidays, on='date', how='left')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "train_ds['holidays'].fillna(0, inplace=True)\n",
    "train_ds['preholidays'].fillna(0, inplace=True)\n",
    "\n",
    "# Convert to int\n",
    "train_ds['holidays'] = train_ds['holidays'].astype(int)\n",
    "train_ds['preholidays'] = train_ds['preholidays'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d592f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-12-31 00:00:00')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_holidays['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ace845c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>holidays</th>\n",
       "      <th>preholidays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, holidays, preholidays]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_holidays[df_holidays.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cdfa6e",
   "metadata": {},
   "source": [
    "#### 1.8 Формирование колонок со значением целевого признака в предыдущие дни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91cfdcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/2993370237.py:7: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_ds['temp_last_day'].fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Добавление колонок с временными лагами\n",
    "\n",
    "# создаем столбец 'temp_last_day'\n",
    "train_ds['temp_last_day'] = train_ds['temp'].shift(24)\n",
    "\n",
    "# заполняем пропущенные значения в 'temp_last_day'\n",
    "train_ds['temp_last_day'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# создаем столбцы с временными лагами для 'target'\n",
    "lags = [24, 48, 72, 7*24, 14*24]\n",
    "for lag in lags:\n",
    "    train_ds[f'target_lag_{lag}'] = train_ds['target'].shift(lag)\n",
    "\n",
    "# заполняем пропущенные значения в столбцах с лагами\n",
    "for lag in lags:\n",
    "    train_ds[f'target_lag_{lag}'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c4022",
   "metadata": {},
   "source": [
    "#### 1.9 Формирование колонок с ВВП и данными о погоде посредством ранее описанных функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79aa7c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>P</th>\n",
       "      <th>U</th>\n",
       "      <th>WW</th>\n",
       "      <th>Td</th>\n",
       "      <th>N</th>\n",
       "      <th>S</th>\n",
       "      <th>W</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-31 00:00:00</td>\n",
       "      <td>763.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>слабый дождь</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-31 00:30:00</td>\n",
       "      <td>764.3</td>\n",
       "      <td>93.0</td>\n",
       "      <td>слабый дождь</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-31 01:00:00</td>\n",
       "      <td>764.3</td>\n",
       "      <td>93.0</td>\n",
       "      <td>слабый дождь</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-31 01:30:00</td>\n",
       "      <td>765.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>слабый дождь</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-31 02:00:00</td>\n",
       "      <td>765.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>нет осадков</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82146</th>\n",
       "      <td>2023-09-30 21:30:00</td>\n",
       "      <td>763.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>нет осадков</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82147</th>\n",
       "      <td>2023-09-30 22:00:00</td>\n",
       "      <td>763.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>нет осадков</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82148</th>\n",
       "      <td>2023-09-30 22:30:00</td>\n",
       "      <td>763.5</td>\n",
       "      <td>77.0</td>\n",
       "      <td>сильный дождь</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82149</th>\n",
       "      <td>2023-09-30 23:00:00</td>\n",
       "      <td>763.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>сильный дождь</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82150</th>\n",
       "      <td>2023-09-30 23:30:00</td>\n",
       "      <td>763.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>нет осадков</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82151 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date      P      U             WW    Td    N    S    W  \\\n",
       "0      2018-12-31 00:00:00  763.5  100.0   слабый дождь   2.0  1.0  0.0  0.0   \n",
       "1      2018-12-31 00:30:00  764.3   93.0   слабый дождь   1.0  1.0  0.0  0.0   \n",
       "2      2018-12-31 01:00:00  764.3   93.0   слабый дождь   1.0  1.0  0.0  0.0   \n",
       "3      2018-12-31 01:30:00  765.0   93.0   слабый дождь   2.0  1.0  0.0  0.0   \n",
       "4      2018-12-31 02:00:00  765.0   93.0    нет осадков   2.0  1.0  0.0  0.0   \n",
       "...                    ...    ...    ...            ...   ...  ...  ...  ...   \n",
       "82146  2023-09-30 21:30:00  763.5   82.0    нет осадков  12.0  0.0  0.0  1.0   \n",
       "82147  2023-09-30 22:00:00  763.5   82.0    нет осадков  12.0  0.5  0.0  1.0   \n",
       "82148  2023-09-30 22:30:00  763.5   77.0  сильный дождь  11.0  0.0  0.0  1.0   \n",
       "82149  2023-09-30 23:00:00  763.5   94.0  сильный дождь  13.0  0.5  0.0  1.0   \n",
       "82150  2023-09-30 23:30:00  763.5   94.0    нет осадков  13.0  0.0  0.5  1.0   \n",
       "\n",
       "         E  \n",
       "0      0.0  \n",
       "1      0.5  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "...    ...  \n",
       "82146  0.0  \n",
       "82147  0.0  \n",
       "82148  0.0  \n",
       "82149  0.0  \n",
       "82150  0.0  \n",
       "\n",
       "[82151 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# применяем функцию добавления ВВП\n",
    "train_ds = add_vvp2(train_ds)\n",
    "\n",
    "# Расшифровка прогноза в колонке 'weather_pred'\n",
    "train_ds = fill_weather_columns(train_ds)\n",
    "\n",
    "\n",
    "# Читаем файл с архивом фактической погоды\n",
    "df_true_weather = pd.read_csv('data/preprocessing_loaded_table.csv')\n",
    "display(df_true_weather)\n",
    "\n",
    "# Форматируем колонки\n",
    "df_true_weather['WW'] = df_true_weather['WW'].apply(true_weather_WW_replace)\n",
    "df_true_weather['date'] = pd.to_datetime(df_true_weather['date'])\n",
    "df_true_weather = df_true_weather.rename(columns={'date':'date_tw'})\n",
    "# Применяем сдвиг на сутки, чтобы не заглядывать в будущее\n",
    "df_true_weather = shift_features_fact(df_true_weather)\n",
    "# Добавляем в датасет\n",
    "train_ds['date_hours'] = train_ds.apply(row_plus_hours_to_index, axis=1)\n",
    "train_ds = train_ds.merge(df_true_weather, left_on='date_hours', right_on='date_tw')\n",
    "train_ds = train_ds.drop(['date_hours', 'date_tw'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 Формирование колонок со средними значениями цели и фактической температуры за предыдущий день и срезы по нему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ba74908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds[['last_day_avg_target', 'last_day_avg_temp']] = train_ds[['date', 'target', 'temp']].groupby(by='date').transform('mean').shift(24)\n",
    "\n",
    "#def mean_evening(values, evening=19):\n",
    "#    return values[evening:].mean()\n",
    "\n",
    "#evening_slices = [19, 22]\n",
    "   \n",
    "#for evening_slice in evening_slices:\n",
    "#    train_ds[['last_evening_avg_target_'+str(evening_slice), 'last_evening_avg_temp_'+str(evening_slice)]] = \\\n",
    "#        train_ds[['date', 'target', 'temp']].groupby(by='date').transform(mean_evening, evening=evening_slice).shift(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5607815",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[['last_day_avg_target', 'last_day_avg_temp']] = train_ds[['date', 'target', 'temp']].groupby(by='date').transform('mean').shift(24)\n",
    "\n",
    "def mean_evening(values, evening=19):\n",
    "    return values[evening:].mean()\n",
    "\n",
    "evening_slices = [19, 22]\n",
    "    \n",
    "for evening_slice in evening_slices:\n",
    "    train_ds[['last_evening_avg_target_'+str(evening_slice), 'last_evening_avg_temp_'+str(evening_slice)]] = \\\n",
    "        train_ds[['date', 'target', 'temp']].groupby(by='date').transform(mean_evening, evening=evening_slice).shift(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9efa20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40027, 806)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.11 Формирование колонок с почасовыми лагами для всех сформированных ранее готовых признаков\n",
    "\n",
    "Сначала готовим список названий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8841abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отбираем признаки. Все лишние колонки здесь отбрасываем, кроме 'date', которую уберем позже \n",
    "\n",
    "feature_cols = list(train_ds.columns)\n",
    "\n",
    "# выбрасываем взгляд в прошлое и расшифрованную погоду\n",
    "drop_list = ['target', 'day_of_year', 'weather_pred', 'weather_fact', 'temp']\n",
    "\n",
    "# выбрасываем признаки, найденные процедурно в процессе оптимизации\n",
    "# КОМАНДЕ: здесь можно добавлять признаки на выброс с целью оптимизации\n",
    "drop_list = drop_list + ['target_lag_48', 'target_lag_168'] #, 'temp_pred'] #, 'target_lag_336'] \n",
    "\n",
    "for name in drop_list:\n",
    "    feature_cols.remove(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потом добавляем лаги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9aab343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/4047160460.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)\n"
     ]
    }
   ],
   "source": [
    "FEATURE_WINDOW_SIZE = 24\n",
    "feature_cols_no_date = feature_cols.copy()\n",
    "feature_cols_no_date.remove('date')\n",
    "\n",
    "\n",
    "for lag in range(1,FEATURE_WINDOW_SIZE):\n",
    "    for column in feature_cols_no_date:\n",
    "        train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.11 Формирование колонок с лагами для цели и фактической температуры\n",
    "\n",
    "Заполняем значениями NaN все поля этих лагов, которые относятся к текущему дню, чтобы не допустить утечки. Т.е. для каждого часа используем только лаги, которые превосходят номер часа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d55bd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/1660665095.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds['target_'+str(lag)] = train_ds.target.shift(lag).where(train_ds['time']<lag, np.NaN)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/1660665095.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds['temp_'+str(lag)] = train_ds['temp'].shift(lag).where(train_ds['time']<lag, np.NaN)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/1660665095.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds['target_'+str(lag)] = train_ds.target.shift(lag).where(train_ds['time']<lag, np.NaN)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/1660665095.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds['temp_'+str(lag)] = train_ds['temp'].shift(lag).where(train_ds['time']<lag, np.NaN)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/1660665095.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds['target_'+str(lag)] = train_ds.target.shift(lag).where(train_ds['time']<lag, np.NaN)\n",
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/1660665095.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds['temp_'+str(lag)] = train_ds['temp'].shift(lag).where(train_ds['time']<lag, np.NaN)\n"
     ]
    }
   ],
   "source": [
    "target_lags=[1, 5, 9]\n",
    "\n",
    "for lag in target_lags:\n",
    "    train_ds['target_'+str(lag)] = train_ds.target.shift(lag).where(train_ds['time']<lag, np.NaN)\n",
    "    train_ds['temp_'+str(lag)] = train_ds['temp'].shift(lag).where(train_ds['time']<lag, np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc76ee",
   "metadata": {},
   "source": [
    "#### 1.12 Демонстрация сформированного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a939b60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'time', 'target', 'temp', 'temp_pred', 'weather_pred',\n",
       "       'weather_fact', 'year', 'month', 'day_of_week',\n",
       "       ...\n",
       "       'last_evening_avg_target_19_23', 'last_evening_avg_temp_19_23',\n",
       "       'last_evening_avg_target_22_23', 'last_evening_avg_temp_22_23',\n",
       "       'target_1', 'temp_1', 'target_5', 'temp_5', 'target_9', 'temp_9'],\n",
       "      dtype='object', length=806)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Итоговый набор колонок\n",
    "train_ds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f51e77ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>target</th>\n",
       "      <th>temp</th>\n",
       "      <th>temp_pred</th>\n",
       "      <th>weather_pred</th>\n",
       "      <th>weather_fact</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>last_evening_avg_target_19_23</th>\n",
       "      <th>last_evening_avg_temp_19_23</th>\n",
       "      <th>last_evening_avg_target_22_23</th>\n",
       "      <th>last_evening_avg_temp_22_23</th>\n",
       "      <th>target_1</th>\n",
       "      <th>temp_1</th>\n",
       "      <th>target_5</th>\n",
       "      <th>temp_5</th>\n",
       "      <th>target_9</th>\n",
       "      <th>temp_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>481.510</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>пасм, ветер</td>\n",
       "      <td>ветер</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>462.872</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>пасм, ветер</td>\n",
       "      <td>ветер</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>449.718</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>пасм, ветер</td>\n",
       "      <td>ветер</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>430.908</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>пасм, ветер</td>\n",
       "      <td>ветер, пасм</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>415.163</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>пасм, ветер</td>\n",
       "      <td>ветер, пасм</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 806 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  time   target  temp  temp_pred weather_pred weather_fact  year  \\\n",
       "0 2019-01-01     0  481.510   2.9        2.0  пасм, ветер        ветер  2019   \n",
       "1 2019-01-01     1  462.872   2.9        2.0  пасм, ветер        ветер  2019   \n",
       "2 2019-01-01     2  449.718   2.9        2.0  пасм, ветер        ветер  2019   \n",
       "3 2019-01-01     3  430.908   4.3        2.0  пасм, ветер  ветер, пасм  2019   \n",
       "4 2019-01-01     4  415.163   4.3        2.0  пасм, ветер  ветер, пасм  2019   \n",
       "\n",
       "   month  day_of_week  ...  last_evening_avg_target_19_23  \\\n",
       "0      1            1  ...                            NaN   \n",
       "1      1            1  ...                            NaN   \n",
       "2      1            1  ...                            NaN   \n",
       "3      1            1  ...                            NaN   \n",
       "4      1            1  ...                            NaN   \n",
       "\n",
       "   last_evening_avg_temp_19_23  last_evening_avg_target_22_23  \\\n",
       "0                          NaN                            NaN   \n",
       "1                          NaN                            NaN   \n",
       "2                          NaN                            NaN   \n",
       "3                          NaN                            NaN   \n",
       "4                          NaN                            NaN   \n",
       "\n",
       "   last_evening_avg_temp_22_23  target_1  temp_1  target_5  temp_5  target_9  \\\n",
       "0                          NaN       NaN     NaN       NaN     NaN       NaN   \n",
       "1                          NaN       NaN     NaN       NaN     NaN       NaN   \n",
       "2                          NaN       NaN     NaN       NaN     NaN       NaN   \n",
       "3                          NaN       NaN     NaN       NaN     NaN       NaN   \n",
       "4                          NaN       NaN     NaN       NaN     NaN       NaN   \n",
       "\n",
       "   temp_9  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "3     NaN  \n",
       "4     NaN  \n",
       "\n",
       "[5 rows x 806 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69383cb",
   "metadata": {},
   "source": [
    "#### 1.13 Исключение лишних колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65d6619d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'time',\n",
       " 'temp_pred',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day_of_week',\n",
       " 'day',\n",
       " 'holidays',\n",
       " 'preholidays',\n",
       " 'temp_last_day',\n",
       " 'target_lag_24',\n",
       " 'target_lag_72',\n",
       " 'target_lag_336',\n",
       " 'VVP',\n",
       " 'cloudy',\n",
       " 'rainy',\n",
       " 'windy',\n",
       " 'clear',\n",
       " 'rain_probability',\n",
       " 'has_rain_probability',\n",
       " 'P',\n",
       " 'U',\n",
       " 'WW',\n",
       " 'Td',\n",
       " 'N',\n",
       " 'S',\n",
       " 'W',\n",
       " 'E',\n",
       " 'last_day_avg_target',\n",
       " 'last_day_avg_temp',\n",
       " 'last_evening_avg_target_19',\n",
       " 'last_evening_avg_temp_19',\n",
       " 'last_evening_avg_target_22',\n",
       " 'last_evening_avg_temp_22',\n",
       " 'time_1',\n",
       " 'temp_pred_1',\n",
       " 'year_1',\n",
       " 'month_1',\n",
       " 'day_of_week_1',\n",
       " 'day_1',\n",
       " 'holidays_1',\n",
       " 'preholidays_1',\n",
       " 'temp_last_day_1',\n",
       " 'target_lag_24_1',\n",
       " 'target_lag_72_1',\n",
       " 'target_lag_336_1',\n",
       " 'VVP_1',\n",
       " 'cloudy_1',\n",
       " 'rainy_1',\n",
       " 'windy_1',\n",
       " 'clear_1',\n",
       " 'rain_probability_1',\n",
       " 'has_rain_probability_1',\n",
       " 'P_1',\n",
       " 'U_1',\n",
       " 'WW_1',\n",
       " 'Td_1',\n",
       " 'N_1',\n",
       " 'S_1',\n",
       " 'W_1',\n",
       " 'E_1',\n",
       " 'last_day_avg_target_1',\n",
       " 'last_day_avg_temp_1',\n",
       " 'last_evening_avg_target_19_1',\n",
       " 'last_evening_avg_temp_19_1',\n",
       " 'last_evening_avg_target_22_1',\n",
       " 'last_evening_avg_temp_22_1',\n",
       " 'time_2',\n",
       " 'temp_pred_2',\n",
       " 'year_2',\n",
       " 'month_2',\n",
       " 'day_of_week_2',\n",
       " 'day_2',\n",
       " 'holidays_2',\n",
       " 'preholidays_2',\n",
       " 'temp_last_day_2',\n",
       " 'target_lag_24_2',\n",
       " 'target_lag_72_2',\n",
       " 'target_lag_336_2',\n",
       " 'VVP_2',\n",
       " 'cloudy_2',\n",
       " 'rainy_2',\n",
       " 'windy_2',\n",
       " 'clear_2',\n",
       " 'rain_probability_2',\n",
       " 'has_rain_probability_2',\n",
       " 'P_2',\n",
       " 'U_2',\n",
       " 'WW_2',\n",
       " 'Td_2',\n",
       " 'N_2',\n",
       " 'S_2',\n",
       " 'W_2',\n",
       " 'E_2',\n",
       " 'last_day_avg_target_2',\n",
       " 'last_day_avg_temp_2',\n",
       " 'last_evening_avg_target_19_2',\n",
       " 'last_evening_avg_temp_19_2',\n",
       " 'last_evening_avg_target_22_2',\n",
       " 'last_evening_avg_temp_22_2',\n",
       " 'time_3',\n",
       " 'temp_pred_3',\n",
       " 'year_3',\n",
       " 'month_3',\n",
       " 'day_of_week_3',\n",
       " 'day_3',\n",
       " 'holidays_3',\n",
       " 'preholidays_3',\n",
       " 'temp_last_day_3',\n",
       " 'target_lag_24_3',\n",
       " 'target_lag_72_3',\n",
       " 'target_lag_336_3',\n",
       " 'VVP_3',\n",
       " 'cloudy_3',\n",
       " 'rainy_3',\n",
       " 'windy_3',\n",
       " 'clear_3',\n",
       " 'rain_probability_3',\n",
       " 'has_rain_probability_3',\n",
       " 'P_3',\n",
       " 'U_3',\n",
       " 'WW_3',\n",
       " 'Td_3',\n",
       " 'N_3',\n",
       " 'S_3',\n",
       " 'W_3',\n",
       " 'E_3',\n",
       " 'last_day_avg_target_3',\n",
       " 'last_day_avg_temp_3',\n",
       " 'last_evening_avg_target_19_3',\n",
       " 'last_evening_avg_temp_19_3',\n",
       " 'last_evening_avg_target_22_3',\n",
       " 'last_evening_avg_temp_22_3',\n",
       " 'time_4',\n",
       " 'temp_pred_4',\n",
       " 'year_4',\n",
       " 'month_4',\n",
       " 'day_of_week_4',\n",
       " 'day_4',\n",
       " 'holidays_4',\n",
       " 'preholidays_4',\n",
       " 'temp_last_day_4',\n",
       " 'target_lag_24_4',\n",
       " 'target_lag_72_4',\n",
       " 'target_lag_336_4',\n",
       " 'VVP_4',\n",
       " 'cloudy_4',\n",
       " 'rainy_4',\n",
       " 'windy_4',\n",
       " 'clear_4',\n",
       " 'rain_probability_4',\n",
       " 'has_rain_probability_4',\n",
       " 'P_4',\n",
       " 'U_4',\n",
       " 'WW_4',\n",
       " 'Td_4',\n",
       " 'N_4',\n",
       " 'S_4',\n",
       " 'W_4',\n",
       " 'E_4',\n",
       " 'last_day_avg_target_4',\n",
       " 'last_day_avg_temp_4',\n",
       " 'last_evening_avg_target_19_4',\n",
       " 'last_evening_avg_temp_19_4',\n",
       " 'last_evening_avg_target_22_4',\n",
       " 'last_evening_avg_temp_22_4',\n",
       " 'time_5',\n",
       " 'temp_pred_5',\n",
       " 'year_5',\n",
       " 'month_5',\n",
       " 'day_of_week_5',\n",
       " 'day_5',\n",
       " 'holidays_5',\n",
       " 'preholidays_5',\n",
       " 'temp_last_day_5',\n",
       " 'target_lag_24_5',\n",
       " 'target_lag_72_5',\n",
       " 'target_lag_336_5',\n",
       " 'VVP_5',\n",
       " 'cloudy_5',\n",
       " 'rainy_5',\n",
       " 'windy_5',\n",
       " 'clear_5',\n",
       " 'rain_probability_5',\n",
       " 'has_rain_probability_5',\n",
       " 'P_5',\n",
       " 'U_5',\n",
       " 'WW_5',\n",
       " 'Td_5',\n",
       " 'N_5',\n",
       " 'S_5',\n",
       " 'W_5',\n",
       " 'E_5',\n",
       " 'last_day_avg_target_5',\n",
       " 'last_day_avg_temp_5',\n",
       " 'last_evening_avg_target_19_5',\n",
       " 'last_evening_avg_temp_19_5',\n",
       " 'last_evening_avg_target_22_5',\n",
       " 'last_evening_avg_temp_22_5',\n",
       " 'time_6',\n",
       " 'temp_pred_6',\n",
       " 'year_6',\n",
       " 'month_6',\n",
       " 'day_of_week_6',\n",
       " 'day_6',\n",
       " 'holidays_6',\n",
       " 'preholidays_6',\n",
       " 'temp_last_day_6',\n",
       " 'target_lag_24_6',\n",
       " 'target_lag_72_6',\n",
       " 'target_lag_336_6',\n",
       " 'VVP_6',\n",
       " 'cloudy_6',\n",
       " 'rainy_6',\n",
       " 'windy_6',\n",
       " 'clear_6',\n",
       " 'rain_probability_6',\n",
       " 'has_rain_probability_6',\n",
       " 'P_6',\n",
       " 'U_6',\n",
       " 'WW_6',\n",
       " 'Td_6',\n",
       " 'N_6',\n",
       " 'S_6',\n",
       " 'W_6',\n",
       " 'E_6',\n",
       " 'last_day_avg_target_6',\n",
       " 'last_day_avg_temp_6',\n",
       " 'last_evening_avg_target_19_6',\n",
       " 'last_evening_avg_temp_19_6',\n",
       " 'last_evening_avg_target_22_6',\n",
       " 'last_evening_avg_temp_22_6',\n",
       " 'time_7',\n",
       " 'temp_pred_7',\n",
       " 'year_7',\n",
       " 'month_7',\n",
       " 'day_of_week_7',\n",
       " 'day_7',\n",
       " 'holidays_7',\n",
       " 'preholidays_7',\n",
       " 'temp_last_day_7',\n",
       " 'target_lag_24_7',\n",
       " 'target_lag_72_7',\n",
       " 'target_lag_336_7',\n",
       " 'VVP_7',\n",
       " 'cloudy_7',\n",
       " 'rainy_7',\n",
       " 'windy_7',\n",
       " 'clear_7',\n",
       " 'rain_probability_7',\n",
       " 'has_rain_probability_7',\n",
       " 'P_7',\n",
       " 'U_7',\n",
       " 'WW_7',\n",
       " 'Td_7',\n",
       " 'N_7',\n",
       " 'S_7',\n",
       " 'W_7',\n",
       " 'E_7',\n",
       " 'last_day_avg_target_7',\n",
       " 'last_day_avg_temp_7',\n",
       " 'last_evening_avg_target_19_7',\n",
       " 'last_evening_avg_temp_19_7',\n",
       " 'last_evening_avg_target_22_7',\n",
       " 'last_evening_avg_temp_22_7',\n",
       " 'time_8',\n",
       " 'temp_pred_8',\n",
       " 'year_8',\n",
       " 'month_8',\n",
       " 'day_of_week_8',\n",
       " 'day_8',\n",
       " 'holidays_8',\n",
       " 'preholidays_8',\n",
       " 'temp_last_day_8',\n",
       " 'target_lag_24_8',\n",
       " 'target_lag_72_8',\n",
       " 'target_lag_336_8',\n",
       " 'VVP_8',\n",
       " 'cloudy_8',\n",
       " 'rainy_8',\n",
       " 'windy_8',\n",
       " 'clear_8',\n",
       " 'rain_probability_8',\n",
       " 'has_rain_probability_8',\n",
       " 'P_8',\n",
       " 'U_8',\n",
       " 'WW_8',\n",
       " 'Td_8',\n",
       " 'N_8',\n",
       " 'S_8',\n",
       " 'W_8',\n",
       " 'E_8',\n",
       " 'last_day_avg_target_8',\n",
       " 'last_day_avg_temp_8',\n",
       " 'last_evening_avg_target_19_8',\n",
       " 'last_evening_avg_temp_19_8',\n",
       " 'last_evening_avg_target_22_8',\n",
       " 'last_evening_avg_temp_22_8',\n",
       " 'time_9',\n",
       " 'temp_pred_9',\n",
       " 'year_9',\n",
       " 'month_9',\n",
       " 'day_of_week_9',\n",
       " 'day_9',\n",
       " 'holidays_9',\n",
       " 'preholidays_9',\n",
       " 'temp_last_day_9',\n",
       " 'target_lag_24_9',\n",
       " 'target_lag_72_9',\n",
       " 'target_lag_336_9',\n",
       " 'VVP_9',\n",
       " 'cloudy_9',\n",
       " 'rainy_9',\n",
       " 'windy_9',\n",
       " 'clear_9',\n",
       " 'rain_probability_9',\n",
       " 'has_rain_probability_9',\n",
       " 'P_9',\n",
       " 'U_9',\n",
       " 'WW_9',\n",
       " 'Td_9',\n",
       " 'N_9',\n",
       " 'S_9',\n",
       " 'W_9',\n",
       " 'E_9',\n",
       " 'last_day_avg_target_9',\n",
       " 'last_day_avg_temp_9',\n",
       " 'last_evening_avg_target_19_9',\n",
       " 'last_evening_avg_temp_19_9',\n",
       " 'last_evening_avg_target_22_9',\n",
       " 'last_evening_avg_temp_22_9',\n",
       " 'time_10',\n",
       " 'temp_pred_10',\n",
       " 'year_10',\n",
       " 'month_10',\n",
       " 'day_of_week_10',\n",
       " 'day_10',\n",
       " 'holidays_10',\n",
       " 'preholidays_10',\n",
       " 'temp_last_day_10',\n",
       " 'target_lag_24_10',\n",
       " 'target_lag_72_10',\n",
       " 'target_lag_336_10',\n",
       " 'VVP_10',\n",
       " 'cloudy_10',\n",
       " 'rainy_10',\n",
       " 'windy_10',\n",
       " 'clear_10',\n",
       " 'rain_probability_10',\n",
       " 'has_rain_probability_10',\n",
       " 'P_10',\n",
       " 'U_10',\n",
       " 'WW_10',\n",
       " 'Td_10',\n",
       " 'N_10',\n",
       " 'S_10',\n",
       " 'W_10',\n",
       " 'E_10',\n",
       " 'last_day_avg_target_10',\n",
       " 'last_day_avg_temp_10',\n",
       " 'last_evening_avg_target_19_10',\n",
       " 'last_evening_avg_temp_19_10',\n",
       " 'last_evening_avg_target_22_10',\n",
       " 'last_evening_avg_temp_22_10',\n",
       " 'time_11',\n",
       " 'temp_pred_11',\n",
       " 'year_11',\n",
       " 'month_11',\n",
       " 'day_of_week_11',\n",
       " 'day_11',\n",
       " 'holidays_11',\n",
       " 'preholidays_11',\n",
       " 'temp_last_day_11',\n",
       " 'target_lag_24_11',\n",
       " 'target_lag_72_11',\n",
       " 'target_lag_336_11',\n",
       " 'VVP_11',\n",
       " 'cloudy_11',\n",
       " 'rainy_11',\n",
       " 'windy_11',\n",
       " 'clear_11',\n",
       " 'rain_probability_11',\n",
       " 'has_rain_probability_11',\n",
       " 'P_11',\n",
       " 'U_11',\n",
       " 'WW_11',\n",
       " 'Td_11',\n",
       " 'N_11',\n",
       " 'S_11',\n",
       " 'W_11',\n",
       " 'E_11',\n",
       " 'last_day_avg_target_11',\n",
       " 'last_day_avg_temp_11',\n",
       " 'last_evening_avg_target_19_11',\n",
       " 'last_evening_avg_temp_19_11',\n",
       " 'last_evening_avg_target_22_11',\n",
       " 'last_evening_avg_temp_22_11',\n",
       " 'time_12',\n",
       " 'temp_pred_12',\n",
       " 'year_12',\n",
       " 'month_12',\n",
       " 'day_of_week_12',\n",
       " 'day_12',\n",
       " 'holidays_12',\n",
       " 'preholidays_12',\n",
       " 'temp_last_day_12',\n",
       " 'target_lag_24_12',\n",
       " 'target_lag_72_12',\n",
       " 'target_lag_336_12',\n",
       " 'VVP_12',\n",
       " 'cloudy_12',\n",
       " 'rainy_12',\n",
       " 'windy_12',\n",
       " 'clear_12',\n",
       " 'rain_probability_12',\n",
       " 'has_rain_probability_12',\n",
       " 'P_12',\n",
       " 'U_12',\n",
       " 'WW_12',\n",
       " 'Td_12',\n",
       " 'N_12',\n",
       " 'S_12',\n",
       " 'W_12',\n",
       " 'E_12',\n",
       " 'last_day_avg_target_12',\n",
       " 'last_day_avg_temp_12',\n",
       " 'last_evening_avg_target_19_12',\n",
       " 'last_evening_avg_temp_19_12',\n",
       " 'last_evening_avg_target_22_12',\n",
       " 'last_evening_avg_temp_22_12',\n",
       " 'time_13',\n",
       " 'temp_pred_13',\n",
       " 'year_13',\n",
       " 'month_13',\n",
       " 'day_of_week_13',\n",
       " 'day_13',\n",
       " 'holidays_13',\n",
       " 'preholidays_13',\n",
       " 'temp_last_day_13',\n",
       " 'target_lag_24_13',\n",
       " 'target_lag_72_13',\n",
       " 'target_lag_336_13',\n",
       " 'VVP_13',\n",
       " 'cloudy_13',\n",
       " 'rainy_13',\n",
       " 'windy_13',\n",
       " 'clear_13',\n",
       " 'rain_probability_13',\n",
       " 'has_rain_probability_13',\n",
       " 'P_13',\n",
       " 'U_13',\n",
       " 'WW_13',\n",
       " 'Td_13',\n",
       " 'N_13',\n",
       " 'S_13',\n",
       " 'W_13',\n",
       " 'E_13',\n",
       " 'last_day_avg_target_13',\n",
       " 'last_day_avg_temp_13',\n",
       " 'last_evening_avg_target_19_13',\n",
       " 'last_evening_avg_temp_19_13',\n",
       " 'last_evening_avg_target_22_13',\n",
       " 'last_evening_avg_temp_22_13',\n",
       " 'time_14',\n",
       " 'temp_pred_14',\n",
       " 'year_14',\n",
       " 'month_14',\n",
       " 'day_of_week_14',\n",
       " 'day_14',\n",
       " 'holidays_14',\n",
       " 'preholidays_14',\n",
       " 'temp_last_day_14',\n",
       " 'target_lag_24_14',\n",
       " 'target_lag_72_14',\n",
       " 'target_lag_336_14',\n",
       " 'VVP_14',\n",
       " 'cloudy_14',\n",
       " 'rainy_14',\n",
       " 'windy_14',\n",
       " 'clear_14',\n",
       " 'rain_probability_14',\n",
       " 'has_rain_probability_14',\n",
       " 'P_14',\n",
       " 'U_14',\n",
       " 'WW_14',\n",
       " 'Td_14',\n",
       " 'N_14',\n",
       " 'S_14',\n",
       " 'W_14',\n",
       " 'E_14',\n",
       " 'last_day_avg_target_14',\n",
       " 'last_day_avg_temp_14',\n",
       " 'last_evening_avg_target_19_14',\n",
       " 'last_evening_avg_temp_19_14',\n",
       " 'last_evening_avg_target_22_14',\n",
       " 'last_evening_avg_temp_22_14',\n",
       " 'time_15',\n",
       " 'temp_pred_15',\n",
       " 'year_15',\n",
       " 'month_15',\n",
       " 'day_of_week_15',\n",
       " 'day_15',\n",
       " 'holidays_15',\n",
       " 'preholidays_15',\n",
       " 'temp_last_day_15',\n",
       " 'target_lag_24_15',\n",
       " 'target_lag_72_15',\n",
       " 'target_lag_336_15',\n",
       " 'VVP_15',\n",
       " 'cloudy_15',\n",
       " 'rainy_15',\n",
       " 'windy_15',\n",
       " 'clear_15',\n",
       " 'rain_probability_15',\n",
       " 'has_rain_probability_15',\n",
       " 'P_15',\n",
       " 'U_15',\n",
       " 'WW_15',\n",
       " 'Td_15',\n",
       " 'N_15',\n",
       " 'S_15',\n",
       " 'W_15',\n",
       " 'E_15',\n",
       " 'last_day_avg_target_15',\n",
       " 'last_day_avg_temp_15',\n",
       " 'last_evening_avg_target_19_15',\n",
       " 'last_evening_avg_temp_19_15',\n",
       " 'last_evening_avg_target_22_15',\n",
       " 'last_evening_avg_temp_22_15',\n",
       " 'time_16',\n",
       " 'temp_pred_16',\n",
       " 'year_16',\n",
       " 'month_16',\n",
       " 'day_of_week_16',\n",
       " 'day_16',\n",
       " 'holidays_16',\n",
       " 'preholidays_16',\n",
       " 'temp_last_day_16',\n",
       " 'target_lag_24_16',\n",
       " 'target_lag_72_16',\n",
       " 'target_lag_336_16',\n",
       " 'VVP_16',\n",
       " 'cloudy_16',\n",
       " 'rainy_16',\n",
       " 'windy_16',\n",
       " 'clear_16',\n",
       " 'rain_probability_16',\n",
       " 'has_rain_probability_16',\n",
       " 'P_16',\n",
       " 'U_16',\n",
       " 'WW_16',\n",
       " 'Td_16',\n",
       " 'N_16',\n",
       " 'S_16',\n",
       " 'W_16',\n",
       " 'E_16',\n",
       " 'last_day_avg_target_16',\n",
       " 'last_day_avg_temp_16',\n",
       " 'last_evening_avg_target_19_16',\n",
       " 'last_evening_avg_temp_19_16',\n",
       " 'last_evening_avg_target_22_16',\n",
       " 'last_evening_avg_temp_22_16',\n",
       " 'time_17',\n",
       " 'temp_pred_17',\n",
       " 'year_17',\n",
       " 'month_17',\n",
       " 'day_of_week_17',\n",
       " 'day_17',\n",
       " 'holidays_17',\n",
       " 'preholidays_17',\n",
       " 'temp_last_day_17',\n",
       " 'target_lag_24_17',\n",
       " 'target_lag_72_17',\n",
       " 'target_lag_336_17',\n",
       " 'VVP_17',\n",
       " 'cloudy_17',\n",
       " 'rainy_17',\n",
       " 'windy_17',\n",
       " 'clear_17',\n",
       " 'rain_probability_17',\n",
       " 'has_rain_probability_17',\n",
       " 'P_17',\n",
       " 'U_17',\n",
       " 'WW_17',\n",
       " 'Td_17',\n",
       " 'N_17',\n",
       " 'S_17',\n",
       " 'W_17',\n",
       " 'E_17',\n",
       " 'last_day_avg_target_17',\n",
       " 'last_day_avg_temp_17',\n",
       " 'last_evening_avg_target_19_17',\n",
       " 'last_evening_avg_temp_19_17',\n",
       " 'last_evening_avg_target_22_17',\n",
       " 'last_evening_avg_temp_22_17',\n",
       " 'time_18',\n",
       " 'temp_pred_18',\n",
       " 'year_18',\n",
       " 'month_18',\n",
       " 'day_of_week_18',\n",
       " 'day_18',\n",
       " 'holidays_18',\n",
       " 'preholidays_18',\n",
       " 'temp_last_day_18',\n",
       " 'target_lag_24_18',\n",
       " 'target_lag_72_18',\n",
       " 'target_lag_336_18',\n",
       " 'VVP_18',\n",
       " 'cloudy_18',\n",
       " 'rainy_18',\n",
       " 'windy_18',\n",
       " 'clear_18',\n",
       " 'rain_probability_18',\n",
       " 'has_rain_probability_18',\n",
       " 'P_18',\n",
       " 'U_18',\n",
       " 'WW_18',\n",
       " 'Td_18',\n",
       " 'N_18',\n",
       " 'S_18',\n",
       " 'W_18',\n",
       " 'E_18',\n",
       " 'last_day_avg_target_18',\n",
       " 'last_day_avg_temp_18',\n",
       " 'last_evening_avg_target_19_18',\n",
       " 'last_evening_avg_temp_19_18',\n",
       " 'last_evening_avg_target_22_18',\n",
       " 'last_evening_avg_temp_22_18',\n",
       " 'time_19',\n",
       " 'temp_pred_19',\n",
       " 'year_19',\n",
       " 'month_19',\n",
       " 'day_of_week_19',\n",
       " 'day_19',\n",
       " 'holidays_19',\n",
       " 'preholidays_19',\n",
       " 'temp_last_day_19',\n",
       " 'target_lag_24_19',\n",
       " 'target_lag_72_19',\n",
       " 'target_lag_336_19',\n",
       " 'VVP_19',\n",
       " 'cloudy_19',\n",
       " 'rainy_19',\n",
       " 'windy_19',\n",
       " 'clear_19',\n",
       " 'rain_probability_19',\n",
       " 'has_rain_probability_19',\n",
       " 'P_19',\n",
       " 'U_19',\n",
       " 'WW_19',\n",
       " 'Td_19',\n",
       " 'N_19',\n",
       " 'S_19',\n",
       " 'W_19',\n",
       " 'E_19',\n",
       " 'last_day_avg_target_19',\n",
       " 'last_day_avg_temp_19',\n",
       " 'last_evening_avg_target_19_19',\n",
       " 'last_evening_avg_temp_19_19',\n",
       " 'last_evening_avg_target_22_19',\n",
       " 'last_evening_avg_temp_22_19',\n",
       " 'time_20',\n",
       " 'temp_pred_20',\n",
       " 'year_20',\n",
       " 'month_20',\n",
       " 'day_of_week_20',\n",
       " 'day_20',\n",
       " 'holidays_20',\n",
       " 'preholidays_20',\n",
       " 'temp_last_day_20',\n",
       " 'target_lag_24_20',\n",
       " 'target_lag_72_20',\n",
       " 'target_lag_336_20',\n",
       " 'VVP_20',\n",
       " 'cloudy_20',\n",
       " 'rainy_20',\n",
       " 'windy_20',\n",
       " 'clear_20',\n",
       " 'rain_probability_20',\n",
       " 'has_rain_probability_20',\n",
       " 'P_20',\n",
       " 'U_20',\n",
       " 'WW_20',\n",
       " 'Td_20',\n",
       " 'N_20',\n",
       " 'S_20',\n",
       " 'W_20',\n",
       " 'E_20',\n",
       " 'last_day_avg_target_20',\n",
       " 'last_day_avg_temp_20',\n",
       " 'last_evening_avg_target_19_20',\n",
       " 'last_evening_avg_temp_19_20',\n",
       " 'last_evening_avg_target_22_20',\n",
       " 'last_evening_avg_temp_22_20',\n",
       " 'time_21',\n",
       " 'temp_pred_21',\n",
       " 'year_21',\n",
       " 'month_21',\n",
       " 'day_of_week_21',\n",
       " 'day_21',\n",
       " 'holidays_21',\n",
       " 'preholidays_21',\n",
       " 'temp_last_day_21',\n",
       " 'target_lag_24_21',\n",
       " 'target_lag_72_21',\n",
       " 'target_lag_336_21',\n",
       " 'VVP_21',\n",
       " 'cloudy_21',\n",
       " 'rainy_21',\n",
       " 'windy_21',\n",
       " 'clear_21',\n",
       " 'rain_probability_21',\n",
       " 'has_rain_probability_21',\n",
       " 'P_21',\n",
       " 'U_21',\n",
       " 'WW_21',\n",
       " 'Td_21',\n",
       " 'N_21',\n",
       " 'S_21',\n",
       " 'W_21',\n",
       " 'E_21',\n",
       " 'last_day_avg_target_21',\n",
       " 'last_day_avg_temp_21',\n",
       " 'last_evening_avg_target_19_21',\n",
       " 'last_evening_avg_temp_19_21',\n",
       " 'last_evening_avg_target_22_21',\n",
       " 'last_evening_avg_temp_22_21',\n",
       " 'time_22',\n",
       " 'temp_pred_22',\n",
       " 'year_22',\n",
       " 'month_22',\n",
       " 'day_of_week_22',\n",
       " 'day_22',\n",
       " 'holidays_22',\n",
       " 'preholidays_22',\n",
       " 'temp_last_day_22',\n",
       " 'target_lag_24_22',\n",
       " 'target_lag_72_22',\n",
       " 'target_lag_336_22',\n",
       " 'VVP_22',\n",
       " 'cloudy_22',\n",
       " 'rainy_22',\n",
       " 'windy_22',\n",
       " 'clear_22',\n",
       " 'rain_probability_22',\n",
       " 'has_rain_probability_22',\n",
       " 'P_22',\n",
       " 'U_22',\n",
       " 'WW_22',\n",
       " 'Td_22',\n",
       " 'N_22',\n",
       " 'S_22',\n",
       " 'W_22',\n",
       " 'E_22',\n",
       " 'last_day_avg_target_22',\n",
       " 'last_day_avg_temp_22',\n",
       " 'last_evening_avg_target_19_22',\n",
       " 'last_evening_avg_temp_19_22',\n",
       " 'last_evening_avg_target_22_22',\n",
       " 'last_evening_avg_temp_22_22',\n",
       " 'time_23',\n",
       " 'temp_pred_23',\n",
       " 'year_23',\n",
       " 'month_23',\n",
       " 'day_of_week_23',\n",
       " 'day_23',\n",
       " 'holidays_23',\n",
       " 'preholidays_23',\n",
       " 'temp_last_day_23',\n",
       " 'target_lag_24_23',\n",
       " 'target_lag_72_23',\n",
       " 'target_lag_336_23',\n",
       " 'VVP_23',\n",
       " 'cloudy_23',\n",
       " 'rainy_23',\n",
       " 'windy_23',\n",
       " 'clear_23',\n",
       " 'rain_probability_23',\n",
       " 'has_rain_probability_23',\n",
       " 'P_23',\n",
       " 'U_23',\n",
       " 'WW_23',\n",
       " 'Td_23',\n",
       " 'N_23',\n",
       " 'S_23',\n",
       " 'W_23',\n",
       " 'E_23',\n",
       " 'last_day_avg_target_23',\n",
       " 'last_day_avg_temp_23',\n",
       " 'last_evening_avg_target_19_23',\n",
       " 'last_evening_avg_temp_19_23',\n",
       " 'last_evening_avg_target_22_23',\n",
       " 'last_evening_avg_temp_22_23',\n",
       " 'target_1',\n",
       " 'temp_1',\n",
       " 'target_5',\n",
       " 'temp_5',\n",
       " 'target_9',\n",
       " 'temp_9']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Отбираем признаки. Список формируем заново из всех текущих колонок.\n",
    "feature_cols = list(train_ds.columns)\n",
    "\n",
    "# Отбрасываем колонки из ранее заготовленного списка на выброс. На этом этапе уходят колонки с утечками.\n",
    "for name in drop_list:\n",
    "    feature_cols.remove(name)\n",
    "\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca6872",
   "metadata": {},
   "source": [
    "#### 1.13.1 Аугументации MIXUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c8bf7f",
   "metadata": {},
   "source": [
    "##### Обозначаем все регрессионные признаки которые в последствии мы изменим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19d3aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_aug = [\n",
    "'temp_pred',\n",
    "'temp_last_day',\n",
    "'target_lag_24',\n",
    "'target_lag_72',\n",
    "'target_lag_336',\n",
    "'VVP',\n",
    "'P',\n",
    "'U',\n",
    "'Td',\n",
    "'last_day_avg_target',\n",
    "'last_day_avg_temp',\n",
    "'last_evening_avg_target_19',\n",
    "'last_evening_avg_temp_19',\n",
    "'last_evening_avg_target_22',\n",
    "'last_evening_avg_temp_22',\n",
    "'temp_pred_1',\n",
    "'temp_last_day_1',\n",
    "'target_lag_24_1',\n",
    "'target_lag_72_1',\n",
    "'target_lag_336_1',\n",
    "'VVP_1',\n",
    "'P_1',\n",
    "'U_1',\n",
    "'Td_1',\n",
    "'last_day_avg_target_1',\n",
    "'last_day_avg_temp_1',\n",
    "'last_evening_avg_target_19_1',\n",
    "'last_evening_avg_temp_19_1',\n",
    "'last_evening_avg_target_22_1',\n",
    "'last_evening_avg_temp_22_1',\n",
    "'temp_pred_2',\n",
    "'temp_last_day_2',\n",
    "'target_lag_24_2',\n",
    "'target_lag_72_2',\n",
    "'target_lag_336_2',\n",
    "'VVP_2',\n",
    "'P_2',\n",
    "'U_2',\n",
    "'Td_2',\n",
    "'last_day_avg_target_2',\n",
    "'last_day_avg_temp_2',\n",
    "'last_evening_avg_target_19_2',\n",
    "'last_evening_avg_temp_19_2',\n",
    "'last_evening_avg_target_22_2',\n",
    "'last_evening_avg_temp_22_2',\n",
    "'temp_pred_3',\n",
    "'temp_last_day_3',\n",
    "'target_lag_24_3',\n",
    "'target_lag_72_3',\n",
    "'target_lag_336_3',\n",
    "'VVP_3',\n",
    "'P_3',\n",
    "'U_3',\n",
    "'Td_3',\n",
    "'last_day_avg_target_3',\n",
    "'last_day_avg_temp_3',\n",
    "'last_evening_avg_target_19_3',\n",
    "'last_evening_avg_temp_19_3',\n",
    "'last_evening_avg_target_22_3',\n",
    "'last_evening_avg_temp_22_3',\n",
    "'temp_pred_4',\n",
    "'temp_last_day_4',\n",
    "'target_lag_24_4',\n",
    "'target_lag_72_4',\n",
    "'target_lag_336_4',\n",
    "'VVP_4',\n",
    "'P_4',\n",
    "'U_4',\n",
    "'Td_4',\n",
    "'last_day_avg_target_4',\n",
    "'last_day_avg_temp_4',\n",
    "'last_evening_avg_target_19_4',\n",
    "'last_evening_avg_temp_19_4',\n",
    "'last_evening_avg_target_22_4',\n",
    "'last_evening_avg_temp_22_4',\n",
    "'temp_pred_5',\n",
    "'temp_last_day_5',\n",
    "'target_lag_24_5',\n",
    "'target_lag_72_5',\n",
    "'target_lag_336_5',\n",
    "'VVP_5',\n",
    "'P_5',\n",
    "'U_5',\n",
    "'Td_5',\n",
    "'last_day_avg_target_5',\n",
    "'last_day_avg_temp_5',\n",
    "'last_evening_avg_target_19_5',\n",
    "'last_evening_avg_temp_19_5',\n",
    "'last_evening_avg_target_22_5',\n",
    "'last_evening_avg_temp_22_5',\n",
    "'temp_pred_6',\n",
    "'temp_last_day_6',\n",
    "'target_lag_24_6',\n",
    "'target_lag_72_6',\n",
    "'target_lag_336_6',\n",
    "'VVP_6',\n",
    "'P_6',\n",
    "'U_6',\n",
    "'Td_6',\n",
    "'last_day_avg_target_6',\n",
    "'last_day_avg_temp_6',\n",
    "'last_evening_avg_target_19_6',\n",
    "'last_evening_avg_temp_19_6',\n",
    "'last_evening_avg_target_22_6',\n",
    "'last_evening_avg_temp_22_6',\n",
    "'temp_pred_7',\n",
    "'temp_last_day_7',\n",
    "'target_lag_24_7',\n",
    "'target_lag_72_7',\n",
    "'target_lag_336_7',\n",
    "'VVP_7',\n",
    "'P_7',\n",
    "'U_7',\n",
    "'Td_7',\n",
    "'last_day_avg_target_7',\n",
    "'last_day_avg_temp_7',\n",
    "'last_evening_avg_target_19_7',\n",
    "'last_evening_avg_temp_19_7',\n",
    "'last_evening_avg_target_22_7',\n",
    "'last_evening_avg_temp_22_7',\n",
    "'temp_pred_8',\n",
    "'temp_last_day_8',\n",
    "'target_lag_24_8',\n",
    "'target_lag_72_8',\n",
    "'target_lag_336_8',\n",
    "'VVP_8',\n",
    "'P_8',\n",
    "'U_8',\n",
    "'Td_8',\n",
    "'last_day_avg_target_8',\n",
    "'last_day_avg_temp_8',\n",
    "'last_evening_avg_target_19_8',\n",
    "'last_evening_avg_temp_19_8',\n",
    "'last_evening_avg_target_22_8',\n",
    "'last_evening_avg_temp_22_8',\n",
    "'temp_pred_9',\n",
    "'temp_last_day_9',\n",
    "'target_lag_24_9',\n",
    "'target_lag_72_9',\n",
    "'target_lag_336_9',\n",
    "'VVP_9',\n",
    "'P_9',\n",
    "'U_9',\n",
    "'Td_9',\n",
    "'last_day_avg_target_9',\n",
    "'last_day_avg_temp_9',\n",
    "'last_evening_avg_target_19_9',\n",
    "'last_evening_avg_temp_19_9',\n",
    "'last_evening_avg_target_22_9',\n",
    "'last_evening_avg_temp_22_9',\n",
    "'temp_pred_10',\n",
    "'temp_last_day_10',\n",
    "'target_lag_24_10',\n",
    "'target_lag_72_10',\n",
    "'target_lag_336_10',\n",
    "'VVP_10',\n",
    "'P_10',\n",
    "'U_10',\n",
    "'Td_10',\n",
    "'last_day_avg_target_10',\n",
    "'last_day_avg_temp_10',\n",
    "'last_evening_avg_target_19_10',\n",
    "'last_evening_avg_temp_19_10',\n",
    "'last_evening_avg_target_22_10',\n",
    "'last_evening_avg_temp_22_10',\n",
    "'temp_pred_11',\n",
    "'temp_last_day_11',\n",
    "'target_lag_24_11',\n",
    "'target_lag_72_11',\n",
    "'target_lag_336_11',\n",
    "'VVP_11',\n",
    "'P_11',\n",
    "'U_11',\n",
    "'Td_11',\n",
    "'last_day_avg_target_11',\n",
    "'last_day_avg_temp_11',\n",
    "'last_evening_avg_target_19_11',\n",
    "'last_evening_avg_temp_19_11',\n",
    "'last_evening_avg_target_22_11',\n",
    "'last_evening_avg_temp_22_11',\n",
    "'temp_pred_12',\n",
    "'temp_last_day_12',\n",
    "'target_lag_24_12',\n",
    "'target_lag_72_12',\n",
    "'target_lag_336_12',\n",
    "'VVP_12',\n",
    "'P_12',\n",
    "'U_12',\n",
    "'Td_12',\n",
    "'last_day_avg_target_12',\n",
    "'last_day_avg_temp_12',\n",
    "'last_evening_avg_target_19_12',\n",
    "'last_evening_avg_temp_19_12',\n",
    "'last_evening_avg_target_22_12',\n",
    "'last_evening_avg_temp_22_12',\n",
    "'temp_pred_13',\n",
    "'temp_last_day_13',\n",
    "'target_lag_24_13',\n",
    "'target_lag_72_13',\n",
    "'target_lag_336_13',\n",
    "'VVP_13',\n",
    "'P_13',\n",
    "'U_13',\n",
    "'Td_13',\n",
    "'last_day_avg_target_13',\n",
    "'last_day_avg_temp_13',\n",
    "'last_evening_avg_target_19_13',\n",
    "'last_evening_avg_temp_19_13',\n",
    "'last_evening_avg_target_22_13',\n",
    "'last_evening_avg_temp_22_13',\n",
    "'temp_pred_14',\n",
    "'temp_last_day_14',\n",
    "'target_lag_24_14',\n",
    "'target_lag_72_14',\n",
    "'target_lag_336_14',\n",
    "'VVP_14',\n",
    "'P_14',\n",
    "'U_14',\n",
    "'Td_14',\n",
    "'last_day_avg_target_14',\n",
    "'last_day_avg_temp_14',\n",
    "'last_evening_avg_target_19_14',\n",
    "'last_evening_avg_temp_19_14',\n",
    "'last_evening_avg_target_22_14',\n",
    "'last_evening_avg_temp_22_14',\n",
    "'temp_pred_15',\n",
    "'temp_last_day_15',\n",
    "'target_lag_24_15',\n",
    "'target_lag_72_15',\n",
    "'target_lag_336_15',\n",
    "'VVP_15',\n",
    "'P_15',\n",
    "'U_15',\n",
    "'Td_15',\n",
    "'last_day_avg_target_15',\n",
    "'last_day_avg_temp_15',\n",
    "'last_evening_avg_target_19_15',\n",
    "'last_evening_avg_temp_19_15',\n",
    "'last_evening_avg_target_22_15',\n",
    "'last_evening_avg_temp_22_15',\n",
    "'temp_pred_16',\n",
    "'temp_last_day_16',\n",
    "'target_lag_24_16',\n",
    "'target_lag_72_16',\n",
    "'target_lag_336_16',\n",
    "'VVP_16',\n",
    "'P_16',\n",
    "'U_16',\n",
    "'Td_16',\n",
    "'last_day_avg_target_16',\n",
    "'last_day_avg_temp_16',\n",
    "'last_evening_avg_target_19_16',\n",
    "'last_evening_avg_temp_19_16',\n",
    "'last_evening_avg_target_22_16',\n",
    "'last_evening_avg_temp_22_16',\n",
    "'temp_pred_17',\n",
    "'temp_last_day_17',\n",
    "'target_lag_24_17',\n",
    "'target_lag_72_17',\n",
    "'target_lag_336_17',\n",
    "'VVP_17',\n",
    "'P_17',\n",
    "'U_17',\n",
    "'Td_17',\n",
    "'last_day_avg_target_17',\n",
    "'last_day_avg_temp_17',\n",
    "'last_evening_avg_target_19_17',\n",
    "'last_evening_avg_temp_19_17',\n",
    "'last_evening_avg_target_22_17',\n",
    "'last_evening_avg_temp_22_17',\n",
    "'temp_pred_18',\n",
    "'temp_last_day_18',\n",
    "'target_lag_24_18',\n",
    "'target_lag_72_18',\n",
    "'target_lag_336_18',\n",
    "'VVP_18',\n",
    "'P_18',\n",
    "'U_18',\n",
    "'Td_18',\n",
    "'last_day_avg_target_18',\n",
    "'last_day_avg_temp_18',\n",
    "'last_evening_avg_target_19_18',\n",
    "'last_evening_avg_temp_19_18',\n",
    "'last_evening_avg_target_22_18',\n",
    "'last_evening_avg_temp_22_18',\n",
    "'temp_pred_19',\n",
    "'temp_last_day_19',\n",
    "'target_lag_24_19',\n",
    "'target_lag_72_19',\n",
    "'target_lag_336_19',\n",
    "'VVP_19',\n",
    "'P_19',\n",
    "'U_19',\n",
    "'Td_19',\n",
    "'last_day_avg_target_19',\n",
    "'last_day_avg_temp_19',\n",
    "'last_evening_avg_target_19_19',\n",
    "'last_evening_avg_temp_19_19',\n",
    "'last_evening_avg_target_22_19',\n",
    "'last_evening_avg_temp_22_19',\n",
    "'temp_pred_20',\n",
    "'temp_last_day_20',\n",
    "'target_lag_24_20',\n",
    "'target_lag_72_20',\n",
    "'target_lag_336_20',\n",
    "'VVP_20',\n",
    "'P_20',\n",
    "'U_20',\n",
    "'Td_20',\n",
    "'last_day_avg_target_20',\n",
    "'last_day_avg_temp_20',\n",
    "'last_evening_avg_target_19_20',\n",
    "'last_evening_avg_temp_19_20',\n",
    "'last_evening_avg_target_22_20',\n",
    "'last_evening_avg_temp_22_20',\n",
    "'temp_pred_21',\n",
    "'temp_last_day_21',\n",
    "'target_lag_24_21',\n",
    "'target_lag_72_21',\n",
    "'target_lag_336_21',\n",
    "'VVP_21',\n",
    "'P_21',\n",
    "'U_21',\n",
    "'Td_21',\n",
    "'last_day_avg_target_21',\n",
    "'last_day_avg_temp_21',\n",
    "'last_evening_avg_target_19_21',\n",
    "'last_evening_avg_temp_19_21',\n",
    "'last_evening_avg_target_22_21',\n",
    "'last_evening_avg_temp_22_21',\n",
    "'temp_pred_22',\n",
    "'temp_last_day_22',\n",
    "'target_lag_24_22',\n",
    "'target_lag_72_22',\n",
    "'target_lag_336_22',\n",
    "'VVP_22',\n",
    "'P_22',\n",
    "'U_22',\n",
    "'Td_22',\n",
    "'last_day_avg_target_22',\n",
    "'last_day_avg_temp_22',\n",
    "'last_evening_avg_target_19_22',\n",
    "'last_evening_avg_temp_19_22',\n",
    "'last_evening_avg_target_22_22',\n",
    "'last_evening_avg_temp_22_22',\n",
    "'temp_pred_23',\n",
    "'temp_last_day_23',\n",
    "'target_lag_24_23',\n",
    "'target_lag_72_23',\n",
    "'target_lag_336_23',\n",
    "'VVP_23',\n",
    "'P_23',\n",
    "'U_23',\n",
    "'Td_23',\n",
    "'last_day_avg_target_23',\n",
    "'last_day_avg_temp_23',\n",
    "'last_evening_avg_target_19_23',\n",
    "'last_evening_avg_temp_19_23',\n",
    "'last_evening_avg_target_22_23',\n",
    "'last_evening_avg_temp_22_23',\n",
    "'target_1',\n",
    "'temp_1',\n",
    "'target_5',\n",
    "'temp_5',\n",
    "'target_9',\n",
    "'temp_9']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0154f8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'time', 'temp_pred', 'year', 'month', 'day_of_week', 'day', 'holidays', 'preholidays', 'temp_last_day', 'target_lag_24', 'target_lag_72', 'target_lag_336', 'VVP', 'cloudy', 'rainy', 'windy', 'clear', 'rain_probability', 'has_rain_probability', 'P', 'U', 'WW', 'Td', 'N', 'S', 'W', 'E', 'last_day_avg_target', 'last_day_avg_temp', 'last_evening_avg_target_19', 'last_evening_avg_temp_19', 'last_evening_avg_target_22', 'last_evening_avg_temp_22', 'time_1', 'temp_pred_1', 'year_1', 'month_1', 'day_of_week_1', 'day_1', 'holidays_1', 'preholidays_1', 'temp_last_day_1', 'target_lag_24_1', 'target_lag_72_1', 'target_lag_336_1', 'VVP_1', 'cloudy_1', 'rainy_1', 'windy_1', 'clear_1', 'rain_probability_1', 'has_rain_probability_1', 'P_1', 'U_1', 'WW_1', 'Td_1', 'N_1', 'S_1', 'W_1', 'E_1', 'last_day_avg_target_1', 'last_day_avg_temp_1', 'last_evening_avg_target_19_1', 'last_evening_avg_temp_19_1', 'last_evening_avg_target_22_1', 'last_evening_avg_temp_22_1', 'time_2', 'temp_pred_2', 'year_2', 'month_2', 'day_of_week_2', 'day_2', 'holidays_2', 'preholidays_2', 'temp_last_day_2', 'target_lag_24_2', 'target_lag_72_2', 'target_lag_336_2', 'VVP_2', 'cloudy_2', 'rainy_2', 'windy_2', 'clear_2', 'rain_probability_2', 'has_rain_probability_2', 'P_2', 'U_2', 'WW_2', 'Td_2', 'N_2', 'S_2', 'W_2', 'E_2', 'last_day_avg_target_2', 'last_day_avg_temp_2', 'last_evening_avg_target_19_2', 'last_evening_avg_temp_19_2', 'last_evening_avg_target_22_2', 'last_evening_avg_temp_22_2', 'time_3', 'temp_pred_3', 'year_3', 'month_3', 'day_of_week_3', 'day_3', 'holidays_3', 'preholidays_3', 'temp_last_day_3', 'target_lag_24_3', 'target_lag_72_3', 'target_lag_336_3', 'VVP_3', 'cloudy_3', 'rainy_3', 'windy_3', 'clear_3', 'rain_probability_3', 'has_rain_probability_3', 'P_3', 'U_3', 'WW_3', 'Td_3', 'N_3', 'S_3', 'W_3', 'E_3', 'last_day_avg_target_3', 'last_day_avg_temp_3', 'last_evening_avg_target_19_3', 'last_evening_avg_temp_19_3', 'last_evening_avg_target_22_3', 'last_evening_avg_temp_22_3', 'time_4', 'temp_pred_4', 'year_4', 'month_4', 'day_of_week_4', 'day_4', 'holidays_4', 'preholidays_4', 'temp_last_day_4', 'target_lag_24_4', 'target_lag_72_4', 'target_lag_336_4', 'VVP_4', 'cloudy_4', 'rainy_4', 'windy_4', 'clear_4', 'rain_probability_4', 'has_rain_probability_4', 'P_4', 'U_4', 'WW_4', 'Td_4', 'N_4', 'S_4', 'W_4', 'E_4', 'last_day_avg_target_4', 'last_day_avg_temp_4', 'last_evening_avg_target_19_4', 'last_evening_avg_temp_19_4', 'last_evening_avg_target_22_4', 'last_evening_avg_temp_22_4', 'time_5', 'temp_pred_5', 'year_5', 'month_5', 'day_of_week_5', 'day_5', 'holidays_5', 'preholidays_5', 'temp_last_day_5', 'target_lag_24_5', 'target_lag_72_5', 'target_lag_336_5', 'VVP_5', 'cloudy_5', 'rainy_5', 'windy_5', 'clear_5', 'rain_probability_5', 'has_rain_probability_5', 'P_5', 'U_5', 'WW_5', 'Td_5', 'N_5', 'S_5', 'W_5', 'E_5', 'last_day_avg_target_5', 'last_day_avg_temp_5', 'last_evening_avg_target_19_5', 'last_evening_avg_temp_19_5', 'last_evening_avg_target_22_5', 'last_evening_avg_temp_22_5', 'time_6', 'temp_pred_6', 'year_6', 'month_6', 'day_of_week_6', 'day_6', 'holidays_6', 'preholidays_6', 'temp_last_day_6', 'target_lag_24_6', 'target_lag_72_6', 'target_lag_336_6', 'VVP_6', 'cloudy_6', 'rainy_6', 'windy_6', 'clear_6', 'rain_probability_6', 'has_rain_probability_6', 'P_6', 'U_6', 'WW_6', 'Td_6', 'N_6', 'S_6', 'W_6', 'E_6', 'last_day_avg_target_6', 'last_day_avg_temp_6', 'last_evening_avg_target_19_6', 'last_evening_avg_temp_19_6', 'last_evening_avg_target_22_6', 'last_evening_avg_temp_22_6', 'time_7', 'temp_pred_7', 'year_7', 'month_7', 'day_of_week_7', 'day_7', 'holidays_7', 'preholidays_7', 'temp_last_day_7', 'target_lag_24_7', 'target_lag_72_7', 'target_lag_336_7', 'VVP_7', 'cloudy_7', 'rainy_7', 'windy_7', 'clear_7', 'rain_probability_7', 'has_rain_probability_7', 'P_7', 'U_7', 'WW_7', 'Td_7', 'N_7', 'S_7', 'W_7', 'E_7', 'last_day_avg_target_7', 'last_day_avg_temp_7', 'last_evening_avg_target_19_7', 'last_evening_avg_temp_19_7', 'last_evening_avg_target_22_7', 'last_evening_avg_temp_22_7', 'time_8', 'temp_pred_8', 'year_8', 'month_8', 'day_of_week_8', 'day_8', 'holidays_8', 'preholidays_8', 'temp_last_day_8', 'target_lag_24_8', 'target_lag_72_8', 'target_lag_336_8', 'VVP_8', 'cloudy_8', 'rainy_8', 'windy_8', 'clear_8', 'rain_probability_8', 'has_rain_probability_8', 'P_8', 'U_8', 'WW_8', 'Td_8', 'N_8', 'S_8', 'W_8', 'E_8', 'last_day_avg_target_8', 'last_day_avg_temp_8', 'last_evening_avg_target_19_8', 'last_evening_avg_temp_19_8', 'last_evening_avg_target_22_8', 'last_evening_avg_temp_22_8', 'time_9', 'temp_pred_9', 'year_9', 'month_9', 'day_of_week_9', 'day_9', 'holidays_9', 'preholidays_9', 'temp_last_day_9', 'target_lag_24_9', 'target_lag_72_9', 'target_lag_336_9', 'VVP_9', 'cloudy_9', 'rainy_9', 'windy_9', 'clear_9', 'rain_probability_9', 'has_rain_probability_9', 'P_9', 'U_9', 'WW_9', 'Td_9', 'N_9', 'S_9', 'W_9', 'E_9', 'last_day_avg_target_9', 'last_day_avg_temp_9', 'last_evening_avg_target_19_9', 'last_evening_avg_temp_19_9', 'last_evening_avg_target_22_9', 'last_evening_avg_temp_22_9', 'time_10', 'temp_pred_10', 'year_10', 'month_10', 'day_of_week_10', 'day_10', 'holidays_10', 'preholidays_10', 'temp_last_day_10', 'target_lag_24_10', 'target_lag_72_10', 'target_lag_336_10', 'VVP_10', 'cloudy_10', 'rainy_10', 'windy_10', 'clear_10', 'rain_probability_10', 'has_rain_probability_10', 'P_10', 'U_10', 'WW_10', 'Td_10', 'N_10', 'S_10', 'W_10', 'E_10', 'last_day_avg_target_10', 'last_day_avg_temp_10', 'last_evening_avg_target_19_10', 'last_evening_avg_temp_19_10', 'last_evening_avg_target_22_10', 'last_evening_avg_temp_22_10', 'time_11', 'temp_pred_11', 'year_11', 'month_11', 'day_of_week_11', 'day_11', 'holidays_11', 'preholidays_11', 'temp_last_day_11', 'target_lag_24_11', 'target_lag_72_11', 'target_lag_336_11', 'VVP_11', 'cloudy_11', 'rainy_11', 'windy_11', 'clear_11', 'rain_probability_11', 'has_rain_probability_11', 'P_11', 'U_11', 'WW_11', 'Td_11', 'N_11', 'S_11', 'W_11', 'E_11', 'last_day_avg_target_11', 'last_day_avg_temp_11', 'last_evening_avg_target_19_11', 'last_evening_avg_temp_19_11', 'last_evening_avg_target_22_11', 'last_evening_avg_temp_22_11', 'time_12', 'temp_pred_12', 'year_12', 'month_12', 'day_of_week_12', 'day_12', 'holidays_12', 'preholidays_12', 'temp_last_day_12', 'target_lag_24_12', 'target_lag_72_12', 'target_lag_336_12', 'VVP_12', 'cloudy_12', 'rainy_12', 'windy_12', 'clear_12', 'rain_probability_12', 'has_rain_probability_12', 'P_12', 'U_12', 'WW_12', 'Td_12', 'N_12', 'S_12', 'W_12', 'E_12', 'last_day_avg_target_12', 'last_day_avg_temp_12', 'last_evening_avg_target_19_12', 'last_evening_avg_temp_19_12', 'last_evening_avg_target_22_12', 'last_evening_avg_temp_22_12', 'time_13', 'temp_pred_13', 'year_13', 'month_13', 'day_of_week_13', 'day_13', 'holidays_13', 'preholidays_13', 'temp_last_day_13', 'target_lag_24_13', 'target_lag_72_13', 'target_lag_336_13', 'VVP_13', 'cloudy_13', 'rainy_13', 'windy_13', 'clear_13', 'rain_probability_13', 'has_rain_probability_13', 'P_13', 'U_13', 'WW_13', 'Td_13', 'N_13', 'S_13', 'W_13', 'E_13', 'last_day_avg_target_13', 'last_day_avg_temp_13', 'last_evening_avg_target_19_13', 'last_evening_avg_temp_19_13', 'last_evening_avg_target_22_13', 'last_evening_avg_temp_22_13', 'time_14', 'temp_pred_14', 'year_14', 'month_14', 'day_of_week_14', 'day_14', 'holidays_14', 'preholidays_14', 'temp_last_day_14', 'target_lag_24_14', 'target_lag_72_14', 'target_lag_336_14', 'VVP_14', 'cloudy_14', 'rainy_14', 'windy_14', 'clear_14', 'rain_probability_14', 'has_rain_probability_14', 'P_14', 'U_14', 'WW_14', 'Td_14', 'N_14', 'S_14', 'W_14', 'E_14', 'last_day_avg_target_14', 'last_day_avg_temp_14', 'last_evening_avg_target_19_14', 'last_evening_avg_temp_19_14', 'last_evening_avg_target_22_14', 'last_evening_avg_temp_22_14', 'time_15', 'temp_pred_15', 'year_15', 'month_15', 'day_of_week_15', 'day_15', 'holidays_15', 'preholidays_15', 'temp_last_day_15', 'target_lag_24_15', 'target_lag_72_15', 'target_lag_336_15', 'VVP_15', 'cloudy_15', 'rainy_15', 'windy_15', 'clear_15', 'rain_probability_15', 'has_rain_probability_15', 'P_15', 'U_15', 'WW_15', 'Td_15', 'N_15', 'S_15', 'W_15', 'E_15', 'last_day_avg_target_15', 'last_day_avg_temp_15', 'last_evening_avg_target_19_15', 'last_evening_avg_temp_19_15', 'last_evening_avg_target_22_15', 'last_evening_avg_temp_22_15', 'time_16', 'temp_pred_16', 'year_16', 'month_16', 'day_of_week_16', 'day_16', 'holidays_16', 'preholidays_16', 'temp_last_day_16', 'target_lag_24_16', 'target_lag_72_16', 'target_lag_336_16', 'VVP_16', 'cloudy_16', 'rainy_16', 'windy_16', 'clear_16', 'rain_probability_16', 'has_rain_probability_16', 'P_16', 'U_16', 'WW_16', 'Td_16', 'N_16', 'S_16', 'W_16', 'E_16', 'last_day_avg_target_16', 'last_day_avg_temp_16', 'last_evening_avg_target_19_16', 'last_evening_avg_temp_19_16', 'last_evening_avg_target_22_16', 'last_evening_avg_temp_22_16', 'time_17', 'temp_pred_17', 'year_17', 'month_17', 'day_of_week_17', 'day_17', 'holidays_17', 'preholidays_17', 'temp_last_day_17', 'target_lag_24_17', 'target_lag_72_17', 'target_lag_336_17', 'VVP_17', 'cloudy_17', 'rainy_17', 'windy_17', 'clear_17', 'rain_probability_17', 'has_rain_probability_17', 'P_17', 'U_17', 'WW_17', 'Td_17', 'N_17', 'S_17', 'W_17', 'E_17', 'last_day_avg_target_17', 'last_day_avg_temp_17', 'last_evening_avg_target_19_17', 'last_evening_avg_temp_19_17', 'last_evening_avg_target_22_17', 'last_evening_avg_temp_22_17', 'time_18', 'temp_pred_18', 'year_18', 'month_18', 'day_of_week_18', 'day_18', 'holidays_18', 'preholidays_18', 'temp_last_day_18', 'target_lag_24_18', 'target_lag_72_18', 'target_lag_336_18', 'VVP_18', 'cloudy_18', 'rainy_18', 'windy_18', 'clear_18', 'rain_probability_18', 'has_rain_probability_18', 'P_18', 'U_18', 'WW_18', 'Td_18', 'N_18', 'S_18', 'W_18', 'E_18', 'last_day_avg_target_18', 'last_day_avg_temp_18', 'last_evening_avg_target_19_18', 'last_evening_avg_temp_19_18', 'last_evening_avg_target_22_18', 'last_evening_avg_temp_22_18', 'time_19', 'temp_pred_19', 'year_19', 'month_19', 'day_of_week_19', 'day_19', 'holidays_19', 'preholidays_19', 'temp_last_day_19', 'target_lag_24_19', 'target_lag_72_19', 'target_lag_336_19', 'VVP_19', 'cloudy_19', 'rainy_19', 'windy_19', 'clear_19', 'rain_probability_19', 'has_rain_probability_19', 'P_19', 'U_19', 'WW_19', 'Td_19', 'N_19', 'S_19', 'W_19', 'E_19', 'last_day_avg_target_19', 'last_day_avg_temp_19', 'last_evening_avg_target_19_19', 'last_evening_avg_temp_19_19', 'last_evening_avg_target_22_19', 'last_evening_avg_temp_22_19', 'time_20', 'temp_pred_20', 'year_20', 'month_20', 'day_of_week_20', 'day_20', 'holidays_20', 'preholidays_20', 'temp_last_day_20', 'target_lag_24_20', 'target_lag_72_20', 'target_lag_336_20', 'VVP_20', 'cloudy_20', 'rainy_20', 'windy_20', 'clear_20', 'rain_probability_20', 'has_rain_probability_20', 'P_20', 'U_20', 'WW_20', 'Td_20', 'N_20', 'S_20', 'W_20', 'E_20', 'last_day_avg_target_20', 'last_day_avg_temp_20', 'last_evening_avg_target_19_20', 'last_evening_avg_temp_19_20', 'last_evening_avg_target_22_20', 'last_evening_avg_temp_22_20', 'time_21', 'temp_pred_21', 'year_21', 'month_21', 'day_of_week_21', 'day_21', 'holidays_21', 'preholidays_21', 'temp_last_day_21', 'target_lag_24_21', 'target_lag_72_21', 'target_lag_336_21', 'VVP_21', 'cloudy_21', 'rainy_21', 'windy_21', 'clear_21', 'rain_probability_21', 'has_rain_probability_21', 'P_21', 'U_21', 'WW_21', 'Td_21', 'N_21', 'S_21', 'W_21', 'E_21', 'last_day_avg_target_21', 'last_day_avg_temp_21', 'last_evening_avg_target_19_21', 'last_evening_avg_temp_19_21', 'last_evening_avg_target_22_21', 'last_evening_avg_temp_22_21', 'time_22', 'temp_pred_22', 'year_22', 'month_22', 'day_of_week_22', 'day_22', 'holidays_22', 'preholidays_22', 'temp_last_day_22', 'target_lag_24_22', 'target_lag_72_22', 'target_lag_336_22', 'VVP_22', 'cloudy_22', 'rainy_22', 'windy_22', 'clear_22', 'rain_probability_22', 'has_rain_probability_22', 'P_22', 'U_22', 'WW_22', 'Td_22', 'N_22', 'S_22', 'W_22', 'E_22', 'last_day_avg_target_22', 'last_day_avg_temp_22', 'last_evening_avg_target_19_22', 'last_evening_avg_temp_19_22', 'last_evening_avg_target_22_22', 'last_evening_avg_temp_22_22', 'time_23', 'temp_pred_23', 'year_23', 'month_23', 'day_of_week_23', 'day_23', 'holidays_23', 'preholidays_23', 'temp_last_day_23', 'target_lag_24_23', 'target_lag_72_23', 'target_lag_336_23', 'VVP_23', 'cloudy_23', 'rainy_23', 'windy_23', 'clear_23', 'rain_probability_23', 'has_rain_probability_23', 'P_23', 'U_23', 'WW_23', 'Td_23', 'N_23', 'S_23', 'W_23', 'E_23', 'last_day_avg_target_23', 'last_day_avg_temp_23', 'last_evening_avg_target_19_23', 'last_evening_avg_temp_19_23', 'last_evening_avg_target_22_23', 'last_evening_avg_temp_22_23', 'target_1', 'temp_1', 'target_5', 'temp_5', 'target_9', 'temp_9', 'target']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "temp_pred         float64\n",
       "temp_last_day     float64\n",
       "target_lag_24     float64\n",
       "target_lag_72     float64\n",
       "target_lag_336    float64\n",
       "                   ...   \n",
       "temp_1            float64\n",
       "target_5          float64\n",
       "temp_5            float64\n",
       "target_9          float64\n",
       "temp_9            float64\n",
       "Length: 366, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_target = feature_cols +['target'] \n",
    "#feature_target.remove('date')\n",
    "print(feature_target)\n",
    "\n",
    "train_ds[feature_aug].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84cd0c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4d3564d4424e5a87972845314f17a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1001916), Label(value='0 / 1001916…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def augment_row(df_to_augment, features_to_augment, alpha, i, random_1):\n",
    "\n",
    "\n",
    "    random.seed(random_1*i) # Постоянно меняем random seed что бы аугументации не повторялись\n",
    "\n",
    "    random_sample1 = random.randint(0000, 50000)\n",
    "    random_sample2 = random.randint(50001, 100000)\n",
    "\n",
    "    df_sample1 = df_to_augment.sample(frac=1,\n",
    "                                     random_state=random_sample1\n",
    "                                     )\n",
    "    df_sample2 = df_to_augment.sample(frac=1,\n",
    "                                     random_state = random_sample2\n",
    "                                     )\n",
    "\n",
    "    lmbda = np.random.beta(alpha, alpha)\n",
    "\n",
    "    df_mixup_sample = df_sample1.copy()\n",
    "    df_mixup_sample[features_to_augment] = df_sample1[features_to_augment] * lmbda + df_sample2[features_to_augment] * (1 - lmbda)\n",
    "        \n",
    "    other_features = list(set(df_to_augment.columns) - set(features_to_augment))\n",
    "    df_mixup_sample[other_features] = df_sample1[other_features]\n",
    "\n",
    "    return df_mixup_sample\n",
    "\n",
    "def mixup(df, alpha, features_to_augment, n_augmentations):\n",
    "    random.seed(random_state) #random.seed(42)\n",
    "    random_1 = random.randint(500, 9500)\n",
    "\n",
    "    \n",
    "    df_to_augment = df[(df['date'] < open_test_begin)] #  датасет будет подвергнут ауггументации\n",
    "    df_to_keep = df[(df['date'] >= open_test_begin)]  # Тестовая часть, не будет аугументированна\n",
    "\n",
    "    df_mixup = pd.concat([augment_row(df_to_augment, features_to_augment, alpha, i, random_1) for i in range(n_augmentations)]).parallel_apply(lambda x: x)\n",
    "\n",
    "    df_final = pd.concat([ df_mixup,\n",
    "                           df_to_augment,  #нужно раскомитить если обучение с нуля\n",
    "                           df_to_keep])\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "n_augmentations = 27 # Количество ауггументаций\n",
    "alpha=0.3 #15   # Влияет на сколько изменятся наши данные\n",
    "n_frac = 1  # какую часть датасета мы берем для изменений, в данный момент функция не принимает этот параметр\n",
    "train_ds_mixup = mixup(train_ds[feature_target], alpha, feature_aug, n_augmentations)\n",
    "FEATURES = '_Aug_27_alpha_3' # Дополнительная информация которую мы записываем во время экспериментов\n",
    "num = 6 #номер модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4def55",
   "metadata": {},
   "source": [
    "##### Проверка нового датасета\n",
    "\n",
    "* Смотрим размер\n",
    "* Обязательно сбрасываем индекс, что бы модели воспринимали новые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "336188f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40027, 800) (1041943, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/zccbjjq17fgd73999h5g3ltr0000gn/T/ipykernel_43142/1146379950.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_ds_mixup.reset_index(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[feature_target].shape, train_ds_mixup.shape)\n",
    "train_ds_mixup.reset_index(inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f34c90",
   "metadata": {},
   "source": [
    "#### 1.14 Выделение наборов данных для обучения, валидации и тестирования\n",
    "\n",
    "Выделялось два набора данных для обучения и валидации:\n",
    "1. Обучение на данных с 2019 по 2021 с валидацией на 2022\n",
    "2. Обучение на данных с 2019 по 2022 с валидацией на первом квартале 2023\n",
    "\n",
    "Первый набор позволяет оценить влияние сезонности на обучение и предсказания, второй позволяет обучить модель на большем объеме данных и на более актуальных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a2b8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формируем набор датасетов для обучения и проверки\n",
    "\n",
    "# здесь мы используем аугументированный dataset\n",
    "\n",
    "features = train_ds_mixup[feature_cols] # train_ds[feature_cols]\n",
    "target = train_ds_mixup['target'] # train_ds['target']\n",
    "\n",
    "# Функция для выделения временных интервалов из таблиц признаков и целей\n",
    "# на этом этапе отбрасываем колонку 'date'\n",
    "def features_interval(features, target, date1, date2):\n",
    "    features_interval = features[ (features['date']>=date1) & (features['date']<date2) ]\n",
    "    target_interval = target[features_interval.index]\n",
    "    features_interval = features_interval.drop('date', axis=1)\n",
    "    return features_interval, target_interval\n",
    "\n",
    "# для первичного подбора гиперпараметров будем обучать на 19-21 годах, валидировать по 2022\n",
    "features_train, target_train = features_interval(features, target, '2019-01-01', '2022-01-01')\n",
    "features_valid, target_valid = features_interval(features, target, '2022-01-01', '2023-01-01')\n",
    "\n",
    "# отбор признаков будем производить, обучая на 19-22 и проверяя по первому кварталу 2023\n",
    "# с дополнительным контролем на вариантах из первичного обучения\n",
    "features_2022, target_2022 = features_interval(features, target, '2019-01-01', '2023-01-01')\n",
    "features_2023, target_2023 = features_interval(features, target, '2023-01-01', open_test_begin)\n",
    "\n",
    "# для проверки на тестовой выборке будем учиться на всем тренировочном датасете\n",
    "features_all_train, target_all_train = features_interval(features, target, '2019-01-01', open_test_begin)\n",
    "features_open_test, target_open_test = features_interval(features, target, open_test_begin, open_test_end)\n",
    "\n",
    "# формируем наборы данных по кварталам 2022 года, чтобы посмотреть по ним метрику отдельно\n",
    "dates = ['2022-01-01', '2022-04-01', '2022-07-01', '2022-10-01', '2023-01-01']\n",
    "quarters = []\n",
    "for i in range(4):\n",
    "    f, t = features_interval(features, target, dates[i], dates[i+1])\n",
    "    quarters.append({'features':f, 'target':t})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967384f",
   "metadata": {},
   "source": [
    "### 2. Обучение моделей\n",
    "\n",
    "В настоящей работе обучаются модели LightGBM и XGBoost, финальное предсказание получается усреднением результатов.\n",
    "\n",
    "#### 2.1 Гиперпараметры LightGBM\n",
    "Были подобраны следующие значения гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7772cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры LGBM без ауггументации\n",
    "params_lgbm = {'num_leaves':15, 'learning_rate':0.02, 'feature_fraction':1, 'num_iterations':NUM_ITERATIONS, 'random_state':random_state, 'objective':'regression_l1', 'n_jobs':-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "183f1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры LGBM для обучения с ауггументации\n",
    "params_lgbm = {'num_leaves': 34, 'min_child_samples': 16, \n",
    "               'max_depth': 8, 'learning_rate': 0.012, \n",
    "               'min_sum_hessian_in_leaf': 1e-4,\n",
    "               'objective': 'regression_l1', 'feature_fraction': 0.9574152630927155,\n",
    "               'n_jobs':-1, 'num_iterations':10000\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f69ef2",
   "metadata": {},
   "source": [
    "### 3 Проверка метрик на тестовом датасете\n",
    "##### 3.1 Модель LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88929019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0001\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9574152630927155, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9574152630927155\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0001\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9574152630927155, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9574152630927155\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.676412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 75507\n",
      "[LightGBM] [Info] Number of data points in the train set: 1039024, number of used features: 798\n",
      "[LightGBM] [Info] Start training from score 473.062988\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(feature_fraction=0.9574152630927155, learning_rate=0.012,\n",
       "              max_depth=8, min_child_samples=16, min_sum_hessian_in_leaf=0.0001,\n",
       "              n_jobs=-1, num_iterations=10000, num_leaves=34,\n",
       "              objective=&#x27;regression_l1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(feature_fraction=0.9574152630927155, learning_rate=0.012,\n",
       "              max_depth=8, min_child_samples=16, min_sum_hessian_in_leaf=0.0001,\n",
       "              n_jobs=-1, num_iterations=10000, num_leaves=34,\n",
       "              objective=&#x27;regression_l1&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(feature_fraction=0.9574152630927155, learning_rate=0.012,\n",
       "              max_depth=8, min_child_samples=16, min_sum_hessian_in_leaf=0.0001,\n",
       "              n_jobs=-1, num_iterations=10000, num_leaves=34,\n",
       "              objective='regression_l1')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка метрики лучшей модели на тестовом датасете\n",
    "# Здесь обучаем на всем тренировочном датасете\n",
    "\n",
    "\n",
    "lgbm_model_all_train = lgb.LGBMRegressor(**params_lgbm)\n",
    "lgbm_model_all_train.fit(features_all_train, target_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed3605e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0001\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9574152630927155, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9574152630927155\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0001\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9574152630927155, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9574152630927155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Выборка</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная LGBM _Aug_27_alpha_3</td>\n",
       "      <td>3.403002</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.996599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая LGBM _Aug_27_alpha_3</td>\n",
       "      <td>6.092151</td>\n",
       "      <td>0.014274</td>\n",
       "      <td>0.985890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Выборка       MAE      MAPE        R2\n",
       "0  тренировочная LGBM _Aug_27_alpha_3  3.403002  0.007062  0.996599\n",
       "1       тестовая LGBM _Aug_27_alpha_3  6.092151  0.014274  0.985890"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_predict_train = lgbm_model_all_train.predict(features_all_train)\n",
    "l_predict_test = lgbm_model_all_train.predict(features_open_test)\n",
    "\n",
    "mae_train, mape_train, r2_train = metrics_hour(target_all_train, l_predict_train)\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test, l_predict_test)\n",
    "\n",
    "\n",
    "results = pd.DataFrame([[f'тренировочная LGBM {FEATURES}', mae_train, mape_train, r2_train], [f'тестовая LGBM {FEATURES}', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c00fb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение важности признаков\n",
    "importance = lgbm_model_all_train.feature_importances_\n",
    "feature_name = features_open_test.columns\n",
    "# создание DataFrame\n",
    "importance_df_lgbm = pd.DataFrame({'feature': feature_name, 'importance': importance})\n",
    "# сортировка по важности\n",
    "importance_df_lgbm = importance_df_lgbm.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41359208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>target_lag_24</td>\n",
       "      <td>5168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>target_lag_336</td>\n",
       "      <td>3262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>target_lag_72</td>\n",
       "      <td>2998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>target_lag_24_1</td>\n",
       "      <td>2148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>temp_pred</td>\n",
       "      <td>2112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>year_14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>year_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>preholidays_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>preholidays_14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>year_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>798 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  importance\n",
       "9      target_lag_24        5168\n",
       "11    target_lag_336        3262\n",
       "10     target_lag_72        2998\n",
       "42   target_lag_24_1        2148\n",
       "1          temp_pred        2112\n",
       "..               ...         ...\n",
       "464          year_14           2\n",
       "332          year_10           1\n",
       "271    preholidays_8           1\n",
       "469   preholidays_14           1\n",
       "134           year_4           0\n",
       "\n",
       "[798 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Модель XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f432964",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['preholidays',\n",
    "            #'has_rain_probability', \n",
    "            # 'W', 'E'\n",
    "            ]\n",
    "n_values = range(1, 24)\n",
    "preholidays = ['preholidays_{}'.format(n) for n in n_values]\n",
    "#has_rain = ['has_rain_probability_{}'.format(n) for n in n_values]\n",
    "#W_wind = ['W_{}'.format(n) for n in n_values]\n",
    "#E_wind = ['E_{}'.format(n) for n in n_values]\n",
    "\n",
    "drop_list = drop_list + preholidays #+ has_rain + W_wind + E_wind\n",
    "\n",
    "feat_xgb_train = features_all_train.drop(columns=drop_list)\n",
    "feat_xgb_test = features_open_test.drop(columns=drop_list)\n",
    "#feat_xgb_train.columns, feat_xgb_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ddbbfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(\n",
    "    max_depth=7,\n",
    "    n_estimators=1190, #n_estimators=195, #\n",
    "    learning_rate=0.009, #learning_rate=0.1, #\n",
    "    tree_method='exact',\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',\n",
    "    gamma=2,\n",
    "    colsample_bytree=1,\n",
    "    random_state=random_state\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85ba1779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/xgboost/sklearn.py:885: UserWarning: `callbacks` in `fit` method is deprecated for better compatibility with scikit-learn, use `callbacks` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Time for last iteration: -26.525530099868774\n",
      "Iteration: 1 Time for last iteration: -11.478789806365967\n",
      "Iteration: 2 Time for last iteration: -11.780966997146606\n",
      "Iteration: 3 Time for last iteration: -12.095326900482178\n",
      "Iteration: 4 Time for last iteration: -11.412700176239014\n",
      "Iteration: 5 Time for last iteration: -12.471737146377563\n",
      "Iteration: 6 Time for last iteration: -12.06488299369812\n",
      "Iteration: 7 Time for last iteration: -11.979055881500244\n",
      "Iteration: 8 Time for last iteration: -11.368681907653809\n",
      "Iteration: 9 Time for last iteration: -11.847079277038574\n",
      "Iteration: 10 Time for last iteration: -11.418037176132202\n",
      "Iteration: 11 Time for last iteration: -11.452046155929565\n",
      "Iteration: 12 Time for last iteration: -11.389868021011353\n",
      "Iteration: 13 Time for last iteration: -11.747148990631104\n",
      "Iteration: 14 Time for last iteration: -11.650840997695923\n",
      "Iteration: 15 Time for last iteration: -11.413532257080078\n",
      "Iteration: 16 Time for last iteration: -11.475651741027832\n",
      "Iteration: 17 Time for last iteration: -11.488332986831665\n",
      "Iteration: 18 Time for last iteration: -11.611287117004395\n",
      "Iteration: 19 Time for last iteration: -11.194981098175049\n",
      "Iteration: 20 Time for last iteration: -11.60469913482666\n",
      "Iteration: 21 Time for last iteration: -11.770851135253906\n",
      "Iteration: 22 Time for last iteration: -11.46267318725586\n",
      "Iteration: 23 Time for last iteration: -11.42304015159607\n",
      "Iteration: 24 Time for last iteration: -11.582270860671997\n",
      "Iteration: 25 Time for last iteration: -11.616139888763428\n",
      "Iteration: 26 Time for last iteration: -11.989091634750366\n",
      "Iteration: 27 Time for last iteration: -12.262509822845459\n",
      "Iteration: 28 Time for last iteration: -12.593429803848267\n",
      "Iteration: 29 Time for last iteration: -12.460919857025146\n",
      "Iteration: 30 Time for last iteration: -11.661944150924683\n",
      "Iteration: 31 Time for last iteration: -11.841509103775024\n",
      "Iteration: 32 Time for last iteration: -12.040615797042847\n",
      "Iteration: 33 Time for last iteration: -11.358396053314209\n",
      "Iteration: 34 Time for last iteration: -11.46649432182312\n",
      "Iteration: 35 Time for last iteration: -11.71081805229187\n",
      "Iteration: 36 Time for last iteration: -11.743047952651978\n",
      "Iteration: 37 Time for last iteration: -11.724770069122314\n",
      "Iteration: 38 Time for last iteration: -11.805984020233154\n",
      "Iteration: 39 Time for last iteration: -11.648681163787842\n",
      "Iteration: 40 Time for last iteration: -11.535384178161621\n",
      "Iteration: 41 Time for last iteration: -11.5677170753479\n",
      "Iteration: 42 Time for last iteration: -11.546430110931396\n",
      "Iteration: 43 Time for last iteration: -11.92715334892273\n",
      "Iteration: 44 Time for last iteration: -11.80744194984436\n",
      "Iteration: 45 Time for last iteration: -11.758838891983032\n",
      "Iteration: 46 Time for last iteration: -11.470273971557617\n",
      "Iteration: 47 Time for last iteration: -11.642278909683228\n",
      "Iteration: 48 Time for last iteration: -11.389724969863892\n",
      "Iteration: 49 Time for last iteration: -11.36824083328247\n",
      "Iteration: 50 Time for last iteration: -11.687673807144165\n",
      "Iteration: 51 Time for last iteration: -11.556591749191284\n",
      "Iteration: 52 Time for last iteration: -11.73573899269104\n",
      "Iteration: 53 Time for last iteration: -11.911283016204834\n",
      "Iteration: 54 Time for last iteration: -11.575739860534668\n",
      "Iteration: 55 Time for last iteration: -11.837928056716919\n",
      "Iteration: 56 Time for last iteration: -11.663111925125122\n",
      "Iteration: 57 Time for last iteration: -11.863016843795776\n",
      "Iteration: 58 Time for last iteration: -11.363282918930054\n",
      "Iteration: 59 Time for last iteration: -11.90512490272522\n",
      "Iteration: 60 Time for last iteration: -11.510180950164795\n",
      "Iteration: 61 Time for last iteration: -11.650364875793457\n",
      "Iteration: 62 Time for last iteration: -11.961221933364868\n",
      "Iteration: 63 Time for last iteration: -11.677497863769531\n",
      "Iteration: 64 Time for last iteration: -11.392179012298584\n",
      "Iteration: 65 Time for last iteration: -11.84392786026001\n",
      "Iteration: 66 Time for last iteration: -11.952457904815674\n",
      "Iteration: 67 Time for last iteration: -11.505199909210205\n",
      "Iteration: 68 Time for last iteration: -11.768896102905273\n",
      "Iteration: 69 Time for last iteration: -11.670636653900146\n",
      "Iteration: 70 Time for last iteration: -11.7276771068573\n",
      "Iteration: 71 Time for last iteration: -11.454869031906128\n",
      "Iteration: 72 Time for last iteration: -11.552257061004639\n",
      "Iteration: 73 Time for last iteration: -11.602551221847534\n",
      "Iteration: 74 Time for last iteration: -11.558446168899536\n",
      "Iteration: 75 Time for last iteration: -11.467943906784058\n",
      "Iteration: 76 Time for last iteration: -11.781644105911255\n",
      "Iteration: 77 Time for last iteration: -11.413450002670288\n",
      "Iteration: 78 Time for last iteration: -11.685306072235107\n",
      "Iteration: 79 Time for last iteration: -11.88439130783081\n",
      "Iteration: 80 Time for last iteration: -11.813830137252808\n",
      "Iteration: 81 Time for last iteration: -11.466555833816528\n",
      "Iteration: 82 Time for last iteration: -11.584990978240967\n",
      "Iteration: 83 Time for last iteration: -11.382812976837158\n",
      "Iteration: 84 Time for last iteration: -11.978854894638062\n",
      "Iteration: 85 Time for last iteration: -11.611021041870117\n",
      "Iteration: 86 Time for last iteration: -11.526034116744995\n",
      "Iteration: 87 Time for last iteration: -11.640209913253784\n",
      "Iteration: 88 Time for last iteration: -11.765803813934326\n",
      "Iteration: 89 Time for last iteration: -11.496247053146362\n",
      "Iteration: 90 Time for last iteration: -11.468525171279907\n",
      "Iteration: 91 Time for last iteration: -11.688397645950317\n",
      "Iteration: 92 Time for last iteration: -11.432785987854004\n",
      "Iteration: 93 Time for last iteration: -11.499771118164062\n",
      "Iteration: 94 Time for last iteration: -11.71568775177002\n",
      "Iteration: 95 Time for last iteration: -11.28787088394165\n",
      "Iteration: 96 Time for last iteration: -12.185382843017578\n",
      "Iteration: 97 Time for last iteration: -11.206897974014282\n",
      "Iteration: 98 Time for last iteration: -11.992508172988892\n",
      "Iteration: 99 Time for last iteration: -11.862145900726318\n",
      "Iteration: 100 Time for last iteration: -11.555880069732666\n",
      "Iteration: 101 Time for last iteration: -11.729414224624634\n",
      "Iteration: 102 Time for last iteration: -11.801775217056274\n",
      "Iteration: 103 Time for last iteration: -11.541419982910156\n",
      "Iteration: 104 Time for last iteration: -11.925533056259155\n",
      "Iteration: 105 Time for last iteration: -11.751849174499512\n",
      "Iteration: 106 Time for last iteration: -11.9603750705719\n",
      "Iteration: 107 Time for last iteration: -12.114062070846558\n",
      "Iteration: 108 Time for last iteration: -11.281893968582153\n",
      "Iteration: 109 Time for last iteration: -11.877264976501465\n",
      "Iteration: 110 Time for last iteration: -11.847295999526978\n",
      "Iteration: 111 Time for last iteration: -11.685926914215088\n",
      "Iteration: 112 Time for last iteration: -11.667927980422974\n",
      "Iteration: 113 Time for last iteration: -11.558389902114868\n",
      "Iteration: 114 Time for last iteration: -11.70612621307373\n",
      "Iteration: 115 Time for last iteration: -11.333895921707153\n",
      "Iteration: 116 Time for last iteration: -11.699156999588013\n",
      "Iteration: 117 Time for last iteration: -11.935707092285156\n",
      "Iteration: 118 Time for last iteration: -11.732535123825073\n",
      "Iteration: 119 Time for last iteration: -11.870747089385986\n",
      "Iteration: 120 Time for last iteration: -11.75677227973938\n",
      "Iteration: 121 Time for last iteration: -11.83970308303833\n",
      "Iteration: 122 Time for last iteration: -11.953563928604126\n",
      "Iteration: 123 Time for last iteration: -11.942416906356812\n",
      "Iteration: 124 Time for last iteration: -11.873678207397461\n",
      "Iteration: 125 Time for last iteration: -11.526128053665161\n",
      "Iteration: 126 Time for last iteration: -12.132142066955566\n",
      "Iteration: 127 Time for last iteration: -12.00307011604309\n",
      "Iteration: 128 Time for last iteration: -11.703396797180176\n",
      "Iteration: 129 Time for last iteration: -12.122759819030762\n",
      "Iteration: 130 Time for last iteration: -11.771378993988037\n",
      "Iteration: 131 Time for last iteration: -11.605300903320312\n",
      "Iteration: 132 Time for last iteration: -11.97668194770813\n",
      "Iteration: 133 Time for last iteration: -11.8883957862854\n",
      "Iteration: 134 Time for last iteration: -12.082802295684814\n",
      "Iteration: 135 Time for last iteration: -12.331146955490112\n",
      "Iteration: 136 Time for last iteration: -12.10534381866455\n",
      "Iteration: 137 Time for last iteration: -11.795784950256348\n",
      "Iteration: 138 Time for last iteration: -12.019905090332031\n",
      "Iteration: 139 Time for last iteration: -11.996257781982422\n",
      "Iteration: 140 Time for last iteration: -11.93832802772522\n",
      "Iteration: 141 Time for last iteration: -11.7694251537323\n",
      "Iteration: 142 Time for last iteration: -12.124632120132446\n",
      "Iteration: 143 Time for last iteration: -11.333553791046143\n",
      "Iteration: 144 Time for last iteration: -11.635269165039062\n",
      "Iteration: 145 Time for last iteration: -12.15987777709961\n",
      "Iteration: 146 Time for last iteration: -12.14749002456665\n",
      "Iteration: 147 Time for last iteration: -11.744619846343994\n",
      "Iteration: 148 Time for last iteration: -11.318804264068604\n",
      "Iteration: 149 Time for last iteration: -12.549102067947388\n",
      "Iteration: 150 Time for last iteration: -11.411502122879028\n",
      "Iteration: 151 Time for last iteration: -12.458599090576172\n",
      "Iteration: 152 Time for last iteration: -11.129379987716675\n",
      "Iteration: 153 Time for last iteration: -12.737339973449707\n",
      "Iteration: 154 Time for last iteration: -12.473563194274902\n",
      "Iteration: 155 Time for last iteration: -12.493052005767822\n",
      "Iteration: 156 Time for last iteration: -11.74419903755188\n",
      "Iteration: 157 Time for last iteration: -12.031776905059814\n",
      "Iteration: 158 Time for last iteration: -11.696730136871338\n",
      "Iteration: 159 Time for last iteration: -11.736583948135376\n",
      "Iteration: 160 Time for last iteration: -11.221394777297974\n",
      "Iteration: 161 Time for last iteration: -11.837735176086426\n",
      "Iteration: 162 Time for last iteration: -12.223501920700073\n",
      "Iteration: 163 Time for last iteration: -12.191362142562866\n",
      "Iteration: 164 Time for last iteration: -11.565666198730469\n",
      "Iteration: 165 Time for last iteration: -12.212390184402466\n",
      "Iteration: 166 Time for last iteration: -11.356953859329224\n",
      "Iteration: 167 Time for last iteration: -11.697495222091675\n",
      "Iteration: 168 Time for last iteration: -11.759586811065674\n",
      "Iteration: 169 Time for last iteration: -11.57865285873413\n",
      "Iteration: 170 Time for last iteration: -11.552408933639526\n",
      "Iteration: 171 Time for last iteration: -12.559931993484497\n",
      "Iteration: 172 Time for last iteration: -11.88737416267395\n",
      "Iteration: 173 Time for last iteration: -11.54777479171753\n",
      "Iteration: 174 Time for last iteration: -12.331126928329468\n",
      "Iteration: 175 Time for last iteration: -12.260877847671509\n",
      "Iteration: 176 Time for last iteration: -11.570649862289429\n",
      "Iteration: 177 Time for last iteration: -11.728193044662476\n",
      "Iteration: 178 Time for last iteration: -11.247465133666992\n",
      "Iteration: 179 Time for last iteration: -11.414726734161377\n",
      "Iteration: 180 Time for last iteration: -11.472103834152222\n",
      "Iteration: 181 Time for last iteration: -11.48323392868042\n",
      "Iteration: 182 Time for last iteration: -12.271521091461182\n",
      "Iteration: 183 Time for last iteration: -12.922943115234375\n",
      "Iteration: 184 Time for last iteration: -13.063835144042969\n",
      "Iteration: 185 Time for last iteration: -12.79461407661438\n",
      "Iteration: 186 Time for last iteration: -12.96834421157837\n",
      "Iteration: 187 Time for last iteration: -13.049617052078247\n",
      "Iteration: 188 Time for last iteration: -12.655428886413574\n",
      "Iteration: 189 Time for last iteration: -13.082266092300415\n",
      "Iteration: 190 Time for last iteration: -13.292427778244019\n",
      "Iteration: 191 Time for last iteration: -13.154906988143921\n",
      "Iteration: 192 Time for last iteration: -13.118135213851929\n",
      "Iteration: 193 Time for last iteration: -13.072928190231323\n",
      "Iteration: 194 Time for last iteration: -12.920053005218506\n",
      "Iteration: 195 Time for last iteration: -12.736032009124756\n",
      "Iteration: 196 Time for last iteration: -12.909523010253906\n",
      "Iteration: 197 Time for last iteration: -13.117124319076538\n",
      "Iteration: 198 Time for last iteration: -13.021294116973877\n",
      "Iteration: 199 Time for last iteration: -13.143131256103516\n",
      "Iteration: 200 Time for last iteration: -12.559852838516235\n",
      "Iteration: 201 Time for last iteration: -12.680623769760132\n",
      "Iteration: 202 Time for last iteration: -13.310039043426514\n",
      "Iteration: 203 Time for last iteration: -11.413070678710938\n",
      "Iteration: 204 Time for last iteration: -11.892746925354004\n",
      "Iteration: 205 Time for last iteration: -11.86506700515747\n",
      "Iteration: 206 Time for last iteration: -13.073061227798462\n",
      "Iteration: 207 Time for last iteration: -12.766049146652222\n",
      "Iteration: 208 Time for last iteration: -11.440691232681274\n",
      "Iteration: 209 Time for last iteration: -11.524276971817017\n",
      "Iteration: 210 Time for last iteration: -12.182574987411499\n",
      "Iteration: 211 Time for last iteration: -12.907212018966675\n",
      "Iteration: 212 Time for last iteration: -13.28217601776123\n",
      "Iteration: 213 Time for last iteration: -11.830053806304932\n",
      "Iteration: 214 Time for last iteration: -11.56020998954773\n",
      "Iteration: 215 Time for last iteration: -11.17622995376587\n",
      "Iteration: 216 Time for last iteration: -14.203754901885986\n",
      "Iteration: 217 Time for last iteration: -11.472705841064453\n",
      "Iteration: 218 Time for last iteration: -12.05901575088501\n",
      "Iteration: 219 Time for last iteration: -11.768630027770996\n",
      "Iteration: 220 Time for last iteration: -13.056763887405396\n",
      "Iteration: 221 Time for last iteration: -11.096735954284668\n",
      "Iteration: 222 Time for last iteration: -11.519842863082886\n",
      "Iteration: 223 Time for last iteration: -11.223877906799316\n",
      "Iteration: 224 Time for last iteration: -10.529320001602173\n",
      "Iteration: 225 Time for last iteration: -10.986436128616333\n",
      "Iteration: 226 Time for last iteration: -11.338942050933838\n",
      "Iteration: 227 Time for last iteration: -11.41252613067627\n",
      "Iteration: 228 Time for last iteration: -12.01112675666809\n",
      "Iteration: 229 Time for last iteration: -11.798382759094238\n",
      "Iteration: 230 Time for last iteration: -11.46425724029541\n",
      "Iteration: 231 Time for last iteration: -11.543172121047974\n",
      "Iteration: 232 Time for last iteration: -10.738736867904663\n",
      "Iteration: 233 Time for last iteration: -11.875277042388916\n",
      "Iteration: 234 Time for last iteration: -11.730977058410645\n",
      "Iteration: 235 Time for last iteration: -11.027787208557129\n",
      "Iteration: 236 Time for last iteration: -12.049249172210693\n",
      "Iteration: 237 Time for last iteration: -12.148007869720459\n",
      "Iteration: 238 Time for last iteration: -12.069962978363037\n",
      "Iteration: 239 Time for last iteration: -11.153225898742676\n",
      "Iteration: 240 Time for last iteration: -11.176806926727295\n",
      "Iteration: 241 Time for last iteration: -10.8579740524292\n",
      "Iteration: 242 Time for last iteration: -11.206955194473267\n",
      "Iteration: 243 Time for last iteration: -10.765982866287231\n",
      "Iteration: 244 Time for last iteration: -10.846595764160156\n",
      "Iteration: 245 Time for last iteration: -10.916120052337646\n",
      "Iteration: 246 Time for last iteration: -10.374613761901855\n",
      "Iteration: 247 Time for last iteration: -11.676184177398682\n",
      "Iteration: 248 Time for last iteration: -12.977122068405151\n",
      "Iteration: 249 Time for last iteration: -11.486044883728027\n",
      "Iteration: 250 Time for last iteration: -11.096517086029053\n",
      "Iteration: 251 Time for last iteration: -10.918598175048828\n",
      "Iteration: 252 Time for last iteration: -10.998666048049927\n",
      "Iteration: 253 Time for last iteration: -10.594903945922852\n",
      "Iteration: 254 Time for last iteration: -11.257094860076904\n",
      "Iteration: 255 Time for last iteration: -10.80430817604065\n",
      "Iteration: 256 Time for last iteration: -10.666086912155151\n",
      "Iteration: 257 Time for last iteration: -11.328295230865479\n",
      "Iteration: 258 Time for last iteration: -11.24940013885498\n",
      "Iteration: 259 Time for last iteration: -10.993366956710815\n",
      "Iteration: 260 Time for last iteration: -11.158275127410889\n",
      "Iteration: 261 Time for last iteration: -11.084831237792969\n",
      "Iteration: 262 Time for last iteration: -11.002514123916626\n",
      "Iteration: 263 Time for last iteration: -11.149590015411377\n",
      "Iteration: 264 Time for last iteration: -11.350535154342651\n",
      "Iteration: 265 Time for last iteration: -11.338644981384277\n",
      "Iteration: 266 Time for last iteration: -10.877142906188965\n",
      "Iteration: 267 Time for last iteration: -11.058125019073486\n",
      "Iteration: 268 Time for last iteration: -10.806809663772583\n",
      "Iteration: 269 Time for last iteration: -11.387838363647461\n",
      "Iteration: 270 Time for last iteration: -11.083828926086426\n",
      "Iteration: 271 Time for last iteration: -11.781106948852539\n",
      "Iteration: 272 Time for last iteration: -11.018637895584106\n",
      "Iteration: 273 Time for last iteration: -11.037560224533081\n",
      "Iteration: 274 Time for last iteration: -11.197164058685303\n",
      "Iteration: 275 Time for last iteration: -11.776927947998047\n",
      "Iteration: 276 Time for last iteration: -11.987413883209229\n",
      "Iteration: 277 Time for last iteration: -11.864393949508667\n",
      "Iteration: 278 Time for last iteration: -11.959285020828247\n",
      "Iteration: 279 Time for last iteration: -11.73555302619934\n",
      "Iteration: 280 Time for last iteration: -11.772680044174194\n",
      "Iteration: 281 Time for last iteration: -12.756629228591919\n",
      "Iteration: 282 Time for last iteration: -12.607874631881714\n",
      "Iteration: 283 Time for last iteration: -11.992067098617554\n",
      "Iteration: 284 Time for last iteration: -11.515753984451294\n",
      "Iteration: 285 Time for last iteration: -11.89530324935913\n",
      "Iteration: 286 Time for last iteration: -11.22947096824646\n",
      "Iteration: 287 Time for last iteration: -11.232342004776001\n",
      "Iteration: 288 Time for last iteration: -10.870171070098877\n",
      "Iteration: 289 Time for last iteration: -11.640419244766235\n",
      "Iteration: 290 Time for last iteration: -12.228319883346558\n",
      "Iteration: 291 Time for last iteration: -10.63762617111206\n",
      "Iteration: 292 Time for last iteration: -11.119715929031372\n",
      "Iteration: 293 Time for last iteration: -11.183734893798828\n",
      "Iteration: 294 Time for last iteration: -11.20348310470581\n",
      "Iteration: 295 Time for last iteration: -11.15009617805481\n",
      "Iteration: 296 Time for last iteration: -11.163043022155762\n",
      "Iteration: 297 Time for last iteration: -11.385891914367676\n",
      "Iteration: 298 Time for last iteration: -11.40560007095337\n",
      "Iteration: 299 Time for last iteration: -11.105406999588013\n",
      "Iteration: 300 Time for last iteration: -11.51806378364563\n",
      "Iteration: 301 Time for last iteration: -11.838294982910156\n",
      "Iteration: 302 Time for last iteration: -11.922158002853394\n",
      "Iteration: 303 Time for last iteration: -11.717287302017212\n",
      "Iteration: 304 Time for last iteration: -11.588114023208618\n",
      "Iteration: 305 Time for last iteration: -11.682626724243164\n",
      "Iteration: 306 Time for last iteration: -12.16880202293396\n",
      "Iteration: 307 Time for last iteration: -11.42041802406311\n",
      "Iteration: 308 Time for last iteration: -12.423671007156372\n",
      "Iteration: 309 Time for last iteration: -13.168501138687134\n",
      "Iteration: 310 Time for last iteration: -11.605411052703857\n",
      "Iteration: 311 Time for last iteration: -12.174366235733032\n",
      "Iteration: 312 Time for last iteration: -12.04950499534607\n",
      "Iteration: 313 Time for last iteration: -11.592849016189575\n",
      "Iteration: 314 Time for last iteration: -12.39436388015747\n",
      "Iteration: 315 Time for last iteration: -12.211050987243652\n",
      "Iteration: 316 Time for last iteration: -12.258199214935303\n",
      "Iteration: 317 Time for last iteration: -11.739817142486572\n",
      "Iteration: 318 Time for last iteration: -11.77827000617981\n",
      "Iteration: 319 Time for last iteration: -11.89241886138916\n",
      "Iteration: 320 Time for last iteration: -11.988532066345215\n",
      "Iteration: 321 Time for last iteration: -11.550113677978516\n",
      "Iteration: 322 Time for last iteration: -11.954804182052612\n",
      "Iteration: 323 Time for last iteration: -12.11093807220459\n",
      "Iteration: 324 Time for last iteration: -11.83345890045166\n",
      "Iteration: 325 Time for last iteration: -11.415392875671387\n",
      "Iteration: 326 Time for last iteration: -11.907861948013306\n",
      "Iteration: 327 Time for last iteration: -12.356251239776611\n",
      "Iteration: 328 Time for last iteration: -11.889312028884888\n",
      "Iteration: 329 Time for last iteration: -11.721704006195068\n",
      "Iteration: 330 Time for last iteration: -11.730508089065552\n",
      "Iteration: 331 Time for last iteration: -12.30928897857666\n",
      "Iteration: 332 Time for last iteration: -12.504631042480469\n",
      "Iteration: 333 Time for last iteration: -12.724106073379517\n",
      "Iteration: 334 Time for last iteration: -11.893412113189697\n",
      "Iteration: 335 Time for last iteration: -11.702753067016602\n",
      "Iteration: 336 Time for last iteration: -11.459604978561401\n",
      "Iteration: 337 Time for last iteration: -11.735103845596313\n",
      "Iteration: 338 Time for last iteration: -11.638155937194824\n",
      "Iteration: 339 Time for last iteration: -11.201746940612793\n",
      "Iteration: 340 Time for last iteration: -10.248500108718872\n",
      "Iteration: 341 Time for last iteration: -10.312924861907959\n",
      "Iteration: 342 Time for last iteration: -11.613929033279419\n",
      "Iteration: 343 Time for last iteration: -11.601432085037231\n",
      "Iteration: 344 Time for last iteration: -11.956027269363403\n",
      "Iteration: 345 Time for last iteration: -11.088150024414062\n",
      "Iteration: 346 Time for last iteration: -11.959058046340942\n",
      "Iteration: 347 Time for last iteration: -11.503763198852539\n",
      "Iteration: 348 Time for last iteration: -12.525848150253296\n",
      "Iteration: 349 Time for last iteration: -12.396453857421875\n",
      "Iteration: 350 Time for last iteration: -12.79824686050415\n",
      "Iteration: 351 Time for last iteration: -13.54589319229126\n",
      "Iteration: 352 Time for last iteration: -12.678639888763428\n",
      "Iteration: 353 Time for last iteration: -11.761186122894287\n",
      "Iteration: 354 Time for last iteration: -13.185770034790039\n",
      "Iteration: 355 Time for last iteration: -12.836146831512451\n",
      "Iteration: 356 Time for last iteration: -12.321458101272583\n",
      "Iteration: 357 Time for last iteration: -12.338100910186768\n",
      "Iteration: 358 Time for last iteration: -11.92944598197937\n",
      "Iteration: 359 Time for last iteration: -10.929270029067993\n",
      "Iteration: 360 Time for last iteration: -10.85418701171875\n",
      "Iteration: 361 Time for last iteration: -10.68268609046936\n",
      "Iteration: 362 Time for last iteration: -10.682206869125366\n",
      "Iteration: 363 Time for last iteration: -10.886181116104126\n",
      "Iteration: 364 Time for last iteration: -11.426052808761597\n",
      "Iteration: 365 Time for last iteration: -10.777903079986572\n",
      "Iteration: 366 Time for last iteration: -12.386384010314941\n",
      "Iteration: 367 Time for last iteration: -12.299881219863892\n",
      "Iteration: 368 Time for last iteration: -11.61192512512207\n",
      "Iteration: 369 Time for last iteration: -11.747772932052612\n",
      "Iteration: 370 Time for last iteration: -11.531116962432861\n",
      "Iteration: 371 Time for last iteration: -11.826951026916504\n",
      "Iteration: 372 Time for last iteration: -11.957425832748413\n",
      "Iteration: 373 Time for last iteration: -12.33687710762024\n",
      "Iteration: 374 Time for last iteration: -11.268430233001709\n",
      "Iteration: 375 Time for last iteration: -10.85994005203247\n",
      "Iteration: 376 Time for last iteration: -11.171470165252686\n",
      "Iteration: 377 Time for last iteration: -10.66276502609253\n",
      "Iteration: 378 Time for last iteration: -11.110897064208984\n",
      "Iteration: 379 Time for last iteration: -11.298487901687622\n",
      "Iteration: 380 Time for last iteration: -11.704369068145752\n",
      "Iteration: 381 Time for last iteration: -11.573164939880371\n",
      "Iteration: 382 Time for last iteration: -11.156047105789185\n",
      "Iteration: 383 Time for last iteration: -11.170678853988647\n",
      "Iteration: 384 Time for last iteration: -11.232503652572632\n",
      "Iteration: 385 Time for last iteration: -11.24637222290039\n",
      "Iteration: 386 Time for last iteration: -11.353652000427246\n",
      "Iteration: 387 Time for last iteration: -11.111708164215088\n",
      "Iteration: 388 Time for last iteration: -11.91963815689087\n",
      "Iteration: 389 Time for last iteration: -13.02234411239624\n",
      "Iteration: 390 Time for last iteration: -11.41438603401184\n",
      "Iteration: 391 Time for last iteration: -11.16066312789917\n",
      "Iteration: 392 Time for last iteration: -11.647938013076782\n",
      "Iteration: 393 Time for last iteration: -11.971166849136353\n",
      "Iteration: 394 Time for last iteration: -11.927198886871338\n",
      "Iteration: 395 Time for last iteration: -11.625771045684814\n",
      "Iteration: 396 Time for last iteration: -11.588602066040039\n",
      "Iteration: 397 Time for last iteration: -12.777621984481812\n",
      "Iteration: 398 Time for last iteration: -12.037345170974731\n",
      "Iteration: 399 Time for last iteration: -12.263031005859375\n",
      "Iteration: 400 Time for last iteration: -11.083372116088867\n",
      "Iteration: 401 Time for last iteration: -11.272732973098755\n",
      "Iteration: 402 Time for last iteration: -11.229727029800415\n",
      "Iteration: 403 Time for last iteration: -11.218760013580322\n",
      "Iteration: 404 Time for last iteration: -12.27173399925232\n",
      "Iteration: 405 Time for last iteration: -12.97931981086731\n",
      "Iteration: 406 Time for last iteration: -13.423089981079102\n",
      "Iteration: 407 Time for last iteration: -13.042328834533691\n",
      "Iteration: 408 Time for last iteration: -10.80584979057312\n",
      "Iteration: 409 Time for last iteration: -11.158744096755981\n",
      "Iteration: 410 Time for last iteration: -12.66983699798584\n",
      "Iteration: 411 Time for last iteration: -11.801257848739624\n",
      "Iteration: 412 Time for last iteration: -10.126255989074707\n",
      "Iteration: 413 Time for last iteration: -11.325690269470215\n",
      "Iteration: 414 Time for last iteration: -11.7514009475708\n",
      "Iteration: 415 Time for last iteration: -11.452484130859375\n",
      "Iteration: 416 Time for last iteration: -11.4788179397583\n",
      "Iteration: 417 Time for last iteration: -11.09100866317749\n",
      "Iteration: 418 Time for last iteration: -10.921991109848022\n",
      "Iteration: 419 Time for last iteration: -11.1129469871521\n",
      "Iteration: 420 Time for last iteration: -12.753089189529419\n",
      "Iteration: 421 Time for last iteration: -11.898568868637085\n",
      "Iteration: 422 Time for last iteration: -11.742430925369263\n",
      "Iteration: 423 Time for last iteration: -12.150868892669678\n",
      "Iteration: 424 Time for last iteration: -12.662901163101196\n",
      "Iteration: 425 Time for last iteration: -11.599832773208618\n",
      "Iteration: 426 Time for last iteration: -11.56774091720581\n",
      "Iteration: 427 Time for last iteration: -12.10011911392212\n",
      "Iteration: 428 Time for last iteration: -12.06858205795288\n",
      "Iteration: 429 Time for last iteration: -12.37751293182373\n",
      "Iteration: 430 Time for last iteration: -11.554503202438354\n",
      "Iteration: 431 Time for last iteration: -11.240310907363892\n",
      "Iteration: 432 Time for last iteration: -11.555619716644287\n",
      "Iteration: 433 Time for last iteration: -12.85402512550354\n",
      "Iteration: 434 Time for last iteration: -11.485221147537231\n",
      "Iteration: 435 Time for last iteration: -11.9690580368042\n",
      "Iteration: 436 Time for last iteration: -12.177208185195923\n",
      "Iteration: 437 Time for last iteration: -11.96566390991211\n",
      "Iteration: 438 Time for last iteration: -12.098620176315308\n",
      "Iteration: 439 Time for last iteration: -12.099905014038086\n",
      "Iteration: 440 Time for last iteration: -12.061893939971924\n",
      "Iteration: 441 Time for last iteration: -12.05172610282898\n",
      "Iteration: 442 Time for last iteration: -11.490375280380249\n",
      "Iteration: 443 Time for last iteration: -12.37811827659607\n",
      "Iteration: 444 Time for last iteration: -10.800068855285645\n",
      "Iteration: 445 Time for last iteration: -11.677415132522583\n",
      "Iteration: 446 Time for last iteration: -11.68393087387085\n",
      "Iteration: 447 Time for last iteration: -11.782382011413574\n",
      "Iteration: 448 Time for last iteration: -11.161170959472656\n",
      "Iteration: 449 Time for last iteration: -11.47625184059143\n",
      "Iteration: 450 Time for last iteration: -11.412829875946045\n",
      "Iteration: 451 Time for last iteration: -11.42408013343811\n",
      "Iteration: 452 Time for last iteration: -12.739831924438477\n",
      "Iteration: 453 Time for last iteration: -10.806169986724854\n",
      "Iteration: 454 Time for last iteration: -11.265448808670044\n",
      "Iteration: 455 Time for last iteration: -11.104976892471313\n",
      "Iteration: 456 Time for last iteration: -11.581092834472656\n",
      "Iteration: 457 Time for last iteration: -11.7424898147583\n",
      "Iteration: 458 Time for last iteration: -11.923046827316284\n",
      "Iteration: 459 Time for last iteration: -12.861416101455688\n",
      "Iteration: 460 Time for last iteration: -11.08008885383606\n",
      "Iteration: 461 Time for last iteration: -11.313646793365479\n",
      "Iteration: 462 Time for last iteration: -12.756339073181152\n",
      "Iteration: 463 Time for last iteration: -11.139909982681274\n",
      "Iteration: 464 Time for last iteration: -11.530004978179932\n",
      "Iteration: 465 Time for last iteration: -12.364296913146973\n",
      "Iteration: 466 Time for last iteration: -11.762387990951538\n",
      "Iteration: 467 Time for last iteration: -11.904219150543213\n",
      "Iteration: 468 Time for last iteration: -12.281845092773438\n",
      "Iteration: 469 Time for last iteration: -11.94354510307312\n",
      "Iteration: 470 Time for last iteration: -12.611667156219482\n",
      "Iteration: 471 Time for last iteration: -11.080552101135254\n",
      "Iteration: 472 Time for last iteration: -11.246588706970215\n",
      "Iteration: 473 Time for last iteration: -11.300822019577026\n",
      "Iteration: 474 Time for last iteration: -11.60391616821289\n",
      "Iteration: 475 Time for last iteration: -11.715991020202637\n",
      "Iteration: 476 Time for last iteration: -11.621579885482788\n",
      "Iteration: 477 Time for last iteration: -11.431027173995972\n",
      "Iteration: 478 Time for last iteration: -11.364940166473389\n",
      "Iteration: 479 Time for last iteration: -11.372017860412598\n",
      "Iteration: 480 Time for last iteration: -11.364974975585938\n",
      "Iteration: 481 Time for last iteration: -11.190868139266968\n",
      "Iteration: 482 Time for last iteration: -11.612335681915283\n",
      "Iteration: 483 Time for last iteration: -14.373713970184326\n",
      "Iteration: 484 Time for last iteration: -11.319673776626587\n",
      "Iteration: 485 Time for last iteration: -10.74041223526001\n",
      "Iteration: 486 Time for last iteration: -11.01164698600769\n",
      "Iteration: 487 Time for last iteration: -13.037410974502563\n",
      "Iteration: 488 Time for last iteration: -10.796645164489746\n",
      "Iteration: 489 Time for last iteration: -11.987432956695557\n",
      "Iteration: 490 Time for last iteration: -11.880483150482178\n",
      "Iteration: 491 Time for last iteration: -11.935075998306274\n",
      "Iteration: 492 Time for last iteration: -11.868760108947754\n",
      "Iteration: 493 Time for last iteration: -11.966531038284302\n",
      "Iteration: 494 Time for last iteration: -11.992691993713379\n",
      "Iteration: 495 Time for last iteration: -11.517802000045776\n",
      "Iteration: 496 Time for last iteration: -11.989136934280396\n",
      "Iteration: 497 Time for last iteration: -12.167031049728394\n",
      "Iteration: 498 Time for last iteration: -12.12963604927063\n",
      "Iteration: 499 Time for last iteration: -11.833047866821289\n",
      "Iteration: 500 Time for last iteration: -12.572369813919067\n",
      "Iteration: 501 Time for last iteration: -11.930724143981934\n",
      "Iteration: 502 Time for last iteration: -11.22144627571106\n",
      "Iteration: 503 Time for last iteration: -10.875429153442383\n",
      "Iteration: 504 Time for last iteration: -12.110638856887817\n",
      "Iteration: 505 Time for last iteration: -12.466750860214233\n",
      "Iteration: 506 Time for last iteration: -11.42123794555664\n",
      "Iteration: 507 Time for last iteration: -12.195855855941772\n",
      "Iteration: 508 Time for last iteration: -13.182233810424805\n",
      "Iteration: 509 Time for last iteration: -12.144788026809692\n",
      "Iteration: 510 Time for last iteration: -11.019593954086304\n",
      "Iteration: 511 Time for last iteration: -11.778571844100952\n",
      "Iteration: 512 Time for last iteration: -11.543010950088501\n",
      "Iteration: 513 Time for last iteration: -11.356226205825806\n",
      "Iteration: 514 Time for last iteration: -10.697030067443848\n",
      "Iteration: 515 Time for last iteration: -11.427618026733398\n",
      "Iteration: 516 Time for last iteration: -11.176253080368042\n",
      "Iteration: 517 Time for last iteration: -11.068200826644897\n",
      "Iteration: 518 Time for last iteration: -11.371881008148193\n",
      "Iteration: 519 Time for last iteration: -11.144509077072144\n",
      "Iteration: 520 Time for last iteration: -11.265424966812134\n",
      "Iteration: 521 Time for last iteration: -10.836902856826782\n",
      "Iteration: 522 Time for last iteration: -11.481829166412354\n",
      "Iteration: 523 Time for last iteration: -11.890928745269775\n",
      "Iteration: 524 Time for last iteration: -10.974733829498291\n",
      "Iteration: 525 Time for last iteration: -11.339293956756592\n",
      "Iteration: 526 Time for last iteration: -11.076688766479492\n",
      "Iteration: 527 Time for last iteration: -11.011945247650146\n",
      "Iteration: 528 Time for last iteration: -11.017352819442749\n",
      "Iteration: 529 Time for last iteration: -10.985727071762085\n",
      "Iteration: 530 Time for last iteration: -10.575263023376465\n",
      "Iteration: 531 Time for last iteration: -12.664983987808228\n",
      "Iteration: 532 Time for last iteration: -12.675865173339844\n",
      "Iteration: 533 Time for last iteration: -11.133981227874756\n",
      "Iteration: 534 Time for last iteration: -12.00114917755127\n",
      "Iteration: 535 Time for last iteration: -11.618752002716064\n",
      "Iteration: 536 Time for last iteration: -12.062824964523315\n",
      "Iteration: 537 Time for last iteration: -12.971799850463867\n",
      "Iteration: 538 Time for last iteration: -11.988789081573486\n",
      "Iteration: 539 Time for last iteration: -12.135358810424805\n",
      "Iteration: 540 Time for last iteration: -11.057127237319946\n",
      "Iteration: 541 Time for last iteration: -11.710454940795898\n",
      "Iteration: 542 Time for last iteration: -11.903523206710815\n",
      "Iteration: 543 Time for last iteration: -12.538528680801392\n",
      "Iteration: 544 Time for last iteration: -11.925175189971924\n",
      "Iteration: 545 Time for last iteration: -12.226315975189209\n",
      "Iteration: 546 Time for last iteration: -11.052116870880127\n",
      "Iteration: 547 Time for last iteration: -10.781975269317627\n",
      "Iteration: 548 Time for last iteration: -12.775348901748657\n",
      "Iteration: 549 Time for last iteration: -12.436988830566406\n",
      "Iteration: 550 Time for last iteration: -12.878730058670044\n",
      "Iteration: 551 Time for last iteration: -12.330162048339844\n",
      "Iteration: 552 Time for last iteration: -12.861716985702515\n",
      "Iteration: 553 Time for last iteration: -11.831820249557495\n",
      "Iteration: 554 Time for last iteration: -13.210967063903809\n",
      "Iteration: 555 Time for last iteration: -11.843329906463623\n",
      "Iteration: 556 Time for last iteration: -11.187626123428345\n",
      "Iteration: 557 Time for last iteration: -11.744336128234863\n",
      "Iteration: 558 Time for last iteration: -11.222963094711304\n",
      "Iteration: 559 Time for last iteration: -11.696250915527344\n",
      "Iteration: 560 Time for last iteration: -11.25303602218628\n",
      "Iteration: 561 Time for last iteration: -10.873375177383423\n",
      "Iteration: 562 Time for last iteration: -13.197108030319214\n",
      "Iteration: 563 Time for last iteration: -13.098922967910767\n",
      "Iteration: 564 Time for last iteration: -12.544352054595947\n",
      "Iteration: 565 Time for last iteration: -12.659170866012573\n",
      "Iteration: 566 Time for last iteration: -13.008430004119873\n",
      "Iteration: 567 Time for last iteration: -12.318572044372559\n",
      "Iteration: 568 Time for last iteration: -11.970260858535767\n",
      "Iteration: 569 Time for last iteration: -13.510779857635498\n",
      "Iteration: 570 Time for last iteration: -11.505600929260254\n",
      "Iteration: 571 Time for last iteration: -10.51229476928711\n",
      "Iteration: 572 Time for last iteration: -10.678272008895874\n",
      "Iteration: 573 Time for last iteration: -11.45019793510437\n",
      "Iteration: 574 Time for last iteration: -11.493468999862671\n",
      "Iteration: 575 Time for last iteration: -10.990072965621948\n",
      "Iteration: 576 Time for last iteration: -10.72799277305603\n",
      "Iteration: 577 Time for last iteration: -10.472915887832642\n",
      "Iteration: 578 Time for last iteration: -10.728707790374756\n",
      "Iteration: 579 Time for last iteration: -11.028272151947021\n",
      "Iteration: 580 Time for last iteration: -11.355897903442383\n",
      "Iteration: 581 Time for last iteration: -11.138236045837402\n",
      "Iteration: 582 Time for last iteration: -11.357731103897095\n",
      "Iteration: 583 Time for last iteration: -11.322291851043701\n",
      "Iteration: 584 Time for last iteration: -10.984086036682129\n",
      "Iteration: 585 Time for last iteration: -10.932745933532715\n",
      "Iteration: 586 Time for last iteration: -11.196472883224487\n",
      "Iteration: 587 Time for last iteration: -10.880744934082031\n",
      "Iteration: 588 Time for last iteration: -11.340096712112427\n",
      "Iteration: 589 Time for last iteration: -11.315425872802734\n",
      "Iteration: 590 Time for last iteration: -12.033353090286255\n",
      "Iteration: 591 Time for last iteration: -11.073457717895508\n",
      "Iteration: 592 Time for last iteration: -11.962263822555542\n",
      "Iteration: 593 Time for last iteration: -13.69994306564331\n",
      "Iteration: 594 Time for last iteration: -12.31404185295105\n",
      "Iteration: 595 Time for last iteration: -11.602526903152466\n",
      "Iteration: 596 Time for last iteration: -10.386749982833862\n",
      "Iteration: 597 Time for last iteration: -12.373025894165039\n",
      "Iteration: 598 Time for last iteration: -13.35713005065918\n",
      "Iteration: 599 Time for last iteration: -11.848718881607056\n",
      "Iteration: 600 Time for last iteration: -12.150591850280762\n",
      "Iteration: 601 Time for last iteration: -13.031933069229126\n",
      "Iteration: 602 Time for last iteration: -11.780197143554688\n",
      "Iteration: 603 Time for last iteration: -11.883202075958252\n",
      "Iteration: 604 Time for last iteration: -11.715087175369263\n",
      "Iteration: 605 Time for last iteration: -12.263584852218628\n",
      "Iteration: 606 Time for last iteration: -13.120085954666138\n",
      "Iteration: 607 Time for last iteration: -10.904555082321167\n",
      "Iteration: 608 Time for last iteration: -10.811947107315063\n",
      "Iteration: 609 Time for last iteration: -10.827945947647095\n",
      "Iteration: 610 Time for last iteration: -10.878174066543579\n",
      "Iteration: 611 Time for last iteration: -11.993093252182007\n",
      "Iteration: 612 Time for last iteration: -11.316267013549805\n",
      "Iteration: 613 Time for last iteration: -11.62942099571228\n",
      "Iteration: 614 Time for last iteration: -11.402336120605469\n",
      "Iteration: 615 Time for last iteration: -12.985572814941406\n",
      "Iteration: 616 Time for last iteration: -13.570449113845825\n",
      "Iteration: 617 Time for last iteration: -12.805178880691528\n",
      "Iteration: 618 Time for last iteration: -11.754607200622559\n",
      "Iteration: 619 Time for last iteration: -12.439640998840332\n",
      "Iteration: 620 Time for last iteration: -12.318394184112549\n",
      "Iteration: 621 Time for last iteration: -11.580935955047607\n",
      "Iteration: 622 Time for last iteration: -11.27727484703064\n",
      "Iteration: 623 Time for last iteration: -10.685468912124634\n",
      "Iteration: 624 Time for last iteration: -11.487972974777222\n",
      "Iteration: 625 Time for last iteration: -12.67245101928711\n",
      "Iteration: 626 Time for last iteration: -12.198560953140259\n",
      "Iteration: 627 Time for last iteration: -12.486078977584839\n",
      "Iteration: 628 Time for last iteration: -12.236121892929077\n",
      "Iteration: 629 Time for last iteration: -12.056396722793579\n",
      "Iteration: 630 Time for last iteration: -11.78259801864624\n",
      "Iteration: 631 Time for last iteration: -12.039031028747559\n",
      "Iteration: 632 Time for last iteration: -12.06594181060791\n",
      "Iteration: 633 Time for last iteration: -11.77074909210205\n",
      "Iteration: 634 Time for last iteration: -11.971625328063965\n",
      "Iteration: 635 Time for last iteration: -11.132152080535889\n",
      "Iteration: 636 Time for last iteration: -12.354683876037598\n",
      "Iteration: 637 Time for last iteration: -11.873025894165039\n",
      "Iteration: 638 Time for last iteration: -10.580543994903564\n",
      "Iteration: 639 Time for last iteration: -11.312593221664429\n",
      "Iteration: 640 Time for last iteration: -10.884718179702759\n",
      "Iteration: 641 Time for last iteration: -10.765631914138794\n",
      "Iteration: 642 Time for last iteration: -12.000118017196655\n",
      "Iteration: 643 Time for last iteration: -11.680091142654419\n",
      "Iteration: 644 Time for last iteration: -13.935148000717163\n",
      "Iteration: 645 Time for last iteration: -10.366720199584961\n",
      "Iteration: 646 Time for last iteration: -10.900690793991089\n",
      "Iteration: 647 Time for last iteration: -11.834693908691406\n",
      "Iteration: 648 Time for last iteration: -11.463340997695923\n",
      "Iteration: 649 Time for last iteration: -11.8489089012146\n",
      "Iteration: 650 Time for last iteration: -11.653383731842041\n",
      "Iteration: 651 Time for last iteration: -10.85263204574585\n",
      "Iteration: 652 Time for last iteration: -10.637825012207031\n",
      "Iteration: 653 Time for last iteration: -10.857183933258057\n",
      "Iteration: 654 Time for last iteration: -11.432322978973389\n",
      "Iteration: 655 Time for last iteration: -11.241048812866211\n",
      "Iteration: 656 Time for last iteration: -11.510933876037598\n",
      "Iteration: 657 Time for last iteration: -12.169437170028687\n",
      "Iteration: 658 Time for last iteration: -11.789867877960205\n",
      "Iteration: 659 Time for last iteration: -11.729578018188477\n",
      "Iteration: 660 Time for last iteration: -12.141291856765747\n",
      "Iteration: 661 Time for last iteration: -12.53886890411377\n",
      "Iteration: 662 Time for last iteration: -10.956264972686768\n",
      "Iteration: 663 Time for last iteration: -10.95524787902832\n",
      "Iteration: 664 Time for last iteration: -11.707154989242554\n",
      "Iteration: 665 Time for last iteration: -10.048349142074585\n",
      "Iteration: 666 Time for last iteration: -10.258686065673828\n",
      "Iteration: 667 Time for last iteration: -11.949459791183472\n",
      "Iteration: 668 Time for last iteration: -10.827630996704102\n",
      "Iteration: 669 Time for last iteration: -10.903003931045532\n",
      "Iteration: 670 Time for last iteration: -11.563637018203735\n",
      "Iteration: 671 Time for last iteration: -11.70261001586914\n",
      "Iteration: 672 Time for last iteration: -11.089581727981567\n",
      "Iteration: 673 Time for last iteration: -10.837130069732666\n",
      "Iteration: 674 Time for last iteration: -10.970424890518188\n",
      "Iteration: 675 Time for last iteration: -11.537249088287354\n",
      "Iteration: 676 Time for last iteration: -10.623534202575684\n",
      "Iteration: 677 Time for last iteration: -11.539650201797485\n",
      "Iteration: 678 Time for last iteration: -12.199957847595215\n",
      "Iteration: 679 Time for last iteration: -11.077600955963135\n",
      "Iteration: 680 Time for last iteration: -11.868579864501953\n",
      "Iteration: 681 Time for last iteration: -11.226506233215332\n",
      "Iteration: 682 Time for last iteration: -10.676027059555054\n",
      "Iteration: 683 Time for last iteration: -11.340521097183228\n",
      "Iteration: 684 Time for last iteration: -11.392900943756104\n",
      "Iteration: 685 Time for last iteration: -11.224134922027588\n",
      "Iteration: 686 Time for last iteration: -10.817241191864014\n",
      "Iteration: 687 Time for last iteration: -11.245814085006714\n",
      "Iteration: 688 Time for last iteration: -11.28384280204773\n",
      "Iteration: 689 Time for last iteration: -11.153274059295654\n",
      "Iteration: 690 Time for last iteration: -10.764655113220215\n",
      "Iteration: 691 Time for last iteration: -11.386067867279053\n",
      "Iteration: 692 Time for last iteration: -11.389286994934082\n",
      "Iteration: 693 Time for last iteration: -10.894378900527954\n",
      "Iteration: 694 Time for last iteration: -12.10743498802185\n",
      "Iteration: 695 Time for last iteration: -13.715989112854004\n",
      "Iteration: 696 Time for last iteration: -10.98379397392273\n",
      "Iteration: 697 Time for last iteration: -10.999860048294067\n",
      "Iteration: 698 Time for last iteration: -11.699917793273926\n",
      "Iteration: 699 Time for last iteration: -11.336467027664185\n",
      "Iteration: 700 Time for last iteration: -12.282158851623535\n",
      "Iteration: 701 Time for last iteration: -11.7515869140625\n",
      "Iteration: 702 Time for last iteration: -11.95059609413147\n",
      "Iteration: 703 Time for last iteration: -10.5916109085083\n",
      "Iteration: 704 Time for last iteration: -11.784248113632202\n",
      "Iteration: 705 Time for last iteration: -11.127532243728638\n",
      "Iteration: 706 Time for last iteration: -11.920673131942749\n",
      "Iteration: 707 Time for last iteration: -11.630177974700928\n",
      "Iteration: 708 Time for last iteration: -12.555910110473633\n",
      "Iteration: 709 Time for last iteration: -11.679625034332275\n",
      "Iteration: 710 Time for last iteration: -12.568134784698486\n",
      "Iteration: 711 Time for last iteration: -12.22610592842102\n",
      "Iteration: 712 Time for last iteration: -12.792487859725952\n",
      "Iteration: 713 Time for last iteration: -12.447668313980103\n",
      "Iteration: 714 Time for last iteration: -12.891257762908936\n",
      "Iteration: 715 Time for last iteration: -10.31598973274231\n",
      "Iteration: 716 Time for last iteration: -10.624216079711914\n",
      "Iteration: 717 Time for last iteration: -11.700354099273682\n",
      "Iteration: 718 Time for last iteration: -10.210029125213623\n",
      "Iteration: 719 Time for last iteration: -10.157137870788574\n",
      "Iteration: 720 Time for last iteration: -11.464006185531616\n",
      "Iteration: 721 Time for last iteration: -10.840691328048706\n",
      "Iteration: 722 Time for last iteration: -11.668174266815186\n",
      "Iteration: 723 Time for last iteration: -12.033557891845703\n",
      "Iteration: 724 Time for last iteration: -11.340110778808594\n",
      "Iteration: 725 Time for last iteration: -11.153234958648682\n",
      "Iteration: 726 Time for last iteration: -11.51677393913269\n",
      "Iteration: 727 Time for last iteration: -11.835909843444824\n",
      "Iteration: 728 Time for last iteration: -11.714792013168335\n",
      "Iteration: 729 Time for last iteration: -12.039085865020752\n",
      "Iteration: 730 Time for last iteration: -11.838982105255127\n",
      "Iteration: 731 Time for last iteration: -13.332592964172363\n",
      "Iteration: 732 Time for last iteration: -12.514308214187622\n",
      "Iteration: 733 Time for last iteration: -12.408767938613892\n",
      "Iteration: 734 Time for last iteration: -11.622309923171997\n",
      "Iteration: 735 Time for last iteration: -11.527045011520386\n",
      "Iteration: 736 Time for last iteration: -10.911847114562988\n",
      "Iteration: 737 Time for last iteration: -10.341001033782959\n",
      "Iteration: 738 Time for last iteration: -10.582633256912231\n",
      "Iteration: 739 Time for last iteration: -10.64733099937439\n",
      "Iteration: 740 Time for last iteration: -10.760893106460571\n",
      "Iteration: 741 Time for last iteration: -11.299475908279419\n",
      "Iteration: 742 Time for last iteration: -11.801826000213623\n",
      "Iteration: 743 Time for last iteration: -11.437219858169556\n",
      "Iteration: 744 Time for last iteration: -10.474614143371582\n",
      "Iteration: 745 Time for last iteration: -11.675532102584839\n",
      "Iteration: 746 Time for last iteration: -10.810930967330933\n",
      "Iteration: 747 Time for last iteration: -11.007286071777344\n",
      "Iteration: 748 Time for last iteration: -12.317975044250488\n",
      "Iteration: 749 Time for last iteration: -12.160299062728882\n",
      "Iteration: 750 Time for last iteration: -12.038596868515015\n",
      "Iteration: 751 Time for last iteration: -11.882957220077515\n",
      "Iteration: 752 Time for last iteration: -12.811080932617188\n",
      "Iteration: 753 Time for last iteration: -12.43152117729187\n",
      "Iteration: 754 Time for last iteration: -12.571974992752075\n",
      "Iteration: 755 Time for last iteration: -12.379319190979004\n",
      "Iteration: 756 Time for last iteration: -12.215389013290405\n",
      "Iteration: 757 Time for last iteration: -10.479605913162231\n",
      "Iteration: 758 Time for last iteration: -10.50645899772644\n",
      "Iteration: 759 Time for last iteration: -11.006251811981201\n",
      "Iteration: 760 Time for last iteration: -11.91417908668518\n",
      "Iteration: 761 Time for last iteration: -11.91661787033081\n",
      "Iteration: 762 Time for last iteration: -12.11115288734436\n",
      "Iteration: 763 Time for last iteration: -12.228668928146362\n",
      "Iteration: 764 Time for last iteration: -12.32796597480774\n",
      "Iteration: 765 Time for last iteration: -12.13484811782837\n",
      "Iteration: 766 Time for last iteration: -12.6060049533844\n",
      "Iteration: 767 Time for last iteration: -12.660158157348633\n",
      "Iteration: 768 Time for last iteration: -11.972922325134277\n",
      "Iteration: 769 Time for last iteration: -12.33644986152649\n",
      "Iteration: 770 Time for last iteration: -12.147305011749268\n",
      "Iteration: 771 Time for last iteration: -11.886368751525879\n",
      "Iteration: 772 Time for last iteration: -11.088518857955933\n",
      "Iteration: 773 Time for last iteration: -10.505752086639404\n",
      "Iteration: 774 Time for last iteration: -10.586374998092651\n",
      "Iteration: 775 Time for last iteration: -10.757936954498291\n",
      "Iteration: 776 Time for last iteration: -10.28299593925476\n",
      "Iteration: 777 Time for last iteration: -11.952699184417725\n",
      "Iteration: 778 Time for last iteration: -11.261447191238403\n",
      "Iteration: 779 Time for last iteration: -10.84761381149292\n",
      "Iteration: 780 Time for last iteration: -10.7445969581604\n",
      "Iteration: 781 Time for last iteration: -11.464985847473145\n",
      "Iteration: 782 Time for last iteration: -11.560852766036987\n",
      "Iteration: 783 Time for last iteration: -10.82420301437378\n",
      "Iteration: 784 Time for last iteration: -10.305301904678345\n",
      "Iteration: 785 Time for last iteration: -10.808438777923584\n",
      "Iteration: 786 Time for last iteration: -10.051669120788574\n",
      "Iteration: 787 Time for last iteration: -10.464760780334473\n",
      "Iteration: 788 Time for last iteration: -10.337821960449219\n",
      "Iteration: 789 Time for last iteration: -10.272449970245361\n",
      "Iteration: 790 Time for last iteration: -9.757828950881958\n",
      "Iteration: 791 Time for last iteration: -8.668862104415894\n",
      "Iteration: 792 Time for last iteration: -8.462610006332397\n",
      "Iteration: 793 Time for last iteration: -8.775090217590332\n",
      "Iteration: 794 Time for last iteration: -8.422198057174683\n",
      "Iteration: 795 Time for last iteration: -8.632384061813354\n",
      "Iteration: 796 Time for last iteration: -8.790307760238647\n",
      "Iteration: 797 Time for last iteration: -9.535354137420654\n",
      "Iteration: 798 Time for last iteration: -9.93229079246521\n",
      "Iteration: 799 Time for last iteration: -8.218428134918213\n",
      "Iteration: 800 Time for last iteration: -8.471713066101074\n",
      "Iteration: 801 Time for last iteration: -8.378926038742065\n",
      "Iteration: 802 Time for last iteration: -9.780066728591919\n",
      "Iteration: 803 Time for last iteration: -9.835088014602661\n",
      "Iteration: 804 Time for last iteration: -9.56197714805603\n",
      "Iteration: 805 Time for last iteration: -10.269036054611206\n",
      "Iteration: 806 Time for last iteration: -10.776928901672363\n",
      "Iteration: 807 Time for last iteration: -11.95123291015625\n",
      "Iteration: 808 Time for last iteration: -10.90788197517395\n",
      "Iteration: 809 Time for last iteration: -11.52771806716919\n",
      "Iteration: 810 Time for last iteration: -12.170921087265015\n",
      "Iteration: 811 Time for last iteration: -12.32817792892456\n",
      "Iteration: 812 Time for last iteration: -11.978055953979492\n",
      "Iteration: 813 Time for last iteration: -11.59977102279663\n",
      "Iteration: 814 Time for last iteration: -11.958908796310425\n",
      "Iteration: 815 Time for last iteration: -11.930633068084717\n",
      "Iteration: 816 Time for last iteration: -12.095693826675415\n",
      "Iteration: 817 Time for last iteration: -11.557056188583374\n",
      "Iteration: 818 Time for last iteration: -12.54289197921753\n",
      "Iteration: 819 Time for last iteration: -10.698952913284302\n",
      "Iteration: 820 Time for last iteration: -11.432642936706543\n",
      "Iteration: 821 Time for last iteration: -10.610954999923706\n",
      "Iteration: 822 Time for last iteration: -10.414163827896118\n",
      "Iteration: 823 Time for last iteration: -12.920163869857788\n",
      "Iteration: 824 Time for last iteration: -11.638120889663696\n",
      "Iteration: 825 Time for last iteration: -11.63201904296875\n",
      "Iteration: 826 Time for last iteration: -11.270456075668335\n",
      "Iteration: 827 Time for last iteration: -11.93555212020874\n",
      "Iteration: 828 Time for last iteration: -11.026926279067993\n",
      "Iteration: 829 Time for last iteration: -11.435472249984741\n",
      "Iteration: 830 Time for last iteration: -11.20957899093628\n",
      "Iteration: 831 Time for last iteration: -11.037980318069458\n",
      "Iteration: 832 Time for last iteration: -11.912846088409424\n",
      "Iteration: 833 Time for last iteration: -13.319011926651001\n",
      "Iteration: 834 Time for last iteration: -12.139732837677002\n",
      "Iteration: 835 Time for last iteration: -14.235356092453003\n",
      "Iteration: 836 Time for last iteration: -12.842912197113037\n",
      "Iteration: 837 Time for last iteration: -14.836962938308716\n",
      "Iteration: 838 Time for last iteration: -13.212139129638672\n",
      "Iteration: 839 Time for last iteration: -12.506049871444702\n",
      "Iteration: 840 Time for last iteration: -12.78714394569397\n",
      "Iteration: 841 Time for last iteration: -13.313462972640991\n",
      "Iteration: 842 Time for last iteration: -13.154609203338623\n",
      "Iteration: 843 Time for last iteration: -12.90358591079712\n",
      "Iteration: 844 Time for last iteration: -13.276283025741577\n",
      "Iteration: 845 Time for last iteration: -12.40337610244751\n",
      "Iteration: 846 Time for last iteration: -12.960694074630737\n",
      "Iteration: 847 Time for last iteration: -13.446360111236572\n",
      "Iteration: 848 Time for last iteration: -12.226544857025146\n",
      "Iteration: 849 Time for last iteration: -12.357522964477539\n",
      "Iteration: 850 Time for last iteration: -10.43715524673462\n",
      "Iteration: 851 Time for last iteration: -10.584778070449829\n",
      "Iteration: 852 Time for last iteration: -12.00451111793518\n",
      "Iteration: 853 Time for last iteration: -12.514451026916504\n",
      "Iteration: 854 Time for last iteration: -12.35883378982544\n",
      "Iteration: 855 Time for last iteration: -12.340911865234375\n",
      "Iteration: 856 Time for last iteration: -12.298782110214233\n",
      "Iteration: 857 Time for last iteration: -12.19068694114685\n",
      "Iteration: 858 Time for last iteration: -12.433154106140137\n",
      "Iteration: 859 Time for last iteration: -12.569813966751099\n",
      "Iteration: 860 Time for last iteration: -12.440473318099976\n",
      "Iteration: 861 Time for last iteration: -11.737420082092285\n",
      "Iteration: 862 Time for last iteration: -11.814082145690918\n",
      "Iteration: 863 Time for last iteration: -12.421579122543335\n",
      "Iteration: 864 Time for last iteration: -12.171393156051636\n",
      "Iteration: 865 Time for last iteration: -11.368956089019775\n",
      "Iteration: 866 Time for last iteration: -11.442540884017944\n",
      "Iteration: 867 Time for last iteration: -11.014418125152588\n",
      "Iteration: 868 Time for last iteration: -11.388190984725952\n",
      "Iteration: 869 Time for last iteration: -13.323347806930542\n",
      "Iteration: 870 Time for last iteration: -11.8693208694458\n",
      "Iteration: 871 Time for last iteration: -10.534016132354736\n",
      "Iteration: 872 Time for last iteration: -10.674051761627197\n",
      "Iteration: 873 Time for last iteration: -11.933204889297485\n",
      "Iteration: 874 Time for last iteration: -10.710917949676514\n",
      "Iteration: 875 Time for last iteration: -10.701122045516968\n",
      "Iteration: 876 Time for last iteration: -10.43290090560913\n",
      "Iteration: 877 Time for last iteration: -10.030560970306396\n",
      "Iteration: 878 Time for last iteration: -10.348655939102173\n",
      "Iteration: 879 Time for last iteration: -11.132613897323608\n",
      "Iteration: 880 Time for last iteration: -12.789278030395508\n",
      "Iteration: 881 Time for last iteration: -12.252380847930908\n",
      "Iteration: 882 Time for last iteration: -14.090355157852173\n",
      "Iteration: 883 Time for last iteration: -12.633660793304443\n",
      "Iteration: 884 Time for last iteration: -13.012896060943604\n",
      "Iteration: 885 Time for last iteration: -11.604453086853027\n",
      "Iteration: 886 Time for last iteration: -11.230590105056763\n",
      "Iteration: 887 Time for last iteration: -11.326285123825073\n",
      "Iteration: 888 Time for last iteration: -11.763638019561768\n",
      "Iteration: 889 Time for last iteration: -11.396131992340088\n",
      "Iteration: 890 Time for last iteration: -10.475656032562256\n",
      "Iteration: 891 Time for last iteration: -10.154894828796387\n",
      "Iteration: 892 Time for last iteration: -8.809749126434326\n",
      "Iteration: 893 Time for last iteration: -9.544113874435425\n",
      "Iteration: 894 Time for last iteration: -10.226162910461426\n",
      "Iteration: 895 Time for last iteration: -8.491483926773071\n",
      "Iteration: 896 Time for last iteration: -8.515923976898193\n",
      "Iteration: 897 Time for last iteration: -8.343842029571533\n",
      "Iteration: 898 Time for last iteration: -8.965135097503662\n",
      "Iteration: 899 Time for last iteration: -8.628470659255981\n",
      "Iteration: 900 Time for last iteration: -8.645229816436768\n",
      "Iteration: 901 Time for last iteration: -8.34141206741333\n",
      "Iteration: 902 Time for last iteration: -9.555378913879395\n",
      "Iteration: 903 Time for last iteration: -9.263170957565308\n",
      "Iteration: 904 Time for last iteration: -8.529378890991211\n",
      "Iteration: 905 Time for last iteration: -8.409801959991455\n",
      "Iteration: 906 Time for last iteration: -8.503628015518188\n",
      "Iteration: 907 Time for last iteration: -8.154969930648804\n",
      "Iteration: 908 Time for last iteration: -8.869271039962769\n",
      "Iteration: 909 Time for last iteration: -9.314176082611084\n",
      "Iteration: 910 Time for last iteration: -9.248993158340454\n",
      "Iteration: 911 Time for last iteration: -8.95604395866394\n",
      "Iteration: 912 Time for last iteration: -9.734667778015137\n",
      "Iteration: 913 Time for last iteration: -9.033684968948364\n",
      "Iteration: 914 Time for last iteration: -8.231993198394775\n",
      "Iteration: 915 Time for last iteration: -8.296265840530396\n",
      "Iteration: 916 Time for last iteration: -8.403472900390625\n",
      "Iteration: 917 Time for last iteration: -8.365073919296265\n",
      "Iteration: 918 Time for last iteration: -8.18790078163147\n",
      "Iteration: 919 Time for last iteration: -9.387026071548462\n",
      "Iteration: 920 Time for last iteration: -10.148032903671265\n",
      "Iteration: 921 Time for last iteration: -9.84262490272522\n",
      "Iteration: 922 Time for last iteration: -9.824121952056885\n",
      "Iteration: 923 Time for last iteration: -9.706494808197021\n",
      "Iteration: 924 Time for last iteration: -9.830400943756104\n",
      "Iteration: 925 Time for last iteration: -9.491005182266235\n",
      "Iteration: 926 Time for last iteration: -11.448099851608276\n",
      "Iteration: 927 Time for last iteration: -12.871096134185791\n",
      "Iteration: 928 Time for last iteration: -12.375097036361694\n",
      "Iteration: 929 Time for last iteration: -11.52731990814209\n",
      "Iteration: 930 Time for last iteration: -10.531474113464355\n",
      "Iteration: 931 Time for last iteration: -10.738065719604492\n",
      "Iteration: 932 Time for last iteration: -10.782868146896362\n",
      "Iteration: 933 Time for last iteration: -10.468880891799927\n",
      "Iteration: 934 Time for last iteration: -10.517058849334717\n",
      "Iteration: 935 Time for last iteration: -10.847437143325806\n",
      "Iteration: 936 Time for last iteration: -10.276142120361328\n",
      "Iteration: 937 Time for last iteration: -10.53811502456665\n",
      "Iteration: 938 Time for last iteration: -11.82014799118042\n",
      "Iteration: 939 Time for last iteration: -12.485961198806763\n",
      "Iteration: 940 Time for last iteration: -12.101254224777222\n",
      "Iteration: 941 Time for last iteration: -11.13484501838684\n",
      "Iteration: 942 Time for last iteration: -10.825090885162354\n",
      "Iteration: 943 Time for last iteration: -11.29845118522644\n",
      "Iteration: 944 Time for last iteration: -10.573697805404663\n",
      "Iteration: 945 Time for last iteration: -11.899379968643188\n",
      "Iteration: 946 Time for last iteration: -12.427304029464722\n",
      "Iteration: 947 Time for last iteration: -11.056658029556274\n",
      "Iteration: 948 Time for last iteration: -10.872243881225586\n",
      "Iteration: 949 Time for last iteration: -11.391979694366455\n",
      "Iteration: 950 Time for last iteration: -9.688263177871704\n",
      "Iteration: 951 Time for last iteration: -10.357255935668945\n",
      "Iteration: 952 Time for last iteration: -11.362691879272461\n",
      "Iteration: 953 Time for last iteration: -11.717893838882446\n",
      "Iteration: 954 Time for last iteration: -15.55898380279541\n",
      "Iteration: 955 Time for last iteration: -15.16826868057251\n",
      "Iteration: 956 Time for last iteration: -14.71110200881958\n",
      "Iteration: 957 Time for last iteration: -14.87935495376587\n",
      "Iteration: 958 Time for last iteration: -15.31155776977539\n",
      "Iteration: 959 Time for last iteration: -15.080018997192383\n",
      "Iteration: 960 Time for last iteration: -13.880702257156372\n",
      "Iteration: 961 Time for last iteration: -13.61837911605835\n",
      "Iteration: 962 Time for last iteration: -12.40591287612915\n",
      "Iteration: 963 Time for last iteration: -13.660523891448975\n",
      "Iteration: 964 Time for last iteration: -14.212586879730225\n",
      "Iteration: 965 Time for last iteration: -15.805101156234741\n",
      "Iteration: 966 Time for last iteration: -15.739693880081177\n",
      "Iteration: 967 Time for last iteration: -16.78525400161743\n",
      "Iteration: 968 Time for last iteration: -13.728216171264648\n",
      "Iteration: 969 Time for last iteration: -13.299490690231323\n",
      "Iteration: 970 Time for last iteration: -15.917850017547607\n",
      "Iteration: 971 Time for last iteration: -17.83999514579773\n",
      "Iteration: 972 Time for last iteration: -16.353708744049072\n",
      "Iteration: 973 Time for last iteration: -16.774986028671265\n",
      "Iteration: 974 Time for last iteration: -16.316596031188965\n",
      "Iteration: 975 Time for last iteration: -15.952788829803467\n",
      "Iteration: 976 Time for last iteration: -15.355447053909302\n",
      "Iteration: 977 Time for last iteration: -15.363004922866821\n",
      "Iteration: 978 Time for last iteration: -14.495690107345581\n",
      "Iteration: 979 Time for last iteration: -13.33697509765625\n",
      "Iteration: 980 Time for last iteration: -13.440191984176636\n",
      "Iteration: 981 Time for last iteration: -13.54978609085083\n",
      "Iteration: 982 Time for last iteration: -14.055685997009277\n",
      "Iteration: 983 Time for last iteration: -13.522634983062744\n",
      "Iteration: 984 Time for last iteration: -13.020977973937988\n",
      "Iteration: 985 Time for last iteration: -13.981537818908691\n",
      "Iteration: 986 Time for last iteration: -13.402659177780151\n",
      "Iteration: 987 Time for last iteration: -13.101699113845825\n",
      "Iteration: 988 Time for last iteration: -13.359480142593384\n",
      "Iteration: 989 Time for last iteration: -13.717866897583008\n",
      "Iteration: 990 Time for last iteration: -12.992459058761597\n",
      "Iteration: 991 Time for last iteration: -13.430706977844238\n",
      "Iteration: 992 Time for last iteration: -13.717674016952515\n",
      "Iteration: 993 Time for last iteration: -13.04768681526184\n",
      "Iteration: 994 Time for last iteration: -13.135385036468506\n",
      "Iteration: 995 Time for last iteration: -14.990915060043335\n",
      "Iteration: 996 Time for last iteration: -13.798036813735962\n",
      "Iteration: 997 Time for last iteration: -14.402894258499146\n",
      "Iteration: 998 Time for last iteration: -13.761602878570557\n",
      "Iteration: 999 Time for last iteration: -14.674062967300415\n",
      "Iteration: 1000 Time for last iteration: -14.31572675704956\n",
      "Iteration: 1001 Time for last iteration: -14.41845989227295\n",
      "Iteration: 1002 Time for last iteration: -14.632164001464844\n",
      "Iteration: 1003 Time for last iteration: -13.926651000976562\n",
      "Iteration: 1004 Time for last iteration: -12.53883409500122\n",
      "Iteration: 1005 Time for last iteration: -12.729035139083862\n",
      "Iteration: 1006 Time for last iteration: -12.458209037780762\n",
      "Iteration: 1007 Time for last iteration: -12.609790086746216\n",
      "Iteration: 1008 Time for last iteration: -12.417749881744385\n",
      "Iteration: 1009 Time for last iteration: -12.671609163284302\n",
      "Iteration: 1010 Time for last iteration: -12.70837926864624\n",
      "Iteration: 1011 Time for last iteration: -12.600013971328735\n",
      "Iteration: 1012 Time for last iteration: -12.422722101211548\n",
      "Iteration: 1013 Time for last iteration: -12.506414890289307\n",
      "Iteration: 1014 Time for last iteration: -12.516518115997314\n",
      "Iteration: 1015 Time for last iteration: -12.845476865768433\n",
      "Iteration: 1016 Time for last iteration: -12.4530029296875\n",
      "Iteration: 1017 Time for last iteration: -12.312432050704956\n",
      "Iteration: 1018 Time for last iteration: -12.470788955688477\n",
      "Iteration: 1019 Time for last iteration: -12.610229015350342\n",
      "Iteration: 1020 Time for last iteration: -12.841557025909424\n",
      "Iteration: 1021 Time for last iteration: -12.776325702667236\n",
      "Iteration: 1022 Time for last iteration: -12.23121976852417\n",
      "Iteration: 1023 Time for last iteration: -12.63583493232727\n",
      "Iteration: 1024 Time for last iteration: -12.707857847213745\n",
      "Iteration: 1025 Time for last iteration: -12.43086576461792\n",
      "Iteration: 1026 Time for last iteration: -12.049278974533081\n",
      "Iteration: 1027 Time for last iteration: -12.365694046020508\n",
      "Iteration: 1028 Time for last iteration: -12.20870304107666\n",
      "Iteration: 1029 Time for last iteration: -12.624749660491943\n",
      "Iteration: 1030 Time for last iteration: -12.504632234573364\n",
      "Iteration: 1031 Time for last iteration: -12.614908933639526\n",
      "Iteration: 1032 Time for last iteration: -12.446007013320923\n",
      "Iteration: 1033 Time for last iteration: -12.485197067260742\n",
      "Iteration: 1034 Time for last iteration: -12.773550748825073\n",
      "Iteration: 1035 Time for last iteration: -12.357141971588135\n",
      "Iteration: 1036 Time for last iteration: -12.446937799453735\n",
      "Iteration: 1037 Time for last iteration: -12.830538988113403\n",
      "Iteration: 1038 Time for last iteration: -12.396493911743164\n",
      "Iteration: 1039 Time for last iteration: -12.759154796600342\n",
      "Iteration: 1040 Time for last iteration: -12.327647924423218\n",
      "Iteration: 1041 Time for last iteration: -12.019430160522461\n",
      "Iteration: 1042 Time for last iteration: -11.624531030654907\n",
      "Iteration: 1043 Time for last iteration: -11.646358013153076\n",
      "Iteration: 1044 Time for last iteration: -12.032169103622437\n",
      "Iteration: 1045 Time for last iteration: -12.736512899398804\n",
      "Iteration: 1046 Time for last iteration: -12.474143981933594\n",
      "Iteration: 1047 Time for last iteration: -12.531170129776001\n",
      "Iteration: 1048 Time for last iteration: -12.560572862625122\n",
      "Iteration: 1049 Time for last iteration: -12.277889013290405\n",
      "Iteration: 1050 Time for last iteration: -11.921033143997192\n",
      "Iteration: 1051 Time for last iteration: -10.343410968780518\n",
      "Iteration: 1052 Time for last iteration: -12.279021978378296\n",
      "Iteration: 1053 Time for last iteration: -11.32733678817749\n",
      "Iteration: 1054 Time for last iteration: -11.810229063034058\n",
      "Iteration: 1055 Time for last iteration: -11.68701696395874\n",
      "Iteration: 1056 Time for last iteration: -11.71478819847107\n",
      "Iteration: 1057 Time for last iteration: -11.840214967727661\n",
      "Iteration: 1058 Time for last iteration: -12.00062894821167\n",
      "Iteration: 1059 Time for last iteration: -11.447006225585938\n",
      "Iteration: 1060 Time for last iteration: -11.87136197090149\n",
      "Iteration: 1061 Time for last iteration: -12.324312925338745\n",
      "Iteration: 1062 Time for last iteration: -11.88369607925415\n",
      "Iteration: 1063 Time for last iteration: -12.263576030731201\n",
      "Iteration: 1064 Time for last iteration: -11.616343021392822\n",
      "Iteration: 1065 Time for last iteration: -11.78928017616272\n",
      "Iteration: 1066 Time for last iteration: -11.873419046401978\n",
      "Iteration: 1067 Time for last iteration: -11.803269147872925\n",
      "Iteration: 1068 Time for last iteration: -11.668587923049927\n",
      "Iteration: 1069 Time for last iteration: -12.073670864105225\n",
      "Iteration: 1070 Time for last iteration: -12.013872146606445\n",
      "Iteration: 1071 Time for last iteration: -11.534116744995117\n",
      "Iteration: 1072 Time for last iteration: -12.646287202835083\n",
      "Iteration: 1073 Time for last iteration: -11.995020151138306\n",
      "Iteration: 1074 Time for last iteration: -12.060450077056885\n",
      "Iteration: 1075 Time for last iteration: -12.38673186302185\n",
      "Iteration: 1076 Time for last iteration: -11.800596952438354\n",
      "Iteration: 1077 Time for last iteration: -12.262897253036499\n",
      "Iteration: 1078 Time for last iteration: -11.899065971374512\n",
      "Iteration: 1079 Time for last iteration: -12.219223976135254\n",
      "Iteration: 1080 Time for last iteration: -11.711318016052246\n",
      "Iteration: 1081 Time for last iteration: -11.662144899368286\n",
      "Iteration: 1082 Time for last iteration: -12.317531108856201\n",
      "Iteration: 1083 Time for last iteration: -11.415780067443848\n",
      "Iteration: 1084 Time for last iteration: -11.79020094871521\n",
      "Iteration: 1085 Time for last iteration: -11.25193190574646\n",
      "Iteration: 1086 Time for last iteration: -14.458561897277832\n",
      "Iteration: 1087 Time for last iteration: -12.629703044891357\n",
      "Iteration: 1088 Time for last iteration: -12.264729022979736\n",
      "Iteration: 1089 Time for last iteration: -13.170899152755737\n",
      "Iteration: 1090 Time for last iteration: -14.578548908233643\n",
      "Iteration: 1091 Time for last iteration: -13.586962938308716\n",
      "Iteration: 1092 Time for last iteration: -14.707951068878174\n",
      "Iteration: 1093 Time for last iteration: -15.76397180557251\n",
      "Iteration: 1094 Time for last iteration: -15.939757823944092\n",
      "Iteration: 1095 Time for last iteration: -12.784955263137817\n",
      "Iteration: 1096 Time for last iteration: -12.204389095306396\n",
      "Iteration: 1097 Time for last iteration: -12.574131965637207\n",
      "Iteration: 1098 Time for last iteration: -11.719552040100098\n",
      "Iteration: 1099 Time for last iteration: -11.667197942733765\n",
      "Iteration: 1100 Time for last iteration: -11.306225061416626\n",
      "Iteration: 1101 Time for last iteration: -9.971264839172363\n",
      "Iteration: 1102 Time for last iteration: -9.567198991775513\n",
      "Iteration: 1103 Time for last iteration: -9.854821681976318\n",
      "Iteration: 1104 Time for last iteration: -10.046069145202637\n",
      "Iteration: 1105 Time for last iteration: -9.567886114120483\n",
      "Iteration: 1106 Time for last iteration: -10.544842004776001\n",
      "Iteration: 1107 Time for last iteration: -11.050548076629639\n",
      "Iteration: 1108 Time for last iteration: -11.392396211624146\n",
      "Iteration: 1109 Time for last iteration: -11.169616222381592\n",
      "Iteration: 1110 Time for last iteration: -11.536823987960815\n",
      "Iteration: 1111 Time for last iteration: -9.933416843414307\n",
      "Iteration: 1112 Time for last iteration: -10.878321886062622\n",
      "Iteration: 1113 Time for last iteration: -9.521050214767456\n",
      "Iteration: 1114 Time for last iteration: -10.016922950744629\n",
      "Iteration: 1115 Time for last iteration: -10.681790113449097\n",
      "Iteration: 1116 Time for last iteration: -9.509234189987183\n",
      "Iteration: 1117 Time for last iteration: -9.854753017425537\n",
      "Iteration: 1118 Time for last iteration: -10.98199987411499\n",
      "Iteration: 1119 Time for last iteration: -10.647112846374512\n",
      "Iteration: 1120 Time for last iteration: -9.488770008087158\n",
      "Iteration: 1121 Time for last iteration: -10.02487301826477\n",
      "Iteration: 1122 Time for last iteration: -10.69458293914795\n",
      "Iteration: 1123 Time for last iteration: -9.527034997940063\n",
      "Iteration: 1124 Time for last iteration: -10.328366041183472\n",
      "Iteration: 1125 Time for last iteration: -10.182642936706543\n",
      "Iteration: 1126 Time for last iteration: -9.416429042816162\n",
      "Iteration: 1127 Time for last iteration: -10.11125922203064\n",
      "Iteration: 1128 Time for last iteration: -9.352322101593018\n",
      "Iteration: 1129 Time for last iteration: -9.787216186523438\n",
      "Iteration: 1130 Time for last iteration: -10.305620908737183\n",
      "Iteration: 1131 Time for last iteration: -10.492413997650146\n",
      "Iteration: 1132 Time for last iteration: -9.778645992279053\n",
      "Iteration: 1133 Time for last iteration: -10.143797636032104\n",
      "Iteration: 1134 Time for last iteration: -10.503399848937988\n",
      "Iteration: 1135 Time for last iteration: -9.717400074005127\n",
      "Iteration: 1136 Time for last iteration: -10.406242847442627\n",
      "Iteration: 1137 Time for last iteration: -11.34018087387085\n",
      "Iteration: 1138 Time for last iteration: -13.34517216682434\n",
      "Iteration: 1139 Time for last iteration: -12.815531015396118\n",
      "Iteration: 1140 Time for last iteration: -12.111391067504883\n",
      "Iteration: 1141 Time for last iteration: -12.590737104415894\n",
      "Iteration: 1142 Time for last iteration: -12.596286296844482\n",
      "Iteration: 1143 Time for last iteration: -12.69890284538269\n",
      "Iteration: 1144 Time for last iteration: -12.345654964447021\n",
      "Iteration: 1145 Time for last iteration: -13.33436107635498\n",
      "Iteration: 1146 Time for last iteration: -12.741779088973999\n",
      "Iteration: 1147 Time for last iteration: -13.171056032180786\n",
      "Iteration: 1148 Time for last iteration: -12.94662880897522\n",
      "Iteration: 1149 Time for last iteration: -14.299797773361206\n",
      "Iteration: 1150 Time for last iteration: -12.902223825454712\n",
      "Iteration: 1151 Time for last iteration: -12.849302053451538\n",
      "Iteration: 1152 Time for last iteration: -13.639818906784058\n",
      "Iteration: 1153 Time for last iteration: -13.12827181816101\n",
      "Iteration: 1154 Time for last iteration: -13.506848096847534\n",
      "Iteration: 1155 Time for last iteration: -13.197793960571289\n",
      "Iteration: 1156 Time for last iteration: -13.205301284790039\n",
      "Iteration: 1157 Time for last iteration: -12.848796844482422\n",
      "Iteration: 1158 Time for last iteration: -13.34845781326294\n",
      "Iteration: 1159 Time for last iteration: -13.495866060256958\n",
      "Iteration: 1160 Time for last iteration: -13.950486898422241\n",
      "Iteration: 1161 Time for last iteration: -13.254664182662964\n",
      "Iteration: 1162 Time for last iteration: -14.215425968170166\n",
      "Iteration: 1163 Time for last iteration: -13.006803035736084\n",
      "Iteration: 1164 Time for last iteration: -13.228049993515015\n",
      "Iteration: 1165 Time for last iteration: -13.807521104812622\n",
      "Iteration: 1166 Time for last iteration: -13.213226079940796\n",
      "Iteration: 1167 Time for last iteration: -14.170382022857666\n",
      "Iteration: 1168 Time for last iteration: -13.375922203063965\n",
      "Iteration: 1169 Time for last iteration: -14.392309188842773\n",
      "Iteration: 1170 Time for last iteration: -13.9198899269104\n",
      "Iteration: 1171 Time for last iteration: -14.223751068115234\n",
      "Iteration: 1172 Time for last iteration: -14.251319169998169\n",
      "Iteration: 1173 Time for last iteration: -12.629791975021362\n",
      "Iteration: 1174 Time for last iteration: -13.847592115402222\n",
      "Iteration: 1175 Time for last iteration: -11.683091402053833\n",
      "Iteration: 1176 Time for last iteration: -11.96158504486084\n",
      "Iteration: 1177 Time for last iteration: -12.88057804107666\n",
      "Iteration: 1178 Time for last iteration: -11.884308338165283\n",
      "Iteration: 1179 Time for last iteration: -13.264863967895508\n",
      "Iteration: 1180 Time for last iteration: -12.565174102783203\n",
      "Iteration: 1181 Time for last iteration: -12.885270833969116\n",
      "Iteration: 1182 Time for last iteration: -13.281089067459106\n",
      "Iteration: 1183 Time for last iteration: -11.592063903808594\n",
      "Iteration: 1184 Time for last iteration: -12.041444063186646\n",
      "Iteration: 1185 Time for last iteration: -11.702136039733887\n",
      "Iteration: 1186 Time for last iteration: -11.239887952804565\n",
      "Iteration: 1187 Time for last iteration: -10.06690788269043\n",
      "Iteration: 1188 Time for last iteration: -13.196019887924194\n",
      "Iteration: 1189 Time for last iteration: -11.90887188911438\n"
     ]
    }
   ],
   "source": [
    "# Проверка метрики лучшей модели на тестовом датасете\n",
    "\n",
    "\n",
    "class IterationInfoCallback(TrainingCallback):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        print('Iteration:', epoch, 'Time for last iteration:', self.start_time - time.time())\n",
    "        self.start_time = time.time()\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "xgb_model_all_train = xgb_model.fit(feat_xgb_train, target_all_train, callbacks=[IterationInfoCallback()])\n",
    "xgb_predict_test = xgb_model_all_train.predict(feat_xgb_test)\n",
    "xgb_predict_train = xgb_model_all_train.predict(feat_xgb_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "299be36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Выборка</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная LGBM _Aug_27_alpha_3</td>\n",
       "      <td>3.403002</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.996599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая LGBM _Aug_27_alpha_3</td>\n",
       "      <td>6.092151</td>\n",
       "      <td>0.014274</td>\n",
       "      <td>0.985890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная XGB _Aug_27_alpha_3</td>\n",
       "      <td>3.828241</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.997552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая XGB _Aug_27_alpha_3</td>\n",
       "      <td>6.141930</td>\n",
       "      <td>0.014345</td>\n",
       "      <td>0.984693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Выборка       MAE      MAPE        R2\n",
       "0  тренировочная LGBM _Aug_27_alpha_3  3.403002  0.007062  0.996599\n",
       "1       тестовая LGBM _Aug_27_alpha_3  6.092151  0.014274  0.985890\n",
       "0   тренировочная XGB _Aug_27_alpha_3  3.828241  0.008054  0.997552\n",
       "1        тестовая XGB _Aug_27_alpha_3  6.141930  0.014345  0.984693"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_train, mape_train, r2_train = metrics_hour(target_all_train, xgb_predict_train )\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test, xgb_predict_test )\n",
    "\n",
    "results = pd.concat([results,\n",
    "pd.DataFrame([[f'тренировочная XGB {FEATURES}', mae_train, mape_train, r2_train], [f'тестовая XGB {FEATURES}', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))\n",
    " ])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ce7f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bf063a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 100.0859457\ttotal: 413ms\tremaining: 1h 8m 53s\n",
      "1:\tlearn: 98.7146801\ttotal: 737ms\tremaining: 1h 1m 23s\n",
      "2:\tlearn: 97.3672413\ttotal: 1.1s\tremaining: 1h 1m 20s\n",
      "3:\tlearn: 96.0436854\ttotal: 1.48s\tremaining: 1h 1m 29s\n",
      "4:\tlearn: 94.7346729\ttotal: 1.83s\tremaining: 1h 1m 6s\n",
      "5:\tlearn: 93.4447521\ttotal: 2.22s\tremaining: 1h 1m 39s\n",
      "6:\tlearn: 92.1675048\ttotal: 2.6s\tremaining: 1h 1m 49s\n",
      "7:\tlearn: 90.9145536\ttotal: 2.96s\tremaining: 1h 1m 38s\n",
      "8:\tlearn: 89.6846397\ttotal: 3.32s\tremaining: 1h 1m 26s\n",
      "9:\tlearn: 88.4743632\ttotal: 3.67s\tremaining: 1h 1m 5s\n",
      "10:\tlearn: 87.2838888\ttotal: 4.1s\tremaining: 1h 2m 2s\n",
      "11:\tlearn: 86.1068307\ttotal: 4.44s\tremaining: 1h 1m 39s\n",
      "12:\tlearn: 84.9475415\ttotal: 4.79s\tremaining: 1h 1m 21s\n",
      "13:\tlearn: 83.8137384\ttotal: 5.15s\tremaining: 1h 1m 13s\n",
      "14:\tlearn: 82.6894440\ttotal: 5.5s\tremaining: 1h 57s\n",
      "15:\tlearn: 81.5848249\ttotal: 5.86s\tremaining: 1h 58s\n",
      "16:\tlearn: 80.4981882\ttotal: 6.19s\tremaining: 1h 36s\n",
      "17:\tlearn: 79.4289236\ttotal: 6.54s\tremaining: 1h 27s\n",
      "18:\tlearn: 78.3689500\ttotal: 6.89s\tremaining: 1h 18s\n",
      "19:\tlearn: 77.3303657\ttotal: 7.24s\tremaining: 1h 10s\n",
      "20:\tlearn: 76.3033352\ttotal: 7.58s\tremaining: 59m 59s\n",
      "21:\tlearn: 75.2960676\ttotal: 7.93s\tremaining: 59m 55s\n",
      "22:\tlearn: 74.3032919\ttotal: 8.27s\tremaining: 59m 45s\n",
      "23:\tlearn: 73.3323493\ttotal: 8.6s\tremaining: 59m 35s\n",
      "24:\tlearn: 72.3560565\ttotal: 8.98s\tremaining: 59m 41s\n",
      "25:\tlearn: 71.3976442\ttotal: 9.36s\tremaining: 59m 50s\n",
      "26:\tlearn: 70.4625197\ttotal: 9.74s\tremaining: 59m 57s\n",
      "27:\tlearn: 69.5321170\ttotal: 10.1s\tremaining: 59m 56s\n",
      "28:\tlearn: 68.6222549\ttotal: 10.4s\tremaining: 59m 48s\n",
      "29:\tlearn: 67.7252458\ttotal: 10.8s\tremaining: 59m 46s\n",
      "30:\tlearn: 66.8395935\ttotal: 11.1s\tremaining: 59m 42s\n",
      "31:\tlearn: 65.9764206\ttotal: 11.5s\tremaining: 59m 38s\n",
      "32:\tlearn: 65.1187478\ttotal: 11.8s\tremaining: 59m 36s\n",
      "33:\tlearn: 64.2817879\ttotal: 12.2s\tremaining: 59m 33s\n",
      "34:\tlearn: 63.4509382\ttotal: 12.5s\tremaining: 59m 26s\n",
      "35:\tlearn: 62.6378951\ttotal: 12.9s\tremaining: 59m 18s\n",
      "36:\tlearn: 61.8307026\ttotal: 13.2s\tremaining: 59m 12s\n",
      "37:\tlearn: 61.0343293\ttotal: 13.5s\tremaining: 59m 2s\n",
      "38:\tlearn: 60.2507700\ttotal: 13.8s\tremaining: 58m 51s\n",
      "39:\tlearn: 59.4706108\ttotal: 14.2s\tremaining: 58m 45s\n",
      "40:\tlearn: 58.7144146\ttotal: 14.5s\tremaining: 58m 50s\n",
      "41:\tlearn: 57.9617486\ttotal: 14.9s\tremaining: 58m 46s\n",
      "42:\tlearn: 57.2222092\ttotal: 15.2s\tremaining: 58m 41s\n",
      "43:\tlearn: 56.4976459\ttotal: 15.5s\tremaining: 58m 36s\n",
      "44:\tlearn: 55.7947310\ttotal: 15.9s\tremaining: 58m 37s\n",
      "45:\tlearn: 55.0925254\ttotal: 16.2s\tremaining: 58m 33s\n",
      "46:\tlearn: 54.4024408\ttotal: 16.6s\tremaining: 58m 33s\n",
      "47:\tlearn: 53.7333874\ttotal: 16.9s\tremaining: 58m 31s\n",
      "48:\tlearn: 53.0697823\ttotal: 17.3s\tremaining: 58m 24s\n",
      "49:\tlearn: 52.4100683\ttotal: 17.6s\tremaining: 58m 23s\n",
      "50:\tlearn: 51.7717602\ttotal: 18s\tremaining: 58m 23s\n",
      "51:\tlearn: 51.1276822\ttotal: 18.3s\tremaining: 58m 14s\n",
      "52:\tlearn: 50.4962376\ttotal: 18.6s\tremaining: 58m 10s\n",
      "53:\tlearn: 49.8821818\ttotal: 18.9s\tremaining: 58m 2s\n",
      "54:\tlearn: 49.2600320\ttotal: 19.3s\tremaining: 58m 2s\n",
      "55:\tlearn: 48.6557764\ttotal: 19.6s\tremaining: 57m 59s\n",
      "56:\tlearn: 48.0651621\ttotal: 19.9s\tremaining: 57m 53s\n",
      "57:\tlearn: 47.4843891\ttotal: 20.3s\tremaining: 57m 51s\n",
      "58:\tlearn: 46.9051019\ttotal: 20.6s\tremaining: 57m 47s\n",
      "59:\tlearn: 46.3427364\ttotal: 20.9s\tremaining: 57m 40s\n",
      "60:\tlearn: 45.7840957\ttotal: 21.2s\tremaining: 57m 37s\n",
      "61:\tlearn: 45.2379044\ttotal: 21.5s\tremaining: 57m 29s\n",
      "62:\tlearn: 44.7046248\ttotal: 21.8s\tremaining: 57m 25s\n",
      "63:\tlearn: 44.1658787\ttotal: 22.2s\tremaining: 57m 24s\n",
      "64:\tlearn: 43.6362200\ttotal: 22.5s\tremaining: 57m 17s\n",
      "65:\tlearn: 43.1192494\ttotal: 22.8s\tremaining: 57m 17s\n",
      "66:\tlearn: 42.6169568\ttotal: 23.2s\tremaining: 57m 15s\n",
      "67:\tlearn: 42.1116806\ttotal: 23.5s\tremaining: 57m 11s\n",
      "68:\tlearn: 41.6199880\ttotal: 23.8s\tremaining: 57m 10s\n",
      "69:\tlearn: 41.1372845\ttotal: 24.1s\tremaining: 57m 3s\n",
      "70:\tlearn: 40.6572570\ttotal: 24.5s\tremaining: 57m\n",
      "71:\tlearn: 40.1805575\ttotal: 24.8s\tremaining: 56m 57s\n",
      "72:\tlearn: 39.7113842\ttotal: 25.1s\tremaining: 56m 55s\n",
      "73:\tlearn: 39.2564013\ttotal: 25.4s\tremaining: 56m 52s\n",
      "74:\tlearn: 38.8055969\ttotal: 25.7s\tremaining: 56m 46s\n",
      "75:\tlearn: 38.3629310\ttotal: 26.1s\tremaining: 56m 42s\n",
      "76:\tlearn: 37.9250079\ttotal: 26.4s\tremaining: 56m 36s\n",
      "77:\tlearn: 37.4985500\ttotal: 26.7s\tremaining: 56m 32s\n",
      "78:\tlearn: 37.0800922\ttotal: 27s\tremaining: 56m 30s\n",
      "79:\tlearn: 36.6652381\ttotal: 27.3s\tremaining: 56m 26s\n",
      "80:\tlearn: 36.2547116\ttotal: 27.6s\tremaining: 56m 21s\n",
      "81:\tlearn: 35.8551140\ttotal: 27.9s\tremaining: 56m 17s\n",
      "82:\tlearn: 35.4585821\ttotal: 28.3s\tremaining: 56m 17s\n",
      "83:\tlearn: 35.0718762\ttotal: 28.6s\tremaining: 56m 16s\n",
      "84:\tlearn: 34.6848711\ttotal: 28.9s\tremaining: 56m 14s\n",
      "85:\tlearn: 34.3064290\ttotal: 29.2s\tremaining: 56m 9s\n",
      "86:\tlearn: 33.9354281\ttotal: 29.5s\tremaining: 56m 6s\n",
      "87:\tlearn: 33.5714316\ttotal: 29.8s\tremaining: 56m\n",
      "88:\tlearn: 33.2142739\ttotal: 30.2s\tremaining: 56m 1s\n",
      "89:\tlearn: 32.8573911\ttotal: 30.5s\tremaining: 56m 2s\n",
      "90:\tlearn: 32.5094212\ttotal: 30.9s\tremaining: 56m 6s\n",
      "91:\tlearn: 32.1634723\ttotal: 31.2s\tremaining: 56m 2s\n",
      "92:\tlearn: 31.8317235\ttotal: 31.5s\tremaining: 55m 58s\n",
      "93:\tlearn: 31.5023968\ttotal: 31.9s\tremaining: 55m 56s\n",
      "94:\tlearn: 31.1783811\ttotal: 32.2s\tremaining: 55m 54s\n",
      "95:\tlearn: 30.8537058\ttotal: 32.5s\tremaining: 55m 53s\n",
      "96:\tlearn: 30.5350163\ttotal: 32.9s\tremaining: 55m 54s\n",
      "97:\tlearn: 30.2171856\ttotal: 33.2s\tremaining: 55m 50s\n",
      "98:\tlearn: 29.9153602\ttotal: 33.5s\tremaining: 55m 51s\n",
      "99:\tlearn: 29.6216817\ttotal: 33.9s\tremaining: 55m 52s\n",
      "100:\tlearn: 29.3261199\ttotal: 34.2s\tremaining: 55m 50s\n",
      "101:\tlearn: 29.0343836\ttotal: 34.5s\tremaining: 55m 47s\n",
      "102:\tlearn: 28.7468185\ttotal: 34.8s\tremaining: 55m 44s\n",
      "103:\tlearn: 28.4669557\ttotal: 35.1s\tremaining: 55m 44s\n",
      "104:\tlearn: 28.1943282\ttotal: 35.5s\tremaining: 55m 41s\n",
      "105:\tlearn: 27.9163866\ttotal: 35.8s\tremaining: 55m 40s\n",
      "106:\tlearn: 27.6511354\ttotal: 36.1s\tremaining: 55m 38s\n",
      "107:\tlearn: 27.3839151\ttotal: 36.4s\tremaining: 55m 34s\n",
      "108:\tlearn: 27.1208752\ttotal: 36.7s\tremaining: 55m 30s\n",
      "109:\tlearn: 26.8661697\ttotal: 37s\tremaining: 55m 27s\n",
      "110:\tlearn: 26.6097732\ttotal: 37.3s\tremaining: 55m 23s\n",
      "111:\tlearn: 26.3620354\ttotal: 37.6s\tremaining: 55m 19s\n",
      "112:\tlearn: 26.1184762\ttotal: 37.9s\tremaining: 55m 18s\n",
      "113:\tlearn: 25.8829016\ttotal: 38.2s\tremaining: 55m 16s\n",
      "114:\tlearn: 25.6472113\ttotal: 38.5s\tremaining: 55m 13s\n",
      "115:\tlearn: 25.4210850\ttotal: 38.9s\tremaining: 55m 18s\n",
      "116:\tlearn: 25.1899123\ttotal: 39.3s\tremaining: 55m 22s\n",
      "117:\tlearn: 24.9669612\ttotal: 39.6s\tremaining: 55m 18s\n",
      "118:\tlearn: 24.7432959\ttotal: 39.9s\tremaining: 55m 17s\n",
      "119:\tlearn: 24.5238629\ttotal: 40.3s\tremaining: 55m 17s\n",
      "120:\tlearn: 24.3122338\ttotal: 40.7s\tremaining: 55m 19s\n",
      "121:\tlearn: 24.1075181\ttotal: 41s\tremaining: 55m 19s\n",
      "122:\tlearn: 23.9074849\ttotal: 41.4s\tremaining: 55m 22s\n",
      "123:\tlearn: 23.7097242\ttotal: 41.7s\tremaining: 55m 22s\n",
      "124:\tlearn: 23.5201145\ttotal: 42s\tremaining: 55m 18s\n",
      "125:\tlearn: 23.3221427\ttotal: 42.3s\tremaining: 55m 14s\n",
      "126:\tlearn: 23.1332391\ttotal: 42.6s\tremaining: 55m 10s\n",
      "127:\tlearn: 22.9515656\ttotal: 42.9s\tremaining: 55m 6s\n",
      "128:\tlearn: 22.7613210\ttotal: 43.2s\tremaining: 55m 5s\n",
      "129:\tlearn: 22.5841507\ttotal: 43.5s\tremaining: 55m 4s\n",
      "130:\tlearn: 22.4098748\ttotal: 43.8s\tremaining: 55m\n",
      "131:\tlearn: 22.2357946\ttotal: 44.1s\tremaining: 54m 58s\n",
      "132:\tlearn: 22.0631138\ttotal: 44.4s\tremaining: 54m 54s\n",
      "133:\tlearn: 21.8926623\ttotal: 44.7s\tremaining: 54m 51s\n",
      "134:\tlearn: 21.7302658\ttotal: 45s\tremaining: 54m 50s\n",
      "135:\tlearn: 21.5637568\ttotal: 45.4s\tremaining: 54m 50s\n",
      "136:\tlearn: 21.4038310\ttotal: 45.7s\tremaining: 54m 49s\n",
      "137:\tlearn: 21.2484111\ttotal: 46s\tremaining: 54m 48s\n",
      "138:\tlearn: 21.0928863\ttotal: 46.4s\tremaining: 54m 48s\n",
      "139:\tlearn: 20.9385576\ttotal: 46.7s\tremaining: 54m 47s\n",
      "140:\tlearn: 20.7902894\ttotal: 47s\tremaining: 54m 44s\n",
      "141:\tlearn: 20.6380055\ttotal: 47.3s\tremaining: 54m 43s\n",
      "142:\tlearn: 20.4931548\ttotal: 47.6s\tremaining: 54m 43s\n",
      "143:\tlearn: 20.3555800\ttotal: 47.9s\tremaining: 54m 41s\n",
      "144:\tlearn: 20.2168892\ttotal: 48.3s\tremaining: 54m 39s\n",
      "145:\tlearn: 20.0760924\ttotal: 48.6s\tremaining: 54m 39s\n",
      "146:\tlearn: 19.9415511\ttotal: 48.9s\tremaining: 54m 37s\n",
      "147:\tlearn: 19.8106243\ttotal: 49.2s\tremaining: 54m 36s\n",
      "148:\tlearn: 19.6815839\ttotal: 49.6s\tremaining: 54m 36s\n",
      "149:\tlearn: 19.5538214\ttotal: 49.9s\tremaining: 54m 35s\n",
      "150:\tlearn: 19.4279068\ttotal: 50.2s\tremaining: 54m 34s\n",
      "151:\tlearn: 19.3048994\ttotal: 50.6s\tremaining: 54m 39s\n",
      "152:\tlearn: 19.1826559\ttotal: 51s\tremaining: 54m 39s\n",
      "153:\tlearn: 19.0633256\ttotal: 51.3s\tremaining: 54m 38s\n",
      "154:\tlearn: 18.9459534\ttotal: 51.6s\tremaining: 54m 38s\n",
      "155:\tlearn: 18.8216195\ttotal: 51.9s\tremaining: 54m 37s\n",
      "156:\tlearn: 18.7010777\ttotal: 52.3s\tremaining: 54m 39s\n",
      "157:\tlearn: 18.5911282\ttotal: 52.6s\tremaining: 54m 37s\n",
      "158:\tlearn: 18.4863964\ttotal: 53s\tremaining: 54m 38s\n",
      "159:\tlearn: 18.3735737\ttotal: 53.3s\tremaining: 54m 36s\n",
      "160:\tlearn: 18.2682901\ttotal: 53.6s\tremaining: 54m 37s\n",
      "161:\tlearn: 18.1616804\ttotal: 54s\tremaining: 54m 37s\n",
      "162:\tlearn: 18.0552805\ttotal: 54.3s\tremaining: 54m 36s\n",
      "163:\tlearn: 17.9512985\ttotal: 54.6s\tremaining: 54m 33s\n",
      "164:\tlearn: 17.8527494\ttotal: 54.9s\tremaining: 54m 32s\n",
      "165:\tlearn: 17.7563095\ttotal: 55.2s\tremaining: 54m 29s\n",
      "166:\tlearn: 17.6646461\ttotal: 55.5s\tremaining: 54m 28s\n",
      "167:\tlearn: 17.5704295\ttotal: 55.8s\tremaining: 54m 26s\n",
      "168:\tlearn: 17.4795369\ttotal: 56.2s\tremaining: 54m 26s\n",
      "169:\tlearn: 17.3891550\ttotal: 56.5s\tremaining: 54m 24s\n",
      "170:\tlearn: 17.3005991\ttotal: 56.8s\tremaining: 54m 23s\n",
      "171:\tlearn: 17.2060093\ttotal: 57.2s\tremaining: 54m 30s\n",
      "172:\tlearn: 17.1246897\ttotal: 57.6s\tremaining: 54m 33s\n",
      "173:\tlearn: 17.0414270\ttotal: 58s\tremaining: 54m 34s\n",
      "174:\tlearn: 16.9603642\ttotal: 58.4s\tremaining: 54m 36s\n",
      "175:\tlearn: 16.8828907\ttotal: 58.7s\tremaining: 54m 35s\n",
      "176:\tlearn: 16.8048221\ttotal: 59s\tremaining: 54m 35s\n",
      "177:\tlearn: 16.7273759\ttotal: 59.4s\tremaining: 54m 35s\n",
      "178:\tlearn: 16.6505091\ttotal: 59.8s\tremaining: 54m 39s\n",
      "179:\tlearn: 16.5672160\ttotal: 1m\tremaining: 54m 40s\n",
      "180:\tlearn: 16.4886255\ttotal: 1m\tremaining: 54m 37s\n",
      "181:\tlearn: 16.4175799\ttotal: 1m\tremaining: 54m 35s\n",
      "182:\tlearn: 16.3466890\ttotal: 1m 1s\tremaining: 54m 32s\n",
      "183:\tlearn: 16.2795881\ttotal: 1m 1s\tremaining: 54m 30s\n",
      "184:\tlearn: 16.2058202\ttotal: 1m 1s\tremaining: 54m 29s\n",
      "185:\tlearn: 16.1375012\ttotal: 1m 1s\tremaining: 54m 27s\n",
      "186:\tlearn: 16.0685519\ttotal: 1m 2s\tremaining: 54m 27s\n",
      "187:\tlearn: 16.0023017\ttotal: 1m 2s\tremaining: 54m 25s\n",
      "188:\tlearn: 15.9351197\ttotal: 1m 2s\tremaining: 54m 25s\n",
      "189:\tlearn: 15.8728119\ttotal: 1m 3s\tremaining: 54m 22s\n",
      "190:\tlearn: 15.8082379\ttotal: 1m 3s\tremaining: 54m 21s\n",
      "191:\tlearn: 15.7448541\ttotal: 1m 3s\tremaining: 54m 25s\n",
      "192:\tlearn: 15.6791190\ttotal: 1m 4s\tremaining: 54m 25s\n",
      "193:\tlearn: 15.6221993\ttotal: 1m 4s\tremaining: 54m 24s\n",
      "194:\tlearn: 15.5663672\ttotal: 1m 4s\tremaining: 54m 22s\n",
      "195:\tlearn: 15.5088700\ttotal: 1m 5s\tremaining: 54m 19s\n",
      "196:\tlearn: 15.4491083\ttotal: 1m 5s\tremaining: 54m 19s\n",
      "197:\tlearn: 15.3892618\ttotal: 1m 5s\tremaining: 54m 19s\n",
      "198:\tlearn: 15.3320884\ttotal: 1m 6s\tremaining: 54m 16s\n",
      "199:\tlearn: 15.2756679\ttotal: 1m 6s\tremaining: 54m 16s\n",
      "200:\tlearn: 15.2240076\ttotal: 1m 6s\tremaining: 54m 16s\n",
      "201:\tlearn: 15.1693116\ttotal: 1m 7s\tremaining: 54m 18s\n",
      "202:\tlearn: 15.1166598\ttotal: 1m 7s\tremaining: 54m 18s\n",
      "203:\tlearn: 15.0675106\ttotal: 1m 7s\tremaining: 54m 15s\n",
      "204:\tlearn: 15.0178889\ttotal: 1m 8s\tremaining: 54m 13s\n",
      "205:\tlearn: 14.9631462\ttotal: 1m 8s\tremaining: 54m 10s\n",
      "206:\tlearn: 14.9102068\ttotal: 1m 8s\tremaining: 54m 9s\n",
      "207:\tlearn: 14.8640901\ttotal: 1m 9s\tremaining: 54m 8s\n",
      "208:\tlearn: 14.8178926\ttotal: 1m 9s\tremaining: 54m 8s\n",
      "209:\tlearn: 14.7726351\ttotal: 1m 9s\tremaining: 54m 6s\n",
      "210:\tlearn: 14.7232717\ttotal: 1m 9s\tremaining: 54m 3s\n",
      "211:\tlearn: 14.6814864\ttotal: 1m 10s\tremaining: 54m\n",
      "212:\tlearn: 14.6286165\ttotal: 1m 10s\tremaining: 53m 59s\n",
      "213:\tlearn: 14.5837326\ttotal: 1m 10s\tremaining: 53m 58s\n",
      "214:\tlearn: 14.5428189\ttotal: 1m 11s\tremaining: 53m 55s\n",
      "215:\tlearn: 14.5044419\ttotal: 1m 11s\tremaining: 53m 54s\n",
      "216:\tlearn: 14.4581854\ttotal: 1m 11s\tremaining: 53m 52s\n",
      "217:\tlearn: 14.4203072\ttotal: 1m 11s\tremaining: 53m 49s\n",
      "218:\tlearn: 14.3771638\ttotal: 1m 12s\tremaining: 53m 47s\n",
      "219:\tlearn: 14.3339757\ttotal: 1m 12s\tremaining: 53m 45s\n",
      "220:\tlearn: 14.2926346\ttotal: 1m 12s\tremaining: 53m 44s\n",
      "221:\tlearn: 14.2572192\ttotal: 1m 13s\tremaining: 53m 42s\n",
      "222:\tlearn: 14.2232684\ttotal: 1m 13s\tremaining: 53m 40s\n",
      "223:\tlearn: 14.1879198\ttotal: 1m 13s\tremaining: 53m 37s\n",
      "224:\tlearn: 14.1491287\ttotal: 1m 14s\tremaining: 53m 36s\n",
      "225:\tlearn: 14.1108764\ttotal: 1m 14s\tremaining: 53m 35s\n",
      "226:\tlearn: 14.0712019\ttotal: 1m 14s\tremaining: 53m 33s\n",
      "227:\tlearn: 14.0300515\ttotal: 1m 14s\tremaining: 53m 33s\n",
      "228:\tlearn: 13.9890363\ttotal: 1m 15s\tremaining: 53m 30s\n",
      "229:\tlearn: 13.9498599\ttotal: 1m 15s\tremaining: 53m 28s\n",
      "230:\tlearn: 13.9171900\ttotal: 1m 15s\tremaining: 53m 26s\n",
      "231:\tlearn: 13.8793673\ttotal: 1m 16s\tremaining: 53m 24s\n",
      "232:\tlearn: 13.8448681\ttotal: 1m 16s\tremaining: 53m 23s\n",
      "233:\tlearn: 13.8142629\ttotal: 1m 16s\tremaining: 53m 19s\n",
      "234:\tlearn: 13.7828782\ttotal: 1m 16s\tremaining: 53m 15s\n",
      "235:\tlearn: 13.7500929\ttotal: 1m 17s\tremaining: 53m 14s\n",
      "236:\tlearn: 13.7188243\ttotal: 1m 17s\tremaining: 53m 13s\n",
      "237:\tlearn: 13.6869799\ttotal: 1m 17s\tremaining: 53m 10s\n",
      "238:\tlearn: 13.6559759\ttotal: 1m 18s\tremaining: 53m 10s\n",
      "239:\tlearn: 13.6244753\ttotal: 1m 18s\tremaining: 53m 11s\n",
      "240:\tlearn: 13.5951879\ttotal: 1m 18s\tremaining: 53m 9s\n",
      "241:\tlearn: 13.5645364\ttotal: 1m 19s\tremaining: 53m 9s\n",
      "242:\tlearn: 13.5382436\ttotal: 1m 19s\tremaining: 53m 6s\n",
      "243:\tlearn: 13.5117043\ttotal: 1m 19s\tremaining: 53m 2s\n",
      "244:\tlearn: 13.4781575\ttotal: 1m 19s\tremaining: 53m 1s\n",
      "245:\tlearn: 13.4510358\ttotal: 1m 20s\tremaining: 53m\n",
      "246:\tlearn: 13.4177894\ttotal: 1m 20s\tremaining: 52m 59s\n",
      "247:\tlearn: 13.3899950\ttotal: 1m 20s\tremaining: 52m 58s\n",
      "248:\tlearn: 13.3642014\ttotal: 1m 21s\tremaining: 52m 54s\n",
      "249:\tlearn: 13.3352422\ttotal: 1m 21s\tremaining: 52m 52s\n",
      "250:\tlearn: 13.3080640\ttotal: 1m 21s\tremaining: 52m 51s\n",
      "251:\tlearn: 13.2773863\ttotal: 1m 21s\tremaining: 52m 50s\n",
      "252:\tlearn: 13.2518470\ttotal: 1m 22s\tremaining: 52m 49s\n",
      "253:\tlearn: 13.2253252\ttotal: 1m 22s\tremaining: 52m 48s\n",
      "254:\tlearn: 13.1951740\ttotal: 1m 22s\tremaining: 52m 48s\n",
      "255:\tlearn: 13.1660670\ttotal: 1m 23s\tremaining: 52m 46s\n",
      "256:\tlearn: 13.1368828\ttotal: 1m 23s\tremaining: 52m 44s\n",
      "257:\tlearn: 13.1129205\ttotal: 1m 23s\tremaining: 52m 41s\n",
      "258:\tlearn: 13.0890208\ttotal: 1m 24s\tremaining: 52m 40s\n",
      "259:\tlearn: 13.0632402\ttotal: 1m 24s\tremaining: 52m 38s\n",
      "260:\tlearn: 13.0391300\ttotal: 1m 24s\tremaining: 52m 38s\n",
      "261:\tlearn: 13.0159196\ttotal: 1m 24s\tremaining: 52m 37s\n",
      "262:\tlearn: 12.9901760\ttotal: 1m 25s\tremaining: 52m 35s\n",
      "263:\tlearn: 12.9663732\ttotal: 1m 25s\tremaining: 52m 33s\n",
      "264:\tlearn: 12.9427774\ttotal: 1m 25s\tremaining: 52m 33s\n",
      "265:\tlearn: 12.9208343\ttotal: 1m 26s\tremaining: 52m 31s\n",
      "266:\tlearn: 12.8977915\ttotal: 1m 26s\tremaining: 52m 28s\n",
      "267:\tlearn: 12.8768441\ttotal: 1m 26s\tremaining: 52m 28s\n",
      "268:\tlearn: 12.8575523\ttotal: 1m 26s\tremaining: 52m 24s\n",
      "269:\tlearn: 12.8356387\ttotal: 1m 27s\tremaining: 52m 24s\n",
      "270:\tlearn: 12.8095339\ttotal: 1m 27s\tremaining: 52m 23s\n",
      "271:\tlearn: 12.7898720\ttotal: 1m 27s\tremaining: 52m 20s\n",
      "272:\tlearn: 12.7691026\ttotal: 1m 28s\tremaining: 52m 18s\n",
      "273:\tlearn: 12.7458752\ttotal: 1m 28s\tremaining: 52m 17s\n",
      "274:\tlearn: 12.7280806\ttotal: 1m 28s\tremaining: 52m 14s\n",
      "275:\tlearn: 12.7064999\ttotal: 1m 28s\tremaining: 52m 14s\n",
      "276:\tlearn: 12.6874794\ttotal: 1m 29s\tremaining: 52m 14s\n",
      "277:\tlearn: 12.6696837\ttotal: 1m 29s\tremaining: 52m 11s\n",
      "278:\tlearn: 12.6461833\ttotal: 1m 29s\tremaining: 52m 12s\n",
      "279:\tlearn: 12.6273415\ttotal: 1m 30s\tremaining: 52m 10s\n",
      "280:\tlearn: 12.6091731\ttotal: 1m 30s\tremaining: 52m 9s\n",
      "281:\tlearn: 12.5929573\ttotal: 1m 30s\tremaining: 52m 5s\n",
      "282:\tlearn: 12.5734048\ttotal: 1m 31s\tremaining: 52m 7s\n",
      "283:\tlearn: 12.5555825\ttotal: 1m 31s\tremaining: 52m 8s\n",
      "284:\tlearn: 12.5356305\ttotal: 1m 31s\tremaining: 52m 11s\n",
      "285:\tlearn: 12.5191486\ttotal: 1m 32s\tremaining: 52m 12s\n",
      "286:\tlearn: 12.4985384\ttotal: 1m 32s\tremaining: 52m 12s\n",
      "287:\tlearn: 12.4768768\ttotal: 1m 32s\tremaining: 52m 11s\n",
      "288:\tlearn: 12.4592876\ttotal: 1m 33s\tremaining: 52m 12s\n",
      "289:\tlearn: 12.4395888\ttotal: 1m 33s\tremaining: 52m 13s\n",
      "290:\tlearn: 12.4231372\ttotal: 1m 33s\tremaining: 52m 13s\n",
      "291:\tlearn: 12.4048375\ttotal: 1m 34s\tremaining: 52m 13s\n",
      "292:\tlearn: 12.3885647\ttotal: 1m 34s\tremaining: 52m 15s\n",
      "293:\tlearn: 12.3706826\ttotal: 1m 34s\tremaining: 52m 15s\n",
      "294:\tlearn: 12.3542059\ttotal: 1m 35s\tremaining: 52m 13s\n",
      "295:\tlearn: 12.3380436\ttotal: 1m 35s\tremaining: 52m 14s\n",
      "296:\tlearn: 12.3245543\ttotal: 1m 35s\tremaining: 52m 13s\n",
      "297:\tlearn: 12.3067031\ttotal: 1m 36s\tremaining: 52m 14s\n",
      "298:\tlearn: 12.2920616\ttotal: 1m 36s\tremaining: 52m 12s\n",
      "299:\tlearn: 12.2752896\ttotal: 1m 36s\tremaining: 52m 12s\n",
      "300:\tlearn: 12.2567483\ttotal: 1m 37s\tremaining: 52m 11s\n",
      "301:\tlearn: 12.2421865\ttotal: 1m 37s\tremaining: 52m 10s\n",
      "302:\tlearn: 12.2264718\ttotal: 1m 37s\tremaining: 52m 9s\n",
      "303:\tlearn: 12.2104766\ttotal: 1m 38s\tremaining: 52m 9s\n",
      "304:\tlearn: 12.1974295\ttotal: 1m 38s\tremaining: 52m 7s\n",
      "305:\tlearn: 12.1794258\ttotal: 1m 38s\tremaining: 52m 8s\n",
      "306:\tlearn: 12.1637587\ttotal: 1m 39s\tremaining: 52m 6s\n",
      "307:\tlearn: 12.1510660\ttotal: 1m 39s\tremaining: 52m 5s\n",
      "308:\tlearn: 12.1366282\ttotal: 1m 39s\tremaining: 52m 6s\n",
      "309:\tlearn: 12.1232469\ttotal: 1m 39s\tremaining: 52m 4s\n",
      "310:\tlearn: 12.1097667\ttotal: 1m 40s\tremaining: 52m 4s\n",
      "311:\tlearn: 12.0927561\ttotal: 1m 40s\tremaining: 52m 3s\n",
      "312:\tlearn: 12.0799351\ttotal: 1m 40s\tremaining: 52m 2s\n",
      "313:\tlearn: 12.0622005\ttotal: 1m 41s\tremaining: 52m 3s\n",
      "314:\tlearn: 12.0484001\ttotal: 1m 41s\tremaining: 52m 2s\n",
      "315:\tlearn: 12.0362062\ttotal: 1m 41s\tremaining: 52m 2s\n",
      "316:\tlearn: 12.0237816\ttotal: 1m 42s\tremaining: 52m 3s\n",
      "317:\tlearn: 12.0072742\ttotal: 1m 42s\tremaining: 52m 3s\n",
      "318:\tlearn: 11.9934593\ttotal: 1m 42s\tremaining: 52m 3s\n",
      "319:\tlearn: 11.9786509\ttotal: 1m 43s\tremaining: 52m 2s\n",
      "320:\tlearn: 11.9666558\ttotal: 1m 43s\tremaining: 52m 2s\n",
      "321:\tlearn: 11.9549068\ttotal: 1m 43s\tremaining: 51m 59s\n",
      "322:\tlearn: 11.9437052\ttotal: 1m 44s\tremaining: 51m 57s\n",
      "323:\tlearn: 11.9288797\ttotal: 1m 44s\tremaining: 51m 56s\n",
      "324:\tlearn: 11.9186720\ttotal: 1m 44s\tremaining: 51m 56s\n",
      "325:\tlearn: 11.9073018\ttotal: 1m 44s\tremaining: 51m 53s\n",
      "326:\tlearn: 11.8966177\ttotal: 1m 45s\tremaining: 51m 51s\n",
      "327:\tlearn: 11.8862154\ttotal: 1m 45s\tremaining: 51m 52s\n",
      "328:\tlearn: 11.8742024\ttotal: 1m 45s\tremaining: 51m 53s\n",
      "329:\tlearn: 11.8625470\ttotal: 1m 46s\tremaining: 51m 51s\n",
      "330:\tlearn: 11.8491571\ttotal: 1m 46s\tremaining: 51m 53s\n",
      "331:\tlearn: 11.8386649\ttotal: 1m 46s\tremaining: 51m 51s\n",
      "332:\tlearn: 11.8270489\ttotal: 1m 47s\tremaining: 51m 54s\n",
      "333:\tlearn: 11.8145616\ttotal: 1m 47s\tremaining: 51m 53s\n",
      "334:\tlearn: 11.8030063\ttotal: 1m 47s\tremaining: 51m 52s\n",
      "335:\tlearn: 11.7913063\ttotal: 1m 48s\tremaining: 51m 53s\n",
      "336:\tlearn: 11.7789366\ttotal: 1m 48s\tremaining: 51m 52s\n",
      "337:\tlearn: 11.7681940\ttotal: 1m 48s\tremaining: 51m 50s\n",
      "338:\tlearn: 11.7592864\ttotal: 1m 49s\tremaining: 51m 48s\n",
      "339:\tlearn: 11.7492649\ttotal: 1m 49s\tremaining: 51m 48s\n",
      "340:\tlearn: 11.7381211\ttotal: 1m 49s\tremaining: 51m 50s\n",
      "341:\tlearn: 11.7289604\ttotal: 1m 50s\tremaining: 51m 48s\n",
      "342:\tlearn: 11.7192123\ttotal: 1m 50s\tremaining: 51m 45s\n",
      "343:\tlearn: 11.7102491\ttotal: 1m 50s\tremaining: 51m 44s\n",
      "344:\tlearn: 11.6993964\ttotal: 1m 50s\tremaining: 51m 44s\n",
      "345:\tlearn: 11.6887215\ttotal: 1m 51s\tremaining: 51m 45s\n",
      "346:\tlearn: 11.6799739\ttotal: 1m 51s\tremaining: 51m 43s\n",
      "347:\tlearn: 11.6710559\ttotal: 1m 51s\tremaining: 51m 42s\n",
      "348:\tlearn: 11.6598336\ttotal: 1m 52s\tremaining: 51m 42s\n",
      "349:\tlearn: 11.6510386\ttotal: 1m 52s\tremaining: 51m 39s\n",
      "350:\tlearn: 11.6397628\ttotal: 1m 52s\tremaining: 51m 37s\n",
      "351:\tlearn: 11.6310365\ttotal: 1m 52s\tremaining: 51m 34s\n",
      "352:\tlearn: 11.6231421\ttotal: 1m 53s\tremaining: 51m 33s\n",
      "353:\tlearn: 11.6134099\ttotal: 1m 53s\tremaining: 51m 34s\n",
      "354:\tlearn: 11.6050628\ttotal: 1m 53s\tremaining: 51m 33s\n",
      "355:\tlearn: 11.5963842\ttotal: 1m 54s\tremaining: 51m 31s\n",
      "356:\tlearn: 11.5869830\ttotal: 1m 54s\tremaining: 51m 28s\n",
      "357:\tlearn: 11.5790578\ttotal: 1m 54s\tremaining: 51m 27s\n",
      "358:\tlearn: 11.5694401\ttotal: 1m 55s\tremaining: 51m 28s\n",
      "359:\tlearn: 11.5612047\ttotal: 1m 55s\tremaining: 51m 27s\n",
      "360:\tlearn: 11.5538835\ttotal: 1m 55s\tremaining: 51m 25s\n",
      "361:\tlearn: 11.5429017\ttotal: 1m 55s\tremaining: 51m 24s\n",
      "362:\tlearn: 11.5337890\ttotal: 1m 56s\tremaining: 51m 21s\n",
      "363:\tlearn: 11.5258718\ttotal: 1m 56s\tremaining: 51m 20s\n",
      "364:\tlearn: 11.5170209\ttotal: 1m 56s\tremaining: 51m 17s\n",
      "365:\tlearn: 11.5089273\ttotal: 1m 56s\tremaining: 51m 16s\n",
      "366:\tlearn: 11.5016773\ttotal: 1m 57s\tremaining: 51m 15s\n",
      "367:\tlearn: 11.4931946\ttotal: 1m 57s\tremaining: 51m 15s\n",
      "368:\tlearn: 11.4789665\ttotal: 1m 57s\tremaining: 51m 15s\n",
      "369:\tlearn: 11.4667538\ttotal: 1m 58s\tremaining: 51m 16s\n",
      "370:\tlearn: 11.4595404\ttotal: 1m 58s\tremaining: 51m 15s\n",
      "371:\tlearn: 11.4504001\ttotal: 1m 58s\tremaining: 51m 13s\n",
      "372:\tlearn: 11.4436455\ttotal: 1m 59s\tremaining: 51m 12s\n",
      "373:\tlearn: 11.4347130\ttotal: 1m 59s\tremaining: 51m 13s\n",
      "374:\tlearn: 11.4258903\ttotal: 1m 59s\tremaining: 51m 11s\n",
      "375:\tlearn: 11.4121584\ttotal: 2m\tremaining: 51m 11s\n",
      "376:\tlearn: 11.4052491\ttotal: 2m\tremaining: 51m 10s\n",
      "377:\tlearn: 11.3959654\ttotal: 2m\tremaining: 51m 9s\n",
      "378:\tlearn: 11.3862573\ttotal: 2m\tremaining: 51m 8s\n",
      "379:\tlearn: 11.3731212\ttotal: 2m 1s\tremaining: 51m 9s\n",
      "380:\tlearn: 11.3660470\ttotal: 2m 1s\tremaining: 51m 7s\n",
      "381:\tlearn: 11.3575191\ttotal: 2m 1s\tremaining: 51m 5s\n",
      "382:\tlearn: 11.3487804\ttotal: 2m 1s\tremaining: 51m 3s\n",
      "383:\tlearn: 11.3407650\ttotal: 2m 2s\tremaining: 51m 2s\n",
      "384:\tlearn: 11.3312804\ttotal: 2m 2s\tremaining: 51m 1s\n",
      "385:\tlearn: 11.3249040\ttotal: 2m 2s\tremaining: 50m 59s\n",
      "386:\tlearn: 11.3171619\ttotal: 2m 3s\tremaining: 50m 57s\n",
      "387:\tlearn: 11.3101373\ttotal: 2m 3s\tremaining: 50m 55s\n",
      "388:\tlearn: 11.3015563\ttotal: 2m 3s\tremaining: 50m 55s\n",
      "389:\tlearn: 11.2939599\ttotal: 2m 4s\tremaining: 50m 55s\n",
      "390:\tlearn: 11.2871561\ttotal: 2m 4s\tremaining: 50m 53s\n",
      "391:\tlearn: 11.2799954\ttotal: 2m 4s\tremaining: 50m 53s\n",
      "392:\tlearn: 11.2739807\ttotal: 2m 4s\tremaining: 50m 51s\n",
      "393:\tlearn: 11.2658994\ttotal: 2m 5s\tremaining: 50m 50s\n",
      "394:\tlearn: 11.2587731\ttotal: 2m 5s\tremaining: 50m 48s\n",
      "395:\tlearn: 11.2523372\ttotal: 2m 5s\tremaining: 50m 47s\n",
      "396:\tlearn: 11.2410712\ttotal: 2m 5s\tremaining: 50m 46s\n",
      "397:\tlearn: 11.2341823\ttotal: 2m 6s\tremaining: 50m 45s\n",
      "398:\tlearn: 11.2273222\ttotal: 2m 6s\tremaining: 50m 45s\n",
      "399:\tlearn: 11.2212105\ttotal: 2m 6s\tremaining: 50m 42s\n",
      "400:\tlearn: 11.2137383\ttotal: 2m 7s\tremaining: 50m 42s\n",
      "401:\tlearn: 11.2013335\ttotal: 2m 7s\tremaining: 50m 42s\n",
      "402:\tlearn: 11.1923732\ttotal: 2m 7s\tremaining: 50m 42s\n",
      "403:\tlearn: 11.1863575\ttotal: 2m 8s\tremaining: 50m 41s\n",
      "404:\tlearn: 11.1799997\ttotal: 2m 8s\tremaining: 50m 40s\n",
      "405:\tlearn: 11.1681101\ttotal: 2m 8s\tremaining: 50m 40s\n",
      "406:\tlearn: 11.1607690\ttotal: 2m 9s\tremaining: 50m 41s\n",
      "407:\tlearn: 11.1501355\ttotal: 2m 9s\tremaining: 50m 40s\n",
      "408:\tlearn: 11.1403409\ttotal: 2m 9s\tremaining: 50m 40s\n",
      "409:\tlearn: 11.1341043\ttotal: 2m 9s\tremaining: 50m 40s\n",
      "410:\tlearn: 11.1282751\ttotal: 2m 10s\tremaining: 50m 39s\n",
      "411:\tlearn: 11.1222828\ttotal: 2m 10s\tremaining: 50m 39s\n",
      "412:\tlearn: 11.1121111\ttotal: 2m 10s\tremaining: 50m 38s\n",
      "413:\tlearn: 11.1067014\ttotal: 2m 11s\tremaining: 50m 36s\n",
      "414:\tlearn: 11.0969672\ttotal: 2m 11s\tremaining: 50m 35s\n",
      "415:\tlearn: 11.0898585\ttotal: 2m 11s\tremaining: 50m 34s\n",
      "416:\tlearn: 11.0846995\ttotal: 2m 11s\tremaining: 50m 33s\n",
      "417:\tlearn: 11.0783338\ttotal: 2m 12s\tremaining: 50m 32s\n",
      "418:\tlearn: 11.0710031\ttotal: 2m 12s\tremaining: 50m 32s\n",
      "419:\tlearn: 11.0618428\ttotal: 2m 12s\tremaining: 50m 32s\n",
      "420:\tlearn: 11.0554291\ttotal: 2m 13s\tremaining: 50m 31s\n",
      "421:\tlearn: 11.0483855\ttotal: 2m 13s\tremaining: 50m 31s\n",
      "422:\tlearn: 11.0416675\ttotal: 2m 13s\tremaining: 50m 30s\n",
      "423:\tlearn: 11.0356818\ttotal: 2m 14s\tremaining: 50m 30s\n",
      "424:\tlearn: 11.0288034\ttotal: 2m 14s\tremaining: 50m 30s\n",
      "425:\tlearn: 11.0220927\ttotal: 2m 14s\tremaining: 50m 29s\n",
      "426:\tlearn: 11.0150258\ttotal: 2m 15s\tremaining: 50m 27s\n",
      "427:\tlearn: 11.0101444\ttotal: 2m 15s\tremaining: 50m 26s\n",
      "428:\tlearn: 11.0001870\ttotal: 2m 15s\tremaining: 50m 26s\n",
      "429:\tlearn: 10.9912401\ttotal: 2m 16s\tremaining: 50m 27s\n",
      "430:\tlearn: 10.9850096\ttotal: 2m 16s\tremaining: 50m 26s\n",
      "431:\tlearn: 10.9801397\ttotal: 2m 16s\tremaining: 50m 24s\n",
      "432:\tlearn: 10.9737744\ttotal: 2m 16s\tremaining: 50m 24s\n",
      "433:\tlearn: 10.9671717\ttotal: 2m 17s\tremaining: 50m 24s\n",
      "434:\tlearn: 10.9606439\ttotal: 2m 17s\tremaining: 50m 25s\n",
      "435:\tlearn: 10.9554396\ttotal: 2m 17s\tremaining: 50m 25s\n",
      "436:\tlearn: 10.9465108\ttotal: 2m 18s\tremaining: 50m 24s\n",
      "437:\tlearn: 10.9401974\ttotal: 2m 18s\tremaining: 50m 23s\n",
      "438:\tlearn: 10.9342050\ttotal: 2m 18s\tremaining: 50m 22s\n",
      "439:\tlearn: 10.9288405\ttotal: 2m 19s\tremaining: 50m 21s\n",
      "440:\tlearn: 10.9243283\ttotal: 2m 19s\tremaining: 50m 20s\n",
      "441:\tlearn: 10.9172641\ttotal: 2m 19s\tremaining: 50m 18s\n",
      "442:\tlearn: 10.9104432\ttotal: 2m 19s\tremaining: 50m 16s\n",
      "443:\tlearn: 10.9054185\ttotal: 2m 20s\tremaining: 50m 15s\n",
      "444:\tlearn: 10.8970134\ttotal: 2m 20s\tremaining: 50m 14s\n",
      "445:\tlearn: 10.8913649\ttotal: 2m 20s\tremaining: 50m 12s\n",
      "446:\tlearn: 10.8852907\ttotal: 2m 20s\tremaining: 50m 12s\n",
      "447:\tlearn: 10.8803545\ttotal: 2m 21s\tremaining: 50m 11s\n",
      "448:\tlearn: 10.8756802\ttotal: 2m 21s\tremaining: 50m 12s\n",
      "449:\tlearn: 10.8684642\ttotal: 2m 21s\tremaining: 50m 11s\n",
      "450:\tlearn: 10.8627320\ttotal: 2m 22s\tremaining: 50m 11s\n",
      "451:\tlearn: 10.8557546\ttotal: 2m 22s\tremaining: 50m 11s\n",
      "452:\tlearn: 10.8508348\ttotal: 2m 22s\tremaining: 50m 11s\n",
      "453:\tlearn: 10.8449795\ttotal: 2m 23s\tremaining: 50m 11s\n",
      "454:\tlearn: 10.8383334\ttotal: 2m 23s\tremaining: 50m 12s\n",
      "455:\tlearn: 10.8320935\ttotal: 2m 23s\tremaining: 50m 9s\n",
      "456:\tlearn: 10.8277939\ttotal: 2m 24s\tremaining: 50m 8s\n",
      "457:\tlearn: 10.8214815\ttotal: 2m 24s\tremaining: 50m 8s\n",
      "458:\tlearn: 10.8144791\ttotal: 2m 24s\tremaining: 50m 7s\n",
      "459:\tlearn: 10.8093628\ttotal: 2m 24s\tremaining: 50m 5s\n",
      "460:\tlearn: 10.8032989\ttotal: 2m 25s\tremaining: 50m 4s\n",
      "461:\tlearn: 10.7946463\ttotal: 2m 25s\tremaining: 50m 3s\n",
      "462:\tlearn: 10.7886325\ttotal: 2m 25s\tremaining: 50m 2s\n",
      "463:\tlearn: 10.7841159\ttotal: 2m 26s\tremaining: 50m 1s\n",
      "464:\tlearn: 10.7786370\ttotal: 2m 26s\tremaining: 50m\n",
      "465:\tlearn: 10.7690745\ttotal: 2m 26s\tremaining: 49m 59s\n",
      "466:\tlearn: 10.7615942\ttotal: 2m 26s\tremaining: 49m 59s\n",
      "467:\tlearn: 10.7547283\ttotal: 2m 27s\tremaining: 49m 58s\n",
      "468:\tlearn: 10.7489017\ttotal: 2m 27s\tremaining: 49m 58s\n",
      "469:\tlearn: 10.7448376\ttotal: 2m 27s\tremaining: 49m 56s\n",
      "470:\tlearn: 10.7391692\ttotal: 2m 28s\tremaining: 49m 55s\n",
      "471:\tlearn: 10.7299757\ttotal: 2m 28s\tremaining: 49m 55s\n",
      "472:\tlearn: 10.7231904\ttotal: 2m 28s\tremaining: 49m 53s\n",
      "473:\tlearn: 10.7167424\ttotal: 2m 28s\tremaining: 49m 53s\n",
      "474:\tlearn: 10.7128745\ttotal: 2m 29s\tremaining: 49m 51s\n",
      "475:\tlearn: 10.7065674\ttotal: 2m 29s\tremaining: 49m 51s\n",
      "476:\tlearn: 10.6997886\ttotal: 2m 29s\tremaining: 49m 50s\n",
      "477:\tlearn: 10.6942909\ttotal: 2m 30s\tremaining: 49m 49s\n",
      "478:\tlearn: 10.6890356\ttotal: 2m 30s\tremaining: 49m 49s\n",
      "479:\tlearn: 10.6835496\ttotal: 2m 30s\tremaining: 49m 47s\n",
      "480:\tlearn: 10.6775820\ttotal: 2m 30s\tremaining: 49m 46s\n",
      "481:\tlearn: 10.6714552\ttotal: 2m 31s\tremaining: 49m 45s\n",
      "482:\tlearn: 10.6658218\ttotal: 2m 31s\tremaining: 49m 46s\n",
      "483:\tlearn: 10.6593177\ttotal: 2m 31s\tremaining: 49m 46s\n",
      "484:\tlearn: 10.6545623\ttotal: 2m 32s\tremaining: 49m 46s\n",
      "485:\tlearn: 10.6494911\ttotal: 2m 32s\tremaining: 49m 45s\n",
      "486:\tlearn: 10.6436137\ttotal: 2m 32s\tremaining: 49m 46s\n",
      "487:\tlearn: 10.6386096\ttotal: 2m 33s\tremaining: 49m 46s\n",
      "488:\tlearn: 10.6338508\ttotal: 2m 33s\tremaining: 49m 46s\n",
      "489:\tlearn: 10.6297663\ttotal: 2m 33s\tremaining: 49m 45s\n",
      "490:\tlearn: 10.6249678\ttotal: 2m 34s\tremaining: 49m 43s\n",
      "491:\tlearn: 10.6190088\ttotal: 2m 34s\tremaining: 49m 42s\n",
      "492:\tlearn: 10.6141162\ttotal: 2m 34s\tremaining: 49m 40s\n",
      "493:\tlearn: 10.6081528\ttotal: 2m 34s\tremaining: 49m 40s\n",
      "494:\tlearn: 10.6040177\ttotal: 2m 35s\tremaining: 49m 39s\n",
      "495:\tlearn: 10.5997786\ttotal: 2m 35s\tremaining: 49m 38s\n",
      "496:\tlearn: 10.5941138\ttotal: 2m 35s\tremaining: 49m 37s\n",
      "497:\tlearn: 10.5901066\ttotal: 2m 35s\tremaining: 49m 36s\n",
      "498:\tlearn: 10.5837378\ttotal: 2m 36s\tremaining: 49m 37s\n",
      "499:\tlearn: 10.5759174\ttotal: 2m 36s\tremaining: 49m 37s\n",
      "500:\tlearn: 10.5696111\ttotal: 2m 36s\tremaining: 49m 36s\n",
      "501:\tlearn: 10.5658539\ttotal: 2m 37s\tremaining: 49m 35s\n",
      "502:\tlearn: 10.5605656\ttotal: 2m 37s\tremaining: 49m 35s\n",
      "503:\tlearn: 10.5514689\ttotal: 2m 37s\tremaining: 49m 35s\n",
      "504:\tlearn: 10.5461056\ttotal: 2m 38s\tremaining: 49m 34s\n",
      "505:\tlearn: 10.5411129\ttotal: 2m 38s\tremaining: 49m 33s\n",
      "506:\tlearn: 10.5356019\ttotal: 2m 38s\tremaining: 49m 31s\n",
      "507:\tlearn: 10.5311893\ttotal: 2m 38s\tremaining: 49m 30s\n",
      "508:\tlearn: 10.5256500\ttotal: 2m 39s\tremaining: 49m 31s\n",
      "509:\tlearn: 10.5208035\ttotal: 2m 39s\tremaining: 49m 31s\n",
      "510:\tlearn: 10.5157037\ttotal: 2m 40s\tremaining: 49m 31s\n",
      "511:\tlearn: 10.5118804\ttotal: 2m 40s\tremaining: 49m 30s\n",
      "512:\tlearn: 10.5077344\ttotal: 2m 40s\tremaining: 49m 28s\n",
      "513:\tlearn: 10.4993592\ttotal: 2m 40s\tremaining: 49m 28s\n",
      "514:\tlearn: 10.4937767\ttotal: 2m 41s\tremaining: 49m 27s\n",
      "515:\tlearn: 10.4893457\ttotal: 2m 41s\tremaining: 49m 26s\n",
      "516:\tlearn: 10.4830112\ttotal: 2m 41s\tremaining: 49m 27s\n",
      "517:\tlearn: 10.4786654\ttotal: 2m 42s\tremaining: 49m 27s\n",
      "518:\tlearn: 10.4739088\ttotal: 2m 42s\tremaining: 49m 28s\n",
      "519:\tlearn: 10.4685286\ttotal: 2m 42s\tremaining: 49m 28s\n",
      "520:\tlearn: 10.4602676\ttotal: 2m 43s\tremaining: 49m 28s\n",
      "521:\tlearn: 10.4545614\ttotal: 2m 43s\tremaining: 49m 28s\n",
      "522:\tlearn: 10.4504988\ttotal: 2m 43s\tremaining: 49m 27s\n",
      "523:\tlearn: 10.4460629\ttotal: 2m 44s\tremaining: 49m 27s\n",
      "524:\tlearn: 10.4405391\ttotal: 2m 44s\tremaining: 49m 27s\n",
      "525:\tlearn: 10.4347104\ttotal: 2m 44s\tremaining: 49m 27s\n",
      "526:\tlearn: 10.4283953\ttotal: 2m 45s\tremaining: 49m 28s\n",
      "527:\tlearn: 10.4237643\ttotal: 2m 45s\tremaining: 49m 27s\n",
      "528:\tlearn: 10.4189215\ttotal: 2m 45s\tremaining: 49m 27s\n",
      "529:\tlearn: 10.4142947\ttotal: 2m 46s\tremaining: 49m 26s\n",
      "530:\tlearn: 10.4097463\ttotal: 2m 46s\tremaining: 49m 26s\n",
      "531:\tlearn: 10.4043736\ttotal: 2m 46s\tremaining: 49m 25s\n",
      "532:\tlearn: 10.3988458\ttotal: 2m 47s\tremaining: 49m 26s\n",
      "533:\tlearn: 10.3932567\ttotal: 2m 47s\tremaining: 49m 24s\n",
      "534:\tlearn: 10.3872972\ttotal: 2m 47s\tremaining: 49m 23s\n",
      "535:\tlearn: 10.3824784\ttotal: 2m 47s\tremaining: 49m 23s\n",
      "536:\tlearn: 10.3753368\ttotal: 2m 48s\tremaining: 49m 23s\n",
      "537:\tlearn: 10.3720544\ttotal: 2m 48s\tremaining: 49m 22s\n",
      "538:\tlearn: 10.3659714\ttotal: 2m 48s\tremaining: 49m 21s\n",
      "539:\tlearn: 10.3588953\ttotal: 2m 49s\tremaining: 49m 20s\n",
      "540:\tlearn: 10.3545988\ttotal: 2m 49s\tremaining: 49m 22s\n",
      "541:\tlearn: 10.3512987\ttotal: 2m 49s\tremaining: 49m 21s\n",
      "542:\tlearn: 10.3469031\ttotal: 2m 49s\tremaining: 49m 19s\n",
      "543:\tlearn: 10.3438602\ttotal: 2m 50s\tremaining: 49m 18s\n",
      "544:\tlearn: 10.3396883\ttotal: 2m 50s\tremaining: 49m 16s\n",
      "545:\tlearn: 10.3332770\ttotal: 2m 50s\tremaining: 49m 15s\n",
      "546:\tlearn: 10.3280872\ttotal: 2m 51s\tremaining: 49m 15s\n",
      "547:\tlearn: 10.3232034\ttotal: 2m 51s\tremaining: 49m 14s\n",
      "548:\tlearn: 10.3177627\ttotal: 2m 51s\tremaining: 49m 14s\n",
      "549:\tlearn: 10.3103071\ttotal: 2m 51s\tremaining: 49m 14s\n",
      "550:\tlearn: 10.3060791\ttotal: 2m 52s\tremaining: 49m 15s\n",
      "551:\tlearn: 10.3023660\ttotal: 2m 52s\tremaining: 49m 13s\n",
      "552:\tlearn: 10.2982514\ttotal: 2m 52s\tremaining: 49m 13s\n",
      "553:\tlearn: 10.2937631\ttotal: 2m 53s\tremaining: 49m 13s\n",
      "554:\tlearn: 10.2889376\ttotal: 2m 53s\tremaining: 49m 13s\n",
      "555:\tlearn: 10.2850658\ttotal: 2m 53s\tremaining: 49m 12s\n",
      "556:\tlearn: 10.2814115\ttotal: 2m 54s\tremaining: 49m 12s\n",
      "557:\tlearn: 10.2738302\ttotal: 2m 54s\tremaining: 49m 11s\n",
      "558:\tlearn: 10.2661060\ttotal: 2m 54s\tremaining: 49m 11s\n",
      "559:\tlearn: 10.2615787\ttotal: 2m 54s\tremaining: 49m 9s\n",
      "560:\tlearn: 10.2550441\ttotal: 2m 55s\tremaining: 49m 9s\n",
      "561:\tlearn: 10.2501076\ttotal: 2m 55s\tremaining: 49m 8s\n",
      "562:\tlearn: 10.2445112\ttotal: 2m 55s\tremaining: 49m 7s\n",
      "563:\tlearn: 10.2397654\ttotal: 2m 56s\tremaining: 49m 6s\n",
      "564:\tlearn: 10.2330524\ttotal: 2m 56s\tremaining: 49m 6s\n",
      "565:\tlearn: 10.2287123\ttotal: 2m 56s\tremaining: 49m 6s\n",
      "566:\tlearn: 10.2245084\ttotal: 2m 57s\tremaining: 49m 5s\n",
      "567:\tlearn: 10.2194813\ttotal: 2m 57s\tremaining: 49m 5s\n",
      "568:\tlearn: 10.2146603\ttotal: 2m 57s\tremaining: 49m 4s\n",
      "569:\tlearn: 10.2109518\ttotal: 2m 57s\tremaining: 49m 4s\n",
      "570:\tlearn: 10.2060264\ttotal: 2m 58s\tremaining: 49m 3s\n",
      "571:\tlearn: 10.2016195\ttotal: 2m 58s\tremaining: 49m 2s\n",
      "572:\tlearn: 10.1959009\ttotal: 2m 58s\tremaining: 49m 2s\n",
      "573:\tlearn: 10.1907108\ttotal: 2m 59s\tremaining: 49m 2s\n",
      "574:\tlearn: 10.1832480\ttotal: 2m 59s\tremaining: 49m 2s\n",
      "575:\tlearn: 10.1775661\ttotal: 2m 59s\tremaining: 49m 1s\n",
      "576:\tlearn: 10.1735732\ttotal: 3m\tremaining: 49m 1s\n",
      "577:\tlearn: 10.1701374\ttotal: 3m\tremaining: 49m\n",
      "578:\tlearn: 10.1630244\ttotal: 3m\tremaining: 49m\n",
      "579:\tlearn: 10.1597300\ttotal: 3m\tremaining: 48m 59s\n",
      "580:\tlearn: 10.1554741\ttotal: 3m 1s\tremaining: 49m\n",
      "581:\tlearn: 10.1509012\ttotal: 3m 1s\tremaining: 49m\n",
      "582:\tlearn: 10.1464702\ttotal: 3m 2s\tremaining: 49m\n",
      "583:\tlearn: 10.1409884\ttotal: 3m 2s\tremaining: 49m\n",
      "584:\tlearn: 10.1366820\ttotal: 3m 2s\tremaining: 48m 58s\n",
      "585:\tlearn: 10.1308219\ttotal: 3m 2s\tremaining: 48m 59s\n",
      "586:\tlearn: 10.1265343\ttotal: 3m 3s\tremaining: 48m 58s\n",
      "587:\tlearn: 10.1237057\ttotal: 3m 3s\tremaining: 48m 57s\n",
      "588:\tlearn: 10.1200720\ttotal: 3m 3s\tremaining: 48m 56s\n",
      "589:\tlearn: 10.1137312\ttotal: 3m 4s\tremaining: 48m 55s\n",
      "590:\tlearn: 10.1107112\ttotal: 3m 4s\tremaining: 48m 54s\n",
      "591:\tlearn: 10.1036509\ttotal: 3m 4s\tremaining: 48m 55s\n",
      "592:\tlearn: 10.0998858\ttotal: 3m 5s\tremaining: 48m 54s\n",
      "593:\tlearn: 10.0945273\ttotal: 3m 5s\tremaining: 48m 54s\n",
      "594:\tlearn: 10.0891977\ttotal: 3m 5s\tremaining: 48m 54s\n",
      "595:\tlearn: 10.0858888\ttotal: 3m 5s\tremaining: 48m 52s\n",
      "596:\tlearn: 10.0825075\ttotal: 3m 6s\tremaining: 48m 52s\n",
      "597:\tlearn: 10.0760390\ttotal: 3m 6s\tremaining: 48m 52s\n",
      "598:\tlearn: 10.0691800\ttotal: 3m 6s\tremaining: 48m 52s\n",
      "599:\tlearn: 10.0652579\ttotal: 3m 7s\tremaining: 48m 52s\n",
      "600:\tlearn: 10.0602738\ttotal: 3m 7s\tremaining: 48m 50s\n",
      "601:\tlearn: 10.0538017\ttotal: 3m 7s\tremaining: 48m 50s\n",
      "602:\tlearn: 10.0493910\ttotal: 3m 7s\tremaining: 48m 49s\n",
      "603:\tlearn: 10.0466599\ttotal: 3m 8s\tremaining: 48m 48s\n",
      "604:\tlearn: 10.0414351\ttotal: 3m 8s\tremaining: 48m 47s\n",
      "605:\tlearn: 10.0378277\ttotal: 3m 8s\tremaining: 48m 46s\n",
      "606:\tlearn: 10.0355203\ttotal: 3m 9s\tremaining: 48m 45s\n",
      "607:\tlearn: 10.0311899\ttotal: 3m 9s\tremaining: 48m 43s\n",
      "608:\tlearn: 10.0262413\ttotal: 3m 9s\tremaining: 48m 43s\n",
      "609:\tlearn: 10.0217351\ttotal: 3m 9s\tremaining: 48m 43s\n",
      "610:\tlearn: 10.0187810\ttotal: 3m 10s\tremaining: 48m 42s\n",
      "611:\tlearn: 10.0148465\ttotal: 3m 10s\tremaining: 48m 40s\n",
      "612:\tlearn: 10.0105186\ttotal: 3m 10s\tremaining: 48m 39s\n",
      "613:\tlearn: 10.0072692\ttotal: 3m 10s\tremaining: 48m 38s\n",
      "614:\tlearn: 10.0029631\ttotal: 3m 11s\tremaining: 48m 37s\n",
      "615:\tlearn: 9.9990580\ttotal: 3m 11s\tremaining: 48m 35s\n",
      "616:\tlearn: 9.9949536\ttotal: 3m 11s\tremaining: 48m 34s\n",
      "617:\tlearn: 9.9919166\ttotal: 3m 11s\tremaining: 48m 34s\n",
      "618:\tlearn: 9.9890626\ttotal: 3m 12s\tremaining: 48m 33s\n",
      "619:\tlearn: 9.9842976\ttotal: 3m 12s\tremaining: 48m 31s\n",
      "620:\tlearn: 9.9812204\ttotal: 3m 12s\tremaining: 48m 30s\n",
      "621:\tlearn: 9.9775978\ttotal: 3m 12s\tremaining: 48m 29s\n",
      "622:\tlearn: 9.9732536\ttotal: 3m 13s\tremaining: 48m 29s\n",
      "623:\tlearn: 9.9689177\ttotal: 3m 13s\tremaining: 48m 28s\n",
      "624:\tlearn: 9.9649155\ttotal: 3m 13s\tremaining: 48m 27s\n",
      "625:\tlearn: 9.9614018\ttotal: 3m 14s\tremaining: 48m 26s\n",
      "626:\tlearn: 9.9566864\ttotal: 3m 14s\tremaining: 48m 25s\n",
      "627:\tlearn: 9.9538436\ttotal: 3m 14s\tremaining: 48m 24s\n",
      "628:\tlearn: 9.9498341\ttotal: 3m 14s\tremaining: 48m 23s\n",
      "629:\tlearn: 9.9455358\ttotal: 3m 15s\tremaining: 48m 23s\n",
      "630:\tlearn: 9.9410409\ttotal: 3m 15s\tremaining: 48m 22s\n",
      "631:\tlearn: 9.9370980\ttotal: 3m 15s\tremaining: 48m 21s\n",
      "632:\tlearn: 9.9334745\ttotal: 3m 16s\tremaining: 48m 20s\n",
      "633:\tlearn: 9.9277754\ttotal: 3m 16s\tremaining: 48m 19s\n",
      "634:\tlearn: 9.9247996\ttotal: 3m 16s\tremaining: 48m 18s\n",
      "635:\tlearn: 9.9203657\ttotal: 3m 16s\tremaining: 48m 17s\n",
      "636:\tlearn: 9.9158948\ttotal: 3m 17s\tremaining: 48m 16s\n",
      "637:\tlearn: 9.9117274\ttotal: 3m 17s\tremaining: 48m 15s\n",
      "638:\tlearn: 9.9073746\ttotal: 3m 17s\tremaining: 48m 14s\n",
      "639:\tlearn: 9.9025324\ttotal: 3m 17s\tremaining: 48m 14s\n",
      "640:\tlearn: 9.8990649\ttotal: 3m 18s\tremaining: 48m 14s\n",
      "641:\tlearn: 9.8939295\ttotal: 3m 18s\tremaining: 48m 13s\n",
      "642:\tlearn: 9.8879460\ttotal: 3m 18s\tremaining: 48m 12s\n",
      "643:\tlearn: 9.8848214\ttotal: 3m 19s\tremaining: 48m 11s\n",
      "644:\tlearn: 9.8814297\ttotal: 3m 19s\tremaining: 48m 9s\n",
      "645:\tlearn: 9.8763189\ttotal: 3m 19s\tremaining: 48m 9s\n",
      "646:\tlearn: 9.8712808\ttotal: 3m 19s\tremaining: 48m 8s\n",
      "647:\tlearn: 9.8670161\ttotal: 3m 20s\tremaining: 48m 8s\n",
      "648:\tlearn: 9.8618650\ttotal: 3m 20s\tremaining: 48m 7s\n",
      "649:\tlearn: 9.8578839\ttotal: 3m 20s\tremaining: 48m 6s\n",
      "650:\tlearn: 9.8537679\ttotal: 3m 21s\tremaining: 48m 6s\n",
      "651:\tlearn: 9.8500770\ttotal: 3m 21s\tremaining: 48m 6s\n",
      "652:\tlearn: 9.8441995\ttotal: 3m 21s\tremaining: 48m 5s\n",
      "653:\tlearn: 9.8409088\ttotal: 3m 21s\tremaining: 48m 5s\n",
      "654:\tlearn: 9.8380601\ttotal: 3m 22s\tremaining: 48m 4s\n",
      "655:\tlearn: 9.8337157\ttotal: 3m 22s\tremaining: 48m 2s\n",
      "656:\tlearn: 9.8290521\ttotal: 3m 22s\tremaining: 48m 2s\n",
      "657:\tlearn: 9.8249463\ttotal: 3m 22s\tremaining: 48m 1s\n",
      "658:\tlearn: 9.8205207\ttotal: 3m 23s\tremaining: 48m\n",
      "659:\tlearn: 9.8166231\ttotal: 3m 23s\tremaining: 47m 58s\n",
      "660:\tlearn: 9.8124780\ttotal: 3m 23s\tremaining: 47m 58s\n",
      "661:\tlearn: 9.8098965\ttotal: 3m 23s\tremaining: 47m 56s\n",
      "662:\tlearn: 9.8069464\ttotal: 3m 24s\tremaining: 47m 55s\n",
      "663:\tlearn: 9.8028737\ttotal: 3m 24s\tremaining: 47m 54s\n",
      "664:\tlearn: 9.8002249\ttotal: 3m 24s\tremaining: 47m 53s\n",
      "665:\tlearn: 9.7965726\ttotal: 3m 24s\tremaining: 47m 52s\n",
      "666:\tlearn: 9.7925591\ttotal: 3m 25s\tremaining: 47m 51s\n",
      "667:\tlearn: 9.7865403\ttotal: 3m 25s\tremaining: 47m 51s\n",
      "668:\tlearn: 9.7799770\ttotal: 3m 25s\tremaining: 47m 50s\n",
      "669:\tlearn: 9.7756149\ttotal: 3m 26s\tremaining: 47m 51s\n",
      "670:\tlearn: 9.7707217\ttotal: 3m 26s\tremaining: 47m 50s\n",
      "671:\tlearn: 9.7665219\ttotal: 3m 26s\tremaining: 47m 50s\n",
      "672:\tlearn: 9.7625802\ttotal: 3m 27s\tremaining: 47m 49s\n",
      "673:\tlearn: 9.7592061\ttotal: 3m 27s\tremaining: 47m 48s\n",
      "674:\tlearn: 9.7549150\ttotal: 3m 27s\tremaining: 47m 47s\n",
      "675:\tlearn: 9.7506099\ttotal: 3m 27s\tremaining: 47m 47s\n",
      "676:\tlearn: 9.7466679\ttotal: 3m 28s\tremaining: 47m 47s\n",
      "677:\tlearn: 9.7423714\ttotal: 3m 28s\tremaining: 47m 47s\n",
      "678:\tlearn: 9.7365522\ttotal: 3m 28s\tremaining: 47m 46s\n",
      "679:\tlearn: 9.7320396\ttotal: 3m 29s\tremaining: 47m 45s\n",
      "680:\tlearn: 9.7281444\ttotal: 3m 29s\tremaining: 47m 44s\n",
      "681:\tlearn: 9.7223951\ttotal: 3m 29s\tremaining: 47m 44s\n",
      "682:\tlearn: 9.7187733\ttotal: 3m 29s\tremaining: 47m 44s\n",
      "683:\tlearn: 9.7146932\ttotal: 3m 30s\tremaining: 47m 43s\n",
      "684:\tlearn: 9.7105395\ttotal: 3m 30s\tremaining: 47m 42s\n",
      "685:\tlearn: 9.7069958\ttotal: 3m 30s\tremaining: 47m 41s\n",
      "686:\tlearn: 9.7043065\ttotal: 3m 30s\tremaining: 47m 40s\n",
      "687:\tlearn: 9.7020800\ttotal: 3m 31s\tremaining: 47m 38s\n",
      "688:\tlearn: 9.6974768\ttotal: 3m 31s\tremaining: 47m 37s\n",
      "689:\tlearn: 9.6948658\ttotal: 3m 31s\tremaining: 47m 36s\n",
      "690:\tlearn: 9.6902584\ttotal: 3m 31s\tremaining: 47m 35s\n",
      "691:\tlearn: 9.6854747\ttotal: 3m 32s\tremaining: 47m 35s\n",
      "692:\tlearn: 9.6816288\ttotal: 3m 32s\tremaining: 47m 35s\n",
      "693:\tlearn: 9.6779639\ttotal: 3m 32s\tremaining: 47m 34s\n",
      "694:\tlearn: 9.6751463\ttotal: 3m 33s\tremaining: 47m 33s\n",
      "695:\tlearn: 9.6711156\ttotal: 3m 33s\tremaining: 47m 33s\n",
      "696:\tlearn: 9.6677348\ttotal: 3m 33s\tremaining: 47m 32s\n",
      "697:\tlearn: 9.6646165\ttotal: 3m 34s\tremaining: 47m 31s\n",
      "698:\tlearn: 9.6613701\ttotal: 3m 34s\tremaining: 47m 31s\n",
      "699:\tlearn: 9.6570206\ttotal: 3m 34s\tremaining: 47m 30s\n",
      "700:\tlearn: 9.6529912\ttotal: 3m 34s\tremaining: 47m 30s\n",
      "701:\tlearn: 9.6492325\ttotal: 3m 35s\tremaining: 47m 30s\n",
      "702:\tlearn: 9.6455370\ttotal: 3m 35s\tremaining: 47m 28s\n",
      "703:\tlearn: 9.6417630\ttotal: 3m 35s\tremaining: 47m 28s\n",
      "704:\tlearn: 9.6376597\ttotal: 3m 36s\tremaining: 47m 28s\n",
      "705:\tlearn: 9.6334967\ttotal: 3m 36s\tremaining: 47m 28s\n",
      "706:\tlearn: 9.6277316\ttotal: 3m 36s\tremaining: 47m 28s\n",
      "707:\tlearn: 9.6242226\ttotal: 3m 36s\tremaining: 47m 27s\n",
      "708:\tlearn: 9.6210933\ttotal: 3m 37s\tremaining: 47m 26s\n",
      "709:\tlearn: 9.6176303\ttotal: 3m 37s\tremaining: 47m 26s\n",
      "710:\tlearn: 9.6138177\ttotal: 3m 37s\tremaining: 47m 24s\n",
      "711:\tlearn: 9.6103297\ttotal: 3m 38s\tremaining: 47m 24s\n",
      "712:\tlearn: 9.6067130\ttotal: 3m 38s\tremaining: 47m 24s\n",
      "713:\tlearn: 9.6032645\ttotal: 3m 38s\tremaining: 47m 23s\n",
      "714:\tlearn: 9.5995381\ttotal: 3m 38s\tremaining: 47m 22s\n",
      "715:\tlearn: 9.5966216\ttotal: 3m 39s\tremaining: 47m 21s\n",
      "716:\tlearn: 9.5912075\ttotal: 3m 39s\tremaining: 47m 20s\n",
      "717:\tlearn: 9.5883625\ttotal: 3m 39s\tremaining: 47m 19s\n",
      "718:\tlearn: 9.5848708\ttotal: 3m 39s\tremaining: 47m 18s\n",
      "719:\tlearn: 9.5801517\ttotal: 3m 40s\tremaining: 47m 18s\n",
      "720:\tlearn: 9.5763658\ttotal: 3m 40s\tremaining: 47m 17s\n",
      "721:\tlearn: 9.5732395\ttotal: 3m 40s\tremaining: 47m 16s\n",
      "722:\tlearn: 9.5695945\ttotal: 3m 41s\tremaining: 47m 16s\n",
      "723:\tlearn: 9.5665789\ttotal: 3m 41s\tremaining: 47m 15s\n",
      "724:\tlearn: 9.5627261\ttotal: 3m 41s\tremaining: 47m 14s\n",
      "725:\tlearn: 9.5587046\ttotal: 3m 41s\tremaining: 47m 14s\n",
      "726:\tlearn: 9.5543875\ttotal: 3m 42s\tremaining: 47m 14s\n",
      "727:\tlearn: 9.5495529\ttotal: 3m 42s\tremaining: 47m 14s\n",
      "728:\tlearn: 9.5466302\ttotal: 3m 42s\tremaining: 47m 14s\n",
      "729:\tlearn: 9.5441560\ttotal: 3m 43s\tremaining: 47m 12s\n",
      "730:\tlearn: 9.5418454\ttotal: 3m 43s\tremaining: 47m 11s\n",
      "731:\tlearn: 9.5386631\ttotal: 3m 43s\tremaining: 47m 12s\n",
      "732:\tlearn: 9.5350194\ttotal: 3m 43s\tremaining: 47m 11s\n",
      "733:\tlearn: 9.5324755\ttotal: 3m 44s\tremaining: 47m 9s\n",
      "734:\tlearn: 9.5278175\ttotal: 3m 44s\tremaining: 47m 9s\n",
      "735:\tlearn: 9.5247964\ttotal: 3m 44s\tremaining: 47m 8s\n",
      "736:\tlearn: 9.5201209\ttotal: 3m 45s\tremaining: 47m 8s\n",
      "737:\tlearn: 9.5153758\ttotal: 3m 45s\tremaining: 47m 8s\n",
      "738:\tlearn: 9.5104900\ttotal: 3m 45s\tremaining: 47m 8s\n",
      "739:\tlearn: 9.5082927\ttotal: 3m 45s\tremaining: 47m 7s\n",
      "740:\tlearn: 9.5057479\ttotal: 3m 46s\tremaining: 47m 6s\n",
      "741:\tlearn: 9.5017423\ttotal: 3m 46s\tremaining: 47m 5s\n",
      "742:\tlearn: 9.4983222\ttotal: 3m 46s\tremaining: 47m 4s\n",
      "743:\tlearn: 9.4931783\ttotal: 3m 47s\tremaining: 47m 4s\n",
      "744:\tlearn: 9.4898371\ttotal: 3m 47s\tremaining: 47m 4s\n",
      "745:\tlearn: 9.4862247\ttotal: 3m 47s\tremaining: 47m 3s\n",
      "746:\tlearn: 9.4835916\ttotal: 3m 47s\tremaining: 47m 2s\n",
      "747:\tlearn: 9.4811844\ttotal: 3m 48s\tremaining: 47m 1s\n",
      "748:\tlearn: 9.4772245\ttotal: 3m 48s\tremaining: 47m 1s\n",
      "749:\tlearn: 9.4734326\ttotal: 3m 48s\tremaining: 47m 1s\n",
      "750:\tlearn: 9.4694851\ttotal: 3m 49s\tremaining: 47m 1s\n",
      "751:\tlearn: 9.4662942\ttotal: 3m 49s\tremaining: 47m\n",
      "752:\tlearn: 9.4634390\ttotal: 3m 49s\tremaining: 46m 59s\n",
      "753:\tlearn: 9.4610852\ttotal: 3m 49s\tremaining: 46m 58s\n",
      "754:\tlearn: 9.4579566\ttotal: 3m 50s\tremaining: 46m 58s\n",
      "755:\tlearn: 9.4530982\ttotal: 3m 50s\tremaining: 46m 57s\n",
      "756:\tlearn: 9.4499423\ttotal: 3m 50s\tremaining: 46m 56s\n",
      "757:\tlearn: 9.4463004\ttotal: 3m 50s\tremaining: 46m 55s\n",
      "758:\tlearn: 9.4440619\ttotal: 3m 51s\tremaining: 46m 54s\n",
      "759:\tlearn: 9.4405458\ttotal: 3m 51s\tremaining: 46m 53s\n",
      "760:\tlearn: 9.4356842\ttotal: 3m 51s\tremaining: 46m 52s\n",
      "761:\tlearn: 9.4321956\ttotal: 3m 51s\tremaining: 46m 52s\n",
      "762:\tlearn: 9.4292653\ttotal: 3m 52s\tremaining: 46m 51s\n",
      "763:\tlearn: 9.4256993\ttotal: 3m 52s\tremaining: 46m 51s\n",
      "764:\tlearn: 9.4195626\ttotal: 3m 52s\tremaining: 46m 51s\n",
      "765:\tlearn: 9.4160774\ttotal: 3m 53s\tremaining: 46m 51s\n",
      "766:\tlearn: 9.4141452\ttotal: 3m 53s\tremaining: 46m 49s\n",
      "767:\tlearn: 9.4104447\ttotal: 3m 53s\tremaining: 46m 49s\n",
      "768:\tlearn: 9.4070448\ttotal: 3m 54s\tremaining: 46m 49s\n",
      "769:\tlearn: 9.4033208\ttotal: 3m 54s\tremaining: 46m 48s\n",
      "770:\tlearn: 9.4012010\ttotal: 3m 54s\tremaining: 46m 46s\n",
      "771:\tlearn: 9.3980433\ttotal: 3m 54s\tremaining: 46m 46s\n",
      "772:\tlearn: 9.3939393\ttotal: 3m 55s\tremaining: 46m 45s\n",
      "773:\tlearn: 9.3896953\ttotal: 3m 55s\tremaining: 46m 44s\n",
      "774:\tlearn: 9.3865360\ttotal: 3m 55s\tremaining: 46m 44s\n",
      "775:\tlearn: 9.3818184\ttotal: 3m 55s\tremaining: 46m 43s\n",
      "776:\tlearn: 9.3792722\ttotal: 3m 56s\tremaining: 46m 42s\n",
      "777:\tlearn: 9.3755400\ttotal: 3m 56s\tremaining: 46m 42s\n",
      "778:\tlearn: 9.3725734\ttotal: 3m 56s\tremaining: 46m 41s\n",
      "779:\tlearn: 9.3701263\ttotal: 3m 56s\tremaining: 46m 40s\n",
      "780:\tlearn: 9.3672085\ttotal: 3m 57s\tremaining: 46m 39s\n",
      "781:\tlearn: 9.3650614\ttotal: 3m 57s\tremaining: 46m 38s\n",
      "782:\tlearn: 9.3610441\ttotal: 3m 57s\tremaining: 46m 38s\n",
      "783:\tlearn: 9.3582021\ttotal: 3m 58s\tremaining: 46m 37s\n",
      "784:\tlearn: 9.3560972\ttotal: 3m 58s\tremaining: 46m 36s\n",
      "785:\tlearn: 9.3515139\ttotal: 3m 58s\tremaining: 46m 35s\n",
      "786:\tlearn: 9.3466992\ttotal: 3m 58s\tremaining: 46m 35s\n",
      "787:\tlearn: 9.3428963\ttotal: 3m 59s\tremaining: 46m 34s\n",
      "788:\tlearn: 9.3385918\ttotal: 3m 59s\tremaining: 46m 34s\n",
      "789:\tlearn: 9.3345762\ttotal: 3m 59s\tremaining: 46m 34s\n",
      "790:\tlearn: 9.3317889\ttotal: 3m 59s\tremaining: 46m 33s\n",
      "791:\tlearn: 9.3284812\ttotal: 4m\tremaining: 46m 33s\n",
      "792:\tlearn: 9.3257830\ttotal: 4m\tremaining: 46m 32s\n",
      "793:\tlearn: 9.3238046\ttotal: 4m\tremaining: 46m 31s\n",
      "794:\tlearn: 9.3200860\ttotal: 4m 1s\tremaining: 46m 31s\n",
      "795:\tlearn: 9.3166418\ttotal: 4m 1s\tremaining: 46m 31s\n",
      "796:\tlearn: 9.3128895\ttotal: 4m 1s\tremaining: 46m 30s\n",
      "797:\tlearn: 9.3094140\ttotal: 4m 2s\tremaining: 46m 30s\n",
      "798:\tlearn: 9.3050784\ttotal: 4m 2s\tremaining: 46m 30s\n",
      "799:\tlearn: 9.3010038\ttotal: 4m 2s\tremaining: 46m 29s\n",
      "800:\tlearn: 9.2977900\ttotal: 4m 2s\tremaining: 46m 28s\n",
      "801:\tlearn: 9.2951807\ttotal: 4m 3s\tremaining: 46m 27s\n",
      "802:\tlearn: 9.2920190\ttotal: 4m 3s\tremaining: 46m 27s\n",
      "803:\tlearn: 9.2886525\ttotal: 4m 3s\tremaining: 46m 26s\n",
      "804:\tlearn: 9.2858418\ttotal: 4m 3s\tremaining: 46m 25s\n",
      "805:\tlearn: 9.2836552\ttotal: 4m 4s\tremaining: 46m 24s\n",
      "806:\tlearn: 9.2798750\ttotal: 4m 4s\tremaining: 46m 24s\n",
      "807:\tlearn: 9.2762410\ttotal: 4m 4s\tremaining: 46m 24s\n",
      "808:\tlearn: 9.2734207\ttotal: 4m 5s\tremaining: 46m 23s\n",
      "809:\tlearn: 9.2697923\ttotal: 4m 5s\tremaining: 46m 23s\n",
      "810:\tlearn: 9.2670087\ttotal: 4m 5s\tremaining: 46m 24s\n",
      "811:\tlearn: 9.2622314\ttotal: 4m 5s\tremaining: 46m 23s\n",
      "812:\tlearn: 9.2585903\ttotal: 4m 6s\tremaining: 46m 23s\n",
      "813:\tlearn: 9.2539157\ttotal: 4m 6s\tremaining: 46m 22s\n",
      "814:\tlearn: 9.2497005\ttotal: 4m 6s\tremaining: 46m 21s\n",
      "815:\tlearn: 9.2461734\ttotal: 4m 7s\tremaining: 46m 20s\n",
      "816:\tlearn: 9.2433406\ttotal: 4m 7s\tremaining: 46m 19s\n",
      "817:\tlearn: 9.2410120\ttotal: 4m 7s\tremaining: 46m 18s\n",
      "818:\tlearn: 9.2374769\ttotal: 4m 7s\tremaining: 46m 18s\n",
      "819:\tlearn: 9.2346879\ttotal: 4m 8s\tremaining: 46m 18s\n",
      "820:\tlearn: 9.2311150\ttotal: 4m 8s\tremaining: 46m 18s\n",
      "821:\tlearn: 9.2279058\ttotal: 4m 8s\tremaining: 46m 18s\n",
      "822:\tlearn: 9.2244252\ttotal: 4m 9s\tremaining: 46m 18s\n",
      "823:\tlearn: 9.2204544\ttotal: 4m 9s\tremaining: 46m 17s\n",
      "824:\tlearn: 9.2172348\ttotal: 4m 9s\tremaining: 46m 16s\n",
      "825:\tlearn: 9.2138875\ttotal: 4m 9s\tremaining: 46m 16s\n",
      "826:\tlearn: 9.2116585\ttotal: 4m 10s\tremaining: 46m 15s\n",
      "827:\tlearn: 9.2092785\ttotal: 4m 10s\tremaining: 46m 14s\n",
      "828:\tlearn: 9.2066574\ttotal: 4m 10s\tremaining: 46m 14s\n",
      "829:\tlearn: 9.2014400\ttotal: 4m 11s\tremaining: 46m 13s\n",
      "830:\tlearn: 9.1990575\ttotal: 4m 11s\tremaining: 46m 12s\n",
      "831:\tlearn: 9.1960385\ttotal: 4m 11s\tremaining: 46m 11s\n",
      "832:\tlearn: 9.1917785\ttotal: 4m 11s\tremaining: 46m 11s\n",
      "833:\tlearn: 9.1887935\ttotal: 4m 12s\tremaining: 46m 10s\n",
      "834:\tlearn: 9.1858449\ttotal: 4m 12s\tremaining: 46m 9s\n",
      "835:\tlearn: 9.1838989\ttotal: 4m 12s\tremaining: 46m 8s\n",
      "836:\tlearn: 9.1808437\ttotal: 4m 12s\tremaining: 46m 8s\n",
      "837:\tlearn: 9.1775625\ttotal: 4m 13s\tremaining: 46m 8s\n",
      "838:\tlearn: 9.1732629\ttotal: 4m 13s\tremaining: 46m 8s\n",
      "839:\tlearn: 9.1686617\ttotal: 4m 13s\tremaining: 46m 7s\n",
      "840:\tlearn: 9.1664099\ttotal: 4m 14s\tremaining: 46m 6s\n",
      "841:\tlearn: 9.1625463\ttotal: 4m 14s\tremaining: 46m 6s\n",
      "842:\tlearn: 9.1595684\ttotal: 4m 14s\tremaining: 46m 5s\n",
      "843:\tlearn: 9.1573451\ttotal: 4m 14s\tremaining: 46m 5s\n",
      "844:\tlearn: 9.1549485\ttotal: 4m 15s\tremaining: 46m 5s\n",
      "845:\tlearn: 9.1513375\ttotal: 4m 15s\tremaining: 46m 5s\n",
      "846:\tlearn: 9.1483697\ttotal: 4m 15s\tremaining: 46m 4s\n",
      "847:\tlearn: 9.1445141\ttotal: 4m 16s\tremaining: 46m 3s\n",
      "848:\tlearn: 9.1420982\ttotal: 4m 16s\tremaining: 46m 2s\n",
      "849:\tlearn: 9.1374379\ttotal: 4m 16s\tremaining: 46m 2s\n",
      "850:\tlearn: 9.1339708\ttotal: 4m 16s\tremaining: 46m 1s\n",
      "851:\tlearn: 9.1314567\ttotal: 4m 17s\tremaining: 46m\n",
      "852:\tlearn: 9.1282331\ttotal: 4m 17s\tremaining: 46m\n",
      "853:\tlearn: 9.1259032\ttotal: 4m 17s\tremaining: 45m 59s\n",
      "854:\tlearn: 9.1238088\ttotal: 4m 17s\tremaining: 45m 58s\n",
      "855:\tlearn: 9.1207032\ttotal: 4m 18s\tremaining: 45m 58s\n",
      "856:\tlearn: 9.1172683\ttotal: 4m 18s\tremaining: 45m 58s\n",
      "857:\tlearn: 9.1150408\ttotal: 4m 18s\tremaining: 45m 57s\n",
      "858:\tlearn: 9.1127992\ttotal: 4m 19s\tremaining: 45m 56s\n",
      "859:\tlearn: 9.1085115\ttotal: 4m 19s\tremaining: 45m 56s\n",
      "860:\tlearn: 9.1038485\ttotal: 4m 19s\tremaining: 45m 57s\n",
      "861:\tlearn: 9.1007362\ttotal: 4m 20s\tremaining: 45m 57s\n",
      "862:\tlearn: 9.0973788\ttotal: 4m 20s\tremaining: 45m 57s\n",
      "863:\tlearn: 9.0945112\ttotal: 4m 20s\tremaining: 45m 56s\n",
      "864:\tlearn: 9.0911374\ttotal: 4m 21s\tremaining: 45m 56s\n",
      "865:\tlearn: 9.0868340\ttotal: 4m 21s\tremaining: 45m 56s\n",
      "866:\tlearn: 9.0840947\ttotal: 4m 21s\tremaining: 45m 56s\n",
      "867:\tlearn: 9.0809966\ttotal: 4m 22s\tremaining: 45m 57s\n",
      "868:\tlearn: 9.0780567\ttotal: 4m 22s\tremaining: 45m 57s\n",
      "869:\tlearn: 9.0763884\ttotal: 4m 22s\tremaining: 45m 57s\n",
      "870:\tlearn: 9.0735069\ttotal: 4m 23s\tremaining: 45m 57s\n",
      "871:\tlearn: 9.0708122\ttotal: 4m 23s\tremaining: 45m 56s\n",
      "872:\tlearn: 9.0663819\ttotal: 4m 23s\tremaining: 45m 56s\n",
      "873:\tlearn: 9.0627678\ttotal: 4m 23s\tremaining: 45m 56s\n",
      "874:\tlearn: 9.0592534\ttotal: 4m 24s\tremaining: 45m 56s\n",
      "875:\tlearn: 9.0564919\ttotal: 4m 24s\tremaining: 45m 55s\n",
      "876:\tlearn: 9.0546837\ttotal: 4m 24s\tremaining: 45m 55s\n",
      "877:\tlearn: 9.0513540\ttotal: 4m 25s\tremaining: 45m 55s\n",
      "878:\tlearn: 9.0485205\ttotal: 4m 25s\tremaining: 45m 54s\n",
      "879:\tlearn: 9.0451049\ttotal: 4m 25s\tremaining: 45m 54s\n",
      "880:\tlearn: 9.0422919\ttotal: 4m 26s\tremaining: 45m 53s\n",
      "881:\tlearn: 9.0390753\ttotal: 4m 26s\tremaining: 45m 54s\n",
      "882:\tlearn: 9.0363201\ttotal: 4m 26s\tremaining: 45m 54s\n",
      "883:\tlearn: 9.0334604\ttotal: 4m 27s\tremaining: 45m 55s\n",
      "884:\tlearn: 9.0302539\ttotal: 4m 27s\tremaining: 45m 55s\n",
      "885:\tlearn: 9.0285662\ttotal: 4m 27s\tremaining: 45m 54s\n",
      "886:\tlearn: 9.0259403\ttotal: 4m 28s\tremaining: 45m 54s\n",
      "887:\tlearn: 9.0237782\ttotal: 4m 28s\tremaining: 45m 53s\n",
      "888:\tlearn: 9.0211595\ttotal: 4m 28s\tremaining: 45m 54s\n",
      "889:\tlearn: 9.0185954\ttotal: 4m 29s\tremaining: 45m 54s\n",
      "890:\tlearn: 9.0148178\ttotal: 4m 29s\tremaining: 45m 54s\n",
      "891:\tlearn: 9.0115693\ttotal: 4m 29s\tremaining: 45m 53s\n",
      "892:\tlearn: 9.0090350\ttotal: 4m 30s\tremaining: 45m 53s\n",
      "893:\tlearn: 9.0061032\ttotal: 4m 30s\tremaining: 45m 53s\n",
      "894:\tlearn: 9.0036583\ttotal: 4m 30s\tremaining: 45m 53s\n",
      "895:\tlearn: 9.0011110\ttotal: 4m 30s\tremaining: 45m 52s\n",
      "896:\tlearn: 8.9985662\ttotal: 4m 31s\tremaining: 45m 51s\n",
      "897:\tlearn: 8.9945138\ttotal: 4m 31s\tremaining: 45m 51s\n",
      "898:\tlearn: 8.9919404\ttotal: 4m 31s\tremaining: 45m 51s\n",
      "899:\tlearn: 8.9889239\ttotal: 4m 32s\tremaining: 45m 51s\n",
      "900:\tlearn: 8.9859092\ttotal: 4m 32s\tremaining: 45m 50s\n",
      "901:\tlearn: 8.9831721\ttotal: 4m 32s\tremaining: 45m 51s\n",
      "902:\tlearn: 8.9797915\ttotal: 4m 33s\tremaining: 45m 51s\n",
      "903:\tlearn: 8.9775507\ttotal: 4m 33s\tremaining: 45m 50s\n",
      "904:\tlearn: 8.9758129\ttotal: 4m 33s\tremaining: 45m 49s\n",
      "905:\tlearn: 8.9730719\ttotal: 4m 33s\tremaining: 45m 48s\n",
      "906:\tlearn: 8.9701940\ttotal: 4m 34s\tremaining: 45m 48s\n",
      "907:\tlearn: 8.9681186\ttotal: 4m 34s\tremaining: 45m 48s\n",
      "908:\tlearn: 8.9651272\ttotal: 4m 34s\tremaining: 45m 48s\n",
      "909:\tlearn: 8.9621640\ttotal: 4m 35s\tremaining: 45m 48s\n",
      "910:\tlearn: 8.9594660\ttotal: 4m 35s\tremaining: 45m 47s\n",
      "911:\tlearn: 8.9579003\ttotal: 4m 35s\tremaining: 45m 47s\n",
      "912:\tlearn: 8.9548407\ttotal: 4m 36s\tremaining: 45m 47s\n",
      "913:\tlearn: 8.9518105\ttotal: 4m 36s\tremaining: 45m 47s\n",
      "914:\tlearn: 8.9503263\ttotal: 4m 36s\tremaining: 45m 46s\n",
      "915:\tlearn: 8.9472546\ttotal: 4m 36s\tremaining: 45m 46s\n",
      "916:\tlearn: 8.9446004\ttotal: 4m 37s\tremaining: 45m 45s\n",
      "917:\tlearn: 8.9420873\ttotal: 4m 37s\tremaining: 45m 45s\n",
      "918:\tlearn: 8.9397753\ttotal: 4m 37s\tremaining: 45m 45s\n",
      "919:\tlearn: 8.9374124\ttotal: 4m 38s\tremaining: 45m 44s\n",
      "920:\tlearn: 8.9348422\ttotal: 4m 38s\tremaining: 45m 43s\n",
      "921:\tlearn: 8.9322167\ttotal: 4m 38s\tremaining: 45m 43s\n",
      "922:\tlearn: 8.9295800\ttotal: 4m 38s\tremaining: 45m 42s\n",
      "923:\tlearn: 8.9264643\ttotal: 4m 39s\tremaining: 45m 43s\n",
      "924:\tlearn: 8.9233896\ttotal: 4m 39s\tremaining: 45m 42s\n",
      "925:\tlearn: 8.9210187\ttotal: 4m 39s\tremaining: 45m 42s\n",
      "926:\tlearn: 8.9179186\ttotal: 4m 40s\tremaining: 45m 41s\n",
      "927:\tlearn: 8.9153737\ttotal: 4m 40s\tremaining: 45m 40s\n",
      "928:\tlearn: 8.9127061\ttotal: 4m 40s\tremaining: 45m 40s\n",
      "929:\tlearn: 8.9107479\ttotal: 4m 40s\tremaining: 45m 40s\n",
      "930:\tlearn: 8.9073967\ttotal: 4m 41s\tremaining: 45m 40s\n",
      "931:\tlearn: 8.9046953\ttotal: 4m 41s\tremaining: 45m 40s\n",
      "932:\tlearn: 8.9026106\ttotal: 4m 41s\tremaining: 45m 39s\n",
      "933:\tlearn: 8.9004173\ttotal: 4m 42s\tremaining: 45m 39s\n",
      "934:\tlearn: 8.8981204\ttotal: 4m 42s\tremaining: 45m 39s\n",
      "935:\tlearn: 8.8947598\ttotal: 4m 42s\tremaining: 45m 39s\n",
      "936:\tlearn: 8.8917317\ttotal: 4m 43s\tremaining: 45m 39s\n",
      "937:\tlearn: 8.8889870\ttotal: 4m 43s\tremaining: 45m 39s\n",
      "938:\tlearn: 8.8854527\ttotal: 4m 43s\tremaining: 45m 39s\n",
      "939:\tlearn: 8.8809798\ttotal: 4m 44s\tremaining: 45m 40s\n",
      "940:\tlearn: 8.8785132\ttotal: 4m 44s\tremaining: 45m 39s\n",
      "941:\tlearn: 8.8758225\ttotal: 4m 44s\tremaining: 45m 39s\n",
      "942:\tlearn: 8.8735500\ttotal: 4m 45s\tremaining: 45m 38s\n",
      "943:\tlearn: 8.8707614\ttotal: 4m 45s\tremaining: 45m 39s\n",
      "944:\tlearn: 8.8678793\ttotal: 4m 45s\tremaining: 45m 38s\n",
      "945:\tlearn: 8.8639122\ttotal: 4m 46s\tremaining: 45m 38s\n",
      "946:\tlearn: 8.8612547\ttotal: 4m 46s\tremaining: 45m 38s\n",
      "947:\tlearn: 8.8575873\ttotal: 4m 46s\tremaining: 45m 38s\n",
      "948:\tlearn: 8.8539441\ttotal: 4m 47s\tremaining: 45m 39s\n",
      "949:\tlearn: 8.8509366\ttotal: 4m 47s\tremaining: 45m 38s\n",
      "950:\tlearn: 8.8487046\ttotal: 4m 47s\tremaining: 45m 38s\n",
      "951:\tlearn: 8.8458397\ttotal: 4m 48s\tremaining: 45m 38s\n",
      "952:\tlearn: 8.8433780\ttotal: 4m 48s\tremaining: 45m 37s\n",
      "953:\tlearn: 8.8394315\ttotal: 4m 48s\tremaining: 45m 37s\n",
      "954:\tlearn: 8.8368846\ttotal: 4m 49s\tremaining: 45m 37s\n",
      "955:\tlearn: 8.8346641\ttotal: 4m 49s\tremaining: 45m 36s\n",
      "956:\tlearn: 8.8308495\ttotal: 4m 49s\tremaining: 45m 36s\n",
      "957:\tlearn: 8.8280243\ttotal: 4m 49s\tremaining: 45m 36s\n",
      "958:\tlearn: 8.8259956\ttotal: 4m 50s\tremaining: 45m 36s\n",
      "959:\tlearn: 8.8224719\ttotal: 4m 50s\tremaining: 45m 36s\n",
      "960:\tlearn: 8.8201662\ttotal: 4m 50s\tremaining: 45m 35s\n",
      "961:\tlearn: 8.8161617\ttotal: 4m 51s\tremaining: 45m 35s\n",
      "962:\tlearn: 8.8120850\ttotal: 4m 51s\tremaining: 45m 36s\n",
      "963:\tlearn: 8.8082037\ttotal: 4m 51s\tremaining: 45m 36s\n",
      "964:\tlearn: 8.8051018\ttotal: 4m 52s\tremaining: 45m 36s\n",
      "965:\tlearn: 8.8025643\ttotal: 4m 52s\tremaining: 45m 37s\n",
      "966:\tlearn: 8.8009763\ttotal: 4m 53s\tremaining: 45m 37s\n",
      "967:\tlearn: 8.7985062\ttotal: 4m 53s\tremaining: 45m 36s\n",
      "968:\tlearn: 8.7967302\ttotal: 4m 53s\tremaining: 45m 35s\n",
      "969:\tlearn: 8.7946756\ttotal: 4m 53s\tremaining: 45m 35s\n",
      "970:\tlearn: 8.7922994\ttotal: 4m 54s\tremaining: 45m 34s\n",
      "971:\tlearn: 8.7888528\ttotal: 4m 54s\tremaining: 45m 35s\n",
      "972:\tlearn: 8.7853851\ttotal: 4m 54s\tremaining: 45m 35s\n",
      "973:\tlearn: 8.7824685\ttotal: 4m 55s\tremaining: 45m 34s\n",
      "974:\tlearn: 8.7802081\ttotal: 4m 55s\tremaining: 45m 34s\n",
      "975:\tlearn: 8.7781172\ttotal: 4m 55s\tremaining: 45m 33s\n",
      "976:\tlearn: 8.7754430\ttotal: 4m 56s\tremaining: 45m 34s\n",
      "977:\tlearn: 8.7729191\ttotal: 4m 56s\tremaining: 45m 35s\n",
      "978:\tlearn: 8.7700436\ttotal: 4m 56s\tremaining: 45m 35s\n",
      "979:\tlearn: 8.7677487\ttotal: 4m 57s\tremaining: 45m 35s\n",
      "980:\tlearn: 8.7653747\ttotal: 4m 57s\tremaining: 45m 35s\n",
      "981:\tlearn: 8.7632184\ttotal: 4m 57s\tremaining: 45m 35s\n",
      "982:\tlearn: 8.7612429\ttotal: 4m 58s\tremaining: 45m 34s\n",
      "983:\tlearn: 8.7592367\ttotal: 4m 58s\tremaining: 45m 33s\n",
      "984:\tlearn: 8.7570684\ttotal: 4m 58s\tremaining: 45m 33s\n",
      "985:\tlearn: 8.7554544\ttotal: 4m 58s\tremaining: 45m 32s\n",
      "986:\tlearn: 8.7526027\ttotal: 4m 59s\tremaining: 45m 32s\n",
      "987:\tlearn: 8.7501011\ttotal: 4m 59s\tremaining: 45m 31s\n",
      "988:\tlearn: 8.7466786\ttotal: 4m 59s\tremaining: 45m 32s\n",
      "989:\tlearn: 8.7428850\ttotal: 5m\tremaining: 45m 32s\n",
      "990:\tlearn: 8.7403997\ttotal: 5m\tremaining: 45m 33s\n",
      "991:\tlearn: 8.7382896\ttotal: 5m 1s\tremaining: 45m 33s\n",
      "992:\tlearn: 8.7349769\ttotal: 5m 1s\tremaining: 45m 33s\n",
      "993:\tlearn: 8.7326763\ttotal: 5m 1s\tremaining: 45m 32s\n",
      "994:\tlearn: 8.7305504\ttotal: 5m 1s\tremaining: 45m 32s\n",
      "995:\tlearn: 8.7271401\ttotal: 5m 2s\tremaining: 45m 31s\n",
      "996:\tlearn: 8.7230886\ttotal: 5m 2s\tremaining: 45m 32s\n",
      "997:\tlearn: 8.7208622\ttotal: 5m 2s\tremaining: 45m 31s\n",
      "998:\tlearn: 8.7187093\ttotal: 5m 3s\tremaining: 45m 31s\n",
      "999:\tlearn: 8.7164703\ttotal: 5m 3s\tremaining: 45m 31s\n",
      "1000:\tlearn: 8.7126170\ttotal: 5m 3s\tremaining: 45m 30s\n",
      "1001:\tlearn: 8.7094536\ttotal: 5m 4s\tremaining: 45m 31s\n",
      "1002:\tlearn: 8.7068956\ttotal: 5m 4s\tremaining: 45m 30s\n",
      "1003:\tlearn: 8.7045937\ttotal: 5m 4s\tremaining: 45m 30s\n",
      "1004:\tlearn: 8.7021507\ttotal: 5m 5s\tremaining: 45m 30s\n",
      "1005:\tlearn: 8.6998389\ttotal: 5m 5s\tremaining: 45m 29s\n",
      "1006:\tlearn: 8.6977718\ttotal: 5m 5s\tremaining: 45m 29s\n",
      "1007:\tlearn: 8.6953150\ttotal: 5m 5s\tremaining: 45m 29s\n",
      "1008:\tlearn: 8.6931863\ttotal: 5m 6s\tremaining: 45m 29s\n",
      "1009:\tlearn: 8.6915406\ttotal: 5m 6s\tremaining: 45m 29s\n",
      "1010:\tlearn: 8.6895721\ttotal: 5m 6s\tremaining: 45m 28s\n",
      "1011:\tlearn: 8.6880073\ttotal: 5m 7s\tremaining: 45m 27s\n",
      "1012:\tlearn: 8.6849424\ttotal: 5m 7s\tremaining: 45m 28s\n",
      "1013:\tlearn: 8.6827412\ttotal: 5m 7s\tremaining: 45m 27s\n",
      "1014:\tlearn: 8.6800292\ttotal: 5m 8s\tremaining: 45m 28s\n",
      "1015:\tlearn: 8.6783790\ttotal: 5m 8s\tremaining: 45m 27s\n",
      "1016:\tlearn: 8.6770359\ttotal: 5m 8s\tremaining: 45m 26s\n",
      "1017:\tlearn: 8.6750732\ttotal: 5m 9s\tremaining: 45m 26s\n",
      "1018:\tlearn: 8.6728695\ttotal: 5m 9s\tremaining: 45m 25s\n",
      "1019:\tlearn: 8.6706271\ttotal: 5m 9s\tremaining: 45m 25s\n",
      "1020:\tlearn: 8.6680814\ttotal: 5m 9s\tremaining: 45m 25s\n",
      "1021:\tlearn: 8.6660193\ttotal: 5m 10s\tremaining: 45m 25s\n",
      "1022:\tlearn: 8.6638431\ttotal: 5m 10s\tremaining: 45m 25s\n",
      "1023:\tlearn: 8.6614330\ttotal: 5m 10s\tremaining: 45m 24s\n",
      "1024:\tlearn: 8.6591495\ttotal: 5m 11s\tremaining: 45m 24s\n",
      "1025:\tlearn: 8.6572388\ttotal: 5m 11s\tremaining: 45m 24s\n",
      "1026:\tlearn: 8.6552877\ttotal: 5m 11s\tremaining: 45m 23s\n",
      "1027:\tlearn: 8.6525427\ttotal: 5m 12s\tremaining: 45m 24s\n",
      "1028:\tlearn: 8.6495615\ttotal: 5m 12s\tremaining: 45m 24s\n",
      "1029:\tlearn: 8.6475485\ttotal: 5m 12s\tremaining: 45m 24s\n",
      "1030:\tlearn: 8.6460523\ttotal: 5m 13s\tremaining: 45m 24s\n",
      "1031:\tlearn: 8.6442981\ttotal: 5m 13s\tremaining: 45m 23s\n",
      "1032:\tlearn: 8.6413878\ttotal: 5m 13s\tremaining: 45m 23s\n",
      "1033:\tlearn: 8.6397002\ttotal: 5m 14s\tremaining: 45m 23s\n",
      "1034:\tlearn: 8.6365388\ttotal: 5m 14s\tremaining: 45m 22s\n",
      "1035:\tlearn: 8.6340663\ttotal: 5m 14s\tremaining: 45m 22s\n",
      "1036:\tlearn: 8.6311499\ttotal: 5m 14s\tremaining: 45m 22s\n",
      "1037:\tlearn: 8.6294053\ttotal: 5m 15s\tremaining: 45m 21s\n",
      "1038:\tlearn: 8.6257283\ttotal: 5m 15s\tremaining: 45m 21s\n",
      "1039:\tlearn: 8.6233250\ttotal: 5m 15s\tremaining: 45m 20s\n",
      "1040:\tlearn: 8.6214478\ttotal: 5m 16s\tremaining: 45m 20s\n",
      "1041:\tlearn: 8.6191321\ttotal: 5m 16s\tremaining: 45m 20s\n",
      "1042:\tlearn: 8.6163241\ttotal: 5m 16s\tremaining: 45m 20s\n",
      "1043:\tlearn: 8.6135283\ttotal: 5m 17s\tremaining: 45m 19s\n",
      "1044:\tlearn: 8.6113198\ttotal: 5m 17s\tremaining: 45m 19s\n",
      "1045:\tlearn: 8.6088211\ttotal: 5m 17s\tremaining: 45m 19s\n",
      "1046:\tlearn: 8.6069470\ttotal: 5m 17s\tremaining: 45m 18s\n",
      "1047:\tlearn: 8.6044166\ttotal: 5m 18s\tremaining: 45m 18s\n",
      "1048:\tlearn: 8.6017044\ttotal: 5m 18s\tremaining: 45m 18s\n",
      "1049:\tlearn: 8.5994647\ttotal: 5m 18s\tremaining: 45m 17s\n",
      "1050:\tlearn: 8.5969594\ttotal: 5m 19s\tremaining: 45m 17s\n",
      "1051:\tlearn: 8.5948246\ttotal: 5m 19s\tremaining: 45m 17s\n",
      "1052:\tlearn: 8.5912117\ttotal: 5m 19s\tremaining: 45m 18s\n",
      "1053:\tlearn: 8.5889242\ttotal: 5m 20s\tremaining: 45m 17s\n",
      "1054:\tlearn: 8.5868295\ttotal: 5m 20s\tremaining: 45m 17s\n",
      "1055:\tlearn: 8.5846737\ttotal: 5m 20s\tremaining: 45m 17s\n",
      "1056:\tlearn: 8.5822721\ttotal: 5m 21s\tremaining: 45m 16s\n",
      "1057:\tlearn: 8.5799165\ttotal: 5m 21s\tremaining: 45m 16s\n",
      "1058:\tlearn: 8.5772148\ttotal: 5m 21s\tremaining: 45m 16s\n",
      "1059:\tlearn: 8.5742309\ttotal: 5m 22s\tremaining: 45m 16s\n",
      "1060:\tlearn: 8.5725224\ttotal: 5m 22s\tremaining: 45m 16s\n",
      "1061:\tlearn: 8.5695650\ttotal: 5m 22s\tremaining: 45m 17s\n",
      "1062:\tlearn: 8.5663158\ttotal: 5m 23s\tremaining: 45m 17s\n",
      "1063:\tlearn: 8.5646669\ttotal: 5m 23s\tremaining: 45m 17s\n",
      "1064:\tlearn: 8.5626513\ttotal: 5m 23s\tremaining: 45m 16s\n",
      "1065:\tlearn: 8.5599037\ttotal: 5m 24s\tremaining: 45m 16s\n",
      "1066:\tlearn: 8.5572145\ttotal: 5m 24s\tremaining: 45m 15s\n",
      "1067:\tlearn: 8.5551627\ttotal: 5m 24s\tremaining: 45m 15s\n",
      "1068:\tlearn: 8.5534407\ttotal: 5m 24s\tremaining: 45m 14s\n",
      "1069:\tlearn: 8.5512432\ttotal: 5m 25s\tremaining: 45m 14s\n",
      "1070:\tlearn: 8.5485005\ttotal: 5m 25s\tremaining: 45m 13s\n",
      "1071:\tlearn: 8.5468524\ttotal: 5m 25s\tremaining: 45m 12s\n",
      "1072:\tlearn: 8.5442045\ttotal: 5m 26s\tremaining: 45m 12s\n",
      "1073:\tlearn: 8.5415104\ttotal: 5m 26s\tremaining: 45m 12s\n",
      "1074:\tlearn: 8.5392216\ttotal: 5m 26s\tremaining: 45m 12s\n",
      "1075:\tlearn: 8.5373204\ttotal: 5m 26s\tremaining: 45m 11s\n",
      "1076:\tlearn: 8.5347115\ttotal: 5m 27s\tremaining: 45m 11s\n",
      "1077:\tlearn: 8.5333039\ttotal: 5m 27s\tremaining: 45m 10s\n",
      "1078:\tlearn: 8.5308468\ttotal: 5m 27s\tremaining: 45m 11s\n",
      "1079:\tlearn: 8.5281754\ttotal: 5m 28s\tremaining: 45m 11s\n",
      "1080:\tlearn: 8.5260043\ttotal: 5m 28s\tremaining: 45m 11s\n",
      "1081:\tlearn: 8.5242874\ttotal: 5m 28s\tremaining: 45m 11s\n",
      "1082:\tlearn: 8.5210605\ttotal: 5m 29s\tremaining: 45m 12s\n",
      "1083:\tlearn: 8.5190665\ttotal: 5m 29s\tremaining: 45m 11s\n",
      "1084:\tlearn: 8.5172373\ttotal: 5m 29s\tremaining: 45m 11s\n",
      "1085:\tlearn: 8.5157538\ttotal: 5m 30s\tremaining: 45m 10s\n",
      "1086:\tlearn: 8.5129039\ttotal: 5m 30s\tremaining: 45m 11s\n",
      "1087:\tlearn: 8.5106802\ttotal: 5m 30s\tremaining: 45m 10s\n",
      "1088:\tlearn: 8.5070961\ttotal: 5m 31s\tremaining: 45m 10s\n",
      "1089:\tlearn: 8.5045343\ttotal: 5m 31s\tremaining: 45m 10s\n",
      "1090:\tlearn: 8.5017619\ttotal: 5m 31s\tremaining: 45m 10s\n",
      "1091:\tlearn: 8.4998226\ttotal: 5m 32s\tremaining: 45m 10s\n",
      "1092:\tlearn: 8.4964101\ttotal: 5m 32s\tremaining: 45m 9s\n",
      "1093:\tlearn: 8.4942604\ttotal: 5m 32s\tremaining: 45m 9s\n",
      "1094:\tlearn: 8.4908991\ttotal: 5m 33s\tremaining: 45m 10s\n",
      "1095:\tlearn: 8.4886445\ttotal: 5m 33s\tremaining: 45m 9s\n",
      "1096:\tlearn: 8.4865950\ttotal: 5m 33s\tremaining: 45m 9s\n",
      "1097:\tlearn: 8.4848735\ttotal: 5m 34s\tremaining: 45m 9s\n",
      "1098:\tlearn: 8.4829789\ttotal: 5m 34s\tremaining: 45m 8s\n",
      "1099:\tlearn: 8.4807114\ttotal: 5m 34s\tremaining: 45m 8s\n",
      "1100:\tlearn: 8.4793075\ttotal: 5m 34s\tremaining: 45m 7s\n",
      "1101:\tlearn: 8.4761335\ttotal: 5m 35s\tremaining: 45m 6s\n",
      "1102:\tlearn: 8.4734821\ttotal: 5m 35s\tremaining: 45m 6s\n",
      "1103:\tlearn: 8.4715585\ttotal: 5m 35s\tremaining: 45m 6s\n",
      "1104:\tlearn: 8.4693946\ttotal: 5m 36s\tremaining: 45m 5s\n",
      "1105:\tlearn: 8.4673934\ttotal: 5m 36s\tremaining: 45m 5s\n",
      "1106:\tlearn: 8.4655046\ttotal: 5m 36s\tremaining: 45m 5s\n",
      "1107:\tlearn: 8.4639446\ttotal: 5m 36s\tremaining: 45m 4s\n",
      "1108:\tlearn: 8.4621170\ttotal: 5m 37s\tremaining: 45m 3s\n",
      "1109:\tlearn: 8.4594214\ttotal: 5m 37s\tremaining: 45m 3s\n",
      "1110:\tlearn: 8.4571155\ttotal: 5m 37s\tremaining: 45m 2s\n",
      "1111:\tlearn: 8.4542358\ttotal: 5m 38s\tremaining: 45m 2s\n",
      "1112:\tlearn: 8.4514682\ttotal: 5m 38s\tremaining: 45m 3s\n",
      "1113:\tlearn: 8.4496984\ttotal: 5m 38s\tremaining: 45m 3s\n",
      "1114:\tlearn: 8.4474546\ttotal: 5m 39s\tremaining: 45m 3s\n",
      "1115:\tlearn: 8.4448554\ttotal: 5m 39s\tremaining: 45m 3s\n",
      "1116:\tlearn: 8.4425135\ttotal: 5m 39s\tremaining: 45m 3s\n",
      "1117:\tlearn: 8.4401847\ttotal: 5m 40s\tremaining: 45m 3s\n",
      "1118:\tlearn: 8.4384084\ttotal: 5m 40s\tremaining: 45m 3s\n",
      "1119:\tlearn: 8.4366002\ttotal: 5m 40s\tremaining: 45m 2s\n",
      "1120:\tlearn: 8.4345451\ttotal: 5m 41s\tremaining: 45m 1s\n",
      "1121:\tlearn: 8.4323552\ttotal: 5m 41s\tremaining: 45m 1s\n",
      "1122:\tlearn: 8.4297925\ttotal: 5m 41s\tremaining: 45m 1s\n",
      "1123:\tlearn: 8.4280030\ttotal: 5m 41s\tremaining: 45m\n",
      "1124:\tlearn: 8.4258161\ttotal: 5m 42s\tremaining: 45m\n",
      "1125:\tlearn: 8.4234930\ttotal: 5m 42s\tremaining: 44m 59s\n",
      "1126:\tlearn: 8.4217275\ttotal: 5m 42s\tremaining: 44m 58s\n",
      "1127:\tlearn: 8.4189090\ttotal: 5m 43s\tremaining: 44m 59s\n",
      "1128:\tlearn: 8.4166939\ttotal: 5m 43s\tremaining: 44m 58s\n",
      "1129:\tlearn: 8.4151917\ttotal: 5m 43s\tremaining: 44m 58s\n",
      "1130:\tlearn: 8.4127283\ttotal: 5m 44s\tremaining: 44m 57s\n",
      "1131:\tlearn: 8.4105956\ttotal: 5m 44s\tremaining: 44m 57s\n",
      "1132:\tlearn: 8.4081585\ttotal: 5m 44s\tremaining: 44m 57s\n",
      "1133:\tlearn: 8.4053066\ttotal: 5m 45s\tremaining: 44m 58s\n",
      "1134:\tlearn: 8.4028344\ttotal: 5m 45s\tremaining: 44m 58s\n",
      "1135:\tlearn: 8.4007177\ttotal: 5m 45s\tremaining: 44m 57s\n",
      "1136:\tlearn: 8.3977053\ttotal: 5m 46s\tremaining: 44m 58s\n",
      "1137:\tlearn: 8.3956614\ttotal: 5m 46s\tremaining: 44m 57s\n",
      "1138:\tlearn: 8.3941161\ttotal: 5m 46s\tremaining: 44m 57s\n",
      "1139:\tlearn: 8.3919872\ttotal: 5m 47s\tremaining: 44m 57s\n",
      "1140:\tlearn: 8.3893064\ttotal: 5m 47s\tremaining: 44m 56s\n",
      "1141:\tlearn: 8.3871209\ttotal: 5m 47s\tremaining: 44m 56s\n",
      "1142:\tlearn: 8.3841865\ttotal: 5m 47s\tremaining: 44m 56s\n",
      "1143:\tlearn: 8.3823224\ttotal: 5m 48s\tremaining: 44m 56s\n",
      "1144:\tlearn: 8.3805602\ttotal: 5m 48s\tremaining: 44m 55s\n",
      "1145:\tlearn: 8.3783794\ttotal: 5m 48s\tremaining: 44m 55s\n",
      "1146:\tlearn: 8.3751984\ttotal: 5m 49s\tremaining: 44m 55s\n",
      "1147:\tlearn: 8.3729959\ttotal: 5m 49s\tremaining: 44m 55s\n",
      "1148:\tlearn: 8.3711503\ttotal: 5m 49s\tremaining: 44m 55s\n",
      "1149:\tlearn: 8.3679473\ttotal: 5m 50s\tremaining: 44m 55s\n",
      "1150:\tlearn: 8.3662263\ttotal: 5m 50s\tremaining: 44m 55s\n",
      "1151:\tlearn: 8.3644723\ttotal: 5m 50s\tremaining: 44m 55s\n",
      "1152:\tlearn: 8.3628026\ttotal: 5m 51s\tremaining: 44m 54s\n",
      "1153:\tlearn: 8.3608098\ttotal: 5m 51s\tremaining: 44m 53s\n",
      "1154:\tlearn: 8.3581655\ttotal: 5m 51s\tremaining: 44m 53s\n",
      "1155:\tlearn: 8.3564380\ttotal: 5m 52s\tremaining: 44m 53s\n",
      "1156:\tlearn: 8.3544133\ttotal: 5m 52s\tremaining: 44m 52s\n",
      "1157:\tlearn: 8.3519458\ttotal: 5m 52s\tremaining: 44m 52s\n",
      "1158:\tlearn: 8.3503195\ttotal: 5m 52s\tremaining: 44m 51s\n",
      "1159:\tlearn: 8.3470923\ttotal: 5m 53s\tremaining: 44m 51s\n",
      "1160:\tlearn: 8.3452843\ttotal: 5m 53s\tremaining: 44m 51s\n",
      "1161:\tlearn: 8.3432316\ttotal: 5m 53s\tremaining: 44m 50s\n",
      "1162:\tlearn: 8.3414963\ttotal: 5m 54s\tremaining: 44m 50s\n",
      "1163:\tlearn: 8.3401507\ttotal: 5m 54s\tremaining: 44m 49s\n",
      "1164:\tlearn: 8.3375685\ttotal: 5m 54s\tremaining: 44m 49s\n",
      "1165:\tlearn: 8.3353666\ttotal: 5m 54s\tremaining: 44m 49s\n",
      "1166:\tlearn: 8.3339673\ttotal: 5m 55s\tremaining: 44m 48s\n",
      "1167:\tlearn: 8.3313010\ttotal: 5m 55s\tremaining: 44m 48s\n",
      "1168:\tlearn: 8.3293530\ttotal: 5m 55s\tremaining: 44m 48s\n",
      "1169:\tlearn: 8.3277387\ttotal: 5m 56s\tremaining: 44m 47s\n",
      "1170:\tlearn: 8.3262764\ttotal: 5m 56s\tremaining: 44m 47s\n",
      "1171:\tlearn: 8.3242620\ttotal: 5m 56s\tremaining: 44m 47s\n",
      "1172:\tlearn: 8.3221871\ttotal: 5m 57s\tremaining: 44m 46s\n",
      "1173:\tlearn: 8.3200666\ttotal: 5m 57s\tremaining: 44m 45s\n",
      "1174:\tlearn: 8.3180864\ttotal: 5m 57s\tremaining: 44m 45s\n",
      "1175:\tlearn: 8.3154397\ttotal: 5m 57s\tremaining: 44m 45s\n",
      "1176:\tlearn: 8.3128462\ttotal: 5m 58s\tremaining: 44m 44s\n",
      "1177:\tlearn: 8.3107681\ttotal: 5m 58s\tremaining: 44m 44s\n",
      "1178:\tlearn: 8.3073199\ttotal: 5m 58s\tremaining: 44m 44s\n",
      "1179:\tlearn: 8.3046370\ttotal: 5m 59s\tremaining: 44m 44s\n",
      "1180:\tlearn: 8.3028292\ttotal: 5m 59s\tremaining: 44m 44s\n",
      "1181:\tlearn: 8.3007153\ttotal: 5m 59s\tremaining: 44m 43s\n",
      "1182:\tlearn: 8.2983515\ttotal: 6m\tremaining: 44m 43s\n",
      "1183:\tlearn: 8.2957891\ttotal: 6m\tremaining: 44m 43s\n",
      "1184:\tlearn: 8.2939053\ttotal: 6m\tremaining: 44m 42s\n",
      "1185:\tlearn: 8.2911157\ttotal: 6m 1s\tremaining: 44m 43s\n",
      "1186:\tlearn: 8.2894719\ttotal: 6m 1s\tremaining: 44m 42s\n",
      "1187:\tlearn: 8.2872632\ttotal: 6m 1s\tremaining: 44m 41s\n",
      "1188:\tlearn: 8.2851830\ttotal: 6m 1s\tremaining: 44m 41s\n",
      "1189:\tlearn: 8.2826363\ttotal: 6m 2s\tremaining: 44m 40s\n",
      "1190:\tlearn: 8.2799055\ttotal: 6m 2s\tremaining: 44m 40s\n",
      "1191:\tlearn: 8.2783929\ttotal: 6m 2s\tremaining: 44m 40s\n",
      "1192:\tlearn: 8.2769266\ttotal: 6m 3s\tremaining: 44m 40s\n",
      "1193:\tlearn: 8.2743796\ttotal: 6m 3s\tremaining: 44m 39s\n",
      "1194:\tlearn: 8.2727291\ttotal: 6m 3s\tremaining: 44m 39s\n",
      "1195:\tlearn: 8.2710933\ttotal: 6m 3s\tremaining: 44m 38s\n",
      "1196:\tlearn: 8.2688841\ttotal: 6m 4s\tremaining: 44m 38s\n",
      "1197:\tlearn: 8.2665162\ttotal: 6m 4s\tremaining: 44m 38s\n",
      "1198:\tlearn: 8.2650969\ttotal: 6m 4s\tremaining: 44m 37s\n",
      "1199:\tlearn: 8.2637795\ttotal: 6m 5s\tremaining: 44m 37s\n",
      "1200:\tlearn: 8.2619959\ttotal: 6m 5s\tremaining: 44m 36s\n",
      "1201:\tlearn: 8.2606652\ttotal: 6m 5s\tremaining: 44m 35s\n",
      "1202:\tlearn: 8.2583421\ttotal: 6m 5s\tremaining: 44m 35s\n",
      "1203:\tlearn: 8.2564819\ttotal: 6m 6s\tremaining: 44m 35s\n",
      "1204:\tlearn: 8.2546025\ttotal: 6m 6s\tremaining: 44m 34s\n",
      "1205:\tlearn: 8.2522800\ttotal: 6m 6s\tremaining: 44m 34s\n",
      "1206:\tlearn: 8.2503908\ttotal: 6m 7s\tremaining: 44m 34s\n",
      "1207:\tlearn: 8.2490564\ttotal: 6m 7s\tremaining: 44m 33s\n",
      "1208:\tlearn: 8.2467127\ttotal: 6m 7s\tremaining: 44m 33s\n",
      "1209:\tlearn: 8.2444725\ttotal: 6m 7s\tremaining: 44m 33s\n",
      "1210:\tlearn: 8.2426116\ttotal: 6m 8s\tremaining: 44m 32s\n",
      "1211:\tlearn: 8.2406726\ttotal: 6m 8s\tremaining: 44m 31s\n",
      "1212:\tlearn: 8.2377871\ttotal: 6m 8s\tremaining: 44m 32s\n",
      "1213:\tlearn: 8.2361231\ttotal: 6m 9s\tremaining: 44m 31s\n",
      "1214:\tlearn: 8.2339008\ttotal: 6m 9s\tremaining: 44m 31s\n",
      "1215:\tlearn: 8.2316039\ttotal: 6m 9s\tremaining: 44m 30s\n",
      "1216:\tlearn: 8.2297489\ttotal: 6m 9s\tremaining: 44m 29s\n",
      "1217:\tlearn: 8.2281180\ttotal: 6m 10s\tremaining: 44m 29s\n",
      "1218:\tlearn: 8.2261534\ttotal: 6m 10s\tremaining: 44m 28s\n",
      "1219:\tlearn: 8.2239381\ttotal: 6m 10s\tremaining: 44m 28s\n",
      "1220:\tlearn: 8.2216637\ttotal: 6m 11s\tremaining: 44m 28s\n",
      "1221:\tlearn: 8.2197586\ttotal: 6m 11s\tremaining: 44m 27s\n",
      "1222:\tlearn: 8.2175319\ttotal: 6m 11s\tremaining: 44m 27s\n",
      "1223:\tlearn: 8.2156355\ttotal: 6m 12s\tremaining: 44m 27s\n",
      "1224:\tlearn: 8.2135964\ttotal: 6m 12s\tremaining: 44m 26s\n",
      "1225:\tlearn: 8.2109952\ttotal: 6m 12s\tremaining: 44m 26s\n",
      "1226:\tlearn: 8.2088076\ttotal: 6m 12s\tremaining: 44m 26s\n",
      "1227:\tlearn: 8.2065199\ttotal: 6m 13s\tremaining: 44m 26s\n",
      "1228:\tlearn: 8.2045779\ttotal: 6m 13s\tremaining: 44m 25s\n",
      "1229:\tlearn: 8.2022412\ttotal: 6m 13s\tremaining: 44m 25s\n",
      "1230:\tlearn: 8.1992987\ttotal: 6m 14s\tremaining: 44m 25s\n",
      "1231:\tlearn: 8.1970813\ttotal: 6m 14s\tremaining: 44m 25s\n",
      "1232:\tlearn: 8.1952699\ttotal: 6m 14s\tremaining: 44m 25s\n",
      "1233:\tlearn: 8.1935561\ttotal: 6m 15s\tremaining: 44m 25s\n",
      "1234:\tlearn: 8.1916965\ttotal: 6m 15s\tremaining: 44m 25s\n",
      "1235:\tlearn: 8.1901143\ttotal: 6m 15s\tremaining: 44m 24s\n",
      "1236:\tlearn: 8.1880926\ttotal: 6m 16s\tremaining: 44m 24s\n",
      "1237:\tlearn: 8.1865293\ttotal: 6m 16s\tremaining: 44m 23s\n",
      "1238:\tlearn: 8.1849585\ttotal: 6m 16s\tremaining: 44m 22s\n",
      "1239:\tlearn: 8.1828577\ttotal: 6m 16s\tremaining: 44m 22s\n",
      "1240:\tlearn: 8.1810141\ttotal: 6m 17s\tremaining: 44m 22s\n",
      "1241:\tlearn: 8.1793556\ttotal: 6m 17s\tremaining: 44m 21s\n",
      "1242:\tlearn: 8.1774922\ttotal: 6m 17s\tremaining: 44m 21s\n",
      "1243:\tlearn: 8.1751525\ttotal: 6m 18s\tremaining: 44m 21s\n",
      "1244:\tlearn: 8.1733835\ttotal: 6m 18s\tremaining: 44m 20s\n",
      "1245:\tlearn: 8.1716509\ttotal: 6m 18s\tremaining: 44m 19s\n",
      "1246:\tlearn: 8.1697181\ttotal: 6m 18s\tremaining: 44m 19s\n",
      "1247:\tlearn: 8.1679369\ttotal: 6m 19s\tremaining: 44m 19s\n",
      "1248:\tlearn: 8.1655625\ttotal: 6m 19s\tremaining: 44m 18s\n",
      "1249:\tlearn: 8.1629685\ttotal: 6m 19s\tremaining: 44m 18s\n",
      "1250:\tlearn: 8.1601038\ttotal: 6m 20s\tremaining: 44m 19s\n",
      "1251:\tlearn: 8.1589455\ttotal: 6m 20s\tremaining: 44m 18s\n",
      "1252:\tlearn: 8.1567860\ttotal: 6m 20s\tremaining: 44m 18s\n",
      "1253:\tlearn: 8.1547191\ttotal: 6m 21s\tremaining: 44m 18s\n",
      "1254:\tlearn: 8.1527965\ttotal: 6m 21s\tremaining: 44m 17s\n",
      "1255:\tlearn: 8.1513069\ttotal: 6m 21s\tremaining: 44m 17s\n",
      "1256:\tlearn: 8.1488965\ttotal: 6m 22s\tremaining: 44m 17s\n",
      "1257:\tlearn: 8.1459035\ttotal: 6m 22s\tremaining: 44m 17s\n",
      "1258:\tlearn: 8.1445600\ttotal: 6m 22s\tremaining: 44m 17s\n",
      "1259:\tlearn: 8.1425534\ttotal: 6m 22s\tremaining: 44m 16s\n",
      "1260:\tlearn: 8.1406103\ttotal: 6m 23s\tremaining: 44m 16s\n",
      "1261:\tlearn: 8.1385928\ttotal: 6m 23s\tremaining: 44m 15s\n",
      "1262:\tlearn: 8.1371160\ttotal: 6m 23s\tremaining: 44m 15s\n",
      "1263:\tlearn: 8.1353639\ttotal: 6m 24s\tremaining: 44m 14s\n",
      "1264:\tlearn: 8.1330048\ttotal: 6m 24s\tremaining: 44m 14s\n",
      "1265:\tlearn: 8.1311905\ttotal: 6m 24s\tremaining: 44m 14s\n",
      "1266:\tlearn: 8.1298328\ttotal: 6m 24s\tremaining: 44m 13s\n",
      "1267:\tlearn: 8.1278715\ttotal: 6m 25s\tremaining: 44m 13s\n",
      "1268:\tlearn: 8.1259654\ttotal: 6m 25s\tremaining: 44m 12s\n",
      "1269:\tlearn: 8.1224546\ttotal: 6m 25s\tremaining: 44m 12s\n",
      "1270:\tlearn: 8.1200001\ttotal: 6m 26s\tremaining: 44m 13s\n",
      "1271:\tlearn: 8.1182123\ttotal: 6m 26s\tremaining: 44m 12s\n",
      "1272:\tlearn: 8.1166518\ttotal: 6m 26s\tremaining: 44m 11s\n",
      "1273:\tlearn: 8.1148131\ttotal: 6m 27s\tremaining: 44m 11s\n",
      "1274:\tlearn: 8.1121877\ttotal: 6m 27s\tremaining: 44m 11s\n",
      "1275:\tlearn: 8.1095252\ttotal: 6m 27s\tremaining: 44m 11s\n",
      "1276:\tlearn: 8.1072180\ttotal: 6m 28s\tremaining: 44m 11s\n",
      "1277:\tlearn: 8.1056610\ttotal: 6m 28s\tremaining: 44m 11s\n",
      "1278:\tlearn: 8.1035333\ttotal: 6m 28s\tremaining: 44m 11s\n",
      "1279:\tlearn: 8.1009572\ttotal: 6m 29s\tremaining: 44m 10s\n",
      "1280:\tlearn: 8.0990476\ttotal: 6m 29s\tremaining: 44m 10s\n",
      "1281:\tlearn: 8.0971181\ttotal: 6m 29s\tremaining: 44m 10s\n",
      "1282:\tlearn: 8.0945971\ttotal: 6m 30s\tremaining: 44m 10s\n",
      "1283:\tlearn: 8.0911413\ttotal: 6m 30s\tremaining: 44m 10s\n",
      "1284:\tlearn: 8.0886170\ttotal: 6m 30s\tremaining: 44m 10s\n",
      "1285:\tlearn: 8.0866634\ttotal: 6m 31s\tremaining: 44m 9s\n",
      "1286:\tlearn: 8.0848616\ttotal: 6m 31s\tremaining: 44m 9s\n",
      "1287:\tlearn: 8.0818580\ttotal: 6m 31s\tremaining: 44m 9s\n",
      "1288:\tlearn: 8.0797932\ttotal: 6m 32s\tremaining: 44m 9s\n",
      "1289:\tlearn: 8.0779369\ttotal: 6m 32s\tremaining: 44m 9s\n",
      "1290:\tlearn: 8.0761767\ttotal: 6m 32s\tremaining: 44m 9s\n",
      "1291:\tlearn: 8.0748855\ttotal: 6m 32s\tremaining: 44m 8s\n",
      "1292:\tlearn: 8.0730200\ttotal: 6m 33s\tremaining: 44m 8s\n",
      "1293:\tlearn: 8.0710906\ttotal: 6m 33s\tremaining: 44m 8s\n",
      "1294:\tlearn: 8.0688617\ttotal: 6m 34s\tremaining: 44m 8s\n",
      "1295:\tlearn: 8.0665926\ttotal: 6m 34s\tremaining: 44m 8s\n",
      "1296:\tlearn: 8.0651076\ttotal: 6m 34s\tremaining: 44m 7s\n",
      "1297:\tlearn: 8.0636601\ttotal: 6m 34s\tremaining: 44m 7s\n",
      "1298:\tlearn: 8.0617677\ttotal: 6m 35s\tremaining: 44m 7s\n",
      "1299:\tlearn: 8.0600303\ttotal: 6m 35s\tremaining: 44m 7s\n",
      "1300:\tlearn: 8.0578669\ttotal: 6m 35s\tremaining: 44m 7s\n",
      "1301:\tlearn: 8.0553672\ttotal: 6m 36s\tremaining: 44m 7s\n",
      "1302:\tlearn: 8.0536199\ttotal: 6m 36s\tremaining: 44m 6s\n",
      "1303:\tlearn: 8.0522364\ttotal: 6m 36s\tremaining: 44m 6s\n",
      "1304:\tlearn: 8.0504808\ttotal: 6m 37s\tremaining: 44m 6s\n",
      "1305:\tlearn: 8.0487140\ttotal: 6m 37s\tremaining: 44m 6s\n",
      "1306:\tlearn: 8.0467067\ttotal: 6m 37s\tremaining: 44m 6s\n",
      "1307:\tlearn: 8.0451800\ttotal: 6m 38s\tremaining: 44m 5s\n",
      "1308:\tlearn: 8.0434623\ttotal: 6m 38s\tremaining: 44m 5s\n",
      "1309:\tlearn: 8.0415545\ttotal: 6m 38s\tremaining: 44m 4s\n",
      "1310:\tlearn: 8.0394063\ttotal: 6m 38s\tremaining: 44m 4s\n",
      "1311:\tlearn: 8.0375925\ttotal: 6m 39s\tremaining: 44m 3s\n",
      "1312:\tlearn: 8.0343047\ttotal: 6m 39s\tremaining: 44m 3s\n",
      "1313:\tlearn: 8.0329401\ttotal: 6m 39s\tremaining: 44m 3s\n",
      "1314:\tlearn: 8.0316336\ttotal: 6m 40s\tremaining: 44m 2s\n",
      "1315:\tlearn: 8.0295129\ttotal: 6m 40s\tremaining: 44m 2s\n",
      "1316:\tlearn: 8.0278319\ttotal: 6m 40s\tremaining: 44m 1s\n",
      "1317:\tlearn: 8.0261091\ttotal: 6m 40s\tremaining: 44m 1s\n",
      "1318:\tlearn: 8.0246493\ttotal: 6m 41s\tremaining: 44m\n",
      "1319:\tlearn: 8.0218801\ttotal: 6m 41s\tremaining: 44m\n",
      "1320:\tlearn: 8.0201764\ttotal: 6m 41s\tremaining: 44m\n",
      "1321:\tlearn: 8.0176204\ttotal: 6m 42s\tremaining: 44m 1s\n",
      "1322:\tlearn: 8.0161397\ttotal: 6m 42s\tremaining: 44m 1s\n",
      "1323:\tlearn: 8.0147766\ttotal: 6m 42s\tremaining: 44m\n",
      "1324:\tlearn: 8.0126608\ttotal: 6m 43s\tremaining: 44m\n",
      "1325:\tlearn: 8.0111533\ttotal: 6m 43s\tremaining: 44m\n",
      "1326:\tlearn: 8.0091205\ttotal: 6m 43s\tremaining: 44m\n",
      "1327:\tlearn: 8.0068383\ttotal: 6m 44s\tremaining: 43m 59s\n",
      "1328:\tlearn: 8.0047005\ttotal: 6m 44s\tremaining: 43m 59s\n",
      "1329:\tlearn: 8.0031038\ttotal: 6m 44s\tremaining: 43m 59s\n",
      "1330:\tlearn: 8.0008108\ttotal: 6m 45s\tremaining: 43m 59s\n",
      "1331:\tlearn: 7.9994852\ttotal: 6m 45s\tremaining: 43m 59s\n",
      "1332:\tlearn: 7.9976525\ttotal: 6m 45s\tremaining: 43m 58s\n",
      "1333:\tlearn: 7.9944374\ttotal: 6m 46s\tremaining: 43m 59s\n",
      "1334:\tlearn: 7.9912095\ttotal: 6m 46s\tremaining: 43m 58s\n",
      "1335:\tlearn: 7.9895507\ttotal: 6m 46s\tremaining: 43m 58s\n",
      "1336:\tlearn: 7.9875482\ttotal: 6m 47s\tremaining: 43m 58s\n",
      "1337:\tlearn: 7.9861892\ttotal: 6m 47s\tremaining: 43m 58s\n",
      "1338:\tlearn: 7.9844819\ttotal: 6m 47s\tremaining: 43m 57s\n",
      "1339:\tlearn: 7.9827666\ttotal: 6m 48s\tremaining: 43m 57s\n",
      "1340:\tlearn: 7.9813007\ttotal: 6m 48s\tremaining: 43m 57s\n",
      "1341:\tlearn: 7.9798778\ttotal: 6m 48s\tremaining: 43m 56s\n",
      "1342:\tlearn: 7.9782583\ttotal: 6m 49s\tremaining: 43m 56s\n",
      "1343:\tlearn: 7.9764257\ttotal: 6m 49s\tremaining: 43m 56s\n",
      "1344:\tlearn: 7.9745426\ttotal: 6m 49s\tremaining: 43m 56s\n",
      "1345:\tlearn: 7.9723841\ttotal: 6m 50s\tremaining: 43m 56s\n",
      "1346:\tlearn: 7.9700248\ttotal: 6m 50s\tremaining: 43m 56s\n",
      "1347:\tlearn: 7.9676826\ttotal: 6m 50s\tremaining: 43m 57s\n",
      "1348:\tlearn: 7.9661612\ttotal: 6m 51s\tremaining: 43m 56s\n",
      "1349:\tlearn: 7.9639614\ttotal: 6m 51s\tremaining: 43m 56s\n",
      "1350:\tlearn: 7.9615289\ttotal: 6m 51s\tremaining: 43m 56s\n",
      "1351:\tlearn: 7.9597791\ttotal: 6m 52s\tremaining: 43m 56s\n",
      "1352:\tlearn: 7.9579446\ttotal: 6m 52s\tremaining: 43m 55s\n",
      "1353:\tlearn: 7.9561733\ttotal: 6m 52s\tremaining: 43m 56s\n",
      "1354:\tlearn: 7.9534414\ttotal: 6m 53s\tremaining: 43m 55s\n",
      "1355:\tlearn: 7.9513889\ttotal: 6m 53s\tremaining: 43m 55s\n",
      "1356:\tlearn: 7.9483404\ttotal: 6m 53s\tremaining: 43m 55s\n",
      "1357:\tlearn: 7.9470054\ttotal: 6m 54s\tremaining: 43m 54s\n",
      "1358:\tlearn: 7.9455383\ttotal: 6m 54s\tremaining: 43m 54s\n",
      "1359:\tlearn: 7.9432575\ttotal: 6m 54s\tremaining: 43m 54s\n",
      "1360:\tlearn: 7.9413882\ttotal: 6m 55s\tremaining: 43m 54s\n",
      "1361:\tlearn: 7.9402684\ttotal: 6m 55s\tremaining: 43m 53s\n",
      "1362:\tlearn: 7.9386605\ttotal: 6m 55s\tremaining: 43m 53s\n",
      "1363:\tlearn: 7.9363797\ttotal: 6m 55s\tremaining: 43m 53s\n",
      "1364:\tlearn: 7.9342945\ttotal: 6m 56s\tremaining: 43m 53s\n",
      "1365:\tlearn: 7.9326308\ttotal: 6m 56s\tremaining: 43m 53s\n",
      "1366:\tlearn: 7.9302384\ttotal: 6m 56s\tremaining: 43m 52s\n",
      "1367:\tlearn: 7.9283204\ttotal: 6m 57s\tremaining: 43m 52s\n",
      "1368:\tlearn: 7.9266593\ttotal: 6m 57s\tremaining: 43m 52s\n",
      "1369:\tlearn: 7.9248354\ttotal: 6m 57s\tremaining: 43m 51s\n",
      "1370:\tlearn: 7.9233617\ttotal: 6m 58s\tremaining: 43m 51s\n",
      "1371:\tlearn: 7.9218658\ttotal: 6m 58s\tremaining: 43m 50s\n",
      "1372:\tlearn: 7.9204486\ttotal: 6m 58s\tremaining: 43m 49s\n",
      "1373:\tlearn: 7.9179349\ttotal: 6m 58s\tremaining: 43m 50s\n",
      "1374:\tlearn: 7.9150907\ttotal: 6m 59s\tremaining: 43m 50s\n",
      "1375:\tlearn: 7.9130691\ttotal: 6m 59s\tremaining: 43m 50s\n",
      "1376:\tlearn: 7.9111813\ttotal: 7m\tremaining: 43m 50s\n",
      "1377:\tlearn: 7.9099223\ttotal: 7m\tremaining: 43m 50s\n",
      "1378:\tlearn: 7.9082249\ttotal: 7m\tremaining: 43m 49s\n",
      "1379:\tlearn: 7.9050965\ttotal: 7m 1s\tremaining: 43m 49s\n",
      "1380:\tlearn: 7.9038104\ttotal: 7m 1s\tremaining: 43m 49s\n",
      "1381:\tlearn: 7.9026381\ttotal: 7m 1s\tremaining: 43m 49s\n",
      "1382:\tlearn: 7.9003727\ttotal: 7m 1s\tremaining: 43m 49s\n",
      "1383:\tlearn: 7.8987739\ttotal: 7m 2s\tremaining: 43m 48s\n",
      "1384:\tlearn: 7.8974730\ttotal: 7m 2s\tremaining: 43m 48s\n",
      "1385:\tlearn: 7.8957820\ttotal: 7m 2s\tremaining: 43m 48s\n",
      "1386:\tlearn: 7.8928191\ttotal: 7m 3s\tremaining: 43m 48s\n",
      "1387:\tlearn: 7.8906880\ttotal: 7m 3s\tremaining: 43m 48s\n",
      "1388:\tlearn: 7.8885598\ttotal: 7m 3s\tremaining: 43m 47s\n",
      "1389:\tlearn: 7.8871530\ttotal: 7m 4s\tremaining: 43m 47s\n",
      "1390:\tlearn: 7.8857383\ttotal: 7m 4s\tremaining: 43m 46s\n",
      "1391:\tlearn: 7.8840450\ttotal: 7m 4s\tremaining: 43m 46s\n",
      "1392:\tlearn: 7.8822291\ttotal: 7m 5s\tremaining: 43m 46s\n",
      "1393:\tlearn: 7.8803542\ttotal: 7m 5s\tremaining: 43m 46s\n",
      "1394:\tlearn: 7.8781193\ttotal: 7m 5s\tremaining: 43m 45s\n",
      "1395:\tlearn: 7.8766373\ttotal: 7m 5s\tremaining: 43m 45s\n",
      "1396:\tlearn: 7.8747529\ttotal: 7m 6s\tremaining: 43m 45s\n",
      "1397:\tlearn: 7.8732103\ttotal: 7m 6s\tremaining: 43m 44s\n",
      "1398:\tlearn: 7.8719625\ttotal: 7m 6s\tremaining: 43m 44s\n",
      "1399:\tlearn: 7.8695955\ttotal: 7m 7s\tremaining: 43m 43s\n",
      "1400:\tlearn: 7.8665983\ttotal: 7m 7s\tremaining: 43m 43s\n",
      "1401:\tlearn: 7.8642315\ttotal: 7m 7s\tremaining: 43m 43s\n",
      "1402:\tlearn: 7.8628749\ttotal: 7m 8s\tremaining: 43m 43s\n",
      "1403:\tlearn: 7.8605689\ttotal: 7m 8s\tremaining: 43m 43s\n",
      "1404:\tlearn: 7.8585263\ttotal: 7m 8s\tremaining: 43m 43s\n",
      "1405:\tlearn: 7.8566319\ttotal: 7m 9s\tremaining: 43m 42s\n",
      "1406:\tlearn: 7.8551457\ttotal: 7m 9s\tremaining: 43m 42s\n",
      "1407:\tlearn: 7.8527042\ttotal: 7m 9s\tremaining: 43m 41s\n",
      "1408:\tlearn: 7.8508922\ttotal: 7m 10s\tremaining: 43m 42s\n",
      "1409:\tlearn: 7.8493988\ttotal: 7m 10s\tremaining: 43m 41s\n",
      "1410:\tlearn: 7.8470943\ttotal: 7m 10s\tremaining: 43m 41s\n",
      "1411:\tlearn: 7.8459633\ttotal: 7m 10s\tremaining: 43m 40s\n",
      "1412:\tlearn: 7.8438169\ttotal: 7m 11s\tremaining: 43m 40s\n",
      "1413:\tlearn: 7.8418049\ttotal: 7m 11s\tremaining: 43m 40s\n",
      "1414:\tlearn: 7.8402281\ttotal: 7m 11s\tremaining: 43m 40s\n",
      "1415:\tlearn: 7.8376558\ttotal: 7m 12s\tremaining: 43m 40s\n",
      "1416:\tlearn: 7.8347484\ttotal: 7m 12s\tremaining: 43m 40s\n",
      "1417:\tlearn: 7.8332941\ttotal: 7m 12s\tremaining: 43m 39s\n",
      "1418:\tlearn: 7.8315858\ttotal: 7m 13s\tremaining: 43m 39s\n",
      "1419:\tlearn: 7.8300243\ttotal: 7m 13s\tremaining: 43m 38s\n",
      "1420:\tlearn: 7.8275702\ttotal: 7m 13s\tremaining: 43m 38s\n",
      "1421:\tlearn: 7.8258268\ttotal: 7m 14s\tremaining: 43m 38s\n",
      "1422:\tlearn: 7.8240757\ttotal: 7m 14s\tremaining: 43m 38s\n",
      "1423:\tlearn: 7.8226926\ttotal: 7m 14s\tremaining: 43m 37s\n",
      "1424:\tlearn: 7.8210921\ttotal: 7m 14s\tremaining: 43m 37s\n",
      "1425:\tlearn: 7.8197288\ttotal: 7m 15s\tremaining: 43m 36s\n",
      "1426:\tlearn: 7.8179302\ttotal: 7m 15s\tremaining: 43m 36s\n",
      "1427:\tlearn: 7.8164067\ttotal: 7m 15s\tremaining: 43m 36s\n",
      "1428:\tlearn: 7.8146525\ttotal: 7m 16s\tremaining: 43m 35s\n",
      "1429:\tlearn: 7.8124910\ttotal: 7m 16s\tremaining: 43m 35s\n",
      "1430:\tlearn: 7.8097252\ttotal: 7m 16s\tremaining: 43m 35s\n",
      "1431:\tlearn: 7.8084089\ttotal: 7m 17s\tremaining: 43m 35s\n",
      "1432:\tlearn: 7.8067253\ttotal: 7m 17s\tremaining: 43m 35s\n",
      "1433:\tlearn: 7.8047041\ttotal: 7m 17s\tremaining: 43m 35s\n",
      "1434:\tlearn: 7.8032204\ttotal: 7m 18s\tremaining: 43m 34s\n",
      "1435:\tlearn: 7.8013608\ttotal: 7m 18s\tremaining: 43m 33s\n",
      "1436:\tlearn: 7.7982939\ttotal: 7m 18s\tremaining: 43m 33s\n",
      "1437:\tlearn: 7.7967486\ttotal: 7m 18s\tremaining: 43m 33s\n",
      "1438:\tlearn: 7.7939729\ttotal: 7m 19s\tremaining: 43m 33s\n",
      "1439:\tlearn: 7.7921818\ttotal: 7m 19s\tremaining: 43m 33s\n",
      "1440:\tlearn: 7.7902203\ttotal: 7m 19s\tremaining: 43m 32s\n",
      "1441:\tlearn: 7.7885160\ttotal: 7m 20s\tremaining: 43m 32s\n",
      "1442:\tlearn: 7.7871695\ttotal: 7m 20s\tremaining: 43m 31s\n",
      "1443:\tlearn: 7.7858244\ttotal: 7m 20s\tremaining: 43m 31s\n",
      "1444:\tlearn: 7.7840587\ttotal: 7m 21s\tremaining: 43m 31s\n",
      "1445:\tlearn: 7.7824781\ttotal: 7m 21s\tremaining: 43m 31s\n",
      "1446:\tlearn: 7.7806170\ttotal: 7m 21s\tremaining: 43m 30s\n",
      "1447:\tlearn: 7.7791292\ttotal: 7m 21s\tremaining: 43m 30s\n",
      "1448:\tlearn: 7.7767432\ttotal: 7m 22s\tremaining: 43m 30s\n",
      "1449:\tlearn: 7.7749098\ttotal: 7m 22s\tremaining: 43m 30s\n",
      "1450:\tlearn: 7.7736256\ttotal: 7m 22s\tremaining: 43m 29s\n",
      "1451:\tlearn: 7.7713636\ttotal: 7m 23s\tremaining: 43m 30s\n",
      "1452:\tlearn: 7.7701070\ttotal: 7m 23s\tremaining: 43m 29s\n",
      "1453:\tlearn: 7.7687188\ttotal: 7m 23s\tremaining: 43m 29s\n",
      "1454:\tlearn: 7.7662844\ttotal: 7m 24s\tremaining: 43m 29s\n",
      "1455:\tlearn: 7.7647945\ttotal: 7m 24s\tremaining: 43m 28s\n",
      "1456:\tlearn: 7.7634870\ttotal: 7m 24s\tremaining: 43m 28s\n",
      "1457:\tlearn: 7.7613284\ttotal: 7m 25s\tremaining: 43m 28s\n",
      "1458:\tlearn: 7.7601914\ttotal: 7m 25s\tremaining: 43m 27s\n",
      "1459:\tlearn: 7.7580622\ttotal: 7m 25s\tremaining: 43m 27s\n",
      "1460:\tlearn: 7.7565333\ttotal: 7m 26s\tremaining: 43m 27s\n",
      "1461:\tlearn: 7.7542092\ttotal: 7m 26s\tremaining: 43m 27s\n",
      "1462:\tlearn: 7.7525223\ttotal: 7m 26s\tremaining: 43m 27s\n",
      "1463:\tlearn: 7.7498954\ttotal: 7m 27s\tremaining: 43m 27s\n",
      "1464:\tlearn: 7.7484570\ttotal: 7m 27s\tremaining: 43m 27s\n",
      "1465:\tlearn: 7.7467421\ttotal: 7m 27s\tremaining: 43m 26s\n",
      "1466:\tlearn: 7.7440960\ttotal: 7m 28s\tremaining: 43m 27s\n",
      "1467:\tlearn: 7.7424916\ttotal: 7m 28s\tremaining: 43m 26s\n",
      "1468:\tlearn: 7.7407335\ttotal: 7m 28s\tremaining: 43m 26s\n",
      "1469:\tlearn: 7.7385720\ttotal: 7m 29s\tremaining: 43m 26s\n",
      "1470:\tlearn: 7.7369821\ttotal: 7m 29s\tremaining: 43m 26s\n",
      "1471:\tlearn: 7.7354280\ttotal: 7m 29s\tremaining: 43m 25s\n",
      "1472:\tlearn: 7.7342387\ttotal: 7m 30s\tremaining: 43m 25s\n",
      "1473:\tlearn: 7.7331823\ttotal: 7m 30s\tremaining: 43m 24s\n",
      "1474:\tlearn: 7.7302822\ttotal: 7m 30s\tremaining: 43m 24s\n",
      "1475:\tlearn: 7.7282694\ttotal: 7m 30s\tremaining: 43m 23s\n",
      "1476:\tlearn: 7.7267000\ttotal: 7m 31s\tremaining: 43m 23s\n",
      "1477:\tlearn: 7.7256993\ttotal: 7m 31s\tremaining: 43m 22s\n",
      "1478:\tlearn: 7.7240146\ttotal: 7m 31s\tremaining: 43m 22s\n",
      "1479:\tlearn: 7.7218141\ttotal: 7m 32s\tremaining: 43m 22s\n",
      "1480:\tlearn: 7.7194347\ttotal: 7m 32s\tremaining: 43m 22s\n",
      "1481:\tlearn: 7.7170884\ttotal: 7m 32s\tremaining: 43m 22s\n",
      "1482:\tlearn: 7.7151011\ttotal: 7m 33s\tremaining: 43m 22s\n",
      "1483:\tlearn: 7.7125312\ttotal: 7m 33s\tremaining: 43m 22s\n",
      "1484:\tlearn: 7.7099131\ttotal: 7m 33s\tremaining: 43m 22s\n",
      "1485:\tlearn: 7.7083718\ttotal: 7m 34s\tremaining: 43m 22s\n",
      "1486:\tlearn: 7.7064443\ttotal: 7m 34s\tremaining: 43m 22s\n",
      "1487:\tlearn: 7.7039002\ttotal: 7m 34s\tremaining: 43m 22s\n",
      "1488:\tlearn: 7.7024611\ttotal: 7m 35s\tremaining: 43m 21s\n",
      "1489:\tlearn: 7.7007799\ttotal: 7m 35s\tremaining: 43m 21s\n",
      "1490:\tlearn: 7.6985952\ttotal: 7m 35s\tremaining: 43m 21s\n",
      "1491:\tlearn: 7.6967886\ttotal: 7m 36s\tremaining: 43m 21s\n",
      "1492:\tlearn: 7.6947868\ttotal: 7m 36s\tremaining: 43m 21s\n",
      "1493:\tlearn: 7.6933164\ttotal: 7m 36s\tremaining: 43m 21s\n",
      "1494:\tlearn: 7.6910338\ttotal: 7m 37s\tremaining: 43m 21s\n",
      "1495:\tlearn: 7.6892678\ttotal: 7m 37s\tremaining: 43m 21s\n",
      "1496:\tlearn: 7.6879554\ttotal: 7m 37s\tremaining: 43m 20s\n",
      "1497:\tlearn: 7.6861230\ttotal: 7m 38s\tremaining: 43m 20s\n",
      "1498:\tlearn: 7.6847395\ttotal: 7m 38s\tremaining: 43m 19s\n",
      "1499:\tlearn: 7.6832420\ttotal: 7m 38s\tremaining: 43m 19s\n",
      "1500:\tlearn: 7.6815643\ttotal: 7m 39s\tremaining: 43m 19s\n",
      "1501:\tlearn: 7.6802912\ttotal: 7m 39s\tremaining: 43m 18s\n",
      "1502:\tlearn: 7.6782695\ttotal: 7m 39s\tremaining: 43m 19s\n",
      "1503:\tlearn: 7.6764842\ttotal: 7m 40s\tremaining: 43m 18s\n",
      "1504:\tlearn: 7.6748034\ttotal: 7m 40s\tremaining: 43m 18s\n",
      "1505:\tlearn: 7.6729937\ttotal: 7m 40s\tremaining: 43m 18s\n",
      "1506:\tlearn: 7.6700958\ttotal: 7m 41s\tremaining: 43m 18s\n",
      "1507:\tlearn: 7.6684943\ttotal: 7m 41s\tremaining: 43m 18s\n",
      "1508:\tlearn: 7.6666413\ttotal: 7m 41s\tremaining: 43m 18s\n",
      "1509:\tlearn: 7.6646130\ttotal: 7m 42s\tremaining: 43m 18s\n",
      "1510:\tlearn: 7.6631047\ttotal: 7m 42s\tremaining: 43m 18s\n",
      "1511:\tlearn: 7.6619170\ttotal: 7m 42s\tremaining: 43m 18s\n",
      "1512:\tlearn: 7.6599193\ttotal: 7m 43s\tremaining: 43m 18s\n",
      "1513:\tlearn: 7.6579912\ttotal: 7m 43s\tremaining: 43m 18s\n",
      "1514:\tlearn: 7.6563606\ttotal: 7m 43s\tremaining: 43m 18s\n",
      "1515:\tlearn: 7.6538675\ttotal: 7m 44s\tremaining: 43m 18s\n",
      "1516:\tlearn: 7.6530351\ttotal: 7m 44s\tremaining: 43m 17s\n",
      "1517:\tlearn: 7.6511064\ttotal: 7m 44s\tremaining: 43m 17s\n",
      "1518:\tlearn: 7.6497183\ttotal: 7m 45s\tremaining: 43m 17s\n",
      "1519:\tlearn: 7.6484329\ttotal: 7m 45s\tremaining: 43m 16s\n",
      "1520:\tlearn: 7.6469230\ttotal: 7m 45s\tremaining: 43m 16s\n",
      "1521:\tlearn: 7.6456593\ttotal: 7m 46s\tremaining: 43m 15s\n",
      "1522:\tlearn: 7.6430648\ttotal: 7m 46s\tremaining: 43m 15s\n",
      "1523:\tlearn: 7.6422510\ttotal: 7m 46s\tremaining: 43m 14s\n",
      "1524:\tlearn: 7.6409475\ttotal: 7m 46s\tremaining: 43m 14s\n",
      "1525:\tlearn: 7.6395793\ttotal: 7m 47s\tremaining: 43m 13s\n",
      "1526:\tlearn: 7.6379764\ttotal: 7m 47s\tremaining: 43m 13s\n",
      "1527:\tlearn: 7.6367815\ttotal: 7m 47s\tremaining: 43m 13s\n",
      "1528:\tlearn: 7.6357422\ttotal: 7m 47s\tremaining: 43m 12s\n",
      "1529:\tlearn: 7.6338935\ttotal: 7m 48s\tremaining: 43m 12s\n",
      "1530:\tlearn: 7.6325537\ttotal: 7m 48s\tremaining: 43m 11s\n",
      "1531:\tlearn: 7.6311357\ttotal: 7m 48s\tremaining: 43m 11s\n",
      "1532:\tlearn: 7.6298550\ttotal: 7m 49s\tremaining: 43m 10s\n",
      "1533:\tlearn: 7.6281565\ttotal: 7m 49s\tremaining: 43m 10s\n",
      "1534:\tlearn: 7.6265729\ttotal: 7m 49s\tremaining: 43m 10s\n",
      "1535:\tlearn: 7.6238116\ttotal: 7m 50s\tremaining: 43m 10s\n",
      "1536:\tlearn: 7.6227104\ttotal: 7m 50s\tremaining: 43m 9s\n",
      "1537:\tlearn: 7.6205354\ttotal: 7m 50s\tremaining: 43m 10s\n",
      "1538:\tlearn: 7.6191548\ttotal: 7m 51s\tremaining: 43m 9s\n",
      "1539:\tlearn: 7.6178669\ttotal: 7m 51s\tremaining: 43m 8s\n",
      "1540:\tlearn: 7.6165765\ttotal: 7m 51s\tremaining: 43m 8s\n",
      "1541:\tlearn: 7.6142627\ttotal: 7m 51s\tremaining: 43m 8s\n",
      "1542:\tlearn: 7.6126946\ttotal: 7m 52s\tremaining: 43m 7s\n",
      "1543:\tlearn: 7.6104945\ttotal: 7m 52s\tremaining: 43m 7s\n",
      "1544:\tlearn: 7.6078337\ttotal: 7m 52s\tremaining: 43m 8s\n",
      "1545:\tlearn: 7.6063403\ttotal: 7m 53s\tremaining: 43m 8s\n",
      "1546:\tlearn: 7.6050558\ttotal: 7m 53s\tremaining: 43m 7s\n",
      "1547:\tlearn: 7.6038549\ttotal: 7m 53s\tremaining: 43m 6s\n",
      "1548:\tlearn: 7.6021594\ttotal: 7m 54s\tremaining: 43m 6s\n",
      "1549:\tlearn: 7.6007813\ttotal: 7m 54s\tremaining: 43m 5s\n",
      "1550:\tlearn: 7.5993595\ttotal: 7m 54s\tremaining: 43m 5s\n",
      "1551:\tlearn: 7.5980260\ttotal: 7m 54s\tremaining: 43m 5s\n",
      "1552:\tlearn: 7.5960163\ttotal: 7m 55s\tremaining: 43m 5s\n",
      "1553:\tlearn: 7.5943954\ttotal: 7m 55s\tremaining: 43m 4s\n",
      "1554:\tlearn: 7.5922538\ttotal: 7m 55s\tremaining: 43m 5s\n",
      "1555:\tlearn: 7.5910353\ttotal: 7m 56s\tremaining: 43m 4s\n",
      "1556:\tlearn: 7.5893377\ttotal: 7m 56s\tremaining: 43m 3s\n",
      "1557:\tlearn: 7.5878547\ttotal: 7m 56s\tremaining: 43m 3s\n",
      "1558:\tlearn: 7.5866479\ttotal: 7m 57s\tremaining: 43m 3s\n",
      "1559:\tlearn: 7.5845724\ttotal: 7m 57s\tremaining: 43m 2s\n",
      "1560:\tlearn: 7.5833839\ttotal: 7m 57s\tremaining: 43m 2s\n",
      "1561:\tlearn: 7.5820081\ttotal: 7m 58s\tremaining: 43m 2s\n",
      "1562:\tlearn: 7.5808171\ttotal: 7m 58s\tremaining: 43m 1s\n",
      "1563:\tlearn: 7.5785064\ttotal: 7m 58s\tremaining: 43m 1s\n",
      "1564:\tlearn: 7.5772754\ttotal: 7m 58s\tremaining: 43m 1s\n",
      "1565:\tlearn: 7.5749587\ttotal: 7m 59s\tremaining: 43m 1s\n",
      "1566:\tlearn: 7.5727733\ttotal: 7m 59s\tremaining: 43m 1s\n",
      "1567:\tlearn: 7.5714973\ttotal: 7m 59s\tremaining: 43m\n",
      "1568:\tlearn: 7.5703841\ttotal: 8m\tremaining: 43m\n",
      "1569:\tlearn: 7.5686364\ttotal: 8m\tremaining: 42m 59s\n",
      "1570:\tlearn: 7.5675024\ttotal: 8m\tremaining: 42m 59s\n",
      "1571:\tlearn: 7.5657673\ttotal: 8m 1s\tremaining: 42m 59s\n",
      "1572:\tlearn: 7.5642567\ttotal: 8m 1s\tremaining: 42m 58s\n",
      "1573:\tlearn: 7.5627933\ttotal: 8m 1s\tremaining: 42m 58s\n",
      "1574:\tlearn: 7.5607929\ttotal: 8m 2s\tremaining: 42m 58s\n",
      "1575:\tlearn: 7.5595212\ttotal: 8m 2s\tremaining: 42m 58s\n",
      "1576:\tlearn: 7.5579782\ttotal: 8m 2s\tremaining: 42m 57s\n",
      "1577:\tlearn: 7.5563839\ttotal: 8m 2s\tremaining: 42m 57s\n",
      "1578:\tlearn: 7.5549092\ttotal: 8m 3s\tremaining: 42m 57s\n",
      "1579:\tlearn: 7.5535663\ttotal: 8m 3s\tremaining: 42m 56s\n",
      "1580:\tlearn: 7.5518776\ttotal: 8m 3s\tremaining: 42m 56s\n",
      "1581:\tlearn: 7.5507807\ttotal: 8m 4s\tremaining: 42m 55s\n",
      "1582:\tlearn: 7.5496729\ttotal: 8m 4s\tremaining: 42m 54s\n",
      "1583:\tlearn: 7.5484947\ttotal: 8m 4s\tremaining: 42m 54s\n",
      "1584:\tlearn: 7.5470493\ttotal: 8m 4s\tremaining: 42m 54s\n",
      "1585:\tlearn: 7.5454955\ttotal: 8m 5s\tremaining: 42m 54s\n",
      "1586:\tlearn: 7.5431772\ttotal: 8m 5s\tremaining: 42m 54s\n",
      "1587:\tlearn: 7.5408801\ttotal: 8m 5s\tremaining: 42m 54s\n",
      "1588:\tlearn: 7.5397380\ttotal: 8m 6s\tremaining: 42m 53s\n",
      "1589:\tlearn: 7.5384005\ttotal: 8m 6s\tremaining: 42m 53s\n",
      "1590:\tlearn: 7.5369578\ttotal: 8m 6s\tremaining: 42m 53s\n",
      "1591:\tlearn: 7.5359616\ttotal: 8m 7s\tremaining: 42m 52s\n",
      "1592:\tlearn: 7.5345869\ttotal: 8m 7s\tremaining: 42m 52s\n",
      "1593:\tlearn: 7.5336052\ttotal: 8m 7s\tremaining: 42m 51s\n",
      "1594:\tlearn: 7.5319979\ttotal: 8m 7s\tremaining: 42m 51s\n",
      "1595:\tlearn: 7.5309745\ttotal: 8m 8s\tremaining: 42m 50s\n",
      "1596:\tlearn: 7.5298803\ttotal: 8m 8s\tremaining: 42m 50s\n",
      "1597:\tlearn: 7.5288014\ttotal: 8m 8s\tremaining: 42m 50s\n",
      "1598:\tlearn: 7.5264419\ttotal: 8m 9s\tremaining: 42m 50s\n",
      "1599:\tlearn: 7.5251200\ttotal: 8m 9s\tremaining: 42m 50s\n",
      "1600:\tlearn: 7.5238810\ttotal: 8m 9s\tremaining: 42m 50s\n",
      "1601:\tlearn: 7.5226192\ttotal: 8m 10s\tremaining: 42m 49s\n",
      "1602:\tlearn: 7.5201331\ttotal: 8m 10s\tremaining: 42m 50s\n",
      "1603:\tlearn: 7.5190079\ttotal: 8m 10s\tremaining: 42m 49s\n",
      "1604:\tlearn: 7.5179168\ttotal: 8m 11s\tremaining: 42m 48s\n",
      "1605:\tlearn: 7.5168171\ttotal: 8m 11s\tremaining: 42m 48s\n",
      "1606:\tlearn: 7.5144497\ttotal: 8m 11s\tremaining: 42m 48s\n",
      "1607:\tlearn: 7.5130329\ttotal: 8m 12s\tremaining: 42m 48s\n",
      "1608:\tlearn: 7.5114161\ttotal: 8m 12s\tremaining: 42m 48s\n",
      "1609:\tlearn: 7.5103108\ttotal: 8m 12s\tremaining: 42m 47s\n",
      "1610:\tlearn: 7.5083114\ttotal: 8m 13s\tremaining: 42m 47s\n",
      "1611:\tlearn: 7.5061870\ttotal: 8m 13s\tremaining: 42m 47s\n",
      "1612:\tlearn: 7.5042666\ttotal: 8m 13s\tremaining: 42m 46s\n",
      "1613:\tlearn: 7.5027356\ttotal: 8m 13s\tremaining: 42m 46s\n",
      "1614:\tlearn: 7.5017387\ttotal: 8m 14s\tremaining: 42m 45s\n",
      "1615:\tlearn: 7.4998719\ttotal: 8m 14s\tremaining: 42m 46s\n",
      "1616:\tlearn: 7.4981597\ttotal: 8m 14s\tremaining: 42m 45s\n",
      "1617:\tlearn: 7.4965913\ttotal: 8m 15s\tremaining: 42m 45s\n",
      "1618:\tlearn: 7.4945266\ttotal: 8m 15s\tremaining: 42m 45s\n",
      "1619:\tlearn: 7.4928485\ttotal: 8m 15s\tremaining: 42m 44s\n",
      "1620:\tlearn: 7.4917709\ttotal: 8m 16s\tremaining: 42m 44s\n",
      "1621:\tlearn: 7.4906523\ttotal: 8m 16s\tremaining: 42m 43s\n",
      "1622:\tlearn: 7.4890028\ttotal: 8m 16s\tremaining: 42m 43s\n",
      "1623:\tlearn: 7.4874913\ttotal: 8m 17s\tremaining: 42m 43s\n",
      "1624:\tlearn: 7.4852800\ttotal: 8m 17s\tremaining: 42m 43s\n",
      "1625:\tlearn: 7.4832190\ttotal: 8m 17s\tremaining: 42m 42s\n",
      "1626:\tlearn: 7.4822801\ttotal: 8m 17s\tremaining: 42m 42s\n",
      "1627:\tlearn: 7.4810562\ttotal: 8m 18s\tremaining: 42m 41s\n",
      "1628:\tlearn: 7.4789746\ttotal: 8m 18s\tremaining: 42m 42s\n",
      "1629:\tlearn: 7.4772496\ttotal: 8m 18s\tremaining: 42m 41s\n",
      "1630:\tlearn: 7.4756734\ttotal: 8m 19s\tremaining: 42m 41s\n",
      "1631:\tlearn: 7.4739024\ttotal: 8m 19s\tremaining: 42m 41s\n",
      "1632:\tlearn: 7.4728129\ttotal: 8m 19s\tremaining: 42m 40s\n",
      "1633:\tlearn: 7.4713671\ttotal: 8m 20s\tremaining: 42m 40s\n",
      "1634:\tlearn: 7.4689779\ttotal: 8m 20s\tremaining: 42m 40s\n",
      "1635:\tlearn: 7.4670200\ttotal: 8m 20s\tremaining: 42m 39s\n",
      "1636:\tlearn: 7.4652873\ttotal: 8m 21s\tremaining: 42m 39s\n",
      "1637:\tlearn: 7.4642833\ttotal: 8m 21s\tremaining: 42m 39s\n",
      "1638:\tlearn: 7.4625746\ttotal: 8m 21s\tremaining: 42m 39s\n",
      "1639:\tlearn: 7.4612392\ttotal: 8m 21s\tremaining: 42m 38s\n",
      "1640:\tlearn: 7.4592683\ttotal: 8m 22s\tremaining: 42m 38s\n",
      "1641:\tlearn: 7.4577040\ttotal: 8m 22s\tremaining: 42m 38s\n",
      "1642:\tlearn: 7.4561686\ttotal: 8m 22s\tremaining: 42m 38s\n",
      "1643:\tlearn: 7.4546398\ttotal: 8m 23s\tremaining: 42m 37s\n",
      "1644:\tlearn: 7.4532920\ttotal: 8m 23s\tremaining: 42m 37s\n",
      "1645:\tlearn: 7.4513985\ttotal: 8m 23s\tremaining: 42m 36s\n",
      "1646:\tlearn: 7.4489950\ttotal: 8m 24s\tremaining: 42m 36s\n",
      "1647:\tlearn: 7.4476747\ttotal: 8m 24s\tremaining: 42m 36s\n",
      "1648:\tlearn: 7.4458446\ttotal: 8m 24s\tremaining: 42m 36s\n",
      "1649:\tlearn: 7.4443264\ttotal: 8m 25s\tremaining: 42m 36s\n",
      "1650:\tlearn: 7.4429536\ttotal: 8m 25s\tremaining: 42m 36s\n",
      "1651:\tlearn: 7.4415233\ttotal: 8m 25s\tremaining: 42m 36s\n",
      "1652:\tlearn: 7.4404862\ttotal: 8m 26s\tremaining: 42m 35s\n",
      "1653:\tlearn: 7.4392532\ttotal: 8m 26s\tremaining: 42m 35s\n",
      "1654:\tlearn: 7.4368588\ttotal: 8m 26s\tremaining: 42m 35s\n",
      "1655:\tlearn: 7.4353517\ttotal: 8m 27s\tremaining: 42m 35s\n",
      "1656:\tlearn: 7.4340059\ttotal: 8m 27s\tremaining: 42m 34s\n",
      "1657:\tlearn: 7.4323719\ttotal: 8m 27s\tremaining: 42m 34s\n",
      "1658:\tlearn: 7.4304486\ttotal: 8m 27s\tremaining: 42m 33s\n",
      "1659:\tlearn: 7.4293939\ttotal: 8m 28s\tremaining: 42m 33s\n",
      "1660:\tlearn: 7.4281916\ttotal: 8m 28s\tremaining: 42m 33s\n",
      "1661:\tlearn: 7.4266993\ttotal: 8m 28s\tremaining: 42m 32s\n",
      "1662:\tlearn: 7.4253001\ttotal: 8m 29s\tremaining: 42m 32s\n",
      "1663:\tlearn: 7.4243439\ttotal: 8m 29s\tremaining: 42m 31s\n",
      "1664:\tlearn: 7.4232895\ttotal: 8m 29s\tremaining: 42m 31s\n",
      "1665:\tlearn: 7.4214575\ttotal: 8m 29s\tremaining: 42m 31s\n",
      "1666:\tlearn: 7.4205195\ttotal: 8m 30s\tremaining: 42m 30s\n",
      "1667:\tlearn: 7.4191397\ttotal: 8m 30s\tremaining: 42m 30s\n",
      "1668:\tlearn: 7.4174772\ttotal: 8m 30s\tremaining: 42m 30s\n",
      "1669:\tlearn: 7.4165118\ttotal: 8m 31s\tremaining: 42m 29s\n",
      "1670:\tlearn: 7.4154558\ttotal: 8m 31s\tremaining: 42m 29s\n",
      "1671:\tlearn: 7.4136475\ttotal: 8m 31s\tremaining: 42m 29s\n",
      "1672:\tlearn: 7.4117139\ttotal: 8m 32s\tremaining: 42m 28s\n",
      "1673:\tlearn: 7.4101027\ttotal: 8m 32s\tremaining: 42m 28s\n",
      "1674:\tlearn: 7.4089028\ttotal: 8m 32s\tremaining: 42m 28s\n",
      "1675:\tlearn: 7.4079352\ttotal: 8m 33s\tremaining: 42m 27s\n",
      "1676:\tlearn: 7.4062267\ttotal: 8m 33s\tremaining: 42m 27s\n",
      "1677:\tlearn: 7.4040421\ttotal: 8m 33s\tremaining: 42m 27s\n",
      "1678:\tlearn: 7.4021946\ttotal: 8m 34s\tremaining: 42m 27s\n",
      "1679:\tlearn: 7.4004550\ttotal: 8m 34s\tremaining: 42m 27s\n",
      "1680:\tlearn: 7.3987375\ttotal: 8m 34s\tremaining: 42m 28s\n",
      "1681:\tlearn: 7.3966181\ttotal: 8m 35s\tremaining: 42m 27s\n",
      "1682:\tlearn: 7.3955945\ttotal: 8m 35s\tremaining: 42m 27s\n",
      "1683:\tlearn: 7.3943257\ttotal: 8m 35s\tremaining: 42m 26s\n",
      "1684:\tlearn: 7.3930111\ttotal: 8m 35s\tremaining: 42m 26s\n",
      "1685:\tlearn: 7.3912678\ttotal: 8m 36s\tremaining: 42m 25s\n",
      "1686:\tlearn: 7.3895988\ttotal: 8m 36s\tremaining: 42m 25s\n",
      "1687:\tlearn: 7.3886382\ttotal: 8m 36s\tremaining: 42m 25s\n",
      "1688:\tlearn: 7.3873808\ttotal: 8m 37s\tremaining: 42m 24s\n",
      "1689:\tlearn: 7.3861446\ttotal: 8m 37s\tremaining: 42m 24s\n",
      "1690:\tlearn: 7.3850173\ttotal: 8m 37s\tremaining: 42m 24s\n",
      "1691:\tlearn: 7.3838674\ttotal: 8m 38s\tremaining: 42m 23s\n",
      "1692:\tlearn: 7.3826738\ttotal: 8m 38s\tremaining: 42m 23s\n",
      "1693:\tlearn: 7.3807643\ttotal: 8m 38s\tremaining: 42m 22s\n",
      "1694:\tlearn: 7.3796247\ttotal: 8m 38s\tremaining: 42m 22s\n",
      "1695:\tlearn: 7.3784416\ttotal: 8m 39s\tremaining: 42m 21s\n",
      "1696:\tlearn: 7.3763893\ttotal: 8m 39s\tremaining: 42m 21s\n",
      "1697:\tlearn: 7.3741031\ttotal: 8m 39s\tremaining: 42m 21s\n",
      "1698:\tlearn: 7.3729350\ttotal: 8m 40s\tremaining: 42m 21s\n",
      "1699:\tlearn: 7.3710330\ttotal: 8m 40s\tremaining: 42m 21s\n",
      "1700:\tlearn: 7.3695878\ttotal: 8m 40s\tremaining: 42m 21s\n",
      "1701:\tlearn: 7.3684897\ttotal: 8m 41s\tremaining: 42m 20s\n",
      "1702:\tlearn: 7.3672811\ttotal: 8m 41s\tremaining: 42m 20s\n",
      "1703:\tlearn: 7.3663032\ttotal: 8m 41s\tremaining: 42m 20s\n",
      "1704:\tlearn: 7.3643677\ttotal: 8m 42s\tremaining: 42m 20s\n",
      "1705:\tlearn: 7.3623927\ttotal: 8m 42s\tremaining: 42m 19s\n",
      "1706:\tlearn: 7.3610034\ttotal: 8m 42s\tremaining: 42m 19s\n",
      "1707:\tlearn: 7.3601166\ttotal: 8m 42s\tremaining: 42m 18s\n",
      "1708:\tlearn: 7.3583283\ttotal: 8m 43s\tremaining: 42m 18s\n",
      "1709:\tlearn: 7.3572168\ttotal: 8m 43s\tremaining: 42m 17s\n",
      "1710:\tlearn: 7.3558341\ttotal: 8m 43s\tremaining: 42m 17s\n",
      "1711:\tlearn: 7.3544763\ttotal: 8m 44s\tremaining: 42m 17s\n",
      "1712:\tlearn: 7.3533980\ttotal: 8m 44s\tremaining: 42m 16s\n",
      "1713:\tlearn: 7.3515767\ttotal: 8m 44s\tremaining: 42m 16s\n",
      "1714:\tlearn: 7.3497525\ttotal: 8m 45s\tremaining: 42m 16s\n",
      "1715:\tlearn: 7.3480163\ttotal: 8m 45s\tremaining: 42m 16s\n",
      "1716:\tlearn: 7.3469238\ttotal: 8m 45s\tremaining: 42m 15s\n",
      "1717:\tlearn: 7.3452124\ttotal: 8m 46s\tremaining: 42m 15s\n",
      "1718:\tlearn: 7.3440724\ttotal: 8m 46s\tremaining: 42m 15s\n",
      "1719:\tlearn: 7.3425109\ttotal: 8m 46s\tremaining: 42m 15s\n",
      "1720:\tlearn: 7.3412078\ttotal: 8m 46s\tremaining: 42m 15s\n",
      "1721:\tlearn: 7.3391378\ttotal: 8m 47s\tremaining: 42m 15s\n",
      "1722:\tlearn: 7.3372255\ttotal: 8m 47s\tremaining: 42m 15s\n",
      "1723:\tlearn: 7.3358176\ttotal: 8m 48s\tremaining: 42m 14s\n",
      "1724:\tlearn: 7.3343008\ttotal: 8m 48s\tremaining: 42m 14s\n",
      "1725:\tlearn: 7.3333786\ttotal: 8m 48s\tremaining: 42m 14s\n",
      "1726:\tlearn: 7.3321042\ttotal: 8m 48s\tremaining: 42m 14s\n",
      "1727:\tlearn: 7.3310334\ttotal: 8m 49s\tremaining: 42m 13s\n",
      "1728:\tlearn: 7.3301419\ttotal: 8m 49s\tremaining: 42m 12s\n",
      "1729:\tlearn: 7.3283505\ttotal: 8m 49s\tremaining: 42m 12s\n",
      "1730:\tlearn: 7.3264798\ttotal: 8m 50s\tremaining: 42m 12s\n",
      "1731:\tlearn: 7.3246010\ttotal: 8m 50s\tremaining: 42m 12s\n",
      "1732:\tlearn: 7.3234514\ttotal: 8m 50s\tremaining: 42m 11s\n",
      "1733:\tlearn: 7.3224664\ttotal: 8m 51s\tremaining: 42m 11s\n",
      "1734:\tlearn: 7.3211331\ttotal: 8m 51s\tremaining: 42m 11s\n",
      "1735:\tlearn: 7.3196157\ttotal: 8m 51s\tremaining: 42m 11s\n",
      "1736:\tlearn: 7.3182082\ttotal: 8m 52s\tremaining: 42m 10s\n",
      "1737:\tlearn: 7.3167127\ttotal: 8m 52s\tremaining: 42m 10s\n",
      "1738:\tlearn: 7.3157093\ttotal: 8m 52s\tremaining: 42m 10s\n",
      "1739:\tlearn: 7.3139255\ttotal: 8m 52s\tremaining: 42m 9s\n",
      "1740:\tlearn: 7.3126974\ttotal: 8m 53s\tremaining: 42m 9s\n",
      "1741:\tlearn: 7.3110454\ttotal: 8m 53s\tremaining: 42m 9s\n",
      "1742:\tlearn: 7.3099047\ttotal: 8m 53s\tremaining: 42m 8s\n",
      "1743:\tlearn: 7.3090136\ttotal: 8m 54s\tremaining: 42m 8s\n",
      "1744:\tlearn: 7.3081980\ttotal: 8m 54s\tremaining: 42m 7s\n",
      "1745:\tlearn: 7.3072542\ttotal: 8m 54s\tremaining: 42m 7s\n",
      "1746:\tlearn: 7.3050895\ttotal: 8m 54s\tremaining: 42m 6s\n",
      "1747:\tlearn: 7.3036733\ttotal: 8m 55s\tremaining: 42m 6s\n",
      "1748:\tlearn: 7.3021310\ttotal: 8m 55s\tremaining: 42m 6s\n",
      "1749:\tlearn: 7.3006618\ttotal: 8m 55s\tremaining: 42m 6s\n",
      "1750:\tlearn: 7.2990946\ttotal: 8m 56s\tremaining: 42m 6s\n",
      "1751:\tlearn: 7.2979435\ttotal: 8m 56s\tremaining: 42m 5s\n",
      "1752:\tlearn: 7.2970324\ttotal: 8m 56s\tremaining: 42m 5s\n",
      "1753:\tlearn: 7.2958001\ttotal: 8m 57s\tremaining: 42m 5s\n",
      "1754:\tlearn: 7.2941638\ttotal: 8m 57s\tremaining: 42m 4s\n",
      "1755:\tlearn: 7.2925004\ttotal: 8m 57s\tremaining: 42m 4s\n",
      "1756:\tlearn: 7.2914184\ttotal: 8m 58s\tremaining: 42m 4s\n",
      "1757:\tlearn: 7.2891847\ttotal: 8m 58s\tremaining: 42m 4s\n",
      "1758:\tlearn: 7.2876850\ttotal: 8m 58s\tremaining: 42m 4s\n",
      "1759:\tlearn: 7.2861458\ttotal: 8m 59s\tremaining: 42m 4s\n",
      "1760:\tlearn: 7.2850829\ttotal: 8m 59s\tremaining: 42m 3s\n",
      "1761:\tlearn: 7.2837603\ttotal: 8m 59s\tremaining: 42m 3s\n",
      "1762:\tlearn: 7.2816763\ttotal: 9m\tremaining: 42m 3s\n",
      "1763:\tlearn: 7.2802906\ttotal: 9m\tremaining: 42m 3s\n",
      "1764:\tlearn: 7.2788440\ttotal: 9m\tremaining: 42m 3s\n",
      "1765:\tlearn: 7.2775150\ttotal: 9m 1s\tremaining: 42m 2s\n",
      "1766:\tlearn: 7.2760112\ttotal: 9m 1s\tremaining: 42m 2s\n",
      "1767:\tlearn: 7.2750002\ttotal: 9m 1s\tremaining: 42m 2s\n",
      "1768:\tlearn: 7.2735897\ttotal: 9m 2s\tremaining: 42m 1s\n",
      "1769:\tlearn: 7.2717355\ttotal: 9m 2s\tremaining: 42m 1s\n",
      "1770:\tlearn: 7.2708487\ttotal: 9m 2s\tremaining: 42m 1s\n",
      "1771:\tlearn: 7.2694801\ttotal: 9m 2s\tremaining: 42m\n",
      "1772:\tlearn: 7.2681876\ttotal: 9m 3s\tremaining: 42m\n",
      "1773:\tlearn: 7.2666422\ttotal: 9m 3s\tremaining: 42m\n",
      "1774:\tlearn: 7.2647560\ttotal: 9m 3s\tremaining: 42m\n",
      "1775:\tlearn: 7.2637718\ttotal: 9m 4s\tremaining: 41m 59s\n",
      "1776:\tlearn: 7.2625458\ttotal: 9m 4s\tremaining: 41m 59s\n",
      "1777:\tlearn: 7.2607485\ttotal: 9m 4s\tremaining: 41m 59s\n",
      "1778:\tlearn: 7.2597313\ttotal: 9m 5s\tremaining: 41m 58s\n",
      "1779:\tlearn: 7.2583881\ttotal: 9m 5s\tremaining: 41m 58s\n",
      "1780:\tlearn: 7.2566612\ttotal: 9m 5s\tremaining: 41m 57s\n",
      "1781:\tlearn: 7.2546935\ttotal: 9m 5s\tremaining: 41m 57s\n",
      "1782:\tlearn: 7.2530610\ttotal: 9m 6s\tremaining: 41m 56s\n",
      "1783:\tlearn: 7.2513514\ttotal: 9m 6s\tremaining: 41m 56s\n",
      "1784:\tlearn: 7.2492773\ttotal: 9m 6s\tremaining: 41m 56s\n",
      "1785:\tlearn: 7.2480616\ttotal: 9m 7s\tremaining: 41m 55s\n",
      "1786:\tlearn: 7.2463138\ttotal: 9m 7s\tremaining: 41m 55s\n",
      "1787:\tlearn: 7.2451653\ttotal: 9m 7s\tremaining: 41m 54s\n",
      "1788:\tlearn: 7.2435901\ttotal: 9m 7s\tremaining: 41m 54s\n",
      "1789:\tlearn: 7.2425175\ttotal: 9m 8s\tremaining: 41m 53s\n",
      "1790:\tlearn: 7.2414801\ttotal: 9m 8s\tremaining: 41m 53s\n",
      "1791:\tlearn: 7.2406123\ttotal: 9m 8s\tremaining: 41m 52s\n",
      "1792:\tlearn: 7.2385993\ttotal: 9m 8s\tremaining: 41m 52s\n",
      "1793:\tlearn: 7.2373442\ttotal: 9m 9s\tremaining: 41m 52s\n",
      "1794:\tlearn: 7.2360964\ttotal: 9m 9s\tremaining: 41m 52s\n",
      "1795:\tlearn: 7.2352948\ttotal: 9m 9s\tremaining: 41m 51s\n",
      "1796:\tlearn: 7.2339923\ttotal: 9m 10s\tremaining: 41m 50s\n",
      "1797:\tlearn: 7.2330544\ttotal: 9m 10s\tremaining: 41m 50s\n",
      "1798:\tlearn: 7.2311907\ttotal: 9m 10s\tremaining: 41m 49s\n",
      "1799:\tlearn: 7.2296250\ttotal: 9m 10s\tremaining: 41m 49s\n",
      "1800:\tlearn: 7.2280577\ttotal: 9m 11s\tremaining: 41m 49s\n",
      "1801:\tlearn: 7.2269796\ttotal: 9m 11s\tremaining: 41m 48s\n",
      "1802:\tlearn: 7.2255406\ttotal: 9m 11s\tremaining: 41m 48s\n",
      "1803:\tlearn: 7.2238362\ttotal: 9m 12s\tremaining: 41m 48s\n",
      "1804:\tlearn: 7.2222972\ttotal: 9m 12s\tremaining: 41m 47s\n",
      "1805:\tlearn: 7.2214389\ttotal: 9m 12s\tremaining: 41m 47s\n",
      "1806:\tlearn: 7.2202905\ttotal: 9m 12s\tremaining: 41m 46s\n",
      "1807:\tlearn: 7.2189125\ttotal: 9m 13s\tremaining: 41m 46s\n",
      "1808:\tlearn: 7.2171805\ttotal: 9m 13s\tremaining: 41m 45s\n",
      "1809:\tlearn: 7.2157067\ttotal: 9m 13s\tremaining: 41m 45s\n",
      "1810:\tlearn: 7.2140034\ttotal: 9m 14s\tremaining: 41m 45s\n",
      "1811:\tlearn: 7.2127585\ttotal: 9m 14s\tremaining: 41m 44s\n",
      "1812:\tlearn: 7.2114179\ttotal: 9m 14s\tremaining: 41m 44s\n",
      "1813:\tlearn: 7.2095957\ttotal: 9m 14s\tremaining: 41m 44s\n",
      "1814:\tlearn: 7.2084589\ttotal: 9m 15s\tremaining: 41m 43s\n",
      "1815:\tlearn: 7.2068024\ttotal: 9m 15s\tremaining: 41m 43s\n",
      "1816:\tlearn: 7.2057329\ttotal: 9m 15s\tremaining: 41m 42s\n",
      "1817:\tlearn: 7.2044569\ttotal: 9m 15s\tremaining: 41m 41s\n",
      "1818:\tlearn: 7.2029257\ttotal: 9m 16s\tremaining: 41m 41s\n",
      "1819:\tlearn: 7.2010343\ttotal: 9m 16s\tremaining: 41m 41s\n",
      "1820:\tlearn: 7.1994380\ttotal: 9m 16s\tremaining: 41m 41s\n",
      "1821:\tlearn: 7.1984076\ttotal: 9m 17s\tremaining: 41m 40s\n",
      "1822:\tlearn: 7.1963202\ttotal: 9m 17s\tremaining: 41m 40s\n",
      "1823:\tlearn: 7.1952244\ttotal: 9m 17s\tremaining: 41m 39s\n",
      "1824:\tlearn: 7.1939956\ttotal: 9m 17s\tremaining: 41m 39s\n",
      "1825:\tlearn: 7.1925358\ttotal: 9m 18s\tremaining: 41m 39s\n",
      "1826:\tlearn: 7.1912806\ttotal: 9m 18s\tremaining: 41m 38s\n",
      "1827:\tlearn: 7.1896280\ttotal: 9m 18s\tremaining: 41m 38s\n",
      "1828:\tlearn: 7.1866824\ttotal: 9m 19s\tremaining: 41m 37s\n",
      "1829:\tlearn: 7.1843768\ttotal: 9m 19s\tremaining: 41m 37s\n",
      "1830:\tlearn: 7.1832413\ttotal: 9m 19s\tremaining: 41m 37s\n",
      "1831:\tlearn: 7.1816394\ttotal: 9m 19s\tremaining: 41m 36s\n",
      "1832:\tlearn: 7.1808404\ttotal: 9m 20s\tremaining: 41m 36s\n",
      "1833:\tlearn: 7.1796076\ttotal: 9m 20s\tremaining: 41m 35s\n",
      "1834:\tlearn: 7.1783476\ttotal: 9m 20s\tremaining: 41m 34s\n",
      "1835:\tlearn: 7.1763053\ttotal: 9m 21s\tremaining: 41m 34s\n",
      "1836:\tlearn: 7.1749953\ttotal: 9m 21s\tremaining: 41m 34s\n",
      "1837:\tlearn: 7.1737863\ttotal: 9m 21s\tremaining: 41m 33s\n",
      "1838:\tlearn: 7.1726551\ttotal: 9m 21s\tremaining: 41m 33s\n",
      "1839:\tlearn: 7.1716411\ttotal: 9m 22s\tremaining: 41m 32s\n",
      "1840:\tlearn: 7.1706410\ttotal: 9m 22s\tremaining: 41m 32s\n",
      "1841:\tlearn: 7.1691326\ttotal: 9m 22s\tremaining: 41m 32s\n",
      "1842:\tlearn: 7.1680197\ttotal: 9m 22s\tremaining: 41m 31s\n",
      "1843:\tlearn: 7.1663817\ttotal: 9m 23s\tremaining: 41m 31s\n",
      "1844:\tlearn: 7.1652237\ttotal: 9m 23s\tremaining: 41m 30s\n",
      "1845:\tlearn: 7.1635791\ttotal: 9m 23s\tremaining: 41m 30s\n",
      "1846:\tlearn: 7.1623101\ttotal: 9m 23s\tremaining: 41m 29s\n",
      "1847:\tlearn: 7.1614715\ttotal: 9m 24s\tremaining: 41m 28s\n",
      "1848:\tlearn: 7.1598268\ttotal: 9m 24s\tremaining: 41m 28s\n",
      "1849:\tlearn: 7.1588837\ttotal: 9m 24s\tremaining: 41m 27s\n",
      "1850:\tlearn: 7.1572175\ttotal: 9m 25s\tremaining: 41m 27s\n",
      "1851:\tlearn: 7.1561923\ttotal: 9m 25s\tremaining: 41m 26s\n",
      "1852:\tlearn: 7.1554484\ttotal: 9m 25s\tremaining: 41m 26s\n",
      "1853:\tlearn: 7.1540246\ttotal: 9m 25s\tremaining: 41m 26s\n",
      "1854:\tlearn: 7.1522997\ttotal: 9m 26s\tremaining: 41m 25s\n",
      "1855:\tlearn: 7.1505442\ttotal: 9m 26s\tremaining: 41m 25s\n",
      "1856:\tlearn: 7.1493838\ttotal: 9m 26s\tremaining: 41m 25s\n",
      "1857:\tlearn: 7.1484345\ttotal: 9m 26s\tremaining: 41m 24s\n",
      "1858:\tlearn: 7.1455897\ttotal: 9m 27s\tremaining: 41m 24s\n",
      "1859:\tlearn: 7.1435789\ttotal: 9m 27s\tremaining: 41m 24s\n",
      "1860:\tlearn: 7.1423070\ttotal: 9m 27s\tremaining: 41m 23s\n",
      "1861:\tlearn: 7.1411315\ttotal: 9m 28s\tremaining: 41m 23s\n",
      "1862:\tlearn: 7.1391712\ttotal: 9m 28s\tremaining: 41m 23s\n",
      "1863:\tlearn: 7.1378739\ttotal: 9m 28s\tremaining: 41m 23s\n",
      "1864:\tlearn: 7.1364061\ttotal: 9m 29s\tremaining: 41m 22s\n",
      "1865:\tlearn: 7.1345377\ttotal: 9m 29s\tremaining: 41m 22s\n",
      "1866:\tlearn: 7.1335874\ttotal: 9m 29s\tremaining: 41m 21s\n",
      "1867:\tlearn: 7.1325869\ttotal: 9m 29s\tremaining: 41m 21s\n",
      "1868:\tlearn: 7.1311137\ttotal: 9m 30s\tremaining: 41m 20s\n",
      "1869:\tlearn: 7.1299754\ttotal: 9m 30s\tremaining: 41m 20s\n",
      "1870:\tlearn: 7.1286309\ttotal: 9m 30s\tremaining: 41m 19s\n",
      "1871:\tlearn: 7.1273348\ttotal: 9m 31s\tremaining: 41m 19s\n",
      "1872:\tlearn: 7.1258996\ttotal: 9m 31s\tremaining: 41m 19s\n",
      "1873:\tlearn: 7.1239532\ttotal: 9m 31s\tremaining: 41m 18s\n",
      "1874:\tlearn: 7.1228876\ttotal: 9m 31s\tremaining: 41m 18s\n",
      "1875:\tlearn: 7.1218137\ttotal: 9m 32s\tremaining: 41m 18s\n",
      "1876:\tlearn: 7.1207588\ttotal: 9m 32s\tremaining: 41m 17s\n",
      "1877:\tlearn: 7.1191603\ttotal: 9m 32s\tremaining: 41m 17s\n",
      "1878:\tlearn: 7.1176251\ttotal: 9m 32s\tremaining: 41m 16s\n",
      "1879:\tlearn: 7.1165703\ttotal: 9m 33s\tremaining: 41m 15s\n",
      "1880:\tlearn: 7.1154509\ttotal: 9m 33s\tremaining: 41m 15s\n",
      "1881:\tlearn: 7.1135236\ttotal: 9m 33s\tremaining: 41m 15s\n",
      "1882:\tlearn: 7.1118067\ttotal: 9m 34s\tremaining: 41m 15s\n",
      "1883:\tlearn: 7.1100445\ttotal: 9m 34s\tremaining: 41m 14s\n",
      "1884:\tlearn: 7.1074781\ttotal: 9m 34s\tremaining: 41m 14s\n",
      "1885:\tlearn: 7.1066232\ttotal: 9m 35s\tremaining: 41m 14s\n",
      "1886:\tlearn: 7.1054246\ttotal: 9m 35s\tremaining: 41m 14s\n",
      "1887:\tlearn: 7.1045077\ttotal: 9m 35s\tremaining: 41m 13s\n",
      "1888:\tlearn: 7.1033963\ttotal: 9m 36s\tremaining: 41m 13s\n",
      "1889:\tlearn: 7.1025339\ttotal: 9m 36s\tremaining: 41m 12s\n",
      "1890:\tlearn: 7.1016379\ttotal: 9m 36s\tremaining: 41m 12s\n",
      "1891:\tlearn: 7.1006554\ttotal: 9m 36s\tremaining: 41m 11s\n",
      "1892:\tlearn: 7.0993248\ttotal: 9m 37s\tremaining: 41m 11s\n",
      "1893:\tlearn: 7.0982351\ttotal: 9m 37s\tremaining: 41m 10s\n",
      "1894:\tlearn: 7.0968120\ttotal: 9m 37s\tremaining: 41m 10s\n",
      "1895:\tlearn: 7.0956906\ttotal: 9m 37s\tremaining: 41m 10s\n",
      "1896:\tlearn: 7.0937747\ttotal: 9m 38s\tremaining: 41m 9s\n",
      "1897:\tlearn: 7.0923994\ttotal: 9m 38s\tremaining: 41m 9s\n",
      "1898:\tlearn: 7.0911769\ttotal: 9m 38s\tremaining: 41m 9s\n",
      "1899:\tlearn: 7.0904226\ttotal: 9m 39s\tremaining: 41m 8s\n",
      "1900:\tlearn: 7.0892178\ttotal: 9m 39s\tremaining: 41m 8s\n",
      "1901:\tlearn: 7.0874901\ttotal: 9m 39s\tremaining: 41m 7s\n",
      "1902:\tlearn: 7.0861452\ttotal: 9m 39s\tremaining: 41m 7s\n",
      "1903:\tlearn: 7.0848351\ttotal: 9m 40s\tremaining: 41m 6s\n",
      "1904:\tlearn: 7.0834349\ttotal: 9m 40s\tremaining: 41m 6s\n",
      "1905:\tlearn: 7.0820156\ttotal: 9m 40s\tremaining: 41m 5s\n",
      "1906:\tlearn: 7.0808189\ttotal: 9m 40s\tremaining: 41m 5s\n",
      "1907:\tlearn: 7.0794783\ttotal: 9m 41s\tremaining: 41m 5s\n",
      "1908:\tlearn: 7.0787331\ttotal: 9m 41s\tremaining: 41m 4s\n",
      "1909:\tlearn: 7.0777911\ttotal: 9m 41s\tremaining: 41m 4s\n",
      "1910:\tlearn: 7.0765688\ttotal: 9m 42s\tremaining: 41m 3s\n",
      "1911:\tlearn: 7.0748622\ttotal: 9m 42s\tremaining: 41m 3s\n",
      "1912:\tlearn: 7.0726717\ttotal: 9m 42s\tremaining: 41m 3s\n",
      "1913:\tlearn: 7.0710385\ttotal: 9m 42s\tremaining: 41m 2s\n",
      "1914:\tlearn: 7.0693555\ttotal: 9m 43s\tremaining: 41m 2s\n",
      "1915:\tlearn: 7.0682930\ttotal: 9m 43s\tremaining: 41m 1s\n",
      "1916:\tlearn: 7.0670679\ttotal: 9m 43s\tremaining: 41m 1s\n",
      "1917:\tlearn: 7.0657325\ttotal: 9m 43s\tremaining: 41m\n",
      "1918:\tlearn: 7.0647163\ttotal: 9m 44s\tremaining: 41m\n",
      "1919:\tlearn: 7.0635911\ttotal: 9m 44s\tremaining: 40m 59s\n",
      "1920:\tlearn: 7.0626965\ttotal: 9m 44s\tremaining: 40m 59s\n",
      "1921:\tlearn: 7.0618909\ttotal: 9m 45s\tremaining: 40m 59s\n",
      "1922:\tlearn: 7.0605525\ttotal: 9m 45s\tremaining: 40m 58s\n",
      "1923:\tlearn: 7.0587605\ttotal: 9m 45s\tremaining: 40m 58s\n",
      "1924:\tlearn: 7.0564118\ttotal: 9m 46s\tremaining: 40m 58s\n",
      "1925:\tlearn: 7.0548544\ttotal: 9m 46s\tremaining: 40m 58s\n",
      "1926:\tlearn: 7.0534307\ttotal: 9m 46s\tremaining: 40m 57s\n",
      "1927:\tlearn: 7.0520805\ttotal: 9m 46s\tremaining: 40m 57s\n",
      "1928:\tlearn: 7.0505318\ttotal: 9m 47s\tremaining: 40m 57s\n",
      "1929:\tlearn: 7.0486971\ttotal: 9m 47s\tremaining: 40m 57s\n",
      "1930:\tlearn: 7.0467997\ttotal: 9m 47s\tremaining: 40m 56s\n",
      "1931:\tlearn: 7.0454635\ttotal: 9m 48s\tremaining: 40m 56s\n",
      "1932:\tlearn: 7.0443391\ttotal: 9m 48s\tremaining: 40m 56s\n",
      "1933:\tlearn: 7.0430299\ttotal: 9m 48s\tremaining: 40m 55s\n",
      "1934:\tlearn: 7.0418597\ttotal: 9m 49s\tremaining: 40m 55s\n",
      "1935:\tlearn: 7.0409432\ttotal: 9m 49s\tremaining: 40m 55s\n",
      "1936:\tlearn: 7.0401571\ttotal: 9m 49s\tremaining: 40m 54s\n",
      "1937:\tlearn: 7.0391140\ttotal: 9m 49s\tremaining: 40m 54s\n",
      "1938:\tlearn: 7.0380924\ttotal: 9m 50s\tremaining: 40m 53s\n",
      "1939:\tlearn: 7.0367380\ttotal: 9m 50s\tremaining: 40m 53s\n",
      "1940:\tlearn: 7.0359442\ttotal: 9m 50s\tremaining: 40m 52s\n",
      "1941:\tlearn: 7.0350796\ttotal: 9m 51s\tremaining: 40m 52s\n",
      "1942:\tlearn: 7.0335690\ttotal: 9m 51s\tremaining: 40m 52s\n",
      "1943:\tlearn: 7.0321326\ttotal: 9m 51s\tremaining: 40m 51s\n",
      "1944:\tlearn: 7.0307643\ttotal: 9m 51s\tremaining: 40m 51s\n",
      "1945:\tlearn: 7.0297764\ttotal: 9m 52s\tremaining: 40m 50s\n",
      "1946:\tlearn: 7.0286745\ttotal: 9m 52s\tremaining: 40m 50s\n",
      "1947:\tlearn: 7.0270783\ttotal: 9m 52s\tremaining: 40m 49s\n",
      "1948:\tlearn: 7.0260254\ttotal: 9m 52s\tremaining: 40m 49s\n",
      "1949:\tlearn: 7.0245435\ttotal: 9m 53s\tremaining: 40m 48s\n",
      "1950:\tlearn: 7.0233280\ttotal: 9m 53s\tremaining: 40m 48s\n",
      "1951:\tlearn: 7.0220993\ttotal: 9m 53s\tremaining: 40m 47s\n",
      "1952:\tlearn: 7.0209755\ttotal: 9m 54s\tremaining: 40m 47s\n",
      "1953:\tlearn: 7.0191690\ttotal: 9m 54s\tremaining: 40m 47s\n",
      "1954:\tlearn: 7.0182227\ttotal: 9m 54s\tremaining: 40m 47s\n",
      "1955:\tlearn: 7.0169509\ttotal: 9m 54s\tremaining: 40m 46s\n",
      "1956:\tlearn: 7.0153901\ttotal: 9m 55s\tremaining: 40m 46s\n",
      "1957:\tlearn: 7.0143411\ttotal: 9m 55s\tremaining: 40m 45s\n",
      "1958:\tlearn: 7.0132200\ttotal: 9m 55s\tremaining: 40m 45s\n",
      "1959:\tlearn: 7.0125297\ttotal: 9m 55s\tremaining: 40m 44s\n",
      "1960:\tlearn: 7.0118094\ttotal: 9m 56s\tremaining: 40m 43s\n",
      "1961:\tlearn: 7.0103101\ttotal: 9m 56s\tremaining: 40m 43s\n",
      "1962:\tlearn: 7.0091980\ttotal: 9m 56s\tremaining: 40m 43s\n",
      "1963:\tlearn: 7.0078170\ttotal: 9m 56s\tremaining: 40m 42s\n",
      "1964:\tlearn: 7.0064329\ttotal: 9m 57s\tremaining: 40m 42s\n",
      "1965:\tlearn: 7.0049660\ttotal: 9m 57s\tremaining: 40m 41s\n",
      "1966:\tlearn: 7.0040941\ttotal: 9m 57s\tremaining: 40m 41s\n",
      "1967:\tlearn: 7.0026467\ttotal: 9m 57s\tremaining: 40m 40s\n",
      "1968:\tlearn: 7.0017011\ttotal: 9m 58s\tremaining: 40m 40s\n",
      "1969:\tlearn: 7.0009144\ttotal: 9m 58s\tremaining: 40m 39s\n",
      "1970:\tlearn: 6.9991355\ttotal: 9m 58s\tremaining: 40m 39s\n",
      "1971:\tlearn: 6.9980990\ttotal: 9m 59s\tremaining: 40m 38s\n",
      "1972:\tlearn: 6.9970424\ttotal: 9m 59s\tremaining: 40m 38s\n",
      "1973:\tlearn: 6.9961660\ttotal: 9m 59s\tremaining: 40m 37s\n",
      "1974:\tlearn: 6.9950084\ttotal: 9m 59s\tremaining: 40m 37s\n",
      "1975:\tlearn: 6.9935692\ttotal: 10m\tremaining: 40m 36s\n",
      "1976:\tlearn: 6.9920542\ttotal: 10m\tremaining: 40m 36s\n",
      "1977:\tlearn: 6.9906984\ttotal: 10m\tremaining: 40m 36s\n",
      "1978:\tlearn: 6.9889796\ttotal: 10m\tremaining: 40m 35s\n",
      "1979:\tlearn: 6.9880265\ttotal: 10m 1s\tremaining: 40m 35s\n",
      "1980:\tlearn: 6.9864544\ttotal: 10m 1s\tremaining: 40m 34s\n",
      "1981:\tlearn: 6.9854711\ttotal: 10m 1s\tremaining: 40m 34s\n",
      "1982:\tlearn: 6.9846183\ttotal: 10m 1s\tremaining: 40m 33s\n",
      "1983:\tlearn: 6.9825486\ttotal: 10m 2s\tremaining: 40m 33s\n",
      "1984:\tlearn: 6.9813163\ttotal: 10m 2s\tremaining: 40m 32s\n",
      "1985:\tlearn: 6.9804523\ttotal: 10m 2s\tremaining: 40m 32s\n",
      "1986:\tlearn: 6.9795675\ttotal: 10m 3s\tremaining: 40m 31s\n",
      "1987:\tlearn: 6.9786577\ttotal: 10m 3s\tremaining: 40m 31s\n",
      "1988:\tlearn: 6.9775940\ttotal: 10m 3s\tremaining: 40m 30s\n",
      "1989:\tlearn: 6.9767002\ttotal: 10m 3s\tremaining: 40m 30s\n",
      "1990:\tlearn: 6.9752793\ttotal: 10m 4s\tremaining: 40m 29s\n",
      "1991:\tlearn: 6.9740518\ttotal: 10m 4s\tremaining: 40m 29s\n",
      "1992:\tlearn: 6.9721533\ttotal: 10m 4s\tremaining: 40m 29s\n",
      "1993:\tlearn: 6.9712600\ttotal: 10m 4s\tremaining: 40m 28s\n",
      "1994:\tlearn: 6.9695948\ttotal: 10m 5s\tremaining: 40m 28s\n",
      "1995:\tlearn: 6.9684771\ttotal: 10m 5s\tremaining: 40m 28s\n",
      "1996:\tlearn: 6.9672944\ttotal: 10m 5s\tremaining: 40m 27s\n",
      "1997:\tlearn: 6.9661002\ttotal: 10m 6s\tremaining: 40m 27s\n",
      "1998:\tlearn: 6.9650808\ttotal: 10m 6s\tremaining: 40m 27s\n",
      "1999:\tlearn: 6.9640492\ttotal: 10m 6s\tremaining: 40m 26s\n",
      "2000:\tlearn: 6.9627820\ttotal: 10m 6s\tremaining: 40m 26s\n",
      "2001:\tlearn: 6.9618490\ttotal: 10m 7s\tremaining: 40m 26s\n",
      "2002:\tlearn: 6.9608623\ttotal: 10m 7s\tremaining: 40m 25s\n",
      "2003:\tlearn: 6.9596183\ttotal: 10m 7s\tremaining: 40m 25s\n",
      "2004:\tlearn: 6.9582275\ttotal: 10m 8s\tremaining: 40m 25s\n",
      "2005:\tlearn: 6.9567190\ttotal: 10m 8s\tremaining: 40m 24s\n",
      "2006:\tlearn: 6.9550912\ttotal: 10m 8s\tremaining: 40m 24s\n",
      "2007:\tlearn: 6.9534779\ttotal: 10m 9s\tremaining: 40m 24s\n",
      "2008:\tlearn: 6.9517542\ttotal: 10m 9s\tremaining: 40m 23s\n",
      "2009:\tlearn: 6.9504139\ttotal: 10m 9s\tremaining: 40m 23s\n",
      "2010:\tlearn: 6.9496896\ttotal: 10m 9s\tremaining: 40m 22s\n",
      "2011:\tlearn: 6.9477135\ttotal: 10m 10s\tremaining: 40m 22s\n",
      "2012:\tlearn: 6.9464336\ttotal: 10m 10s\tremaining: 40m 22s\n",
      "2013:\tlearn: 6.9455831\ttotal: 10m 10s\tremaining: 40m 21s\n",
      "2014:\tlearn: 6.9446913\ttotal: 10m 10s\tremaining: 40m 21s\n",
      "2015:\tlearn: 6.9433869\ttotal: 10m 11s\tremaining: 40m 20s\n",
      "2016:\tlearn: 6.9418572\ttotal: 10m 11s\tremaining: 40m 20s\n",
      "2017:\tlearn: 6.9406318\ttotal: 10m 11s\tremaining: 40m 20s\n",
      "2018:\tlearn: 6.9393866\ttotal: 10m 12s\tremaining: 40m 19s\n",
      "2019:\tlearn: 6.9379750\ttotal: 10m 12s\tremaining: 40m 19s\n",
      "2020:\tlearn: 6.9366689\ttotal: 10m 12s\tremaining: 40m 19s\n",
      "2021:\tlearn: 6.9356035\ttotal: 10m 13s\tremaining: 40m 18s\n",
      "2022:\tlearn: 6.9341511\ttotal: 10m 13s\tremaining: 40m 18s\n",
      "2023:\tlearn: 6.9331258\ttotal: 10m 13s\tremaining: 40m 18s\n",
      "2024:\tlearn: 6.9315204\ttotal: 10m 13s\tremaining: 40m 17s\n",
      "2025:\tlearn: 6.9300476\ttotal: 10m 14s\tremaining: 40m 17s\n",
      "2026:\tlearn: 6.9284702\ttotal: 10m 14s\tremaining: 40m 16s\n",
      "2027:\tlearn: 6.9267069\ttotal: 10m 14s\tremaining: 40m 16s\n",
      "2028:\tlearn: 6.9257813\ttotal: 10m 15s\tremaining: 40m 16s\n",
      "2029:\tlearn: 6.9243458\ttotal: 10m 15s\tremaining: 40m 15s\n",
      "2030:\tlearn: 6.9230007\ttotal: 10m 15s\tremaining: 40m 15s\n",
      "2031:\tlearn: 6.9215677\ttotal: 10m 16s\tremaining: 40m 15s\n",
      "2032:\tlearn: 6.9206711\ttotal: 10m 16s\tremaining: 40m 15s\n",
      "2033:\tlearn: 6.9191078\ttotal: 10m 16s\tremaining: 40m 14s\n",
      "2034:\tlearn: 6.9179465\ttotal: 10m 16s\tremaining: 40m 14s\n",
      "2035:\tlearn: 6.9171861\ttotal: 10m 17s\tremaining: 40m 14s\n",
      "2036:\tlearn: 6.9159145\ttotal: 10m 17s\tremaining: 40m 13s\n",
      "2037:\tlearn: 6.9146173\ttotal: 10m 17s\tremaining: 40m 13s\n",
      "2038:\tlearn: 6.9137077\ttotal: 10m 18s\tremaining: 40m 13s\n",
      "2039:\tlearn: 6.9122568\ttotal: 10m 18s\tremaining: 40m 12s\n",
      "2040:\tlearn: 6.9112187\ttotal: 10m 18s\tremaining: 40m 12s\n",
      "2041:\tlearn: 6.9100123\ttotal: 10m 18s\tremaining: 40m 11s\n",
      "2042:\tlearn: 6.9079039\ttotal: 10m 19s\tremaining: 40m 11s\n",
      "2043:\tlearn: 6.9067396\ttotal: 10m 19s\tremaining: 40m 11s\n",
      "2044:\tlearn: 6.9052773\ttotal: 10m 19s\tremaining: 40m 10s\n",
      "2045:\tlearn: 6.9037775\ttotal: 10m 20s\tremaining: 40m 10s\n",
      "2046:\tlearn: 6.9026826\ttotal: 10m 20s\tremaining: 40m 9s\n",
      "2047:\tlearn: 6.9017350\ttotal: 10m 20s\tremaining: 40m 9s\n",
      "2048:\tlearn: 6.8993773\ttotal: 10m 20s\tremaining: 40m 9s\n",
      "2049:\tlearn: 6.8984582\ttotal: 10m 21s\tremaining: 40m 8s\n",
      "2050:\tlearn: 6.8964428\ttotal: 10m 21s\tremaining: 40m 8s\n",
      "2051:\tlearn: 6.8950617\ttotal: 10m 21s\tremaining: 40m 8s\n",
      "2052:\tlearn: 6.8935348\ttotal: 10m 22s\tremaining: 40m 7s\n",
      "2053:\tlearn: 6.8918726\ttotal: 10m 22s\tremaining: 40m 7s\n",
      "2054:\tlearn: 6.8907967\ttotal: 10m 22s\tremaining: 40m 7s\n",
      "2055:\tlearn: 6.8895976\ttotal: 10m 22s\tremaining: 40m 6s\n",
      "2056:\tlearn: 6.8887311\ttotal: 10m 23s\tremaining: 40m 6s\n",
      "2057:\tlearn: 6.8876690\ttotal: 10m 23s\tremaining: 40m 5s\n",
      "2058:\tlearn: 6.8863992\ttotal: 10m 23s\tremaining: 40m 5s\n",
      "2059:\tlearn: 6.8854992\ttotal: 10m 24s\tremaining: 40m 5s\n",
      "2060:\tlearn: 6.8837825\ttotal: 10m 24s\tremaining: 40m 5s\n",
      "2061:\tlearn: 6.8828524\ttotal: 10m 24s\tremaining: 40m 5s\n",
      "2062:\tlearn: 6.8818823\ttotal: 10m 25s\tremaining: 40m 4s\n",
      "2063:\tlearn: 6.8806248\ttotal: 10m 25s\tremaining: 40m 4s\n",
      "2064:\tlearn: 6.8796332\ttotal: 10m 25s\tremaining: 40m 4s\n",
      "2065:\tlearn: 6.8786549\ttotal: 10m 25s\tremaining: 40m 3s\n",
      "2066:\tlearn: 6.8775263\ttotal: 10m 26s\tremaining: 40m 3s\n",
      "2067:\tlearn: 6.8759832\ttotal: 10m 26s\tremaining: 40m 2s\n",
      "2068:\tlearn: 6.8745221\ttotal: 10m 26s\tremaining: 40m 2s\n",
      "2069:\tlearn: 6.8735842\ttotal: 10m 27s\tremaining: 40m 2s\n",
      "2070:\tlearn: 6.8726914\ttotal: 10m 27s\tremaining: 40m 2s\n",
      "2071:\tlearn: 6.8715067\ttotal: 10m 27s\tremaining: 40m 1s\n",
      "2072:\tlearn: 6.8699028\ttotal: 10m 27s\tremaining: 40m 1s\n",
      "2073:\tlearn: 6.8688097\ttotal: 10m 28s\tremaining: 40m\n",
      "2074:\tlearn: 6.8667999\ttotal: 10m 28s\tremaining: 40m\n",
      "2075:\tlearn: 6.8649957\ttotal: 10m 28s\tremaining: 40m\n",
      "2076:\tlearn: 6.8639509\ttotal: 10m 29s\tremaining: 39m 59s\n",
      "2077:\tlearn: 6.8627229\ttotal: 10m 29s\tremaining: 39m 59s\n",
      "2078:\tlearn: 6.8616790\ttotal: 10m 29s\tremaining: 39m 59s\n",
      "2079:\tlearn: 6.8608262\ttotal: 10m 29s\tremaining: 39m 58s\n",
      "2080:\tlearn: 6.8597922\ttotal: 10m 30s\tremaining: 39m 58s\n",
      "2081:\tlearn: 6.8590131\ttotal: 10m 30s\tremaining: 39m 57s\n",
      "2082:\tlearn: 6.8574889\ttotal: 10m 30s\tremaining: 39m 57s\n",
      "2083:\tlearn: 6.8566371\ttotal: 10m 31s\tremaining: 39m 57s\n",
      "2084:\tlearn: 6.8551245\ttotal: 10m 31s\tremaining: 39m 56s\n",
      "2085:\tlearn: 6.8535856\ttotal: 10m 31s\tremaining: 39m 56s\n",
      "2086:\tlearn: 6.8523752\ttotal: 10m 32s\tremaining: 39m 56s\n",
      "2087:\tlearn: 6.8513501\ttotal: 10m 32s\tremaining: 39m 55s\n",
      "2088:\tlearn: 6.8497343\ttotal: 10m 32s\tremaining: 39m 55s\n",
      "2089:\tlearn: 6.8485859\ttotal: 10m 32s\tremaining: 39m 55s\n",
      "2090:\tlearn: 6.8473575\ttotal: 10m 33s\tremaining: 39m 54s\n",
      "2091:\tlearn: 6.8454933\ttotal: 10m 33s\tremaining: 39m 54s\n",
      "2092:\tlearn: 6.8442790\ttotal: 10m 33s\tremaining: 39m 54s\n",
      "2093:\tlearn: 6.8433743\ttotal: 10m 34s\tremaining: 39m 54s\n",
      "2094:\tlearn: 6.8422163\ttotal: 10m 34s\tremaining: 39m 53s\n",
      "2095:\tlearn: 6.8412828\ttotal: 10m 34s\tremaining: 39m 53s\n",
      "2096:\tlearn: 6.8396318\ttotal: 10m 34s\tremaining: 39m 53s\n",
      "2097:\tlearn: 6.8381716\ttotal: 10m 35s\tremaining: 39m 52s\n",
      "2098:\tlearn: 6.8371189\ttotal: 10m 35s\tremaining: 39m 52s\n",
      "2099:\tlearn: 6.8359225\ttotal: 10m 35s\tremaining: 39m 52s\n",
      "2100:\tlearn: 6.8344060\ttotal: 10m 36s\tremaining: 39m 51s\n",
      "2101:\tlearn: 6.8333793\ttotal: 10m 36s\tremaining: 39m 51s\n",
      "2102:\tlearn: 6.8319819\ttotal: 10m 36s\tremaining: 39m 51s\n",
      "2103:\tlearn: 6.8308882\ttotal: 10m 37s\tremaining: 39m 50s\n",
      "2104:\tlearn: 6.8295514\ttotal: 10m 37s\tremaining: 39m 50s\n",
      "2105:\tlearn: 6.8283575\ttotal: 10m 37s\tremaining: 39m 50s\n",
      "2106:\tlearn: 6.8274886\ttotal: 10m 37s\tremaining: 39m 49s\n",
      "2107:\tlearn: 6.8264021\ttotal: 10m 38s\tremaining: 39m 49s\n",
      "2108:\tlearn: 6.8251538\ttotal: 10m 38s\tremaining: 39m 49s\n",
      "2109:\tlearn: 6.8240468\ttotal: 10m 38s\tremaining: 39m 48s\n",
      "2110:\tlearn: 6.8220927\ttotal: 10m 39s\tremaining: 39m 48s\n",
      "2111:\tlearn: 6.8207792\ttotal: 10m 39s\tremaining: 39m 48s\n",
      "2112:\tlearn: 6.8200700\ttotal: 10m 39s\tremaining: 39m 48s\n",
      "2113:\tlearn: 6.8189474\ttotal: 10m 40s\tremaining: 39m 47s\n",
      "2114:\tlearn: 6.8176752\ttotal: 10m 40s\tremaining: 39m 47s\n",
      "2115:\tlearn: 6.8160205\ttotal: 10m 40s\tremaining: 39m 47s\n",
      "2116:\tlearn: 6.8154414\ttotal: 10m 41s\tremaining: 39m 47s\n",
      "2117:\tlearn: 6.8139046\ttotal: 10m 41s\tremaining: 39m 46s\n",
      "2118:\tlearn: 6.8130490\ttotal: 10m 41s\tremaining: 39m 46s\n",
      "2119:\tlearn: 6.8121948\ttotal: 10m 41s\tremaining: 39m 46s\n",
      "2120:\tlearn: 6.8107470\ttotal: 10m 42s\tremaining: 39m 45s\n",
      "2121:\tlearn: 6.8098529\ttotal: 10m 42s\tremaining: 39m 45s\n",
      "2122:\tlearn: 6.8079341\ttotal: 10m 42s\tremaining: 39m 45s\n",
      "2123:\tlearn: 6.8070048\ttotal: 10m 43s\tremaining: 39m 44s\n",
      "2124:\tlearn: 6.8050757\ttotal: 10m 43s\tremaining: 39m 44s\n",
      "2125:\tlearn: 6.8037310\ttotal: 10m 43s\tremaining: 39m 44s\n",
      "2126:\tlearn: 6.8023632\ttotal: 10m 44s\tremaining: 39m 44s\n",
      "2127:\tlearn: 6.8012125\ttotal: 10m 44s\tremaining: 39m 43s\n",
      "2128:\tlearn: 6.8001653\ttotal: 10m 44s\tremaining: 39m 43s\n",
      "2129:\tlearn: 6.7991933\ttotal: 10m 44s\tremaining: 39m 42s\n",
      "2130:\tlearn: 6.7981328\ttotal: 10m 45s\tremaining: 39m 42s\n",
      "2131:\tlearn: 6.7973096\ttotal: 10m 45s\tremaining: 39m 42s\n",
      "2132:\tlearn: 6.7961057\ttotal: 10m 45s\tremaining: 39m 41s\n",
      "2133:\tlearn: 6.7949501\ttotal: 10m 46s\tremaining: 39m 41s\n",
      "2134:\tlearn: 6.7938513\ttotal: 10m 46s\tremaining: 39m 41s\n",
      "2135:\tlearn: 6.7923671\ttotal: 10m 46s\tremaining: 39m 41s\n",
      "2136:\tlearn: 6.7905422\ttotal: 10m 47s\tremaining: 39m 41s\n",
      "2137:\tlearn: 6.7888460\ttotal: 10m 47s\tremaining: 39m 40s\n",
      "2138:\tlearn: 6.7872199\ttotal: 10m 47s\tremaining: 39m 40s\n",
      "2139:\tlearn: 6.7855755\ttotal: 10m 48s\tremaining: 39m 40s\n",
      "2140:\tlearn: 6.7840961\ttotal: 10m 48s\tremaining: 39m 40s\n",
      "2141:\tlearn: 6.7826929\ttotal: 10m 48s\tremaining: 39m 39s\n",
      "2142:\tlearn: 6.7815600\ttotal: 10m 49s\tremaining: 39m 39s\n",
      "2143:\tlearn: 6.7807765\ttotal: 10m 49s\tremaining: 39m 39s\n",
      "2144:\tlearn: 6.7791063\ttotal: 10m 49s\tremaining: 39m 39s\n",
      "2145:\tlearn: 6.7784896\ttotal: 10m 50s\tremaining: 39m 38s\n",
      "2146:\tlearn: 6.7771044\ttotal: 10m 50s\tremaining: 39m 38s\n",
      "2147:\tlearn: 6.7762479\ttotal: 10m 50s\tremaining: 39m 38s\n",
      "2148:\tlearn: 6.7748835\ttotal: 10m 50s\tremaining: 39m 38s\n",
      "2149:\tlearn: 6.7735393\ttotal: 10m 51s\tremaining: 39m 37s\n",
      "2150:\tlearn: 6.7722457\ttotal: 10m 51s\tremaining: 39m 37s\n",
      "2151:\tlearn: 6.7712543\ttotal: 10m 51s\tremaining: 39m 36s\n",
      "2152:\tlearn: 6.7696862\ttotal: 10m 52s\tremaining: 39m 36s\n",
      "2153:\tlearn: 6.7682633\ttotal: 10m 52s\tremaining: 39m 36s\n",
      "2154:\tlearn: 6.7671699\ttotal: 10m 52s\tremaining: 39m 36s\n",
      "2155:\tlearn: 6.7658638\ttotal: 10m 52s\tremaining: 39m 35s\n",
      "2156:\tlearn: 6.7646412\ttotal: 10m 53s\tremaining: 39m 35s\n",
      "2157:\tlearn: 6.7630027\ttotal: 10m 53s\tremaining: 39m 35s\n",
      "2158:\tlearn: 6.7617576\ttotal: 10m 53s\tremaining: 39m 35s\n",
      "2159:\tlearn: 6.7603691\ttotal: 10m 54s\tremaining: 39m 34s\n",
      "2160:\tlearn: 6.7590635\ttotal: 10m 54s\tremaining: 39m 34s\n",
      "2161:\tlearn: 6.7582806\ttotal: 10m 54s\tremaining: 39m 34s\n",
      "2162:\tlearn: 6.7572481\ttotal: 10m 55s\tremaining: 39m 34s\n",
      "2163:\tlearn: 6.7564001\ttotal: 10m 55s\tremaining: 39m 33s\n",
      "2164:\tlearn: 6.7547049\ttotal: 10m 55s\tremaining: 39m 33s\n",
      "2165:\tlearn: 6.7536172\ttotal: 10m 56s\tremaining: 39m 33s\n",
      "2166:\tlearn: 6.7521596\ttotal: 10m 56s\tremaining: 39m 33s\n",
      "2167:\tlearn: 6.7512647\ttotal: 10m 56s\tremaining: 39m 32s\n",
      "2168:\tlearn: 6.7504873\ttotal: 10m 57s\tremaining: 39m 32s\n",
      "2169:\tlearn: 6.7497095\ttotal: 10m 57s\tremaining: 39m 32s\n",
      "2170:\tlearn: 6.7483813\ttotal: 10m 57s\tremaining: 39m 32s\n",
      "2171:\tlearn: 6.7462582\ttotal: 10m 58s\tremaining: 39m 31s\n",
      "2172:\tlearn: 6.7445202\ttotal: 10m 58s\tremaining: 39m 31s\n",
      "2173:\tlearn: 6.7432702\ttotal: 10m 58s\tremaining: 39m 31s\n",
      "2174:\tlearn: 6.7420335\ttotal: 10m 58s\tremaining: 39m 30s\n",
      "2175:\tlearn: 6.7410637\ttotal: 10m 59s\tremaining: 39m 30s\n",
      "2176:\tlearn: 6.7397045\ttotal: 10m 59s\tremaining: 39m 29s\n",
      "2177:\tlearn: 6.7385942\ttotal: 10m 59s\tremaining: 39m 29s\n",
      "2178:\tlearn: 6.7378469\ttotal: 11m\tremaining: 39m 29s\n",
      "2179:\tlearn: 6.7360363\ttotal: 11m\tremaining: 39m 29s\n",
      "2180:\tlearn: 6.7349180\ttotal: 11m\tremaining: 39m 29s\n",
      "2181:\tlearn: 6.7331114\ttotal: 11m 1s\tremaining: 39m 28s\n",
      "2182:\tlearn: 6.7321507\ttotal: 11m 1s\tremaining: 39m 28s\n",
      "2183:\tlearn: 6.7303132\ttotal: 11m 1s\tremaining: 39m 28s\n",
      "2184:\tlearn: 6.7289309\ttotal: 11m 2s\tremaining: 39m 28s\n",
      "2185:\tlearn: 6.7280484\ttotal: 11m 2s\tremaining: 39m 27s\n",
      "2186:\tlearn: 6.7269784\ttotal: 11m 2s\tremaining: 39m 27s\n",
      "2187:\tlearn: 6.7253860\ttotal: 11m 3s\tremaining: 39m 27s\n",
      "2188:\tlearn: 6.7247024\ttotal: 11m 3s\tremaining: 39m 27s\n",
      "2189:\tlearn: 6.7236454\ttotal: 11m 3s\tremaining: 39m 26s\n",
      "2190:\tlearn: 6.7222988\ttotal: 11m 4s\tremaining: 39m 26s\n",
      "2191:\tlearn: 6.7209313\ttotal: 11m 4s\tremaining: 39m 26s\n",
      "2192:\tlearn: 6.7192223\ttotal: 11m 4s\tremaining: 39m 26s\n",
      "2193:\tlearn: 6.7181221\ttotal: 11m 5s\tremaining: 39m 26s\n",
      "2194:\tlearn: 6.7163245\ttotal: 11m 5s\tremaining: 39m 25s\n",
      "2195:\tlearn: 6.7151719\ttotal: 11m 5s\tremaining: 39m 25s\n",
      "2196:\tlearn: 6.7144168\ttotal: 11m 5s\tremaining: 39m 25s\n",
      "2197:\tlearn: 6.7129249\ttotal: 11m 6s\tremaining: 39m 24s\n",
      "2198:\tlearn: 6.7117961\ttotal: 11m 6s\tremaining: 39m 24s\n",
      "2199:\tlearn: 6.7102717\ttotal: 11m 6s\tremaining: 39m 23s\n",
      "2200:\tlearn: 6.7092305\ttotal: 11m 6s\tremaining: 39m 23s\n",
      "2201:\tlearn: 6.7080903\ttotal: 11m 7s\tremaining: 39m 22s\n",
      "2202:\tlearn: 6.7069296\ttotal: 11m 7s\tremaining: 39m 22s\n",
      "2203:\tlearn: 6.7059307\ttotal: 11m 7s\tremaining: 39m 22s\n",
      "2204:\tlearn: 6.7044234\ttotal: 11m 8s\tremaining: 39m 21s\n",
      "2205:\tlearn: 6.7037569\ttotal: 11m 8s\tremaining: 39m 21s\n",
      "2206:\tlearn: 6.7023631\ttotal: 11m 8s\tremaining: 39m 20s\n",
      "2207:\tlearn: 6.7014705\ttotal: 11m 8s\tremaining: 39m 20s\n",
      "2208:\tlearn: 6.7007100\ttotal: 11m 9s\tremaining: 39m 19s\n",
      "2209:\tlearn: 6.6990436\ttotal: 11m 9s\tremaining: 39m 19s\n",
      "2210:\tlearn: 6.6980004\ttotal: 11m 9s\tremaining: 39m 19s\n",
      "2211:\tlearn: 6.6971159\ttotal: 11m 10s\tremaining: 39m 19s\n",
      "2212:\tlearn: 6.6956438\ttotal: 11m 10s\tremaining: 39m 19s\n",
      "2213:\tlearn: 6.6945651\ttotal: 11m 10s\tremaining: 39m 18s\n",
      "2214:\tlearn: 6.6929735\ttotal: 11m 11s\tremaining: 39m 18s\n",
      "2215:\tlearn: 6.6920334\ttotal: 11m 11s\tremaining: 39m 18s\n",
      "2216:\tlearn: 6.6908350\ttotal: 11m 11s\tremaining: 39m 17s\n",
      "2217:\tlearn: 6.6895980\ttotal: 11m 11s\tremaining: 39m 17s\n",
      "2218:\tlearn: 6.6885973\ttotal: 11m 12s\tremaining: 39m 17s\n",
      "2219:\tlearn: 6.6874556\ttotal: 11m 12s\tremaining: 39m 16s\n",
      "2220:\tlearn: 6.6864172\ttotal: 11m 12s\tremaining: 39m 16s\n",
      "2221:\tlearn: 6.6853775\ttotal: 11m 13s\tremaining: 39m 15s\n",
      "2222:\tlearn: 6.6843031\ttotal: 11m 13s\tremaining: 39m 15s\n",
      "2223:\tlearn: 6.6831197\ttotal: 11m 13s\tremaining: 39m 14s\n",
      "2224:\tlearn: 6.6822854\ttotal: 11m 13s\tremaining: 39m 14s\n",
      "2225:\tlearn: 6.6810580\ttotal: 11m 14s\tremaining: 39m 13s\n",
      "2226:\tlearn: 6.6800019\ttotal: 11m 14s\tremaining: 39m 13s\n",
      "2227:\tlearn: 6.6792411\ttotal: 11m 14s\tremaining: 39m 13s\n",
      "2228:\tlearn: 6.6780036\ttotal: 11m 14s\tremaining: 39m 12s\n",
      "2229:\tlearn: 6.6769118\ttotal: 11m 15s\tremaining: 39m 12s\n",
      "2230:\tlearn: 6.6755648\ttotal: 11m 15s\tremaining: 39m 11s\n",
      "2231:\tlearn: 6.6738304\ttotal: 11m 15s\tremaining: 39m 11s\n",
      "2232:\tlearn: 6.6724761\ttotal: 11m 15s\tremaining: 39m 11s\n",
      "2233:\tlearn: 6.6714564\ttotal: 11m 16s\tremaining: 39m 11s\n",
      "2234:\tlearn: 6.6705966\ttotal: 11m 16s\tremaining: 39m 10s\n",
      "2235:\tlearn: 6.6694899\ttotal: 11m 16s\tremaining: 39m 9s\n",
      "2236:\tlearn: 6.6679592\ttotal: 11m 17s\tremaining: 39m 9s\n",
      "2237:\tlearn: 6.6664496\ttotal: 11m 17s\tremaining: 39m 9s\n",
      "2238:\tlearn: 6.6656310\ttotal: 11m 17s\tremaining: 39m 8s\n",
      "2239:\tlearn: 6.6644184\ttotal: 11m 17s\tremaining: 39m 8s\n",
      "2240:\tlearn: 6.6630518\ttotal: 11m 18s\tremaining: 39m 7s\n",
      "2241:\tlearn: 6.6621881\ttotal: 11m 18s\tremaining: 39m 7s\n",
      "2242:\tlearn: 6.6609752\ttotal: 11m 18s\tremaining: 39m 7s\n",
      "2243:\tlearn: 6.6595959\ttotal: 11m 19s\tremaining: 39m 6s\n",
      "2244:\tlearn: 6.6587451\ttotal: 11m 19s\tremaining: 39m 6s\n",
      "2245:\tlearn: 6.6580195\ttotal: 11m 19s\tremaining: 39m 5s\n",
      "2246:\tlearn: 6.6571162\ttotal: 11m 19s\tremaining: 39m 5s\n",
      "2247:\tlearn: 6.6560001\ttotal: 11m 19s\tremaining: 39m 4s\n",
      "2248:\tlearn: 6.6545039\ttotal: 11m 20s\tremaining: 39m 4s\n",
      "2249:\tlearn: 6.6533293\ttotal: 11m 20s\tremaining: 39m 3s\n",
      "2250:\tlearn: 6.6518597\ttotal: 11m 20s\tremaining: 39m 3s\n",
      "2251:\tlearn: 6.6509550\ttotal: 11m 21s\tremaining: 39m 3s\n",
      "2252:\tlearn: 6.6495298\ttotal: 11m 21s\tremaining: 39m 3s\n",
      "2253:\tlearn: 6.6480486\ttotal: 11m 21s\tremaining: 39m 2s\n",
      "2254:\tlearn: 6.6467897\ttotal: 11m 22s\tremaining: 39m 2s\n",
      "2255:\tlearn: 6.6456303\ttotal: 11m 22s\tremaining: 39m 1s\n",
      "2256:\tlearn: 6.6444477\ttotal: 11m 22s\tremaining: 39m 1s\n",
      "2257:\tlearn: 6.6431466\ttotal: 11m 22s\tremaining: 39m 1s\n",
      "2258:\tlearn: 6.6417783\ttotal: 11m 23s\tremaining: 39m\n",
      "2259:\tlearn: 6.6407508\ttotal: 11m 23s\tremaining: 39m\n",
      "2260:\tlearn: 6.6399418\ttotal: 11m 23s\tremaining: 39m\n",
      "2261:\tlearn: 6.6388596\ttotal: 11m 23s\tremaining: 38m 59s\n",
      "2262:\tlearn: 6.6372759\ttotal: 11m 24s\tremaining: 38m 59s\n",
      "2263:\tlearn: 6.6359669\ttotal: 11m 24s\tremaining: 38m 59s\n",
      "2264:\tlearn: 6.6346186\ttotal: 11m 24s\tremaining: 38m 58s\n",
      "2265:\tlearn: 6.6340430\ttotal: 11m 25s\tremaining: 38m 58s\n",
      "2266:\tlearn: 6.6327676\ttotal: 11m 25s\tremaining: 38m 57s\n",
      "2267:\tlearn: 6.6313445\ttotal: 11m 25s\tremaining: 38m 57s\n",
      "2268:\tlearn: 6.6302453\ttotal: 11m 25s\tremaining: 38m 56s\n",
      "2269:\tlearn: 6.6291709\ttotal: 11m 26s\tremaining: 38m 56s\n",
      "2270:\tlearn: 6.6275429\ttotal: 11m 26s\tremaining: 38m 55s\n",
      "2271:\tlearn: 6.6268317\ttotal: 11m 26s\tremaining: 38m 55s\n",
      "2272:\tlearn: 6.6256297\ttotal: 11m 26s\tremaining: 38m 55s\n",
      "2273:\tlearn: 6.6246964\ttotal: 11m 27s\tremaining: 38m 54s\n",
      "2274:\tlearn: 6.6242660\ttotal: 11m 27s\tremaining: 38m 53s\n",
      "2275:\tlearn: 6.6234455\ttotal: 11m 27s\tremaining: 38m 53s\n",
      "2276:\tlearn: 6.6221026\ttotal: 11m 27s\tremaining: 38m 52s\n",
      "2277:\tlearn: 6.6210856\ttotal: 11m 28s\tremaining: 38m 52s\n",
      "2278:\tlearn: 6.6200507\ttotal: 11m 28s\tremaining: 38m 51s\n",
      "2279:\tlearn: 6.6191468\ttotal: 11m 28s\tremaining: 38m 51s\n",
      "2280:\tlearn: 6.6180769\ttotal: 11m 28s\tremaining: 38m 51s\n",
      "2281:\tlearn: 6.6165994\ttotal: 11m 29s\tremaining: 38m 50s\n",
      "2282:\tlearn: 6.6154847\ttotal: 11m 29s\tremaining: 38m 50s\n",
      "2283:\tlearn: 6.6147623\ttotal: 11m 29s\tremaining: 38m 49s\n",
      "2284:\tlearn: 6.6131940\ttotal: 11m 29s\tremaining: 38m 49s\n",
      "2285:\tlearn: 6.6120302\ttotal: 11m 30s\tremaining: 38m 49s\n",
      "2286:\tlearn: 6.6113897\ttotal: 11m 30s\tremaining: 38m 48s\n",
      "2287:\tlearn: 6.6106647\ttotal: 11m 30s\tremaining: 38m 48s\n",
      "2288:\tlearn: 6.6093294\ttotal: 11m 31s\tremaining: 38m 47s\n",
      "2289:\tlearn: 6.6080914\ttotal: 11m 31s\tremaining: 38m 47s\n",
      "2290:\tlearn: 6.6062773\ttotal: 11m 31s\tremaining: 38m 47s\n",
      "2291:\tlearn: 6.6051364\ttotal: 11m 31s\tremaining: 38m 46s\n",
      "2292:\tlearn: 6.6036565\ttotal: 11m 32s\tremaining: 38m 46s\n",
      "2293:\tlearn: 6.6027137\ttotal: 11m 32s\tremaining: 38m 45s\n",
      "2294:\tlearn: 6.6016658\ttotal: 11m 32s\tremaining: 38m 45s\n",
      "2295:\tlearn: 6.6003133\ttotal: 11m 32s\tremaining: 38m 44s\n",
      "2296:\tlearn: 6.5992482\ttotal: 11m 33s\tremaining: 38m 44s\n",
      "2297:\tlearn: 6.5982218\ttotal: 11m 33s\tremaining: 38m 44s\n",
      "2298:\tlearn: 6.5971067\ttotal: 11m 33s\tremaining: 38m 43s\n",
      "2299:\tlearn: 6.5965212\ttotal: 11m 33s\tremaining: 38m 43s\n",
      "2300:\tlearn: 6.5951528\ttotal: 11m 34s\tremaining: 38m 42s\n",
      "2301:\tlearn: 6.5936175\ttotal: 11m 34s\tremaining: 38m 42s\n",
      "2302:\tlearn: 6.5920879\ttotal: 11m 34s\tremaining: 38m 42s\n",
      "2303:\tlearn: 6.5912368\ttotal: 11m 35s\tremaining: 38m 42s\n",
      "2304:\tlearn: 6.5904994\ttotal: 11m 35s\tremaining: 38m 41s\n",
      "2305:\tlearn: 6.5896032\ttotal: 11m 35s\tremaining: 38m 41s\n",
      "2306:\tlearn: 6.5879430\ttotal: 11m 35s\tremaining: 38m 40s\n",
      "2307:\tlearn: 6.5869905\ttotal: 11m 36s\tremaining: 38m 40s\n",
      "2308:\tlearn: 6.5862306\ttotal: 11m 36s\tremaining: 38m 39s\n",
      "2309:\tlearn: 6.5849671\ttotal: 11m 36s\tremaining: 38m 39s\n",
      "2310:\tlearn: 6.5841279\ttotal: 11m 37s\tremaining: 38m 39s\n",
      "2311:\tlearn: 6.5825183\ttotal: 11m 37s\tremaining: 38m 38s\n",
      "2312:\tlearn: 6.5811642\ttotal: 11m 37s\tremaining: 38m 38s\n",
      "2313:\tlearn: 6.5802858\ttotal: 11m 37s\tremaining: 38m 38s\n",
      "2314:\tlearn: 6.5791601\ttotal: 11m 38s\tremaining: 38m 37s\n",
      "2315:\tlearn: 6.5778791\ttotal: 11m 38s\tremaining: 38m 37s\n",
      "2316:\tlearn: 6.5767018\ttotal: 11m 38s\tremaining: 38m 36s\n",
      "2317:\tlearn: 6.5755475\ttotal: 11m 38s\tremaining: 38m 36s\n",
      "2318:\tlearn: 6.5743487\ttotal: 11m 39s\tremaining: 38m 35s\n",
      "2319:\tlearn: 6.5730924\ttotal: 11m 39s\tremaining: 38m 35s\n",
      "2320:\tlearn: 6.5722429\ttotal: 11m 39s\tremaining: 38m 35s\n",
      "2321:\tlearn: 6.5707716\ttotal: 11m 40s\tremaining: 38m 34s\n",
      "2322:\tlearn: 6.5699163\ttotal: 11m 40s\tremaining: 38m 34s\n",
      "2323:\tlearn: 6.5686358\ttotal: 11m 40s\tremaining: 38m 33s\n",
      "2324:\tlearn: 6.5678797\ttotal: 11m 40s\tremaining: 38m 33s\n",
      "2325:\tlearn: 6.5659716\ttotal: 11m 41s\tremaining: 38m 33s\n",
      "2326:\tlearn: 6.5653148\ttotal: 11m 41s\tremaining: 38m 32s\n",
      "2327:\tlearn: 6.5636078\ttotal: 11m 41s\tremaining: 38m 32s\n",
      "2328:\tlearn: 6.5626199\ttotal: 11m 42s\tremaining: 38m 32s\n",
      "2329:\tlearn: 6.5620448\ttotal: 11m 42s\tremaining: 38m 31s\n",
      "2330:\tlearn: 6.5609003\ttotal: 11m 42s\tremaining: 38m 31s\n",
      "2331:\tlearn: 6.5593029\ttotal: 11m 42s\tremaining: 38m 31s\n",
      "2332:\tlearn: 6.5580436\ttotal: 11m 43s\tremaining: 38m 30s\n",
      "2333:\tlearn: 6.5570144\ttotal: 11m 43s\tremaining: 38m 30s\n",
      "2334:\tlearn: 6.5562210\ttotal: 11m 43s\tremaining: 38m 29s\n",
      "2335:\tlearn: 6.5550052\ttotal: 11m 43s\tremaining: 38m 29s\n",
      "2336:\tlearn: 6.5541647\ttotal: 11m 44s\tremaining: 38m 29s\n",
      "2337:\tlearn: 6.5531072\ttotal: 11m 44s\tremaining: 38m 28s\n",
      "2338:\tlearn: 6.5518462\ttotal: 11m 44s\tremaining: 38m 28s\n",
      "2339:\tlearn: 6.5506737\ttotal: 11m 45s\tremaining: 38m 28s\n",
      "2340:\tlearn: 6.5490859\ttotal: 11m 45s\tremaining: 38m 28s\n",
      "2341:\tlearn: 6.5473910\ttotal: 11m 45s\tremaining: 38m 27s\n",
      "2342:\tlearn: 6.5464845\ttotal: 11m 46s\tremaining: 38m 27s\n",
      "2343:\tlearn: 6.5453343\ttotal: 11m 46s\tremaining: 38m 26s\n",
      "2344:\tlearn: 6.5446803\ttotal: 11m 46s\tremaining: 38m 26s\n",
      "2345:\tlearn: 6.5428499\ttotal: 11m 46s\tremaining: 38m 26s\n",
      "2346:\tlearn: 6.5409440\ttotal: 11m 47s\tremaining: 38m 25s\n",
      "2347:\tlearn: 6.5397105\ttotal: 11m 47s\tremaining: 38m 25s\n",
      "2348:\tlearn: 6.5385693\ttotal: 11m 47s\tremaining: 38m 25s\n",
      "2349:\tlearn: 6.5380151\ttotal: 11m 47s\tremaining: 38m 24s\n",
      "2350:\tlearn: 6.5366727\ttotal: 11m 48s\tremaining: 38m 24s\n",
      "2351:\tlearn: 6.5348294\ttotal: 11m 48s\tremaining: 38m 24s\n",
      "2352:\tlearn: 6.5338185\ttotal: 11m 48s\tremaining: 38m 23s\n",
      "2353:\tlearn: 6.5324393\ttotal: 11m 49s\tremaining: 38m 23s\n",
      "2354:\tlearn: 6.5315857\ttotal: 11m 49s\tremaining: 38m 22s\n",
      "2355:\tlearn: 6.5301891\ttotal: 11m 49s\tremaining: 38m 22s\n",
      "2356:\tlearn: 6.5282899\ttotal: 11m 50s\tremaining: 38m 22s\n",
      "2357:\tlearn: 6.5268998\ttotal: 11m 50s\tremaining: 38m 22s\n",
      "2358:\tlearn: 6.5250715\ttotal: 11m 50s\tremaining: 38m 22s\n",
      "2359:\tlearn: 6.5238628\ttotal: 11m 51s\tremaining: 38m 22s\n",
      "2360:\tlearn: 6.5227809\ttotal: 11m 51s\tremaining: 38m 21s\n",
      "2361:\tlearn: 6.5220548\ttotal: 11m 51s\tremaining: 38m 21s\n",
      "2362:\tlearn: 6.5214650\ttotal: 11m 51s\tremaining: 38m 20s\n",
      "2363:\tlearn: 6.5205157\ttotal: 11m 52s\tremaining: 38m 20s\n",
      "2364:\tlearn: 6.5191320\ttotal: 11m 52s\tremaining: 38m 19s\n",
      "2365:\tlearn: 6.5181648\ttotal: 11m 52s\tremaining: 38m 19s\n",
      "2366:\tlearn: 6.5166134\ttotal: 11m 53s\tremaining: 38m 19s\n",
      "2367:\tlearn: 6.5153699\ttotal: 11m 53s\tremaining: 38m 18s\n",
      "2368:\tlearn: 6.5146187\ttotal: 11m 53s\tremaining: 38m 18s\n",
      "2369:\tlearn: 6.5135922\ttotal: 11m 53s\tremaining: 38m 18s\n",
      "2370:\tlearn: 6.5127674\ttotal: 11m 54s\tremaining: 38m 17s\n",
      "2371:\tlearn: 6.5116894\ttotal: 11m 54s\tremaining: 38m 17s\n",
      "2372:\tlearn: 6.5109875\ttotal: 11m 54s\tremaining: 38m 16s\n",
      "2373:\tlearn: 6.5099342\ttotal: 11m 54s\tremaining: 38m 16s\n",
      "2374:\tlearn: 6.5089232\ttotal: 11m 55s\tremaining: 38m 16s\n",
      "2375:\tlearn: 6.5081849\ttotal: 11m 55s\tremaining: 38m 15s\n",
      "2376:\tlearn: 6.5069142\ttotal: 11m 55s\tremaining: 38m 15s\n",
      "2377:\tlearn: 6.5059852\ttotal: 11m 56s\tremaining: 38m 15s\n",
      "2378:\tlearn: 6.5054320\ttotal: 11m 56s\tremaining: 38m 14s\n",
      "2379:\tlearn: 6.5042976\ttotal: 11m 56s\tremaining: 38m 14s\n",
      "2380:\tlearn: 6.5031663\ttotal: 11m 56s\tremaining: 38m 14s\n",
      "2381:\tlearn: 6.5018986\ttotal: 11m 57s\tremaining: 38m 14s\n",
      "2382:\tlearn: 6.5008570\ttotal: 11m 57s\tremaining: 38m 13s\n",
      "2383:\tlearn: 6.4994461\ttotal: 11m 58s\tremaining: 38m 13s\n",
      "2384:\tlearn: 6.4986023\ttotal: 11m 58s\tremaining: 38m 13s\n",
      "2385:\tlearn: 6.4980399\ttotal: 11m 58s\tremaining: 38m 12s\n",
      "2386:\tlearn: 6.4970189\ttotal: 11m 58s\tremaining: 38m 12s\n",
      "2387:\tlearn: 6.4957844\ttotal: 11m 59s\tremaining: 38m 12s\n",
      "2388:\tlearn: 6.4949906\ttotal: 11m 59s\tremaining: 38m 11s\n",
      "2389:\tlearn: 6.4937366\ttotal: 11m 59s\tremaining: 38m 11s\n",
      "2390:\tlearn: 6.4919711\ttotal: 12m\tremaining: 38m 11s\n",
      "2391:\tlearn: 6.4912295\ttotal: 12m\tremaining: 38m 10s\n",
      "2392:\tlearn: 6.4901742\ttotal: 12m\tremaining: 38m 10s\n",
      "2393:\tlearn: 6.4893440\ttotal: 12m\tremaining: 38m 10s\n",
      "2394:\tlearn: 6.4882374\ttotal: 12m 1s\tremaining: 38m 10s\n",
      "2395:\tlearn: 6.4873178\ttotal: 12m 1s\tremaining: 38m 9s\n",
      "2396:\tlearn: 6.4862303\ttotal: 12m 1s\tremaining: 38m 9s\n",
      "2397:\tlearn: 6.4851717\ttotal: 12m 2s\tremaining: 38m 9s\n",
      "2398:\tlearn: 6.4836541\ttotal: 12m 2s\tremaining: 38m 8s\n",
      "2399:\tlearn: 6.4831337\ttotal: 12m 2s\tremaining: 38m 8s\n",
      "2400:\tlearn: 6.4826415\ttotal: 12m 2s\tremaining: 38m 7s\n",
      "2401:\tlearn: 6.4814780\ttotal: 12m 3s\tremaining: 38m 7s\n",
      "2402:\tlearn: 6.4805336\ttotal: 12m 3s\tremaining: 38m 7s\n",
      "2403:\tlearn: 6.4793091\ttotal: 12m 3s\tremaining: 38m 6s\n",
      "2404:\tlearn: 6.4778455\ttotal: 12m 3s\tremaining: 38m 6s\n",
      "2405:\tlearn: 6.4768018\ttotal: 12m 4s\tremaining: 38m 5s\n",
      "2406:\tlearn: 6.4749811\ttotal: 12m 4s\tremaining: 38m 5s\n",
      "2407:\tlearn: 6.4741424\ttotal: 12m 4s\tremaining: 38m 5s\n",
      "2408:\tlearn: 6.4727948\ttotal: 12m 5s\tremaining: 38m 5s\n",
      "2409:\tlearn: 6.4719338\ttotal: 12m 5s\tremaining: 38m 4s\n",
      "2410:\tlearn: 6.4708818\ttotal: 12m 5s\tremaining: 38m 4s\n",
      "2411:\tlearn: 6.4700870\ttotal: 12m 5s\tremaining: 38m 3s\n",
      "2412:\tlearn: 6.4692069\ttotal: 12m 6s\tremaining: 38m 3s\n",
      "2413:\tlearn: 6.4684345\ttotal: 12m 6s\tremaining: 38m 3s\n",
      "2414:\tlearn: 6.4670818\ttotal: 12m 6s\tremaining: 38m 2s\n",
      "2415:\tlearn: 6.4654585\ttotal: 12m 7s\tremaining: 38m 2s\n",
      "2416:\tlearn: 6.4640526\ttotal: 12m 7s\tremaining: 38m 2s\n",
      "2417:\tlearn: 6.4621774\ttotal: 12m 7s\tremaining: 38m 2s\n",
      "2418:\tlearn: 6.4606406\ttotal: 12m 8s\tremaining: 38m 1s\n",
      "2419:\tlearn: 6.4593168\ttotal: 12m 8s\tremaining: 38m 1s\n",
      "2420:\tlearn: 6.4583505\ttotal: 12m 8s\tremaining: 38m 1s\n",
      "2421:\tlearn: 6.4577993\ttotal: 12m 8s\tremaining: 38m\n",
      "2422:\tlearn: 6.4569482\ttotal: 12m 9s\tremaining: 38m\n",
      "2423:\tlearn: 6.4556885\ttotal: 12m 9s\tremaining: 37m 59s\n",
      "2424:\tlearn: 6.4541419\ttotal: 12m 9s\tremaining: 37m 59s\n",
      "2425:\tlearn: 6.4527036\ttotal: 12m 10s\tremaining: 37m 59s\n",
      "2426:\tlearn: 6.4516399\ttotal: 12m 10s\tremaining: 37m 58s\n",
      "2427:\tlearn: 6.4506305\ttotal: 12m 10s\tremaining: 37m 58s\n",
      "2428:\tlearn: 6.4497516\ttotal: 12m 10s\tremaining: 37m 58s\n",
      "2429:\tlearn: 6.4489559\ttotal: 12m 11s\tremaining: 37m 57s\n",
      "2430:\tlearn: 6.4476480\ttotal: 12m 11s\tremaining: 37m 57s\n",
      "2431:\tlearn: 6.4468458\ttotal: 12m 11s\tremaining: 37m 57s\n",
      "2432:\tlearn: 6.4459476\ttotal: 12m 11s\tremaining: 37m 56s\n",
      "2433:\tlearn: 6.4443795\ttotal: 12m 12s\tremaining: 37m 56s\n",
      "2434:\tlearn: 6.4432200\ttotal: 12m 12s\tremaining: 37m 56s\n",
      "2435:\tlearn: 6.4422348\ttotal: 12m 12s\tremaining: 37m 55s\n",
      "2436:\tlearn: 6.4413745\ttotal: 12m 13s\tremaining: 37m 55s\n",
      "2437:\tlearn: 6.4405954\ttotal: 12m 13s\tremaining: 37m 55s\n",
      "2438:\tlearn: 6.4402102\ttotal: 12m 13s\tremaining: 37m 54s\n",
      "2439:\tlearn: 6.4395963\ttotal: 12m 13s\tremaining: 37m 53s\n",
      "2440:\tlearn: 6.4384598\ttotal: 12m 14s\tremaining: 37m 53s\n",
      "2441:\tlearn: 6.4372157\ttotal: 12m 14s\tremaining: 37m 53s\n",
      "2442:\tlearn: 6.4363163\ttotal: 12m 14s\tremaining: 37m 53s\n",
      "2443:\tlearn: 6.4355261\ttotal: 12m 15s\tremaining: 37m 52s\n",
      "2444:\tlearn: 6.4336783\ttotal: 12m 15s\tremaining: 37m 52s\n",
      "2445:\tlearn: 6.4324004\ttotal: 12m 15s\tremaining: 37m 52s\n",
      "2446:\tlearn: 6.4314422\ttotal: 12m 16s\tremaining: 37m 51s\n",
      "2447:\tlearn: 6.4305943\ttotal: 12m 16s\tremaining: 37m 51s\n",
      "2448:\tlearn: 6.4297124\ttotal: 12m 16s\tremaining: 37m 51s\n",
      "2449:\tlearn: 6.4292069\ttotal: 12m 16s\tremaining: 37m 50s\n",
      "2450:\tlearn: 6.4286356\ttotal: 12m 17s\tremaining: 37m 50s\n",
      "2451:\tlearn: 6.4278039\ttotal: 12m 17s\tremaining: 37m 49s\n",
      "2452:\tlearn: 6.4267523\ttotal: 12m 17s\tremaining: 37m 49s\n",
      "2453:\tlearn: 6.4250082\ttotal: 12m 17s\tremaining: 37m 48s\n",
      "2454:\tlearn: 6.4241204\ttotal: 12m 18s\tremaining: 37m 48s\n",
      "2455:\tlearn: 6.4231931\ttotal: 12m 18s\tremaining: 37m 48s\n",
      "2456:\tlearn: 6.4219760\ttotal: 12m 18s\tremaining: 37m 47s\n",
      "2457:\tlearn: 6.4212227\ttotal: 12m 18s\tremaining: 37m 47s\n",
      "2458:\tlearn: 6.4201393\ttotal: 12m 19s\tremaining: 37m 46s\n",
      "2459:\tlearn: 6.4184633\ttotal: 12m 19s\tremaining: 37m 46s\n",
      "2460:\tlearn: 6.4176911\ttotal: 12m 19s\tremaining: 37m 46s\n",
      "2461:\tlearn: 6.4163167\ttotal: 12m 20s\tremaining: 37m 45s\n",
      "2462:\tlearn: 6.4152784\ttotal: 12m 20s\tremaining: 37m 45s\n",
      "2463:\tlearn: 6.4139099\ttotal: 12m 20s\tremaining: 37m 45s\n",
      "2464:\tlearn: 6.4125788\ttotal: 12m 21s\tremaining: 37m 45s\n",
      "2465:\tlearn: 6.4116129\ttotal: 12m 21s\tremaining: 37m 44s\n",
      "2466:\tlearn: 6.4105340\ttotal: 12m 21s\tremaining: 37m 44s\n",
      "2467:\tlearn: 6.4095927\ttotal: 12m 22s\tremaining: 37m 44s\n",
      "2468:\tlearn: 6.4087010\ttotal: 12m 22s\tremaining: 37m 44s\n",
      "2469:\tlearn: 6.4079385\ttotal: 12m 22s\tremaining: 37m 43s\n",
      "2470:\tlearn: 6.4071067\ttotal: 12m 22s\tremaining: 37m 43s\n",
      "2471:\tlearn: 6.4063374\ttotal: 12m 23s\tremaining: 37m 42s\n",
      "2472:\tlearn: 6.4052133\ttotal: 12m 23s\tremaining: 37m 42s\n",
      "2473:\tlearn: 6.4044204\ttotal: 12m 23s\tremaining: 37m 42s\n",
      "2474:\tlearn: 6.4035918\ttotal: 12m 23s\tremaining: 37m 41s\n",
      "2475:\tlearn: 6.4027978\ttotal: 12m 24s\tremaining: 37m 41s\n",
      "2476:\tlearn: 6.4011202\ttotal: 12m 24s\tremaining: 37m 41s\n",
      "2477:\tlearn: 6.3997760\ttotal: 12m 24s\tremaining: 37m 40s\n",
      "2478:\tlearn: 6.3991351\ttotal: 12m 24s\tremaining: 37m 40s\n",
      "2479:\tlearn: 6.3980691\ttotal: 12m 25s\tremaining: 37m 39s\n",
      "2480:\tlearn: 6.3974969\ttotal: 12m 25s\tremaining: 37m 39s\n",
      "2481:\tlearn: 6.3965110\ttotal: 12m 25s\tremaining: 37m 39s\n",
      "2482:\tlearn: 6.3951823\ttotal: 12m 26s\tremaining: 37m 39s\n",
      "2483:\tlearn: 6.3943700\ttotal: 12m 26s\tremaining: 37m 38s\n",
      "2484:\tlearn: 6.3932090\ttotal: 12m 26s\tremaining: 37m 38s\n",
      "2485:\tlearn: 6.3921154\ttotal: 12m 27s\tremaining: 37m 38s\n",
      "2486:\tlearn: 6.3911158\ttotal: 12m 27s\tremaining: 37m 37s\n",
      "2487:\tlearn: 6.3901108\ttotal: 12m 27s\tremaining: 37m 37s\n",
      "2488:\tlearn: 6.3889388\ttotal: 12m 27s\tremaining: 37m 37s\n",
      "2489:\tlearn: 6.3878277\ttotal: 12m 28s\tremaining: 37m 36s\n",
      "2490:\tlearn: 6.3867752\ttotal: 12m 28s\tremaining: 37m 36s\n",
      "2491:\tlearn: 6.3853249\ttotal: 12m 28s\tremaining: 37m 36s\n",
      "2492:\tlearn: 6.3847345\ttotal: 12m 29s\tremaining: 37m 35s\n",
      "2493:\tlearn: 6.3835134\ttotal: 12m 29s\tremaining: 37m 35s\n",
      "2494:\tlearn: 6.3825847\ttotal: 12m 29s\tremaining: 37m 34s\n",
      "2495:\tlearn: 6.3818428\ttotal: 12m 29s\tremaining: 37m 34s\n",
      "2496:\tlearn: 6.3810493\ttotal: 12m 30s\tremaining: 37m 33s\n",
      "2497:\tlearn: 6.3800740\ttotal: 12m 30s\tremaining: 37m 33s\n",
      "2498:\tlearn: 6.3785681\ttotal: 12m 30s\tremaining: 37m 33s\n",
      "2499:\tlearn: 6.3777100\ttotal: 12m 31s\tremaining: 37m 33s\n",
      "2500:\tlearn: 6.3767741\ttotal: 12m 31s\tremaining: 37m 33s\n",
      "2501:\tlearn: 6.3759386\ttotal: 12m 31s\tremaining: 37m 32s\n",
      "2502:\tlearn: 6.3744497\ttotal: 12m 32s\tremaining: 37m 32s\n",
      "2503:\tlearn: 6.3734429\ttotal: 12m 32s\tremaining: 37m 32s\n",
      "2504:\tlearn: 6.3725247\ttotal: 12m 32s\tremaining: 37m 31s\n",
      "2505:\tlearn: 6.3716966\ttotal: 12m 32s\tremaining: 37m 31s\n",
      "2506:\tlearn: 6.3710640\ttotal: 12m 33s\tremaining: 37m 30s\n",
      "2507:\tlearn: 6.3700585\ttotal: 12m 33s\tremaining: 37m 30s\n",
      "2508:\tlearn: 6.3690054\ttotal: 12m 33s\tremaining: 37m 30s\n",
      "2509:\tlearn: 6.3679404\ttotal: 12m 34s\tremaining: 37m 29s\n",
      "2510:\tlearn: 6.3669641\ttotal: 12m 34s\tremaining: 37m 29s\n",
      "2511:\tlearn: 6.3657178\ttotal: 12m 34s\tremaining: 37m 29s\n",
      "2512:\tlearn: 6.3642807\ttotal: 12m 34s\tremaining: 37m 29s\n",
      "2513:\tlearn: 6.3633716\ttotal: 12m 35s\tremaining: 37m 28s\n",
      "2514:\tlearn: 6.3625475\ttotal: 12m 35s\tremaining: 37m 28s\n",
      "2515:\tlearn: 6.3616462\ttotal: 12m 35s\tremaining: 37m 27s\n",
      "2516:\tlearn: 6.3599799\ttotal: 12m 36s\tremaining: 37m 27s\n",
      "2517:\tlearn: 6.3593173\ttotal: 12m 36s\tremaining: 37m 27s\n",
      "2518:\tlearn: 6.3584741\ttotal: 12m 36s\tremaining: 37m 26s\n",
      "2519:\tlearn: 6.3575763\ttotal: 12m 36s\tremaining: 37m 26s\n",
      "2520:\tlearn: 6.3564866\ttotal: 12m 37s\tremaining: 37m 26s\n",
      "2521:\tlearn: 6.3550172\ttotal: 12m 37s\tremaining: 37m 25s\n",
      "2522:\tlearn: 6.3543183\ttotal: 12m 37s\tremaining: 37m 25s\n",
      "2523:\tlearn: 6.3535147\ttotal: 12m 37s\tremaining: 37m 24s\n",
      "2524:\tlearn: 6.3526141\ttotal: 12m 38s\tremaining: 37m 24s\n",
      "2525:\tlearn: 6.3511444\ttotal: 12m 38s\tremaining: 37m 24s\n",
      "2526:\tlearn: 6.3492113\ttotal: 12m 38s\tremaining: 37m 24s\n",
      "2527:\tlearn: 6.3480937\ttotal: 12m 39s\tremaining: 37m 23s\n",
      "2528:\tlearn: 6.3471014\ttotal: 12m 39s\tremaining: 37m 23s\n",
      "2529:\tlearn: 6.3460063\ttotal: 12m 39s\tremaining: 37m 23s\n",
      "2530:\tlearn: 6.3449435\ttotal: 12m 40s\tremaining: 37m 22s\n",
      "2531:\tlearn: 6.3442287\ttotal: 12m 40s\tremaining: 37m 22s\n",
      "2532:\tlearn: 6.3435575\ttotal: 12m 40s\tremaining: 37m 22s\n",
      "2533:\tlearn: 6.3428084\ttotal: 12m 40s\tremaining: 37m 21s\n",
      "2534:\tlearn: 6.3421559\ttotal: 12m 41s\tremaining: 37m 21s\n",
      "2535:\tlearn: 6.3410387\ttotal: 12m 41s\tremaining: 37m 20s\n",
      "2536:\tlearn: 6.3402223\ttotal: 12m 41s\tremaining: 37m 20s\n",
      "2537:\tlearn: 6.3387363\ttotal: 12m 41s\tremaining: 37m 19s\n",
      "2538:\tlearn: 6.3370566\ttotal: 12m 42s\tremaining: 37m 19s\n",
      "2539:\tlearn: 6.3360265\ttotal: 12m 42s\tremaining: 37m 19s\n",
      "2540:\tlearn: 6.3347734\ttotal: 12m 42s\tremaining: 37m 19s\n",
      "2541:\tlearn: 6.3339623\ttotal: 12m 43s\tremaining: 37m 18s\n",
      "2542:\tlearn: 6.3329442\ttotal: 12m 43s\tremaining: 37m 18s\n",
      "2543:\tlearn: 6.3322576\ttotal: 12m 43s\tremaining: 37m 18s\n",
      "2544:\tlearn: 6.3311908\ttotal: 12m 43s\tremaining: 37m 17s\n",
      "2545:\tlearn: 6.3303043\ttotal: 12m 44s\tremaining: 37m 17s\n",
      "2546:\tlearn: 6.3291791\ttotal: 12m 44s\tremaining: 37m 17s\n",
      "2547:\tlearn: 6.3284705\ttotal: 12m 44s\tremaining: 37m 16s\n",
      "2548:\tlearn: 6.3270461\ttotal: 12m 45s\tremaining: 37m 16s\n",
      "2549:\tlearn: 6.3260749\ttotal: 12m 45s\tremaining: 37m 16s\n",
      "2550:\tlearn: 6.3249583\ttotal: 12m 45s\tremaining: 37m 15s\n",
      "2551:\tlearn: 6.3241104\ttotal: 12m 45s\tremaining: 37m 15s\n",
      "2552:\tlearn: 6.3232519\ttotal: 12m 46s\tremaining: 37m 14s\n",
      "2553:\tlearn: 6.3220965\ttotal: 12m 46s\tremaining: 37m 14s\n",
      "2554:\tlearn: 6.3211674\ttotal: 12m 46s\tremaining: 37m 14s\n",
      "2555:\tlearn: 6.3202427\ttotal: 12m 47s\tremaining: 37m 14s\n",
      "2556:\tlearn: 6.3194348\ttotal: 12m 47s\tremaining: 37m 13s\n",
      "2557:\tlearn: 6.3184641\ttotal: 12m 47s\tremaining: 37m 13s\n",
      "2558:\tlearn: 6.3171166\ttotal: 12m 47s\tremaining: 37m 13s\n",
      "2559:\tlearn: 6.3161330\ttotal: 12m 48s\tremaining: 37m 12s\n",
      "2560:\tlearn: 6.3152982\ttotal: 12m 48s\tremaining: 37m 12s\n",
      "2561:\tlearn: 6.3143729\ttotal: 12m 48s\tremaining: 37m 11s\n",
      "2562:\tlearn: 6.3128731\ttotal: 12m 49s\tremaining: 37m 11s\n",
      "2563:\tlearn: 6.3118289\ttotal: 12m 49s\tremaining: 37m 11s\n",
      "2564:\tlearn: 6.3100034\ttotal: 12m 49s\tremaining: 37m 11s\n",
      "2565:\tlearn: 6.3092551\ttotal: 12m 49s\tremaining: 37m 10s\n",
      "2566:\tlearn: 6.3083392\ttotal: 12m 50s\tremaining: 37m 10s\n",
      "2567:\tlearn: 6.3071325\ttotal: 12m 50s\tremaining: 37m 9s\n",
      "2568:\tlearn: 6.3060983\ttotal: 12m 50s\tremaining: 37m 9s\n",
      "2569:\tlearn: 6.3049122\ttotal: 12m 51s\tremaining: 37m 9s\n",
      "2570:\tlearn: 6.3035109\ttotal: 12m 51s\tremaining: 37m 9s\n",
      "2571:\tlearn: 6.3024022\ttotal: 12m 51s\tremaining: 37m 9s\n",
      "2572:\tlearn: 6.3015924\ttotal: 12m 52s\tremaining: 37m 8s\n",
      "2573:\tlearn: 6.3005661\ttotal: 12m 52s\tremaining: 37m 8s\n",
      "2574:\tlearn: 6.2995793\ttotal: 12m 52s\tremaining: 37m 7s\n",
      "2575:\tlearn: 6.2990090\ttotal: 12m 52s\tremaining: 37m 7s\n",
      "2576:\tlearn: 6.2982225\ttotal: 12m 53s\tremaining: 37m 7s\n",
      "2577:\tlearn: 6.2972132\ttotal: 12m 53s\tremaining: 37m 6s\n",
      "2578:\tlearn: 6.2958877\ttotal: 12m 53s\tremaining: 37m 6s\n",
      "2579:\tlearn: 6.2949857\ttotal: 12m 53s\tremaining: 37m 5s\n",
      "2580:\tlearn: 6.2942380\ttotal: 12m 54s\tremaining: 37m 5s\n",
      "2581:\tlearn: 6.2927123\ttotal: 12m 54s\tremaining: 37m 5s\n",
      "2582:\tlearn: 6.2912478\ttotal: 12m 54s\tremaining: 37m 5s\n",
      "2583:\tlearn: 6.2899905\ttotal: 12m 55s\tremaining: 37m 4s\n",
      "2584:\tlearn: 6.2888896\ttotal: 12m 55s\tremaining: 37m 4s\n",
      "2585:\tlearn: 6.2879973\ttotal: 12m 55s\tremaining: 37m 4s\n",
      "2586:\tlearn: 6.2873611\ttotal: 12m 56s\tremaining: 37m 3s\n",
      "2587:\tlearn: 6.2862444\ttotal: 12m 56s\tremaining: 37m 3s\n",
      "2588:\tlearn: 6.2848355\ttotal: 12m 56s\tremaining: 37m 3s\n",
      "2589:\tlearn: 6.2839222\ttotal: 12m 56s\tremaining: 37m 2s\n",
      "2590:\tlearn: 6.2828031\ttotal: 12m 57s\tremaining: 37m 2s\n",
      "2591:\tlearn: 6.2821677\ttotal: 12m 57s\tremaining: 37m 1s\n",
      "2592:\tlearn: 6.2809746\ttotal: 12m 57s\tremaining: 37m 1s\n",
      "2593:\tlearn: 6.2795360\ttotal: 12m 58s\tremaining: 37m 1s\n",
      "2594:\tlearn: 6.2784887\ttotal: 12m 58s\tremaining: 37m 1s\n",
      "2595:\tlearn: 6.2774452\ttotal: 12m 58s\tremaining: 37m\n",
      "2596:\tlearn: 6.2765994\ttotal: 12m 58s\tremaining: 37m\n",
      "2597:\tlearn: 6.2754591\ttotal: 12m 59s\tremaining: 36m 59s\n",
      "2598:\tlearn: 6.2742255\ttotal: 12m 59s\tremaining: 36m 59s\n",
      "2599:\tlearn: 6.2736151\ttotal: 12m 59s\tremaining: 36m 59s\n",
      "2600:\tlearn: 6.2726795\ttotal: 13m\tremaining: 36m 59s\n",
      "2601:\tlearn: 6.2715430\ttotal: 13m\tremaining: 36m 59s\n",
      "2602:\tlearn: 6.2704572\ttotal: 13m\tremaining: 36m 58s\n",
      "2603:\tlearn: 6.2694746\ttotal: 13m 1s\tremaining: 36m 58s\n",
      "2604:\tlearn: 6.2686428\ttotal: 13m 1s\tremaining: 36m 58s\n",
      "2605:\tlearn: 6.2677869\ttotal: 13m 1s\tremaining: 36m 57s\n",
      "2606:\tlearn: 6.2667401\ttotal: 13m 1s\tremaining: 36m 57s\n",
      "2607:\tlearn: 6.2659531\ttotal: 13m 2s\tremaining: 36m 57s\n",
      "2608:\tlearn: 6.2649893\ttotal: 13m 2s\tremaining: 36m 56s\n",
      "2609:\tlearn: 6.2642986\ttotal: 13m 2s\tremaining: 36m 56s\n",
      "2610:\tlearn: 6.2626731\ttotal: 13m 3s\tremaining: 36m 56s\n",
      "2611:\tlearn: 6.2622087\ttotal: 13m 3s\tremaining: 36m 55s\n",
      "2612:\tlearn: 6.2609991\ttotal: 13m 3s\tremaining: 36m 55s\n",
      "2613:\tlearn: 6.2597077\ttotal: 13m 4s\tremaining: 36m 55s\n",
      "2614:\tlearn: 6.2583239\ttotal: 13m 4s\tremaining: 36m 55s\n",
      "2615:\tlearn: 6.2572548\ttotal: 13m 4s\tremaining: 36m 54s\n",
      "2616:\tlearn: 6.2563489\ttotal: 13m 5s\tremaining: 36m 54s\n",
      "2617:\tlearn: 6.2555540\ttotal: 13m 5s\tremaining: 36m 54s\n",
      "2618:\tlearn: 6.2546733\ttotal: 13m 5s\tremaining: 36m 54s\n",
      "2619:\tlearn: 6.2538134\ttotal: 13m 5s\tremaining: 36m 53s\n",
      "2620:\tlearn: 6.2530363\ttotal: 13m 6s\tremaining: 36m 53s\n",
      "2621:\tlearn: 6.2512597\ttotal: 13m 6s\tremaining: 36m 53s\n",
      "2622:\tlearn: 6.2502343\ttotal: 13m 6s\tremaining: 36m 52s\n",
      "2623:\tlearn: 6.2494337\ttotal: 13m 7s\tremaining: 36m 52s\n",
      "2624:\tlearn: 6.2482884\ttotal: 13m 7s\tremaining: 36m 52s\n",
      "2625:\tlearn: 6.2471150\ttotal: 13m 7s\tremaining: 36m 51s\n",
      "2626:\tlearn: 6.2456405\ttotal: 13m 8s\tremaining: 36m 51s\n",
      "2627:\tlearn: 6.2449830\ttotal: 13m 8s\tremaining: 36m 51s\n",
      "2628:\tlearn: 6.2435542\ttotal: 13m 8s\tremaining: 36m 50s\n",
      "2629:\tlearn: 6.2428712\ttotal: 13m 8s\tremaining: 36m 50s\n",
      "2630:\tlearn: 6.2417154\ttotal: 13m 9s\tremaining: 36m 50s\n",
      "2631:\tlearn: 6.2405791\ttotal: 13m 9s\tremaining: 36m 50s\n",
      "2632:\tlearn: 6.2394391\ttotal: 13m 9s\tremaining: 36m 49s\n",
      "2633:\tlearn: 6.2385076\ttotal: 13m 10s\tremaining: 36m 49s\n",
      "2634:\tlearn: 6.2378533\ttotal: 13m 10s\tremaining: 36m 49s\n",
      "2635:\tlearn: 6.2370488\ttotal: 13m 10s\tremaining: 36m 48s\n",
      "2636:\tlearn: 6.2362314\ttotal: 13m 10s\tremaining: 36m 48s\n",
      "2637:\tlearn: 6.2353280\ttotal: 13m 11s\tremaining: 36m 47s\n",
      "2638:\tlearn: 6.2346621\ttotal: 13m 11s\tremaining: 36m 47s\n",
      "2639:\tlearn: 6.2336535\ttotal: 13m 11s\tremaining: 36m 47s\n",
      "2640:\tlearn: 6.2331792\ttotal: 13m 11s\tremaining: 36m 46s\n",
      "2641:\tlearn: 6.2322503\ttotal: 13m 12s\tremaining: 36m 46s\n",
      "2642:\tlearn: 6.2312583\ttotal: 13m 12s\tremaining: 36m 46s\n",
      "2643:\tlearn: 6.2295860\ttotal: 13m 13s\tremaining: 36m 46s\n",
      "2644:\tlearn: 6.2287648\ttotal: 13m 13s\tremaining: 36m 45s\n",
      "2645:\tlearn: 6.2276570\ttotal: 13m 13s\tremaining: 36m 45s\n",
      "2646:\tlearn: 6.2266327\ttotal: 13m 13s\tremaining: 36m 45s\n",
      "2647:\tlearn: 6.2260625\ttotal: 13m 14s\tremaining: 36m 44s\n",
      "2648:\tlearn: 6.2249957\ttotal: 13m 14s\tremaining: 36m 44s\n",
      "2649:\tlearn: 6.2233750\ttotal: 13m 14s\tremaining: 36m 44s\n",
      "2650:\tlearn: 6.2219624\ttotal: 13m 15s\tremaining: 36m 44s\n",
      "2651:\tlearn: 6.2211487\ttotal: 13m 15s\tremaining: 36m 43s\n",
      "2652:\tlearn: 6.2204754\ttotal: 13m 15s\tremaining: 36m 43s\n",
      "2653:\tlearn: 6.2194497\ttotal: 13m 15s\tremaining: 36m 42s\n",
      "2654:\tlearn: 6.2182708\ttotal: 13m 16s\tremaining: 36m 42s\n",
      "2655:\tlearn: 6.2174308\ttotal: 13m 16s\tremaining: 36m 42s\n",
      "2656:\tlearn: 6.2163795\ttotal: 13m 16s\tremaining: 36m 41s\n",
      "2657:\tlearn: 6.2157114\ttotal: 13m 16s\tremaining: 36m 41s\n",
      "2658:\tlearn: 6.2148284\ttotal: 13m 17s\tremaining: 36m 40s\n",
      "2659:\tlearn: 6.2139776\ttotal: 13m 17s\tremaining: 36m 40s\n",
      "2660:\tlearn: 6.2129480\ttotal: 13m 17s\tremaining: 36m 40s\n",
      "2661:\tlearn: 6.2121725\ttotal: 13m 18s\tremaining: 36m 39s\n",
      "2662:\tlearn: 6.2114474\ttotal: 13m 18s\tremaining: 36m 39s\n",
      "2663:\tlearn: 6.2104268\ttotal: 13m 18s\tremaining: 36m 39s\n",
      "2664:\tlearn: 6.2094682\ttotal: 13m 19s\tremaining: 36m 39s\n",
      "2665:\tlearn: 6.2085612\ttotal: 13m 19s\tremaining: 36m 38s\n",
      "2666:\tlearn: 6.2078002\ttotal: 13m 19s\tremaining: 36m 38s\n",
      "2667:\tlearn: 6.2069434\ttotal: 13m 19s\tremaining: 36m 38s\n",
      "2668:\tlearn: 6.2054508\ttotal: 13m 20s\tremaining: 36m 37s\n",
      "2669:\tlearn: 6.2046585\ttotal: 13m 20s\tremaining: 36m 37s\n",
      "2670:\tlearn: 6.2038361\ttotal: 13m 20s\tremaining: 36m 36s\n",
      "2671:\tlearn: 6.2032295\ttotal: 13m 20s\tremaining: 36m 36s\n",
      "2672:\tlearn: 6.2022171\ttotal: 13m 21s\tremaining: 36m 36s\n",
      "2673:\tlearn: 6.2008624\ttotal: 13m 21s\tremaining: 36m 35s\n",
      "2674:\tlearn: 6.1997014\ttotal: 13m 21s\tremaining: 36m 35s\n",
      "2675:\tlearn: 6.1991206\ttotal: 13m 22s\tremaining: 36m 35s\n",
      "2676:\tlearn: 6.1982715\ttotal: 13m 22s\tremaining: 36m 34s\n",
      "2677:\tlearn: 6.1974715\ttotal: 13m 22s\tremaining: 36m 34s\n",
      "2678:\tlearn: 6.1966152\ttotal: 13m 22s\tremaining: 36m 34s\n",
      "2679:\tlearn: 6.1951908\ttotal: 13m 23s\tremaining: 36m 34s\n",
      "2680:\tlearn: 6.1942857\ttotal: 13m 23s\tremaining: 36m 33s\n",
      "2681:\tlearn: 6.1934701\ttotal: 13m 23s\tremaining: 36m 33s\n",
      "2682:\tlearn: 6.1925748\ttotal: 13m 24s\tremaining: 36m 33s\n",
      "2683:\tlearn: 6.1920299\ttotal: 13m 24s\tremaining: 36m 32s\n",
      "2684:\tlearn: 6.1905970\ttotal: 13m 24s\tremaining: 36m 32s\n",
      "2685:\tlearn: 6.1895591\ttotal: 13m 25s\tremaining: 36m 32s\n",
      "2686:\tlearn: 6.1889645\ttotal: 13m 25s\tremaining: 36m 32s\n",
      "2687:\tlearn: 6.1880946\ttotal: 13m 25s\tremaining: 36m 31s\n",
      "2688:\tlearn: 6.1872918\ttotal: 13m 26s\tremaining: 36m 31s\n",
      "2689:\tlearn: 6.1859353\ttotal: 13m 26s\tremaining: 36m 31s\n",
      "2690:\tlearn: 6.1845883\ttotal: 13m 26s\tremaining: 36m 31s\n",
      "2691:\tlearn: 6.1835900\ttotal: 13m 27s\tremaining: 36m 30s\n",
      "2692:\tlearn: 6.1826529\ttotal: 13m 27s\tremaining: 36m 30s\n",
      "2693:\tlearn: 6.1816364\ttotal: 13m 27s\tremaining: 36m 30s\n",
      "2694:\tlearn: 6.1810569\ttotal: 13m 27s\tremaining: 36m 29s\n",
      "2695:\tlearn: 6.1799870\ttotal: 13m 28s\tremaining: 36m 29s\n",
      "2696:\tlearn: 6.1791154\ttotal: 13m 28s\tremaining: 36m 28s\n",
      "2697:\tlearn: 6.1779432\ttotal: 13m 28s\tremaining: 36m 28s\n",
      "2698:\tlearn: 6.1773032\ttotal: 13m 28s\tremaining: 36m 28s\n",
      "2699:\tlearn: 6.1763181\ttotal: 13m 29s\tremaining: 36m 28s\n",
      "2700:\tlearn: 6.1753825\ttotal: 13m 29s\tremaining: 36m 27s\n",
      "2701:\tlearn: 6.1740527\ttotal: 13m 29s\tremaining: 36m 27s\n",
      "2702:\tlearn: 6.1730556\ttotal: 13m 30s\tremaining: 36m 27s\n",
      "2703:\tlearn: 6.1723184\ttotal: 13m 30s\tremaining: 36m 26s\n",
      "2704:\tlearn: 6.1713548\ttotal: 13m 30s\tremaining: 36m 26s\n",
      "2705:\tlearn: 6.1703745\ttotal: 13m 30s\tremaining: 36m 25s\n",
      "2706:\tlearn: 6.1698538\ttotal: 13m 31s\tremaining: 36m 25s\n",
      "2707:\tlearn: 6.1688850\ttotal: 13m 31s\tremaining: 36m 24s\n",
      "2708:\tlearn: 6.1684297\ttotal: 13m 31s\tremaining: 36m 24s\n",
      "2709:\tlearn: 6.1673699\ttotal: 13m 31s\tremaining: 36m 24s\n",
      "2710:\tlearn: 6.1663108\ttotal: 13m 32s\tremaining: 36m 24s\n",
      "2711:\tlearn: 6.1654037\ttotal: 13m 32s\tremaining: 36m 23s\n",
      "2712:\tlearn: 6.1640428\ttotal: 13m 32s\tremaining: 36m 23s\n",
      "2713:\tlearn: 6.1631306\ttotal: 13m 33s\tremaining: 36m 23s\n",
      "2714:\tlearn: 6.1619514\ttotal: 13m 33s\tremaining: 36m 23s\n",
      "2715:\tlearn: 6.1607514\ttotal: 13m 33s\tremaining: 36m 22s\n",
      "2716:\tlearn: 6.1597956\ttotal: 13m 34s\tremaining: 36m 22s\n",
      "2717:\tlearn: 6.1586953\ttotal: 13m 34s\tremaining: 36m 22s\n",
      "2718:\tlearn: 6.1582096\ttotal: 13m 34s\tremaining: 36m 21s\n",
      "2719:\tlearn: 6.1575936\ttotal: 13m 34s\tremaining: 36m 21s\n",
      "2720:\tlearn: 6.1569174\ttotal: 13m 35s\tremaining: 36m 20s\n",
      "2721:\tlearn: 6.1555207\ttotal: 13m 35s\tremaining: 36m 20s\n",
      "2722:\tlearn: 6.1549268\ttotal: 13m 35s\tremaining: 36m 20s\n",
      "2723:\tlearn: 6.1539170\ttotal: 13m 36s\tremaining: 36m 20s\n",
      "2724:\tlearn: 6.1532512\ttotal: 13m 36s\tremaining: 36m 19s\n",
      "2725:\tlearn: 6.1520651\ttotal: 13m 36s\tremaining: 36m 19s\n",
      "2726:\tlearn: 6.1513818\ttotal: 13m 37s\tremaining: 36m 19s\n",
      "2727:\tlearn: 6.1502956\ttotal: 13m 37s\tremaining: 36m 18s\n",
      "2728:\tlearn: 6.1497116\ttotal: 13m 37s\tremaining: 36m 18s\n",
      "2729:\tlearn: 6.1485934\ttotal: 13m 37s\tremaining: 36m 18s\n",
      "2730:\tlearn: 6.1481544\ttotal: 13m 38s\tremaining: 36m 17s\n",
      "2731:\tlearn: 6.1471783\ttotal: 13m 38s\tremaining: 36m 17s\n",
      "2732:\tlearn: 6.1464156\ttotal: 13m 38s\tremaining: 36m 17s\n",
      "2733:\tlearn: 6.1459083\ttotal: 13m 39s\tremaining: 36m 16s\n",
      "2734:\tlearn: 6.1445318\ttotal: 13m 39s\tremaining: 36m 16s\n",
      "2735:\tlearn: 6.1432630\ttotal: 13m 39s\tremaining: 36m 16s\n",
      "2736:\tlearn: 6.1425508\ttotal: 13m 39s\tremaining: 36m 15s\n",
      "2737:\tlearn: 6.1415040\ttotal: 13m 40s\tremaining: 36m 15s\n",
      "2738:\tlearn: 6.1404567\ttotal: 13m 40s\tremaining: 36m 15s\n",
      "2739:\tlearn: 6.1396374\ttotal: 13m 40s\tremaining: 36m 14s\n",
      "2740:\tlearn: 6.1388824\ttotal: 13m 41s\tremaining: 36m 14s\n",
      "2741:\tlearn: 6.1380144\ttotal: 13m 41s\tremaining: 36m 13s\n",
      "2742:\tlearn: 6.1368776\ttotal: 13m 41s\tremaining: 36m 13s\n",
      "2743:\tlearn: 6.1361131\ttotal: 13m 41s\tremaining: 36m 13s\n",
      "2744:\tlearn: 6.1352906\ttotal: 13m 42s\tremaining: 36m 13s\n",
      "2745:\tlearn: 6.1346393\ttotal: 13m 42s\tremaining: 36m 12s\n",
      "2746:\tlearn: 6.1336641\ttotal: 13m 42s\tremaining: 36m 12s\n",
      "2747:\tlearn: 6.1324126\ttotal: 13m 43s\tremaining: 36m 11s\n",
      "2748:\tlearn: 6.1317234\ttotal: 13m 43s\tremaining: 36m 11s\n",
      "2749:\tlearn: 6.1310898\ttotal: 13m 43s\tremaining: 36m 11s\n",
      "2750:\tlearn: 6.1297037\ttotal: 13m 43s\tremaining: 36m 11s\n",
      "2751:\tlearn: 6.1288182\ttotal: 13m 44s\tremaining: 36m 10s\n",
      "2752:\tlearn: 6.1279259\ttotal: 13m 44s\tremaining: 36m 10s\n",
      "2753:\tlearn: 6.1270782\ttotal: 13m 44s\tremaining: 36m 10s\n",
      "2754:\tlearn: 6.1263153\ttotal: 13m 45s\tremaining: 36m 9s\n",
      "2755:\tlearn: 6.1254619\ttotal: 13m 45s\tremaining: 36m 9s\n",
      "2756:\tlearn: 6.1247037\ttotal: 13m 45s\tremaining: 36m 9s\n",
      "2757:\tlearn: 6.1239218\ttotal: 13m 46s\tremaining: 36m 8s\n",
      "2758:\tlearn: 6.1234477\ttotal: 13m 46s\tremaining: 36m 8s\n",
      "2759:\tlearn: 6.1222533\ttotal: 13m 46s\tremaining: 36m 8s\n",
      "2760:\tlearn: 6.1216747\ttotal: 13m 46s\tremaining: 36m 7s\n",
      "2761:\tlearn: 6.1206847\ttotal: 13m 47s\tremaining: 36m 7s\n",
      "2762:\tlearn: 6.1199661\ttotal: 13m 47s\tremaining: 36m 7s\n",
      "2763:\tlearn: 6.1187601\ttotal: 13m 47s\tremaining: 36m 7s\n",
      "2764:\tlearn: 6.1179845\ttotal: 13m 48s\tremaining: 36m 6s\n",
      "2765:\tlearn: 6.1174125\ttotal: 13m 48s\tremaining: 36m 6s\n",
      "2766:\tlearn: 6.1164121\ttotal: 13m 48s\tremaining: 36m 6s\n",
      "2767:\tlearn: 6.1152117\ttotal: 13m 48s\tremaining: 36m 5s\n",
      "2768:\tlearn: 6.1143220\ttotal: 13m 49s\tremaining: 36m 5s\n",
      "2769:\tlearn: 6.1131754\ttotal: 13m 49s\tremaining: 36m 5s\n",
      "2770:\tlearn: 6.1120866\ttotal: 13m 49s\tremaining: 36m 5s\n",
      "2771:\tlearn: 6.1112910\ttotal: 13m 50s\tremaining: 36m 4s\n",
      "2772:\tlearn: 6.1106404\ttotal: 13m 50s\tremaining: 36m 4s\n",
      "2773:\tlearn: 6.1098096\ttotal: 13m 50s\tremaining: 36m 4s\n",
      "2774:\tlearn: 6.1093037\ttotal: 13m 50s\tremaining: 36m 3s\n",
      "2775:\tlearn: 6.1082866\ttotal: 13m 51s\tremaining: 36m 3s\n",
      "2776:\tlearn: 6.1068222\ttotal: 13m 51s\tremaining: 36m 2s\n",
      "2777:\tlearn: 6.1062114\ttotal: 13m 51s\tremaining: 36m 2s\n",
      "2778:\tlearn: 6.1051244\ttotal: 13m 52s\tremaining: 36m 2s\n",
      "2779:\tlearn: 6.1043772\ttotal: 13m 52s\tremaining: 36m 1s\n",
      "2780:\tlearn: 6.1026975\ttotal: 13m 52s\tremaining: 36m 1s\n",
      "2781:\tlearn: 6.1021415\ttotal: 13m 52s\tremaining: 36m 1s\n",
      "2782:\tlearn: 6.1015026\ttotal: 13m 53s\tremaining: 36m\n",
      "2783:\tlearn: 6.1005212\ttotal: 13m 53s\tremaining: 36m\n",
      "2784:\tlearn: 6.0996646\ttotal: 13m 53s\tremaining: 35m 59s\n",
      "2785:\tlearn: 6.0986453\ttotal: 13m 54s\tremaining: 35m 59s\n",
      "2786:\tlearn: 6.0972156\ttotal: 13m 54s\tremaining: 35m 59s\n",
      "2787:\tlearn: 6.0962491\ttotal: 13m 54s\tremaining: 35m 58s\n",
      "2788:\tlearn: 6.0952891\ttotal: 13m 54s\tremaining: 35m 58s\n",
      "2789:\tlearn: 6.0938768\ttotal: 13m 55s\tremaining: 35m 58s\n",
      "2790:\tlearn: 6.0928963\ttotal: 13m 55s\tremaining: 35m 58s\n",
      "2791:\tlearn: 6.0920209\ttotal: 13m 55s\tremaining: 35m 57s\n",
      "2792:\tlearn: 6.0914709\ttotal: 13m 56s\tremaining: 35m 57s\n",
      "2793:\tlearn: 6.0900798\ttotal: 13m 56s\tremaining: 35m 57s\n",
      "2794:\tlearn: 6.0894473\ttotal: 13m 56s\tremaining: 35m 56s\n",
      "2795:\tlearn: 6.0881264\ttotal: 13m 57s\tremaining: 35m 56s\n",
      "2796:\tlearn: 6.0875291\ttotal: 13m 57s\tremaining: 35m 56s\n",
      "2797:\tlearn: 6.0862874\ttotal: 13m 57s\tremaining: 35m 56s\n",
      "2798:\tlearn: 6.0852478\ttotal: 13m 57s\tremaining: 35m 55s\n",
      "2799:\tlearn: 6.0842736\ttotal: 13m 58s\tremaining: 35m 55s\n",
      "2800:\tlearn: 6.0834652\ttotal: 13m 58s\tremaining: 35m 55s\n",
      "2801:\tlearn: 6.0823797\ttotal: 13m 58s\tremaining: 35m 55s\n",
      "2802:\tlearn: 6.0816767\ttotal: 13m 59s\tremaining: 35m 54s\n",
      "2803:\tlearn: 6.0809148\ttotal: 13m 59s\tremaining: 35m 54s\n",
      "2804:\tlearn: 6.0800863\ttotal: 13m 59s\tremaining: 35m 54s\n",
      "2805:\tlearn: 6.0791238\ttotal: 14m\tremaining: 35m 53s\n",
      "2806:\tlearn: 6.0782783\ttotal: 14m\tremaining: 35m 53s\n",
      "2807:\tlearn: 6.0777015\ttotal: 14m\tremaining: 35m 53s\n",
      "2808:\tlearn: 6.0760765\ttotal: 14m 1s\tremaining: 35m 53s\n",
      "2809:\tlearn: 6.0755002\ttotal: 14m 1s\tremaining: 35m 52s\n",
      "2810:\tlearn: 6.0742943\ttotal: 14m 1s\tremaining: 35m 52s\n",
      "2811:\tlearn: 6.0735256\ttotal: 14m 1s\tremaining: 35m 52s\n",
      "2812:\tlearn: 6.0729609\ttotal: 14m 2s\tremaining: 35m 51s\n",
      "2813:\tlearn: 6.0721788\ttotal: 14m 2s\tremaining: 35m 51s\n",
      "2814:\tlearn: 6.0713921\ttotal: 14m 2s\tremaining: 35m 50s\n",
      "2815:\tlearn: 6.0704093\ttotal: 14m 3s\tremaining: 35m 50s\n",
      "2816:\tlearn: 6.0690560\ttotal: 14m 3s\tremaining: 35m 50s\n",
      "2817:\tlearn: 6.0680127\ttotal: 14m 3s\tremaining: 35m 50s\n",
      "2818:\tlearn: 6.0666489\ttotal: 14m 4s\tremaining: 35m 50s\n",
      "2819:\tlearn: 6.0655041\ttotal: 14m 4s\tremaining: 35m 50s\n",
      "2820:\tlearn: 6.0645745\ttotal: 14m 4s\tremaining: 35m 49s\n",
      "2821:\tlearn: 6.0635177\ttotal: 14m 5s\tremaining: 35m 49s\n",
      "2822:\tlearn: 6.0622298\ttotal: 14m 5s\tremaining: 35m 49s\n",
      "2823:\tlearn: 6.0614812\ttotal: 14m 5s\tremaining: 35m 49s\n",
      "2824:\tlearn: 6.0607545\ttotal: 14m 6s\tremaining: 35m 48s\n",
      "2825:\tlearn: 6.0596725\ttotal: 14m 6s\tremaining: 35m 48s\n",
      "2826:\tlearn: 6.0586792\ttotal: 14m 6s\tremaining: 35m 48s\n",
      "2827:\tlearn: 6.0579601\ttotal: 14m 6s\tremaining: 35m 48s\n",
      "2828:\tlearn: 6.0573625\ttotal: 14m 7s\tremaining: 35m 47s\n",
      "2829:\tlearn: 6.0564307\ttotal: 14m 7s\tremaining: 35m 47s\n",
      "2830:\tlearn: 6.0554382\ttotal: 14m 7s\tremaining: 35m 47s\n",
      "2831:\tlearn: 6.0544407\ttotal: 14m 8s\tremaining: 35m 46s\n",
      "2832:\tlearn: 6.0536526\ttotal: 14m 8s\tremaining: 35m 46s\n",
      "2833:\tlearn: 6.0525877\ttotal: 14m 8s\tremaining: 35m 46s\n",
      "2834:\tlearn: 6.0518012\ttotal: 14m 9s\tremaining: 35m 45s\n",
      "2835:\tlearn: 6.0511170\ttotal: 14m 9s\tremaining: 35m 45s\n",
      "2836:\tlearn: 6.0503850\ttotal: 14m 9s\tremaining: 35m 45s\n",
      "2837:\tlearn: 6.0496425\ttotal: 14m 9s\tremaining: 35m 44s\n",
      "2838:\tlearn: 6.0490499\ttotal: 14m 10s\tremaining: 35m 44s\n",
      "2839:\tlearn: 6.0479759\ttotal: 14m 10s\tremaining: 35m 44s\n",
      "2840:\tlearn: 6.0467073\ttotal: 14m 10s\tremaining: 35m 43s\n",
      "2841:\tlearn: 6.0454564\ttotal: 14m 11s\tremaining: 35m 43s\n",
      "2842:\tlearn: 6.0441950\ttotal: 14m 11s\tremaining: 35m 43s\n",
      "2843:\tlearn: 6.0430778\ttotal: 14m 11s\tremaining: 35m 43s\n",
      "2844:\tlearn: 6.0423958\ttotal: 14m 12s\tremaining: 35m 42s\n",
      "2845:\tlearn: 6.0414308\ttotal: 14m 12s\tremaining: 35m 42s\n",
      "2846:\tlearn: 6.0408892\ttotal: 14m 12s\tremaining: 35m 42s\n",
      "2847:\tlearn: 6.0400574\ttotal: 14m 12s\tremaining: 35m 41s\n",
      "2848:\tlearn: 6.0394532\ttotal: 14m 13s\tremaining: 35m 41s\n",
      "2849:\tlearn: 6.0385182\ttotal: 14m 13s\tremaining: 35m 41s\n",
      "2850:\tlearn: 6.0373910\ttotal: 14m 13s\tremaining: 35m 40s\n",
      "2851:\tlearn: 6.0365749\ttotal: 14m 14s\tremaining: 35m 40s\n",
      "2852:\tlearn: 6.0359420\ttotal: 14m 14s\tremaining: 35m 40s\n",
      "2853:\tlearn: 6.0351906\ttotal: 14m 14s\tremaining: 35m 39s\n",
      "2854:\tlearn: 6.0340683\ttotal: 14m 14s\tremaining: 35m 39s\n",
      "2855:\tlearn: 6.0333666\ttotal: 14m 15s\tremaining: 35m 38s\n",
      "2856:\tlearn: 6.0324763\ttotal: 14m 15s\tremaining: 35m 38s\n",
      "2857:\tlearn: 6.0310924\ttotal: 14m 15s\tremaining: 35m 38s\n",
      "2858:\tlearn: 6.0306053\ttotal: 14m 15s\tremaining: 35m 37s\n",
      "2859:\tlearn: 6.0298078\ttotal: 14m 16s\tremaining: 35m 37s\n",
      "2860:\tlearn: 6.0289446\ttotal: 14m 16s\tremaining: 35m 37s\n",
      "2861:\tlearn: 6.0279946\ttotal: 14m 16s\tremaining: 35m 36s\n",
      "2862:\tlearn: 6.0266750\ttotal: 14m 17s\tremaining: 35m 36s\n",
      "2863:\tlearn: 6.0253004\ttotal: 14m 17s\tremaining: 35m 36s\n",
      "2864:\tlearn: 6.0246852\ttotal: 14m 17s\tremaining: 35m 35s\n",
      "2865:\tlearn: 6.0238616\ttotal: 14m 17s\tremaining: 35m 35s\n",
      "2866:\tlearn: 6.0223248\ttotal: 14m 18s\tremaining: 35m 35s\n",
      "2867:\tlearn: 6.0209905\ttotal: 14m 18s\tremaining: 35m 35s\n",
      "2868:\tlearn: 6.0199129\ttotal: 14m 19s\tremaining: 35m 35s\n",
      "2869:\tlearn: 6.0191999\ttotal: 14m 19s\tremaining: 35m 35s\n",
      "2870:\tlearn: 6.0181657\ttotal: 14m 19s\tremaining: 35m 34s\n",
      "2871:\tlearn: 6.0172465\ttotal: 14m 20s\tremaining: 35m 34s\n",
      "2872:\tlearn: 6.0166931\ttotal: 14m 20s\tremaining: 35m 34s\n",
      "2873:\tlearn: 6.0160619\ttotal: 14m 20s\tremaining: 35m 33s\n",
      "2874:\tlearn: 6.0154293\ttotal: 14m 20s\tremaining: 35m 33s\n",
      "2875:\tlearn: 6.0142052\ttotal: 14m 21s\tremaining: 35m 33s\n",
      "2876:\tlearn: 6.0137369\ttotal: 14m 21s\tremaining: 35m 32s\n",
      "2877:\tlearn: 6.0124549\ttotal: 14m 21s\tremaining: 35m 32s\n",
      "2878:\tlearn: 6.0114644\ttotal: 14m 22s\tremaining: 35m 32s\n",
      "2879:\tlearn: 6.0108337\ttotal: 14m 22s\tremaining: 35m 32s\n",
      "2880:\tlearn: 6.0101402\ttotal: 14m 22s\tremaining: 35m 31s\n",
      "2881:\tlearn: 6.0092979\ttotal: 14m 23s\tremaining: 35m 31s\n",
      "2882:\tlearn: 6.0086310\ttotal: 14m 23s\tremaining: 35m 31s\n",
      "2883:\tlearn: 6.0075541\ttotal: 14m 23s\tremaining: 35m 30s\n",
      "2884:\tlearn: 6.0068222\ttotal: 14m 23s\tremaining: 35m 30s\n",
      "2885:\tlearn: 6.0060951\ttotal: 14m 24s\tremaining: 35m 30s\n",
      "2886:\tlearn: 6.0055529\ttotal: 14m 24s\tremaining: 35m 29s\n",
      "2887:\tlearn: 6.0044062\ttotal: 14m 24s\tremaining: 35m 29s\n",
      "2888:\tlearn: 6.0036065\ttotal: 14m 24s\tremaining: 35m 29s\n",
      "2889:\tlearn: 6.0027796\ttotal: 14m 25s\tremaining: 35m 28s\n",
      "2890:\tlearn: 6.0021207\ttotal: 14m 25s\tremaining: 35m 28s\n",
      "2891:\tlearn: 6.0012057\ttotal: 14m 26s\tremaining: 35m 28s\n",
      "2892:\tlearn: 6.0005987\ttotal: 14m 26s\tremaining: 35m 28s\n",
      "2893:\tlearn: 5.9995944\ttotal: 14m 26s\tremaining: 35m 27s\n",
      "2894:\tlearn: 5.9990015\ttotal: 14m 26s\tremaining: 35m 27s\n",
      "2895:\tlearn: 5.9977365\ttotal: 14m 27s\tremaining: 35m 27s\n",
      "2896:\tlearn: 5.9965598\ttotal: 14m 27s\tremaining: 35m 27s\n",
      "2897:\tlearn: 5.9952742\ttotal: 14m 27s\tremaining: 35m 27s\n",
      "2898:\tlearn: 5.9939712\ttotal: 14m 28s\tremaining: 35m 26s\n",
      "2899:\tlearn: 5.9930436\ttotal: 14m 28s\tremaining: 35m 26s\n",
      "2900:\tlearn: 5.9924807\ttotal: 14m 28s\tremaining: 35m 26s\n",
      "2901:\tlearn: 5.9918337\ttotal: 14m 29s\tremaining: 35m 25s\n",
      "2902:\tlearn: 5.9911885\ttotal: 14m 29s\tremaining: 35m 25s\n",
      "2903:\tlearn: 5.9903679\ttotal: 14m 29s\tremaining: 35m 25s\n",
      "2904:\tlearn: 5.9892441\ttotal: 14m 30s\tremaining: 35m 24s\n",
      "2905:\tlearn: 5.9885596\ttotal: 14m 30s\tremaining: 35m 24s\n",
      "2906:\tlearn: 5.9877315\ttotal: 14m 30s\tremaining: 35m 24s\n",
      "2907:\tlearn: 5.9863252\ttotal: 14m 31s\tremaining: 35m 24s\n",
      "2908:\tlearn: 5.9857141\ttotal: 14m 31s\tremaining: 35m 23s\n",
      "2909:\tlearn: 5.9846706\ttotal: 14m 31s\tremaining: 35m 23s\n",
      "2910:\tlearn: 5.9841153\ttotal: 14m 31s\tremaining: 35m 23s\n",
      "2911:\tlearn: 5.9834089\ttotal: 14m 32s\tremaining: 35m 23s\n",
      "2912:\tlearn: 5.9823571\ttotal: 14m 32s\tremaining: 35m 23s\n",
      "2913:\tlearn: 5.9811238\ttotal: 14m 33s\tremaining: 35m 23s\n",
      "2914:\tlearn: 5.9805274\ttotal: 14m 33s\tremaining: 35m 22s\n",
      "2915:\tlearn: 5.9796708\ttotal: 14m 33s\tremaining: 35m 22s\n",
      "2916:\tlearn: 5.9784584\ttotal: 14m 33s\tremaining: 35m 22s\n",
      "2917:\tlearn: 5.9776064\ttotal: 14m 34s\tremaining: 35m 21s\n",
      "2918:\tlearn: 5.9764636\ttotal: 14m 34s\tremaining: 35m 21s\n",
      "2919:\tlearn: 5.9751650\ttotal: 14m 35s\tremaining: 35m 21s\n",
      "2920:\tlearn: 5.9744662\ttotal: 14m 35s\tremaining: 35m 21s\n",
      "2921:\tlearn: 5.9733459\ttotal: 14m 35s\tremaining: 35m 21s\n",
      "2922:\tlearn: 5.9721705\ttotal: 14m 36s\tremaining: 35m 20s\n",
      "2923:\tlearn: 5.9713706\ttotal: 14m 36s\tremaining: 35m 20s\n",
      "2924:\tlearn: 5.9702166\ttotal: 14m 36s\tremaining: 35m 20s\n",
      "2925:\tlearn: 5.9689596\ttotal: 14m 37s\tremaining: 35m 20s\n",
      "2926:\tlearn: 5.9679830\ttotal: 14m 37s\tremaining: 35m 20s\n",
      "2927:\tlearn: 5.9671242\ttotal: 14m 37s\tremaining: 35m 20s\n",
      "2928:\tlearn: 5.9659758\ttotal: 14m 38s\tremaining: 35m 20s\n",
      "2929:\tlearn: 5.9654028\ttotal: 14m 38s\tremaining: 35m 19s\n",
      "2930:\tlearn: 5.9646181\ttotal: 14m 38s\tremaining: 35m 19s\n",
      "2931:\tlearn: 5.9637767\ttotal: 14m 39s\tremaining: 35m 19s\n",
      "2932:\tlearn: 5.9632525\ttotal: 14m 39s\tremaining: 35m 18s\n",
      "2933:\tlearn: 5.9624235\ttotal: 14m 39s\tremaining: 35m 18s\n",
      "2934:\tlearn: 5.9617834\ttotal: 14m 39s\tremaining: 35m 17s\n",
      "2935:\tlearn: 5.9606391\ttotal: 14m 40s\tremaining: 35m 17s\n",
      "2936:\tlearn: 5.9598207\ttotal: 14m 40s\tremaining: 35m 17s\n",
      "2937:\tlearn: 5.9591625\ttotal: 14m 40s\tremaining: 35m 17s\n",
      "2938:\tlearn: 5.9586421\ttotal: 14m 41s\tremaining: 35m 16s\n",
      "2939:\tlearn: 5.9578428\ttotal: 14m 41s\tremaining: 35m 16s\n",
      "2940:\tlearn: 5.9571861\ttotal: 14m 41s\tremaining: 35m 16s\n",
      "2941:\tlearn: 5.9565227\ttotal: 14m 41s\tremaining: 35m 15s\n",
      "2942:\tlearn: 5.9555207\ttotal: 14m 42s\tremaining: 35m 15s\n",
      "2943:\tlearn: 5.9546090\ttotal: 14m 42s\tremaining: 35m 15s\n",
      "2944:\tlearn: 5.9537070\ttotal: 14m 42s\tremaining: 35m 15s\n",
      "2945:\tlearn: 5.9529560\ttotal: 14m 43s\tremaining: 35m 14s\n",
      "2946:\tlearn: 5.9518710\ttotal: 14m 43s\tremaining: 35m 14s\n",
      "2947:\tlearn: 5.9512667\ttotal: 14m 43s\tremaining: 35m 14s\n",
      "2948:\tlearn: 5.9507468\ttotal: 14m 44s\tremaining: 35m 13s\n",
      "2949:\tlearn: 5.9501512\ttotal: 14m 44s\tremaining: 35m 13s\n",
      "2950:\tlearn: 5.9491456\ttotal: 14m 44s\tremaining: 35m 13s\n",
      "2951:\tlearn: 5.9480907\ttotal: 14m 45s\tremaining: 35m 13s\n",
      "2952:\tlearn: 5.9475205\ttotal: 14m 45s\tremaining: 35m 12s\n",
      "2953:\tlearn: 5.9468976\ttotal: 14m 45s\tremaining: 35m 12s\n",
      "2954:\tlearn: 5.9458209\ttotal: 14m 45s\tremaining: 35m 12s\n",
      "2955:\tlearn: 5.9446132\ttotal: 14m 46s\tremaining: 35m 11s\n",
      "2956:\tlearn: 5.9440314\ttotal: 14m 46s\tremaining: 35m 11s\n",
      "2957:\tlearn: 5.9433851\ttotal: 14m 46s\tremaining: 35m 11s\n",
      "2958:\tlearn: 5.9422174\ttotal: 14m 47s\tremaining: 35m 11s\n",
      "2959:\tlearn: 5.9415264\ttotal: 14m 47s\tremaining: 35m 10s\n",
      "2960:\tlearn: 5.9410252\ttotal: 14m 47s\tremaining: 35m 10s\n",
      "2961:\tlearn: 5.9402550\ttotal: 14m 48s\tremaining: 35m 10s\n",
      "2962:\tlearn: 5.9392295\ttotal: 14m 48s\tremaining: 35m 9s\n",
      "2963:\tlearn: 5.9387191\ttotal: 14m 48s\tremaining: 35m 9s\n",
      "2964:\tlearn: 5.9379758\ttotal: 14m 48s\tremaining: 35m 9s\n",
      "2965:\tlearn: 5.9371589\ttotal: 14m 49s\tremaining: 35m 8s\n",
      "2966:\tlearn: 5.9365301\ttotal: 14m 49s\tremaining: 35m 8s\n",
      "2967:\tlearn: 5.9358091\ttotal: 14m 49s\tremaining: 35m 8s\n",
      "2968:\tlearn: 5.9352354\ttotal: 14m 50s\tremaining: 35m 7s\n",
      "2969:\tlearn: 5.9343459\ttotal: 14m 50s\tremaining: 35m 7s\n",
      "2970:\tlearn: 5.9337054\ttotal: 14m 50s\tremaining: 35m 7s\n",
      "2971:\tlearn: 5.9332025\ttotal: 14m 50s\tremaining: 35m 6s\n",
      "2972:\tlearn: 5.9324985\ttotal: 14m 51s\tremaining: 35m 6s\n",
      "2973:\tlearn: 5.9316420\ttotal: 14m 51s\tremaining: 35m 6s\n",
      "2974:\tlearn: 5.9307125\ttotal: 14m 51s\tremaining: 35m 5s\n",
      "2975:\tlearn: 5.9302230\ttotal: 14m 52s\tremaining: 35m 5s\n",
      "2976:\tlearn: 5.9289436\ttotal: 14m 52s\tremaining: 35m 5s\n",
      "2977:\tlearn: 5.9278650\ttotal: 14m 52s\tremaining: 35m 5s\n",
      "2978:\tlearn: 5.9269406\ttotal: 14m 53s\tremaining: 35m 5s\n",
      "2979:\tlearn: 5.9263817\ttotal: 14m 53s\tremaining: 35m 4s\n",
      "2980:\tlearn: 5.9256211\ttotal: 14m 53s\tremaining: 35m 4s\n",
      "2981:\tlearn: 5.9240396\ttotal: 14m 54s\tremaining: 35m 4s\n",
      "2982:\tlearn: 5.9232303\ttotal: 14m 54s\tremaining: 35m 3s\n",
      "2983:\tlearn: 5.9222349\ttotal: 14m 54s\tremaining: 35m 3s\n",
      "2984:\tlearn: 5.9215119\ttotal: 14m 54s\tremaining: 35m 3s\n",
      "2985:\tlearn: 5.9204083\ttotal: 14m 55s\tremaining: 35m 2s\n",
      "2986:\tlearn: 5.9200425\ttotal: 14m 55s\tremaining: 35m 2s\n",
      "2987:\tlearn: 5.9190682\ttotal: 14m 55s\tremaining: 35m 2s\n",
      "2988:\tlearn: 5.9183039\ttotal: 14m 55s\tremaining: 35m 1s\n",
      "2989:\tlearn: 5.9172410\ttotal: 14m 56s\tremaining: 35m 1s\n",
      "2990:\tlearn: 5.9164323\ttotal: 14m 56s\tremaining: 35m 1s\n",
      "2991:\tlearn: 5.9158118\ttotal: 14m 56s\tremaining: 35m\n",
      "2992:\tlearn: 5.9151141\ttotal: 14m 57s\tremaining: 35m\n",
      "2993:\tlearn: 5.9143738\ttotal: 14m 57s\tremaining: 35m\n",
      "2994:\tlearn: 5.9134479\ttotal: 14m 57s\tremaining: 34m 59s\n",
      "2995:\tlearn: 5.9127826\ttotal: 14m 58s\tremaining: 34m 59s\n",
      "2996:\tlearn: 5.9119404\ttotal: 14m 58s\tremaining: 34m 59s\n",
      "2997:\tlearn: 5.9111679\ttotal: 14m 58s\tremaining: 34m 58s\n",
      "2998:\tlearn: 5.9105414\ttotal: 14m 59s\tremaining: 34m 58s\n",
      "2999:\tlearn: 5.9095692\ttotal: 14m 59s\tremaining: 34m 58s\n",
      "3000:\tlearn: 5.9086762\ttotal: 14m 59s\tremaining: 34m 58s\n",
      "3001:\tlearn: 5.9080239\ttotal: 14m 59s\tremaining: 34m 57s\n",
      "3002:\tlearn: 5.9071244\ttotal: 15m\tremaining: 34m 57s\n",
      "3003:\tlearn: 5.9065811\ttotal: 15m\tremaining: 34m 57s\n",
      "3004:\tlearn: 5.9058179\ttotal: 15m\tremaining: 34m 56s\n",
      "3005:\tlearn: 5.9044473\ttotal: 15m 1s\tremaining: 34m 56s\n",
      "3006:\tlearn: 5.9030673\ttotal: 15m 1s\tremaining: 34m 56s\n",
      "3007:\tlearn: 5.9020843\ttotal: 15m 1s\tremaining: 34m 56s\n",
      "3008:\tlearn: 5.9008315\ttotal: 15m 2s\tremaining: 34m 56s\n",
      "3009:\tlearn: 5.8998272\ttotal: 15m 2s\tremaining: 34m 55s\n",
      "3010:\tlearn: 5.8988918\ttotal: 15m 2s\tremaining: 34m 55s\n",
      "3011:\tlearn: 5.8979827\ttotal: 15m 3s\tremaining: 34m 55s\n",
      "3012:\tlearn: 5.8974252\ttotal: 15m 3s\tremaining: 34m 55s\n",
      "3013:\tlearn: 5.8966430\ttotal: 15m 3s\tremaining: 34m 54s\n",
      "3014:\tlearn: 5.8956709\ttotal: 15m 4s\tremaining: 34m 54s\n",
      "3015:\tlearn: 5.8945787\ttotal: 15m 4s\tremaining: 34m 54s\n",
      "3016:\tlearn: 5.8936853\ttotal: 15m 4s\tremaining: 34m 54s\n",
      "3017:\tlearn: 5.8924876\ttotal: 15m 5s\tremaining: 34m 54s\n",
      "3018:\tlearn: 5.8915634\ttotal: 15m 5s\tremaining: 34m 54s\n",
      "3019:\tlearn: 5.8905932\ttotal: 15m 5s\tremaining: 34m 53s\n",
      "3020:\tlearn: 5.8895672\ttotal: 15m 6s\tremaining: 34m 53s\n",
      "3021:\tlearn: 5.8883241\ttotal: 15m 6s\tremaining: 34m 53s\n",
      "3022:\tlearn: 5.8873406\ttotal: 15m 6s\tremaining: 34m 53s\n",
      "3023:\tlearn: 5.8867019\ttotal: 15m 7s\tremaining: 34m 53s\n",
      "3024:\tlearn: 5.8857394\ttotal: 15m 7s\tremaining: 34m 52s\n",
      "3025:\tlearn: 5.8846429\ttotal: 15m 8s\tremaining: 34m 53s\n",
      "3026:\tlearn: 5.8841753\ttotal: 15m 8s\tremaining: 34m 52s\n",
      "3027:\tlearn: 5.8829979\ttotal: 15m 8s\tremaining: 34m 52s\n",
      "3028:\tlearn: 5.8821732\ttotal: 15m 9s\tremaining: 34m 52s\n",
      "3029:\tlearn: 5.8814205\ttotal: 15m 9s\tremaining: 34m 52s\n",
      "3030:\tlearn: 5.8807947\ttotal: 15m 10s\tremaining: 34m 52s\n",
      "3031:\tlearn: 5.8801327\ttotal: 15m 10s\tremaining: 34m 52s\n",
      "3032:\tlearn: 5.8791258\ttotal: 15m 10s\tremaining: 34m 52s\n",
      "3033:\tlearn: 5.8786820\ttotal: 15m 11s\tremaining: 34m 51s\n",
      "3034:\tlearn: 5.8776436\ttotal: 15m 11s\tremaining: 34m 51s\n",
      "3035:\tlearn: 5.8770354\ttotal: 15m 11s\tremaining: 34m 51s\n",
      "3036:\tlearn: 5.8761274\ttotal: 15m 11s\tremaining: 34m 50s\n",
      "3037:\tlearn: 5.8756343\ttotal: 15m 12s\tremaining: 34m 50s\n",
      "3038:\tlearn: 5.8749571\ttotal: 15m 12s\tremaining: 34m 50s\n",
      "3039:\tlearn: 5.8741314\ttotal: 15m 12s\tremaining: 34m 49s\n",
      "3040:\tlearn: 5.8737701\ttotal: 15m 13s\tremaining: 34m 49s\n",
      "3041:\tlearn: 5.8726185\ttotal: 15m 13s\tremaining: 34m 49s\n",
      "3042:\tlearn: 5.8718756\ttotal: 15m 13s\tremaining: 34m 48s\n",
      "3043:\tlearn: 5.8712103\ttotal: 15m 14s\tremaining: 34m 48s\n",
      "3044:\tlearn: 5.8704392\ttotal: 15m 14s\tremaining: 34m 48s\n",
      "3045:\tlearn: 5.8694009\ttotal: 15m 14s\tremaining: 34m 48s\n",
      "3046:\tlearn: 5.8687732\ttotal: 15m 14s\tremaining: 34m 47s\n",
      "3047:\tlearn: 5.8676612\ttotal: 15m 15s\tremaining: 34m 47s\n",
      "3048:\tlearn: 5.8669205\ttotal: 15m 15s\tremaining: 34m 47s\n",
      "3049:\tlearn: 5.8655968\ttotal: 15m 15s\tremaining: 34m 47s\n",
      "3050:\tlearn: 5.8651475\ttotal: 15m 16s\tremaining: 34m 46s\n",
      "3051:\tlearn: 5.8643070\ttotal: 15m 16s\tremaining: 34m 46s\n",
      "3052:\tlearn: 5.8635790\ttotal: 15m 16s\tremaining: 34m 45s\n",
      "3053:\tlearn: 5.8626265\ttotal: 15m 16s\tremaining: 34m 45s\n",
      "3054:\tlearn: 5.8618186\ttotal: 15m 17s\tremaining: 34m 45s\n",
      "3055:\tlearn: 5.8610250\ttotal: 15m 17s\tremaining: 34m 44s\n",
      "3056:\tlearn: 5.8601412\ttotal: 15m 17s\tremaining: 34m 44s\n",
      "3057:\tlearn: 5.8596031\ttotal: 15m 18s\tremaining: 34m 44s\n",
      "3058:\tlearn: 5.8589968\ttotal: 15m 18s\tremaining: 34m 43s\n",
      "3059:\tlearn: 5.8581031\ttotal: 15m 18s\tremaining: 34m 43s\n",
      "3060:\tlearn: 5.8571469\ttotal: 15m 19s\tremaining: 34m 43s\n",
      "3061:\tlearn: 5.8563464\ttotal: 15m 19s\tremaining: 34m 43s\n",
      "3062:\tlearn: 5.8558064\ttotal: 15m 19s\tremaining: 34m 42s\n",
      "3063:\tlearn: 5.8546302\ttotal: 15m 19s\tremaining: 34m 42s\n",
      "3064:\tlearn: 5.8537436\ttotal: 15m 20s\tremaining: 34m 42s\n",
      "3065:\tlearn: 5.8530641\ttotal: 15m 20s\tremaining: 34m 41s\n",
      "3066:\tlearn: 5.8524949\ttotal: 15m 20s\tremaining: 34m 41s\n",
      "3067:\tlearn: 5.8517505\ttotal: 15m 21s\tremaining: 34m 41s\n",
      "3068:\tlearn: 5.8507231\ttotal: 15m 21s\tremaining: 34m 40s\n",
      "3069:\tlearn: 5.8500393\ttotal: 15m 21s\tremaining: 34m 40s\n",
      "3070:\tlearn: 5.8491930\ttotal: 15m 21s\tremaining: 34m 40s\n",
      "3071:\tlearn: 5.8485540\ttotal: 15m 22s\tremaining: 34m 39s\n",
      "3072:\tlearn: 5.8476539\ttotal: 15m 22s\tremaining: 34m 39s\n",
      "3073:\tlearn: 5.8472311\ttotal: 15m 22s\tremaining: 34m 38s\n",
      "3074:\tlearn: 5.8467013\ttotal: 15m 22s\tremaining: 34m 38s\n",
      "3075:\tlearn: 5.8460986\ttotal: 15m 23s\tremaining: 34m 38s\n",
      "3076:\tlearn: 5.8454591\ttotal: 15m 23s\tremaining: 34m 37s\n",
      "3077:\tlearn: 5.8443790\ttotal: 15m 23s\tremaining: 34m 37s\n",
      "3078:\tlearn: 5.8437987\ttotal: 15m 24s\tremaining: 34m 37s\n",
      "3079:\tlearn: 5.8426850\ttotal: 15m 24s\tremaining: 34m 36s\n",
      "3080:\tlearn: 5.8418175\ttotal: 15m 24s\tremaining: 34m 36s\n",
      "3081:\tlearn: 5.8413181\ttotal: 15m 24s\tremaining: 34m 36s\n",
      "3082:\tlearn: 5.8407030\ttotal: 15m 25s\tremaining: 34m 35s\n",
      "3083:\tlearn: 5.8397633\ttotal: 15m 25s\tremaining: 34m 35s\n",
      "3084:\tlearn: 5.8390531\ttotal: 15m 25s\tremaining: 34m 35s\n",
      "3085:\tlearn: 5.8380307\ttotal: 15m 26s\tremaining: 34m 34s\n",
      "3086:\tlearn: 5.8373721\ttotal: 15m 26s\tremaining: 34m 34s\n",
      "3087:\tlearn: 5.8366008\ttotal: 15m 26s\tremaining: 34m 34s\n",
      "3088:\tlearn: 5.8359832\ttotal: 15m 26s\tremaining: 34m 33s\n",
      "3089:\tlearn: 5.8350971\ttotal: 15m 27s\tremaining: 34m 33s\n",
      "3090:\tlearn: 5.8335936\ttotal: 15m 27s\tremaining: 34m 33s\n",
      "3091:\tlearn: 5.8326972\ttotal: 15m 27s\tremaining: 34m 32s\n",
      "3092:\tlearn: 5.8320249\ttotal: 15m 27s\tremaining: 34m 32s\n",
      "3093:\tlearn: 5.8309099\ttotal: 15m 28s\tremaining: 34m 32s\n",
      "3094:\tlearn: 5.8301401\ttotal: 15m 28s\tremaining: 34m 31s\n",
      "3095:\tlearn: 5.8293370\ttotal: 15m 28s\tremaining: 34m 31s\n",
      "3096:\tlearn: 5.8285190\ttotal: 15m 29s\tremaining: 34m 31s\n",
      "3097:\tlearn: 5.8280017\ttotal: 15m 29s\tremaining: 34m 30s\n",
      "3098:\tlearn: 5.8271237\ttotal: 15m 29s\tremaining: 34m 30s\n",
      "3099:\tlearn: 5.8264061\ttotal: 15m 30s\tremaining: 34m 30s\n",
      "3100:\tlearn: 5.8254791\ttotal: 15m 30s\tremaining: 34m 29s\n",
      "3101:\tlearn: 5.8242290\ttotal: 15m 30s\tremaining: 34m 29s\n",
      "3102:\tlearn: 5.8231986\ttotal: 15m 31s\tremaining: 34m 29s\n",
      "3103:\tlearn: 5.8220786\ttotal: 15m 31s\tremaining: 34m 29s\n",
      "3104:\tlearn: 5.8210525\ttotal: 15m 31s\tremaining: 34m 29s\n",
      "3105:\tlearn: 5.8203440\ttotal: 15m 32s\tremaining: 34m 28s\n",
      "3106:\tlearn: 5.8193215\ttotal: 15m 32s\tremaining: 34m 28s\n",
      "3107:\tlearn: 5.8188374\ttotal: 15m 32s\tremaining: 34m 28s\n",
      "3108:\tlearn: 5.8180642\ttotal: 15m 32s\tremaining: 34m 27s\n",
      "3109:\tlearn: 5.8175095\ttotal: 15m 33s\tremaining: 34m 27s\n",
      "3110:\tlearn: 5.8165203\ttotal: 15m 33s\tremaining: 34m 27s\n",
      "3111:\tlearn: 5.8157823\ttotal: 15m 33s\tremaining: 34m 26s\n",
      "3112:\tlearn: 5.8147470\ttotal: 15m 34s\tremaining: 34m 26s\n",
      "3113:\tlearn: 5.8137834\ttotal: 15m 34s\tremaining: 34m 26s\n",
      "3114:\tlearn: 5.8129970\ttotal: 15m 34s\tremaining: 34m 25s\n",
      "3115:\tlearn: 5.8121784\ttotal: 15m 34s\tremaining: 34m 25s\n",
      "3116:\tlearn: 5.8106404\ttotal: 15m 35s\tremaining: 34m 25s\n",
      "3117:\tlearn: 5.8098611\ttotal: 15m 35s\tremaining: 34m 25s\n",
      "3118:\tlearn: 5.8092318\ttotal: 15m 35s\tremaining: 34m 24s\n",
      "3119:\tlearn: 5.8087676\ttotal: 15m 36s\tremaining: 34m 24s\n",
      "3120:\tlearn: 5.8073747\ttotal: 15m 36s\tremaining: 34m 24s\n",
      "3121:\tlearn: 5.8064769\ttotal: 15m 36s\tremaining: 34m 24s\n",
      "3122:\tlearn: 5.8057624\ttotal: 15m 37s\tremaining: 34m 23s\n",
      "3123:\tlearn: 5.8050058\ttotal: 15m 37s\tremaining: 34m 23s\n",
      "3124:\tlearn: 5.8041529\ttotal: 15m 37s\tremaining: 34m 23s\n",
      "3125:\tlearn: 5.8031843\ttotal: 15m 38s\tremaining: 34m 23s\n",
      "3126:\tlearn: 5.8025903\ttotal: 15m 38s\tremaining: 34m 22s\n",
      "3127:\tlearn: 5.8010984\ttotal: 15m 38s\tremaining: 34m 22s\n",
      "3128:\tlearn: 5.8002845\ttotal: 15m 39s\tremaining: 34m 22s\n",
      "3129:\tlearn: 5.7990720\ttotal: 15m 39s\tremaining: 34m 22s\n",
      "3130:\tlearn: 5.7984913\ttotal: 15m 39s\tremaining: 34m 22s\n",
      "3131:\tlearn: 5.7975952\ttotal: 15m 40s\tremaining: 34m 21s\n",
      "3132:\tlearn: 5.7969482\ttotal: 15m 40s\tremaining: 34m 21s\n",
      "3133:\tlearn: 5.7960664\ttotal: 15m 41s\tremaining: 34m 21s\n",
      "3134:\tlearn: 5.7953463\ttotal: 15m 41s\tremaining: 34m 21s\n",
      "3135:\tlearn: 5.7947604\ttotal: 15m 41s\tremaining: 34m 20s\n",
      "3136:\tlearn: 5.7941387\ttotal: 15m 41s\tremaining: 34m 20s\n",
      "3137:\tlearn: 5.7934896\ttotal: 15m 42s\tremaining: 34m 20s\n",
      "3138:\tlearn: 5.7929283\ttotal: 15m 42s\tremaining: 34m 20s\n",
      "3139:\tlearn: 5.7923098\ttotal: 15m 42s\tremaining: 34m 19s\n",
      "3140:\tlearn: 5.7918019\ttotal: 15m 43s\tremaining: 34m 19s\n",
      "3141:\tlearn: 5.7910178\ttotal: 15m 43s\tremaining: 34m 19s\n",
      "3142:\tlearn: 5.7904864\ttotal: 15m 43s\tremaining: 34m 19s\n",
      "3143:\tlearn: 5.7894751\ttotal: 15m 44s\tremaining: 34m 19s\n",
      "3144:\tlearn: 5.7886810\ttotal: 15m 44s\tremaining: 34m 18s\n",
      "3145:\tlearn: 5.7880926\ttotal: 15m 44s\tremaining: 34m 18s\n",
      "3146:\tlearn: 5.7876018\ttotal: 15m 45s\tremaining: 34m 18s\n",
      "3147:\tlearn: 5.7870233\ttotal: 15m 45s\tremaining: 34m 17s\n",
      "3148:\tlearn: 5.7861421\ttotal: 15m 45s\tremaining: 34m 17s\n",
      "3149:\tlearn: 5.7857013\ttotal: 15m 45s\tremaining: 34m 17s\n",
      "3150:\tlearn: 5.7851856\ttotal: 15m 46s\tremaining: 34m 16s\n",
      "3151:\tlearn: 5.7842473\ttotal: 15m 46s\tremaining: 34m 16s\n",
      "3152:\tlearn: 5.7835706\ttotal: 15m 46s\tremaining: 34m 16s\n",
      "3153:\tlearn: 5.7824346\ttotal: 15m 47s\tremaining: 34m 16s\n",
      "3154:\tlearn: 5.7816341\ttotal: 15m 47s\tremaining: 34m 16s\n",
      "3155:\tlearn: 5.7806419\ttotal: 15m 48s\tremaining: 34m 16s\n",
      "3156:\tlearn: 5.7796685\ttotal: 15m 48s\tremaining: 34m 15s\n",
      "3157:\tlearn: 5.7788537\ttotal: 15m 48s\tremaining: 34m 15s\n",
      "3158:\tlearn: 5.7781324\ttotal: 15m 49s\tremaining: 34m 15s\n",
      "3159:\tlearn: 5.7771522\ttotal: 15m 49s\tremaining: 34m 15s\n",
      "3160:\tlearn: 5.7763189\ttotal: 15m 49s\tremaining: 34m 15s\n",
      "3161:\tlearn: 5.7756332\ttotal: 15m 50s\tremaining: 34m 15s\n",
      "3162:\tlearn: 5.7744556\ttotal: 15m 50s\tremaining: 34m 15s\n",
      "3163:\tlearn: 5.7733684\ttotal: 15m 51s\tremaining: 34m 14s\n",
      "3164:\tlearn: 5.7725866\ttotal: 15m 51s\tremaining: 34m 14s\n",
      "3165:\tlearn: 5.7716078\ttotal: 15m 51s\tremaining: 34m 14s\n",
      "3166:\tlearn: 5.7702865\ttotal: 15m 52s\tremaining: 34m 14s\n",
      "3167:\tlearn: 5.7695219\ttotal: 15m 52s\tremaining: 34m 14s\n",
      "3168:\tlearn: 5.7685933\ttotal: 15m 52s\tremaining: 34m 14s\n",
      "3169:\tlearn: 5.7680214\ttotal: 15m 53s\tremaining: 34m 13s\n",
      "3170:\tlearn: 5.7671284\ttotal: 15m 53s\tremaining: 34m 13s\n",
      "3171:\tlearn: 5.7665366\ttotal: 15m 54s\tremaining: 34m 13s\n",
      "3172:\tlearn: 5.7659337\ttotal: 15m 54s\tremaining: 34m 13s\n",
      "3173:\tlearn: 5.7651237\ttotal: 15m 55s\tremaining: 34m 14s\n",
      "3174:\tlearn: 5.7638273\ttotal: 15m 55s\tremaining: 34m 13s\n",
      "3175:\tlearn: 5.7633915\ttotal: 15m 55s\tremaining: 34m 13s\n",
      "3176:\tlearn: 5.7622310\ttotal: 15m 56s\tremaining: 34m 13s\n",
      "3177:\tlearn: 5.7616069\ttotal: 15m 56s\tremaining: 34m 13s\n",
      "3178:\tlearn: 5.7611163\ttotal: 15m 56s\tremaining: 34m 12s\n",
      "3179:\tlearn: 5.7599213\ttotal: 15m 57s\tremaining: 34m 12s\n",
      "3180:\tlearn: 5.7593885\ttotal: 15m 57s\tremaining: 34m 12s\n",
      "3181:\tlearn: 5.7583004\ttotal: 15m 57s\tremaining: 34m 12s\n",
      "3182:\tlearn: 5.7571964\ttotal: 15m 58s\tremaining: 34m 12s\n",
      "3183:\tlearn: 5.7564367\ttotal: 15m 58s\tremaining: 34m 12s\n",
      "3184:\tlearn: 5.7556702\ttotal: 15m 59s\tremaining: 34m 12s\n",
      "3185:\tlearn: 5.7547537\ttotal: 15m 59s\tremaining: 34m 12s\n",
      "3186:\tlearn: 5.7541652\ttotal: 15m 59s\tremaining: 34m 11s\n",
      "3187:\tlearn: 5.7534519\ttotal: 16m\tremaining: 34m 11s\n",
      "3188:\tlearn: 5.7524651\ttotal: 16m\tremaining: 34m 11s\n",
      "3189:\tlearn: 5.7517321\ttotal: 16m 1s\tremaining: 34m 11s\n",
      "3190:\tlearn: 5.7505611\ttotal: 16m 1s\tremaining: 34m 11s\n",
      "3191:\tlearn: 5.7494823\ttotal: 16m 1s\tremaining: 34m 11s\n",
      "3192:\tlearn: 5.7487759\ttotal: 16m 2s\tremaining: 34m 10s\n",
      "3193:\tlearn: 5.7480252\ttotal: 16m 2s\tremaining: 34m 10s\n",
      "3194:\tlearn: 5.7475245\ttotal: 16m 2s\tremaining: 34m 10s\n",
      "3195:\tlearn: 5.7467167\ttotal: 16m 3s\tremaining: 34m 10s\n",
      "3196:\tlearn: 5.7461825\ttotal: 16m 3s\tremaining: 34m 9s\n",
      "3197:\tlearn: 5.7454751\ttotal: 16m 3s\tremaining: 34m 9s\n",
      "3198:\tlearn: 5.7450025\ttotal: 16m 3s\tremaining: 34m 9s\n",
      "3199:\tlearn: 5.7443530\ttotal: 16m 4s\tremaining: 34m 8s\n",
      "3200:\tlearn: 5.7436769\ttotal: 16m 4s\tremaining: 34m 8s\n",
      "3201:\tlearn: 5.7431793\ttotal: 16m 4s\tremaining: 34m 8s\n",
      "3202:\tlearn: 5.7424995\ttotal: 16m 5s\tremaining: 34m 7s\n",
      "3203:\tlearn: 5.7420016\ttotal: 16m 5s\tremaining: 34m 7s\n",
      "3204:\tlearn: 5.7411230\ttotal: 16m 5s\tremaining: 34m 7s\n",
      "3205:\tlearn: 5.7404482\ttotal: 16m 5s\tremaining: 34m 7s\n",
      "3206:\tlearn: 5.7394052\ttotal: 16m 6s\tremaining: 34m 7s\n",
      "3207:\tlearn: 5.7386978\ttotal: 16m 6s\tremaining: 34m 6s\n",
      "3208:\tlearn: 5.7378973\ttotal: 16m 7s\tremaining: 34m 6s\n",
      "3209:\tlearn: 5.7368372\ttotal: 16m 7s\tremaining: 34m 6s\n",
      "3210:\tlearn: 5.7361235\ttotal: 16m 7s\tremaining: 34m 5s\n",
      "3211:\tlearn: 5.7349297\ttotal: 16m 8s\tremaining: 34m 5s\n",
      "3212:\tlearn: 5.7341482\ttotal: 16m 8s\tremaining: 34m 5s\n",
      "3213:\tlearn: 5.7335302\ttotal: 16m 8s\tremaining: 34m 5s\n",
      "3214:\tlearn: 5.7326377\ttotal: 16m 9s\tremaining: 34m 5s\n",
      "3215:\tlearn: 5.7315952\ttotal: 16m 9s\tremaining: 34m 4s\n",
      "3216:\tlearn: 5.7307247\ttotal: 16m 9s\tremaining: 34m 4s\n",
      "3217:\tlearn: 5.7292724\ttotal: 16m 10s\tremaining: 34m 4s\n",
      "3218:\tlearn: 5.7279953\ttotal: 16m 10s\tremaining: 34m 4s\n",
      "3219:\tlearn: 5.7269423\ttotal: 16m 10s\tremaining: 34m 4s\n",
      "3220:\tlearn: 5.7264832\ttotal: 16m 11s\tremaining: 34m 4s\n",
      "3221:\tlearn: 5.7254871\ttotal: 16m 11s\tremaining: 34m 3s\n",
      "3222:\tlearn: 5.7248911\ttotal: 16m 11s\tremaining: 34m 3s\n",
      "3223:\tlearn: 5.7241087\ttotal: 16m 12s\tremaining: 34m 3s\n",
      "3224:\tlearn: 5.7232633\ttotal: 16m 12s\tremaining: 34m 3s\n",
      "3225:\tlearn: 5.7223411\ttotal: 16m 12s\tremaining: 34m 2s\n",
      "3226:\tlearn: 5.7214079\ttotal: 16m 13s\tremaining: 34m 2s\n",
      "3227:\tlearn: 5.7206734\ttotal: 16m 13s\tremaining: 34m 2s\n",
      "3228:\tlearn: 5.7197788\ttotal: 16m 13s\tremaining: 34m 2s\n",
      "3229:\tlearn: 5.7186315\ttotal: 16m 14s\tremaining: 34m 1s\n",
      "3230:\tlearn: 5.7175432\ttotal: 16m 14s\tremaining: 34m 1s\n",
      "3231:\tlearn: 5.7171105\ttotal: 16m 14s\tremaining: 34m 1s\n",
      "3232:\tlearn: 5.7157963\ttotal: 16m 15s\tremaining: 34m 1s\n",
      "3233:\tlearn: 5.7151266\ttotal: 16m 15s\tremaining: 34m\n",
      "3234:\tlearn: 5.7144954\ttotal: 16m 15s\tremaining: 34m\n",
      "3235:\tlearn: 5.7139102\ttotal: 16m 15s\tremaining: 34m\n",
      "3236:\tlearn: 5.7133030\ttotal: 16m 16s\tremaining: 33m 59s\n",
      "3237:\tlearn: 5.7122711\ttotal: 16m 16s\tremaining: 33m 59s\n",
      "3238:\tlearn: 5.7116887\ttotal: 16m 16s\tremaining: 33m 59s\n",
      "3239:\tlearn: 5.7110137\ttotal: 16m 17s\tremaining: 33m 58s\n",
      "3240:\tlearn: 5.7102553\ttotal: 16m 17s\tremaining: 33m 58s\n",
      "3241:\tlearn: 5.7090514\ttotal: 16m 17s\tremaining: 33m 58s\n",
      "3242:\tlearn: 5.7085404\ttotal: 16m 18s\tremaining: 33m 58s\n",
      "3243:\tlearn: 5.7077949\ttotal: 16m 18s\tremaining: 33m 57s\n",
      "3244:\tlearn: 5.7072610\ttotal: 16m 18s\tremaining: 33m 57s\n",
      "3245:\tlearn: 5.7065340\ttotal: 16m 19s\tremaining: 33m 57s\n",
      "3246:\tlearn: 5.7057431\ttotal: 16m 19s\tremaining: 33m 57s\n",
      "3247:\tlearn: 5.7048819\ttotal: 16m 19s\tremaining: 33m 56s\n",
      "3248:\tlearn: 5.7044762\ttotal: 16m 20s\tremaining: 33m 56s\n",
      "3249:\tlearn: 5.7035678\ttotal: 16m 20s\tremaining: 33m 56s\n",
      "3250:\tlearn: 5.7027688\ttotal: 16m 20s\tremaining: 33m 56s\n",
      "3251:\tlearn: 5.7021806\ttotal: 16m 21s\tremaining: 33m 55s\n",
      "3252:\tlearn: 5.7013558\ttotal: 16m 21s\tremaining: 33m 55s\n",
      "3253:\tlearn: 5.7008312\ttotal: 16m 21s\tremaining: 33m 55s\n",
      "3254:\tlearn: 5.6999256\ttotal: 16m 22s\tremaining: 33m 55s\n",
      "3255:\tlearn: 5.6990576\ttotal: 16m 22s\tremaining: 33m 54s\n",
      "3256:\tlearn: 5.6985468\ttotal: 16m 22s\tremaining: 33m 54s\n",
      "3257:\tlearn: 5.6976283\ttotal: 16m 23s\tremaining: 33m 54s\n",
      "3258:\tlearn: 5.6967857\ttotal: 16m 23s\tremaining: 33m 53s\n",
      "3259:\tlearn: 5.6957457\ttotal: 16m 23s\tremaining: 33m 53s\n",
      "3260:\tlearn: 5.6949755\ttotal: 16m 23s\tremaining: 33m 53s\n",
      "3261:\tlearn: 5.6940684\ttotal: 16m 24s\tremaining: 33m 53s\n",
      "3262:\tlearn: 5.6933221\ttotal: 16m 24s\tremaining: 33m 53s\n",
      "3263:\tlearn: 5.6927561\ttotal: 16m 25s\tremaining: 33m 52s\n",
      "3264:\tlearn: 5.6921902\ttotal: 16m 25s\tremaining: 33m 52s\n",
      "3265:\tlearn: 5.6913074\ttotal: 16m 25s\tremaining: 33m 52s\n",
      "3266:\tlearn: 5.6904873\ttotal: 16m 25s\tremaining: 33m 52s\n",
      "3267:\tlearn: 5.6895204\ttotal: 16m 26s\tremaining: 33m 51s\n",
      "3268:\tlearn: 5.6880827\ttotal: 16m 26s\tremaining: 33m 51s\n",
      "3269:\tlearn: 5.6877201\ttotal: 16m 26s\tremaining: 33m 51s\n",
      "3270:\tlearn: 5.6863217\ttotal: 16m 27s\tremaining: 33m 51s\n",
      "3271:\tlearn: 5.6856709\ttotal: 16m 27s\tremaining: 33m 50s\n",
      "3272:\tlearn: 5.6849958\ttotal: 16m 27s\tremaining: 33m 50s\n",
      "3273:\tlearn: 5.6845163\ttotal: 16m 28s\tremaining: 33m 50s\n",
      "3274:\tlearn: 5.6836737\ttotal: 16m 28s\tremaining: 33m 49s\n",
      "3275:\tlearn: 5.6832460\ttotal: 16m 28s\tremaining: 33m 49s\n",
      "3276:\tlearn: 5.6825716\ttotal: 16m 29s\tremaining: 33m 49s\n",
      "3277:\tlearn: 5.6814003\ttotal: 16m 29s\tremaining: 33m 48s\n",
      "3278:\tlearn: 5.6806679\ttotal: 16m 29s\tremaining: 33m 48s\n",
      "3279:\tlearn: 5.6800145\ttotal: 16m 30s\tremaining: 33m 48s\n",
      "3280:\tlearn: 5.6794603\ttotal: 16m 30s\tremaining: 33m 48s\n",
      "3281:\tlearn: 5.6789747\ttotal: 16m 30s\tremaining: 33m 47s\n",
      "3282:\tlearn: 5.6782736\ttotal: 16m 30s\tremaining: 33m 47s\n",
      "3283:\tlearn: 5.6776482\ttotal: 16m 31s\tremaining: 33m 47s\n",
      "3284:\tlearn: 5.6766813\ttotal: 16m 31s\tremaining: 33m 46s\n",
      "3285:\tlearn: 5.6757895\ttotal: 16m 31s\tremaining: 33m 46s\n",
      "3286:\tlearn: 5.6745865\ttotal: 16m 32s\tremaining: 33m 46s\n",
      "3287:\tlearn: 5.6738603\ttotal: 16m 32s\tremaining: 33m 46s\n",
      "3288:\tlearn: 5.6731153\ttotal: 16m 32s\tremaining: 33m 45s\n",
      "3289:\tlearn: 5.6721380\ttotal: 16m 33s\tremaining: 33m 45s\n",
      "3290:\tlearn: 5.6715464\ttotal: 16m 33s\tremaining: 33m 45s\n",
      "3291:\tlearn: 5.6710237\ttotal: 16m 33s\tremaining: 33m 45s\n",
      "3292:\tlearn: 5.6704288\ttotal: 16m 34s\tremaining: 33m 44s\n",
      "3293:\tlearn: 5.6696103\ttotal: 16m 34s\tremaining: 33m 44s\n",
      "3294:\tlearn: 5.6691683\ttotal: 16m 34s\tremaining: 33m 44s\n",
      "3295:\tlearn: 5.6684190\ttotal: 16m 35s\tremaining: 33m 44s\n",
      "3296:\tlearn: 5.6672891\ttotal: 16m 35s\tremaining: 33m 44s\n",
      "3297:\tlearn: 5.6665489\ttotal: 16m 35s\tremaining: 33m 43s\n",
      "3298:\tlearn: 5.6660503\ttotal: 16m 36s\tremaining: 33m 43s\n",
      "3299:\tlearn: 5.6653053\ttotal: 16m 36s\tremaining: 33m 43s\n",
      "3300:\tlearn: 5.6643905\ttotal: 16m 36s\tremaining: 33m 43s\n",
      "3301:\tlearn: 5.6631379\ttotal: 16m 37s\tremaining: 33m 43s\n",
      "3302:\tlearn: 5.6622286\ttotal: 16m 37s\tremaining: 33m 42s\n",
      "3303:\tlearn: 5.6608876\ttotal: 16m 38s\tremaining: 33m 42s\n",
      "3304:\tlearn: 5.6602610\ttotal: 16m 38s\tremaining: 33m 42s\n",
      "3305:\tlearn: 5.6593560\ttotal: 16m 38s\tremaining: 33m 42s\n",
      "3306:\tlearn: 5.6588081\ttotal: 16m 38s\tremaining: 33m 41s\n",
      "3307:\tlearn: 5.6578819\ttotal: 16m 39s\tremaining: 33m 41s\n",
      "3308:\tlearn: 5.6573039\ttotal: 16m 39s\tremaining: 33m 41s\n",
      "3309:\tlearn: 5.6566959\ttotal: 16m 40s\tremaining: 33m 41s\n",
      "3310:\tlearn: 5.6559115\ttotal: 16m 40s\tremaining: 33m 41s\n",
      "3311:\tlearn: 5.6550573\ttotal: 16m 40s\tremaining: 33m 40s\n",
      "3312:\tlearn: 5.6543297\ttotal: 16m 41s\tremaining: 33m 40s\n",
      "3313:\tlearn: 5.6535171\ttotal: 16m 41s\tremaining: 33m 40s\n",
      "3314:\tlearn: 5.6528496\ttotal: 16m 41s\tremaining: 33m 40s\n",
      "3315:\tlearn: 5.6520923\ttotal: 16m 41s\tremaining: 33m 39s\n",
      "3316:\tlearn: 5.6514346\ttotal: 16m 42s\tremaining: 33m 39s\n",
      "3317:\tlearn: 5.6508716\ttotal: 16m 42s\tremaining: 33m 39s\n",
      "3318:\tlearn: 5.6501216\ttotal: 16m 42s\tremaining: 33m 38s\n",
      "3319:\tlearn: 5.6493487\ttotal: 16m 43s\tremaining: 33m 38s\n",
      "3320:\tlearn: 5.6485637\ttotal: 16m 43s\tremaining: 33m 38s\n",
      "3321:\tlearn: 5.6476160\ttotal: 16m 43s\tremaining: 33m 38s\n",
      "3322:\tlearn: 5.6466554\ttotal: 16m 44s\tremaining: 33m 38s\n",
      "3323:\tlearn: 5.6459784\ttotal: 16m 44s\tremaining: 33m 37s\n",
      "3324:\tlearn: 5.6451219\ttotal: 16m 45s\tremaining: 33m 37s\n",
      "3325:\tlearn: 5.6438449\ttotal: 16m 45s\tremaining: 33m 37s\n",
      "3326:\tlearn: 5.6433486\ttotal: 16m 45s\tremaining: 33m 37s\n",
      "3327:\tlearn: 5.6422568\ttotal: 16m 46s\tremaining: 33m 37s\n",
      "3328:\tlearn: 5.6414314\ttotal: 16m 46s\tremaining: 33m 37s\n",
      "3329:\tlearn: 5.6404638\ttotal: 16m 46s\tremaining: 33m 36s\n",
      "3330:\tlearn: 5.6397700\ttotal: 16m 47s\tremaining: 33m 36s\n",
      "3331:\tlearn: 5.6392345\ttotal: 16m 47s\tremaining: 33m 36s\n",
      "3332:\tlearn: 5.6386466\ttotal: 16m 47s\tremaining: 33m 35s\n",
      "3333:\tlearn: 5.6378760\ttotal: 16m 48s\tremaining: 33m 35s\n",
      "3334:\tlearn: 5.6372519\ttotal: 16m 48s\tremaining: 33m 35s\n",
      "3335:\tlearn: 5.6365604\ttotal: 16m 48s\tremaining: 33m 35s\n",
      "3336:\tlearn: 5.6353568\ttotal: 16m 49s\tremaining: 33m 35s\n",
      "3337:\tlearn: 5.6345270\ttotal: 16m 49s\tremaining: 33m 34s\n",
      "3338:\tlearn: 5.6335834\ttotal: 16m 49s\tremaining: 33m 34s\n",
      "3339:\tlearn: 5.6330442\ttotal: 16m 50s\tremaining: 33m 34s\n",
      "3340:\tlearn: 5.6319725\ttotal: 16m 50s\tremaining: 33m 34s\n",
      "3341:\tlearn: 5.6314965\ttotal: 16m 50s\tremaining: 33m 33s\n",
      "3342:\tlearn: 5.6306931\ttotal: 16m 51s\tremaining: 33m 33s\n",
      "3343:\tlearn: 5.6300589\ttotal: 16m 51s\tremaining: 33m 33s\n",
      "3344:\tlearn: 5.6293710\ttotal: 16m 51s\tremaining: 33m 33s\n",
      "3345:\tlearn: 5.6285201\ttotal: 16m 52s\tremaining: 33m 32s\n",
      "3346:\tlearn: 5.6276028\ttotal: 16m 52s\tremaining: 33m 32s\n",
      "3347:\tlearn: 5.6268636\ttotal: 16m 52s\tremaining: 33m 32s\n",
      "3348:\tlearn: 5.6262590\ttotal: 16m 53s\tremaining: 33m 32s\n",
      "3349:\tlearn: 5.6257160\ttotal: 16m 53s\tremaining: 33m 31s\n",
      "3350:\tlearn: 5.6248764\ttotal: 16m 53s\tremaining: 33m 31s\n",
      "3351:\tlearn: 5.6239876\ttotal: 16m 54s\tremaining: 33m 31s\n",
      "3352:\tlearn: 5.6233331\ttotal: 16m 54s\tremaining: 33m 31s\n",
      "3353:\tlearn: 5.6228877\ttotal: 16m 54s\tremaining: 33m 30s\n",
      "3354:\tlearn: 5.6221826\ttotal: 16m 55s\tremaining: 33m 30s\n",
      "3355:\tlearn: 5.6218399\ttotal: 16m 55s\tremaining: 33m 30s\n",
      "3356:\tlearn: 5.6207418\ttotal: 16m 55s\tremaining: 33m 30s\n",
      "3357:\tlearn: 5.6195528\ttotal: 16m 56s\tremaining: 33m 30s\n",
      "3358:\tlearn: 5.6185781\ttotal: 16m 56s\tremaining: 33m 29s\n",
      "3359:\tlearn: 5.6174673\ttotal: 16m 57s\tremaining: 33m 29s\n",
      "3360:\tlearn: 5.6164293\ttotal: 16m 57s\tremaining: 33m 29s\n",
      "3361:\tlearn: 5.6158157\ttotal: 16m 57s\tremaining: 33m 29s\n",
      "3362:\tlearn: 5.6146775\ttotal: 16m 58s\tremaining: 33m 29s\n",
      "3363:\tlearn: 5.6140346\ttotal: 16m 58s\tremaining: 33m 28s\n",
      "3364:\tlearn: 5.6132709\ttotal: 16m 58s\tremaining: 33m 28s\n",
      "3365:\tlearn: 5.6125793\ttotal: 16m 59s\tremaining: 33m 28s\n",
      "3366:\tlearn: 5.6119173\ttotal: 16m 59s\tremaining: 33m 28s\n",
      "3367:\tlearn: 5.6111570\ttotal: 16m 59s\tremaining: 33m 27s\n",
      "3368:\tlearn: 5.6104251\ttotal: 16m 59s\tremaining: 33m 27s\n",
      "3369:\tlearn: 5.6096469\ttotal: 17m\tremaining: 33m 27s\n",
      "3370:\tlearn: 5.6087642\ttotal: 17m\tremaining: 33m 27s\n",
      "3371:\tlearn: 5.6079996\ttotal: 17m 1s\tremaining: 33m 26s\n",
      "3372:\tlearn: 5.6069031\ttotal: 17m 1s\tremaining: 33m 26s\n",
      "3373:\tlearn: 5.6061533\ttotal: 17m 1s\tremaining: 33m 26s\n",
      "3374:\tlearn: 5.6051847\ttotal: 17m 2s\tremaining: 33m 26s\n",
      "3375:\tlearn: 5.6044792\ttotal: 17m 2s\tremaining: 33m 25s\n",
      "3376:\tlearn: 5.6038539\ttotal: 17m 2s\tremaining: 33m 25s\n",
      "3377:\tlearn: 5.6028872\ttotal: 17m 2s\tremaining: 33m 25s\n",
      "3378:\tlearn: 5.6017947\ttotal: 17m 3s\tremaining: 33m 25s\n",
      "3379:\tlearn: 5.6010805\ttotal: 17m 3s\tremaining: 33m 25s\n",
      "3380:\tlearn: 5.6003643\ttotal: 17m 4s\tremaining: 33m 24s\n",
      "3381:\tlearn: 5.5997060\ttotal: 17m 4s\tremaining: 33m 24s\n",
      "3382:\tlearn: 5.5987245\ttotal: 17m 4s\tremaining: 33m 24s\n",
      "3383:\tlearn: 5.5981389\ttotal: 17m 5s\tremaining: 33m 24s\n",
      "3384:\tlearn: 5.5975192\ttotal: 17m 5s\tremaining: 33m 23s\n",
      "3385:\tlearn: 5.5969671\ttotal: 17m 5s\tremaining: 33m 23s\n",
      "3386:\tlearn: 5.5958818\ttotal: 17m 6s\tremaining: 33m 23s\n",
      "3387:\tlearn: 5.5951645\ttotal: 17m 6s\tremaining: 33m 23s\n",
      "3388:\tlearn: 5.5943785\ttotal: 17m 6s\tremaining: 33m 22s\n",
      "3389:\tlearn: 5.5938767\ttotal: 17m 7s\tremaining: 33m 22s\n",
      "3390:\tlearn: 5.5933368\ttotal: 17m 7s\tremaining: 33m 22s\n",
      "3391:\tlearn: 5.5928871\ttotal: 17m 7s\tremaining: 33m 22s\n",
      "3392:\tlearn: 5.5922089\ttotal: 17m 7s\tremaining: 33m 21s\n",
      "3393:\tlearn: 5.5917909\ttotal: 17m 8s\tremaining: 33m 21s\n",
      "3394:\tlearn: 5.5912587\ttotal: 17m 8s\tremaining: 33m 21s\n",
      "3395:\tlearn: 5.5900791\ttotal: 17m 8s\tremaining: 33m 20s\n",
      "3396:\tlearn: 5.5888112\ttotal: 17m 9s\tremaining: 33m 20s\n",
      "3397:\tlearn: 5.5879004\ttotal: 17m 9s\tremaining: 33m 20s\n",
      "3398:\tlearn: 5.5872382\ttotal: 17m 9s\tremaining: 33m 20s\n",
      "3399:\tlearn: 5.5863909\ttotal: 17m 10s\tremaining: 33m 19s\n",
      "3400:\tlearn: 5.5856492\ttotal: 17m 10s\tremaining: 33m 19s\n",
      "3401:\tlearn: 5.5851771\ttotal: 17m 10s\tremaining: 33m 19s\n",
      "3402:\tlearn: 5.5848682\ttotal: 17m 11s\tremaining: 33m 18s\n",
      "3403:\tlearn: 5.5841482\ttotal: 17m 11s\tremaining: 33m 18s\n",
      "3404:\tlearn: 5.5833454\ttotal: 17m 11s\tremaining: 33m 18s\n",
      "3405:\tlearn: 5.5822517\ttotal: 17m 12s\tremaining: 33m 18s\n",
      "3406:\tlearn: 5.5815610\ttotal: 17m 12s\tremaining: 33m 18s\n",
      "3407:\tlearn: 5.5808739\ttotal: 17m 12s\tremaining: 33m 17s\n",
      "3408:\tlearn: 5.5799058\ttotal: 17m 13s\tremaining: 33m 17s\n",
      "3409:\tlearn: 5.5795160\ttotal: 17m 13s\tremaining: 33m 17s\n",
      "3410:\tlearn: 5.5786573\ttotal: 17m 13s\tremaining: 33m 17s\n",
      "3411:\tlearn: 5.5779311\ttotal: 17m 14s\tremaining: 33m 16s\n",
      "3412:\tlearn: 5.5774855\ttotal: 17m 14s\tremaining: 33m 16s\n",
      "3413:\tlearn: 5.5767560\ttotal: 17m 14s\tremaining: 33m 16s\n",
      "3414:\tlearn: 5.5762351\ttotal: 17m 15s\tremaining: 33m 16s\n",
      "3415:\tlearn: 5.5751761\ttotal: 17m 15s\tremaining: 33m 16s\n",
      "3416:\tlearn: 5.5742561\ttotal: 17m 15s\tremaining: 33m 15s\n",
      "3417:\tlearn: 5.5736935\ttotal: 17m 16s\tremaining: 33m 15s\n",
      "3418:\tlearn: 5.5729950\ttotal: 17m 16s\tremaining: 33m 15s\n",
      "3419:\tlearn: 5.5724209\ttotal: 17m 17s\tremaining: 33m 15s\n",
      "3420:\tlearn: 5.5718155\ttotal: 17m 17s\tremaining: 33m 14s\n",
      "3421:\tlearn: 5.5712901\ttotal: 17m 17s\tremaining: 33m 14s\n",
      "3422:\tlearn: 5.5707641\ttotal: 17m 18s\tremaining: 33m 14s\n",
      "3423:\tlearn: 5.5702271\ttotal: 17m 18s\tremaining: 33m 14s\n",
      "3424:\tlearn: 5.5693781\ttotal: 17m 18s\tremaining: 33m 14s\n",
      "3425:\tlearn: 5.5686462\ttotal: 17m 19s\tremaining: 33m 14s\n",
      "3426:\tlearn: 5.5675894\ttotal: 17m 19s\tremaining: 33m 13s\n",
      "3427:\tlearn: 5.5666356\ttotal: 17m 19s\tremaining: 33m 13s\n",
      "3428:\tlearn: 5.5652765\ttotal: 17m 20s\tremaining: 33m 13s\n",
      "3429:\tlearn: 5.5643501\ttotal: 17m 20s\tremaining: 33m 13s\n",
      "3430:\tlearn: 5.5635769\ttotal: 17m 21s\tremaining: 33m 13s\n",
      "3431:\tlearn: 5.5624798\ttotal: 17m 21s\tremaining: 33m 13s\n",
      "3432:\tlearn: 5.5617787\ttotal: 17m 21s\tremaining: 33m 12s\n",
      "3433:\tlearn: 5.5607634\ttotal: 17m 22s\tremaining: 33m 12s\n",
      "3434:\tlearn: 5.5602473\ttotal: 17m 22s\tremaining: 33m 12s\n",
      "3435:\tlearn: 5.5596511\ttotal: 17m 22s\tremaining: 33m 12s\n",
      "3436:\tlearn: 5.5591200\ttotal: 17m 23s\tremaining: 33m 12s\n",
      "3437:\tlearn: 5.5582799\ttotal: 17m 23s\tremaining: 33m 12s\n",
      "3438:\tlearn: 5.5574216\ttotal: 17m 24s\tremaining: 33m 12s\n",
      "3439:\tlearn: 5.5566661\ttotal: 17m 24s\tremaining: 33m 11s\n",
      "3440:\tlearn: 5.5561522\ttotal: 17m 24s\tremaining: 33m 11s\n",
      "3441:\tlearn: 5.5552274\ttotal: 17m 25s\tremaining: 33m 11s\n",
      "3442:\tlearn: 5.5547329\ttotal: 17m 25s\tremaining: 33m 11s\n",
      "3443:\tlearn: 5.5538897\ttotal: 17m 25s\tremaining: 33m 10s\n",
      "3444:\tlearn: 5.5532652\ttotal: 17m 26s\tremaining: 33m 10s\n",
      "3445:\tlearn: 5.5525911\ttotal: 17m 26s\tremaining: 33m 10s\n",
      "3446:\tlearn: 5.5518258\ttotal: 17m 26s\tremaining: 33m 9s\n",
      "3447:\tlearn: 5.5513999\ttotal: 17m 26s\tremaining: 33m 9s\n",
      "3448:\tlearn: 5.5510197\ttotal: 17m 27s\tremaining: 33m 8s\n",
      "3449:\tlearn: 5.5500657\ttotal: 17m 27s\tremaining: 33m 8s\n",
      "3450:\tlearn: 5.5490537\ttotal: 17m 27s\tremaining: 33m 8s\n",
      "3451:\tlearn: 5.5484840\ttotal: 17m 27s\tremaining: 33m 7s\n",
      "3452:\tlearn: 5.5473448\ttotal: 17m 28s\tremaining: 33m 7s\n",
      "3453:\tlearn: 5.5468785\ttotal: 17m 28s\tremaining: 33m 7s\n",
      "3454:\tlearn: 5.5461342\ttotal: 17m 28s\tremaining: 33m 7s\n",
      "3455:\tlearn: 5.5453906\ttotal: 17m 29s\tremaining: 33m 6s\n",
      "3456:\tlearn: 5.5449996\ttotal: 17m 29s\tremaining: 33m 6s\n",
      "3457:\tlearn: 5.5443035\ttotal: 17m 29s\tremaining: 33m 6s\n",
      "3458:\tlearn: 5.5439043\ttotal: 17m 30s\tremaining: 33m 5s\n",
      "3459:\tlearn: 5.5435220\ttotal: 17m 30s\tremaining: 33m 5s\n",
      "3460:\tlearn: 5.5429922\ttotal: 17m 30s\tremaining: 33m 5s\n",
      "3461:\tlearn: 5.5424929\ttotal: 17m 30s\tremaining: 33m 4s\n",
      "3462:\tlearn: 5.5417992\ttotal: 17m 31s\tremaining: 33m 4s\n",
      "3463:\tlearn: 5.5412805\ttotal: 17m 31s\tremaining: 33m 4s\n",
      "3464:\tlearn: 5.5407786\ttotal: 17m 31s\tremaining: 33m 3s\n",
      "3465:\tlearn: 5.5399704\ttotal: 17m 32s\tremaining: 33m 3s\n",
      "3466:\tlearn: 5.5393547\ttotal: 17m 32s\tremaining: 33m 3s\n",
      "3467:\tlearn: 5.5382357\ttotal: 17m 32s\tremaining: 33m 2s\n",
      "3468:\tlearn: 5.5374293\ttotal: 17m 33s\tremaining: 33m 2s\n",
      "3469:\tlearn: 5.5371937\ttotal: 17m 33s\tremaining: 33m 2s\n",
      "3470:\tlearn: 5.5365037\ttotal: 17m 33s\tremaining: 33m 2s\n",
      "3471:\tlearn: 5.5360469\ttotal: 17m 33s\tremaining: 33m 1s\n",
      "3472:\tlearn: 5.5352471\ttotal: 17m 34s\tremaining: 33m 1s\n",
      "3473:\tlearn: 5.5345895\ttotal: 17m 34s\tremaining: 33m 1s\n",
      "3474:\tlearn: 5.5340613\ttotal: 17m 35s\tremaining: 33m 1s\n",
      "3475:\tlearn: 5.5333437\ttotal: 17m 35s\tremaining: 33m\n",
      "3476:\tlearn: 5.5324794\ttotal: 17m 35s\tremaining: 33m\n",
      "3477:\tlearn: 5.5319560\ttotal: 17m 36s\tremaining: 33m\n",
      "3478:\tlearn: 5.5310642\ttotal: 17m 36s\tremaining: 33m\n",
      "3479:\tlearn: 5.5301678\ttotal: 17m 36s\tremaining: 33m\n",
      "3480:\tlearn: 5.5293305\ttotal: 17m 37s\tremaining: 32m 59s\n",
      "3481:\tlearn: 5.5288411\ttotal: 17m 37s\tremaining: 32m 59s\n",
      "3482:\tlearn: 5.5277923\ttotal: 17m 37s\tremaining: 32m 59s\n",
      "3483:\tlearn: 5.5272855\ttotal: 17m 38s\tremaining: 32m 59s\n",
      "3484:\tlearn: 5.5266603\ttotal: 17m 38s\tremaining: 32m 59s\n",
      "3485:\tlearn: 5.5259619\ttotal: 17m 38s\tremaining: 32m 58s\n",
      "3486:\tlearn: 5.5250214\ttotal: 17m 39s\tremaining: 32m 58s\n",
      "3487:\tlearn: 5.5242100\ttotal: 17m 39s\tremaining: 32m 58s\n",
      "3488:\tlearn: 5.5233221\ttotal: 17m 39s\tremaining: 32m 57s\n",
      "3489:\tlearn: 5.5227117\ttotal: 17m 40s\tremaining: 32m 57s\n",
      "3490:\tlearn: 5.5217959\ttotal: 17m 40s\tremaining: 32m 57s\n",
      "3491:\tlearn: 5.5211780\ttotal: 17m 40s\tremaining: 32m 56s\n",
      "3492:\tlearn: 5.5201834\ttotal: 17m 41s\tremaining: 32m 56s\n",
      "3493:\tlearn: 5.5192653\ttotal: 17m 41s\tremaining: 32m 56s\n",
      "3494:\tlearn: 5.5182311\ttotal: 17m 41s\tremaining: 32m 56s\n",
      "3495:\tlearn: 5.5175568\ttotal: 17m 42s\tremaining: 32m 55s\n",
      "3496:\tlearn: 5.5167179\ttotal: 17m 42s\tremaining: 32m 55s\n",
      "3497:\tlearn: 5.5156967\ttotal: 17m 42s\tremaining: 32m 55s\n",
      "3498:\tlearn: 5.5151095\ttotal: 17m 43s\tremaining: 32m 55s\n",
      "3499:\tlearn: 5.5143407\ttotal: 17m 43s\tremaining: 32m 55s\n",
      "3500:\tlearn: 5.5138284\ttotal: 17m 43s\tremaining: 32m 54s\n",
      "3501:\tlearn: 5.5131265\ttotal: 17m 44s\tremaining: 32m 54s\n",
      "3502:\tlearn: 5.5121928\ttotal: 17m 44s\tremaining: 32m 54s\n",
      "3503:\tlearn: 5.5116580\ttotal: 17m 44s\tremaining: 32m 53s\n",
      "3504:\tlearn: 5.5113399\ttotal: 17m 45s\tremaining: 32m 53s\n",
      "3505:\tlearn: 5.5107264\ttotal: 17m 45s\tremaining: 32m 53s\n",
      "3506:\tlearn: 5.5101721\ttotal: 17m 45s\tremaining: 32m 53s\n",
      "3507:\tlearn: 5.5088736\ttotal: 17m 46s\tremaining: 32m 52s\n",
      "3508:\tlearn: 5.5080398\ttotal: 17m 46s\tremaining: 32m 52s\n",
      "3509:\tlearn: 5.5074465\ttotal: 17m 46s\tremaining: 32m 52s\n",
      "3510:\tlearn: 5.5065312\ttotal: 17m 47s\tremaining: 32m 52s\n",
      "3511:\tlearn: 5.5053847\ttotal: 17m 47s\tremaining: 32m 51s\n",
      "3512:\tlearn: 5.5039284\ttotal: 17m 47s\tremaining: 32m 51s\n",
      "3513:\tlearn: 5.5033658\ttotal: 17m 48s\tremaining: 32m 51s\n",
      "3514:\tlearn: 5.5027846\ttotal: 17m 48s\tremaining: 32m 51s\n",
      "3515:\tlearn: 5.5022813\ttotal: 17m 48s\tremaining: 32m 50s\n",
      "3516:\tlearn: 5.5012948\ttotal: 17m 49s\tremaining: 32m 50s\n",
      "3517:\tlearn: 5.5002632\ttotal: 17m 49s\tremaining: 32m 50s\n",
      "3518:\tlearn: 5.4995058\ttotal: 17m 49s\tremaining: 32m 50s\n",
      "3519:\tlearn: 5.4989146\ttotal: 17m 50s\tremaining: 32m 49s\n",
      "3520:\tlearn: 5.4981214\ttotal: 17m 50s\tremaining: 32m 49s\n",
      "3521:\tlearn: 5.4971571\ttotal: 17m 50s\tremaining: 32m 49s\n",
      "3522:\tlearn: 5.4964166\ttotal: 17m 50s\tremaining: 32m 48s\n",
      "3523:\tlearn: 5.4955670\ttotal: 17m 51s\tremaining: 32m 48s\n",
      "3524:\tlearn: 5.4947046\ttotal: 17m 51s\tremaining: 32m 48s\n",
      "3525:\tlearn: 5.4939754\ttotal: 17m 52s\tremaining: 32m 48s\n",
      "3526:\tlearn: 5.4930882\ttotal: 17m 52s\tremaining: 32m 48s\n",
      "3527:\tlearn: 5.4928346\ttotal: 17m 52s\tremaining: 32m 48s\n",
      "3528:\tlearn: 5.4919052\ttotal: 17m 53s\tremaining: 32m 47s\n",
      "3529:\tlearn: 5.4912694\ttotal: 17m 53s\tremaining: 32m 47s\n",
      "3530:\tlearn: 5.4903327\ttotal: 17m 53s\tremaining: 32m 47s\n",
      "3531:\tlearn: 5.4896136\ttotal: 17m 54s\tremaining: 32m 46s\n",
      "3532:\tlearn: 5.4890859\ttotal: 17m 54s\tremaining: 32m 46s\n",
      "3533:\tlearn: 5.4880828\ttotal: 17m 54s\tremaining: 32m 46s\n",
      "3534:\tlearn: 5.4875529\ttotal: 17m 54s\tremaining: 32m 45s\n",
      "3535:\tlearn: 5.4868725\ttotal: 17m 55s\tremaining: 32m 45s\n",
      "3536:\tlearn: 5.4857152\ttotal: 17m 55s\tremaining: 32m 45s\n",
      "3537:\tlearn: 5.4853132\ttotal: 17m 55s\tremaining: 32m 45s\n",
      "3538:\tlearn: 5.4845416\ttotal: 17m 56s\tremaining: 32m 44s\n",
      "3539:\tlearn: 5.4836008\ttotal: 17m 56s\tremaining: 32m 44s\n",
      "3540:\tlearn: 5.4829653\ttotal: 17m 56s\tremaining: 32m 44s\n",
      "3541:\tlearn: 5.4821061\ttotal: 17m 57s\tremaining: 32m 44s\n",
      "3542:\tlearn: 5.4810535\ttotal: 17m 57s\tremaining: 32m 43s\n",
      "3543:\tlearn: 5.4804621\ttotal: 17m 57s\tremaining: 32m 43s\n",
      "3544:\tlearn: 5.4800186\ttotal: 17m 58s\tremaining: 32m 43s\n",
      "3545:\tlearn: 5.4792285\ttotal: 17m 58s\tremaining: 32m 42s\n",
      "3546:\tlearn: 5.4781479\ttotal: 17m 58s\tremaining: 32m 42s\n",
      "3547:\tlearn: 5.4777631\ttotal: 17m 59s\tremaining: 32m 42s\n",
      "3548:\tlearn: 5.4766316\ttotal: 17m 59s\tremaining: 32m 42s\n",
      "3549:\tlearn: 5.4759938\ttotal: 17m 59s\tremaining: 32m 41s\n",
      "3550:\tlearn: 5.4750055\ttotal: 18m\tremaining: 32m 41s\n",
      "3551:\tlearn: 5.4743199\ttotal: 18m\tremaining: 32m 41s\n",
      "3552:\tlearn: 5.4737129\ttotal: 18m\tremaining: 32m 41s\n",
      "3553:\tlearn: 5.4731141\ttotal: 18m 1s\tremaining: 32m 40s\n",
      "3554:\tlearn: 5.4726039\ttotal: 18m 1s\tremaining: 32m 40s\n",
      "3555:\tlearn: 5.4716957\ttotal: 18m 1s\tremaining: 32m 40s\n",
      "3556:\tlearn: 5.4712071\ttotal: 18m 1s\tremaining: 32m 39s\n",
      "3557:\tlearn: 5.4702889\ttotal: 18m 2s\tremaining: 32m 39s\n",
      "3558:\tlearn: 5.4697421\ttotal: 18m 2s\tremaining: 32m 39s\n",
      "3559:\tlearn: 5.4689596\ttotal: 18m 2s\tremaining: 32m 39s\n",
      "3560:\tlearn: 5.4682869\ttotal: 18m 3s\tremaining: 32m 38s\n",
      "3561:\tlearn: 5.4671217\ttotal: 18m 3s\tremaining: 32m 38s\n",
      "3562:\tlearn: 5.4664655\ttotal: 18m 3s\tremaining: 32m 38s\n",
      "3563:\tlearn: 5.4661283\ttotal: 18m 4s\tremaining: 32m 37s\n",
      "3564:\tlearn: 5.4653542\ttotal: 18m 4s\tremaining: 32m 37s\n",
      "3565:\tlearn: 5.4645967\ttotal: 18m 4s\tremaining: 32m 37s\n",
      "3566:\tlearn: 5.4639147\ttotal: 18m 5s\tremaining: 32m 36s\n",
      "3567:\tlearn: 5.4634139\ttotal: 18m 5s\tremaining: 32m 36s\n",
      "3568:\tlearn: 5.4624991\ttotal: 18m 5s\tremaining: 32m 36s\n",
      "3569:\tlearn: 5.4619854\ttotal: 18m 5s\tremaining: 32m 35s\n",
      "3570:\tlearn: 5.4613730\ttotal: 18m 6s\tremaining: 32m 35s\n",
      "3571:\tlearn: 5.4609400\ttotal: 18m 6s\tremaining: 32m 35s\n",
      "3572:\tlearn: 5.4602783\ttotal: 18m 6s\tremaining: 32m 34s\n",
      "3573:\tlearn: 5.4594296\ttotal: 18m 7s\tremaining: 32m 34s\n",
      "3574:\tlearn: 5.4586700\ttotal: 18m 7s\tremaining: 32m 34s\n",
      "3575:\tlearn: 5.4581519\ttotal: 18m 7s\tremaining: 32m 33s\n",
      "3576:\tlearn: 5.4573503\ttotal: 18m 7s\tremaining: 32m 33s\n",
      "3577:\tlearn: 5.4566205\ttotal: 18m 8s\tremaining: 32m 33s\n",
      "3578:\tlearn: 5.4559365\ttotal: 18m 8s\tremaining: 32m 33s\n",
      "3579:\tlearn: 5.4550312\ttotal: 18m 8s\tremaining: 32m 32s\n",
      "3580:\tlearn: 5.4542815\ttotal: 18m 9s\tremaining: 32m 32s\n",
      "3581:\tlearn: 5.4538589\ttotal: 18m 9s\tremaining: 32m 32s\n",
      "3582:\tlearn: 5.4529410\ttotal: 18m 9s\tremaining: 32m 31s\n",
      "3583:\tlearn: 5.4525493\ttotal: 18m 10s\tremaining: 32m 31s\n",
      "3584:\tlearn: 5.4516438\ttotal: 18m 10s\tremaining: 32m 31s\n",
      "3585:\tlearn: 5.4510353\ttotal: 18m 10s\tremaining: 32m 30s\n",
      "3586:\tlearn: 5.4504398\ttotal: 18m 10s\tremaining: 32m 30s\n",
      "3587:\tlearn: 5.4499615\ttotal: 18m 11s\tremaining: 32m 30s\n",
      "3588:\tlearn: 5.4492358\ttotal: 18m 11s\tremaining: 32m 29s\n",
      "3589:\tlearn: 5.4486564\ttotal: 18m 11s\tremaining: 32m 29s\n",
      "3590:\tlearn: 5.4479586\ttotal: 18m 12s\tremaining: 32m 28s\n",
      "3591:\tlearn: 5.4468283\ttotal: 18m 12s\tremaining: 32m 28s\n",
      "3592:\tlearn: 5.4462710\ttotal: 18m 12s\tremaining: 32m 28s\n",
      "3593:\tlearn: 5.4454160\ttotal: 18m 12s\tremaining: 32m 28s\n",
      "3594:\tlearn: 5.4450930\ttotal: 18m 13s\tremaining: 32m 27s\n",
      "3595:\tlearn: 5.4445216\ttotal: 18m 13s\tremaining: 32m 27s\n",
      "3596:\tlearn: 5.4439564\ttotal: 18m 13s\tremaining: 32m 26s\n",
      "3597:\tlearn: 5.4429983\ttotal: 18m 13s\tremaining: 32m 26s\n",
      "3598:\tlearn: 5.4424937\ttotal: 18m 14s\tremaining: 32m 26s\n",
      "3599:\tlearn: 5.4420819\ttotal: 18m 14s\tremaining: 32m 25s\n",
      "3600:\tlearn: 5.4411291\ttotal: 18m 14s\tremaining: 32m 25s\n",
      "3601:\tlearn: 5.4406337\ttotal: 18m 15s\tremaining: 32m 25s\n",
      "3602:\tlearn: 5.4401996\ttotal: 18m 15s\tremaining: 32m 24s\n",
      "3603:\tlearn: 5.4393674\ttotal: 18m 15s\tremaining: 32m 24s\n",
      "3604:\tlearn: 5.4386359\ttotal: 18m 15s\tremaining: 32m 24s\n",
      "3605:\tlearn: 5.4379986\ttotal: 18m 16s\tremaining: 32m 23s\n",
      "3606:\tlearn: 5.4371309\ttotal: 18m 16s\tremaining: 32m 23s\n",
      "3607:\tlearn: 5.4366153\ttotal: 18m 16s\tremaining: 32m 23s\n",
      "3608:\tlearn: 5.4360535\ttotal: 18m 17s\tremaining: 32m 23s\n",
      "3609:\tlearn: 5.4351316\ttotal: 18m 17s\tremaining: 32m 22s\n",
      "3610:\tlearn: 5.4344385\ttotal: 18m 17s\tremaining: 32m 22s\n",
      "3611:\tlearn: 5.4340143\ttotal: 18m 18s\tremaining: 32m 21s\n",
      "3612:\tlearn: 5.4333278\ttotal: 18m 18s\tremaining: 32m 21s\n",
      "3613:\tlearn: 5.4324877\ttotal: 18m 18s\tremaining: 32m 21s\n",
      "3614:\tlearn: 5.4316402\ttotal: 18m 18s\tremaining: 32m 21s\n",
      "3615:\tlearn: 5.4309278\ttotal: 18m 19s\tremaining: 32m 20s\n",
      "3616:\tlearn: 5.4303989\ttotal: 18m 19s\tremaining: 32m 20s\n",
      "3617:\tlearn: 5.4296923\ttotal: 18m 19s\tremaining: 32m 19s\n",
      "3618:\tlearn: 5.4291446\ttotal: 18m 20s\tremaining: 32m 19s\n",
      "3619:\tlearn: 5.4288036\ttotal: 18m 20s\tremaining: 32m 19s\n",
      "3620:\tlearn: 5.4284164\ttotal: 18m 20s\tremaining: 32m 18s\n",
      "3621:\tlearn: 5.4277839\ttotal: 18m 20s\tremaining: 32m 18s\n",
      "3622:\tlearn: 5.4267973\ttotal: 18m 21s\tremaining: 32m 18s\n",
      "3623:\tlearn: 5.4259425\ttotal: 18m 21s\tremaining: 32m 17s\n",
      "3624:\tlearn: 5.4254818\ttotal: 18m 21s\tremaining: 32m 17s\n",
      "3625:\tlearn: 5.4246603\ttotal: 18m 21s\tremaining: 32m 17s\n",
      "3626:\tlearn: 5.4241266\ttotal: 18m 22s\tremaining: 32m 16s\n",
      "3627:\tlearn: 5.4233044\ttotal: 18m 22s\tremaining: 32m 16s\n",
      "3628:\tlearn: 5.4226622\ttotal: 18m 22s\tremaining: 32m 15s\n",
      "3629:\tlearn: 5.4220982\ttotal: 18m 22s\tremaining: 32m 15s\n",
      "3630:\tlearn: 5.4216803\ttotal: 18m 23s\tremaining: 32m 15s\n",
      "3631:\tlearn: 5.4212240\ttotal: 18m 23s\tremaining: 32m 14s\n",
      "3632:\tlearn: 5.4203114\ttotal: 18m 23s\tremaining: 32m 14s\n",
      "3633:\tlearn: 5.4191488\ttotal: 18m 24s\tremaining: 32m 14s\n",
      "3634:\tlearn: 5.4187680\ttotal: 18m 24s\tremaining: 32m 13s\n",
      "3635:\tlearn: 5.4182060\ttotal: 18m 24s\tremaining: 32m 13s\n",
      "3636:\tlearn: 5.4174252\ttotal: 18m 24s\tremaining: 32m 13s\n",
      "3637:\tlearn: 5.4167590\ttotal: 18m 25s\tremaining: 32m 12s\n",
      "3638:\tlearn: 5.4163334\ttotal: 18m 25s\tremaining: 32m 12s\n",
      "3639:\tlearn: 5.4157348\ttotal: 18m 25s\tremaining: 32m 11s\n",
      "3640:\tlearn: 5.4146942\ttotal: 18m 26s\tremaining: 32m 11s\n",
      "3641:\tlearn: 5.4140058\ttotal: 18m 26s\tremaining: 32m 11s\n",
      "3642:\tlearn: 5.4133061\ttotal: 18m 26s\tremaining: 32m 11s\n",
      "3643:\tlearn: 5.4129665\ttotal: 18m 26s\tremaining: 32m 10s\n",
      "3644:\tlearn: 5.4118845\ttotal: 18m 27s\tremaining: 32m 10s\n",
      "3645:\tlearn: 5.4111110\ttotal: 18m 27s\tremaining: 32m 10s\n",
      "3646:\tlearn: 5.4100603\ttotal: 18m 27s\tremaining: 32m 9s\n",
      "3647:\tlearn: 5.4097669\ttotal: 18m 28s\tremaining: 32m 9s\n",
      "3648:\tlearn: 5.4095194\ttotal: 18m 28s\tremaining: 32m 9s\n",
      "3649:\tlearn: 5.4088171\ttotal: 18m 28s\tremaining: 32m 8s\n",
      "3650:\tlearn: 5.4082103\ttotal: 18m 28s\tremaining: 32m 8s\n",
      "3651:\tlearn: 5.4072430\ttotal: 18m 29s\tremaining: 32m 8s\n",
      "3652:\tlearn: 5.4065437\ttotal: 18m 29s\tremaining: 32m 7s\n",
      "3653:\tlearn: 5.4060707\ttotal: 18m 29s\tremaining: 32m 7s\n",
      "3654:\tlearn: 5.4056519\ttotal: 18m 30s\tremaining: 32m 7s\n",
      "3655:\tlearn: 5.4052284\ttotal: 18m 30s\tremaining: 32m 6s\n",
      "3656:\tlearn: 5.4047308\ttotal: 18m 30s\tremaining: 32m 6s\n",
      "3657:\tlearn: 5.4041144\ttotal: 18m 30s\tremaining: 32m 5s\n",
      "3658:\tlearn: 5.4035535\ttotal: 18m 31s\tremaining: 32m 5s\n",
      "3659:\tlearn: 5.4031571\ttotal: 18m 31s\tremaining: 32m 4s\n",
      "3660:\tlearn: 5.4024994\ttotal: 18m 31s\tremaining: 32m 4s\n",
      "3661:\tlearn: 5.4014541\ttotal: 18m 31s\tremaining: 32m 4s\n",
      "3662:\tlearn: 5.4002991\ttotal: 18m 32s\tremaining: 32m 4s\n",
      "3663:\tlearn: 5.3994269\ttotal: 18m 32s\tremaining: 32m 3s\n",
      "3664:\tlearn: 5.3984630\ttotal: 18m 32s\tremaining: 32m 3s\n",
      "3665:\tlearn: 5.3979888\ttotal: 18m 33s\tremaining: 32m 3s\n",
      "3666:\tlearn: 5.3971997\ttotal: 18m 33s\tremaining: 32m 2s\n",
      "3667:\tlearn: 5.3964002\ttotal: 18m 33s\tremaining: 32m 2s\n",
      "3668:\tlearn: 5.3956405\ttotal: 18m 34s\tremaining: 32m 2s\n",
      "3669:\tlearn: 5.3947174\ttotal: 18m 34s\tremaining: 32m 2s\n",
      "3670:\tlearn: 5.3936852\ttotal: 18m 34s\tremaining: 32m 1s\n",
      "3671:\tlearn: 5.3929360\ttotal: 18m 35s\tremaining: 32m 1s\n",
      "3672:\tlearn: 5.3924320\ttotal: 18m 35s\tremaining: 32m 1s\n",
      "3673:\tlearn: 5.3917166\ttotal: 18m 35s\tremaining: 32m 1s\n",
      "3674:\tlearn: 5.3911082\ttotal: 18m 36s\tremaining: 32m\n",
      "3675:\tlearn: 5.3902954\ttotal: 18m 36s\tremaining: 32m\n",
      "3676:\tlearn: 5.3898563\ttotal: 18m 36s\tremaining: 32m\n",
      "3677:\tlearn: 5.3894080\ttotal: 18m 36s\tremaining: 31m 59s\n",
      "3678:\tlearn: 5.3887493\ttotal: 18m 37s\tremaining: 31m 59s\n",
      "3679:\tlearn: 5.3881984\ttotal: 18m 37s\tremaining: 31m 59s\n",
      "3680:\tlearn: 5.3873085\ttotal: 18m 37s\tremaining: 31m 59s\n",
      "3681:\tlearn: 5.3868641\ttotal: 18m 38s\tremaining: 31m 58s\n",
      "3682:\tlearn: 5.3862242\ttotal: 18m 38s\tremaining: 31m 58s\n",
      "3683:\tlearn: 5.3858016\ttotal: 18m 38s\tremaining: 31m 58s\n",
      "3684:\tlearn: 5.3852782\ttotal: 18m 39s\tremaining: 31m 57s\n",
      "3685:\tlearn: 5.3842139\ttotal: 18m 39s\tremaining: 31m 57s\n",
      "3686:\tlearn: 5.3838595\ttotal: 18m 39s\tremaining: 31m 57s\n",
      "3687:\tlearn: 5.3831899\ttotal: 18m 40s\tremaining: 31m 57s\n",
      "3688:\tlearn: 5.3821191\ttotal: 18m 40s\tremaining: 31m 56s\n",
      "3689:\tlearn: 5.3814108\ttotal: 18m 40s\tremaining: 31m 56s\n",
      "3690:\tlearn: 5.3810509\ttotal: 18m 41s\tremaining: 31m 56s\n",
      "3691:\tlearn: 5.3804500\ttotal: 18m 41s\tremaining: 31m 56s\n",
      "3692:\tlearn: 5.3797858\ttotal: 18m 41s\tremaining: 31m 55s\n",
      "3693:\tlearn: 5.3794329\ttotal: 18m 42s\tremaining: 31m 55s\n",
      "3694:\tlearn: 5.3785224\ttotal: 18m 42s\tremaining: 31m 55s\n",
      "3695:\tlearn: 5.3780908\ttotal: 18m 42s\tremaining: 31m 54s\n",
      "3696:\tlearn: 5.3774039\ttotal: 18m 43s\tremaining: 31m 54s\n",
      "3697:\tlearn: 5.3771696\ttotal: 18m 43s\tremaining: 31m 54s\n",
      "3698:\tlearn: 5.3762262\ttotal: 18m 43s\tremaining: 31m 54s\n",
      "3699:\tlearn: 5.3754219\ttotal: 18m 44s\tremaining: 31m 54s\n",
      "3700:\tlearn: 5.3748933\ttotal: 18m 44s\tremaining: 31m 53s\n",
      "3701:\tlearn: 5.3738506\ttotal: 18m 44s\tremaining: 31m 53s\n",
      "3702:\tlearn: 5.3732834\ttotal: 18m 45s\tremaining: 31m 53s\n",
      "3703:\tlearn: 5.3726956\ttotal: 18m 45s\tremaining: 31m 53s\n",
      "3704:\tlearn: 5.3722146\ttotal: 18m 45s\tremaining: 31m 52s\n",
      "3705:\tlearn: 5.3711650\ttotal: 18m 46s\tremaining: 31m 52s\n",
      "3706:\tlearn: 5.3705906\ttotal: 18m 46s\tremaining: 31m 52s\n",
      "3707:\tlearn: 5.3700382\ttotal: 18m 46s\tremaining: 31m 51s\n",
      "3708:\tlearn: 5.3694792\ttotal: 18m 46s\tremaining: 31m 51s\n",
      "3709:\tlearn: 5.3689318\ttotal: 18m 47s\tremaining: 31m 51s\n",
      "3710:\tlearn: 5.3680916\ttotal: 18m 47s\tremaining: 31m 50s\n",
      "3711:\tlearn: 5.3674435\ttotal: 18m 47s\tremaining: 31m 50s\n",
      "3712:\tlearn: 5.3666842\ttotal: 18m 48s\tremaining: 31m 50s\n",
      "3713:\tlearn: 5.3661414\ttotal: 18m 48s\tremaining: 31m 50s\n",
      "3714:\tlearn: 5.3654503\ttotal: 18m 49s\tremaining: 31m 50s\n",
      "3715:\tlearn: 5.3650264\ttotal: 18m 49s\tremaining: 31m 49s\n",
      "3716:\tlearn: 5.3646126\ttotal: 18m 49s\tremaining: 31m 49s\n",
      "3717:\tlearn: 5.3639809\ttotal: 18m 49s\tremaining: 31m 48s\n",
      "3718:\tlearn: 5.3631439\ttotal: 18m 50s\tremaining: 31m 48s\n",
      "3719:\tlearn: 5.3623194\ttotal: 18m 50s\tremaining: 31m 48s\n",
      "3720:\tlearn: 5.3616619\ttotal: 18m 50s\tremaining: 31m 48s\n",
      "3721:\tlearn: 5.3608993\ttotal: 18m 51s\tremaining: 31m 47s\n",
      "3722:\tlearn: 5.3601114\ttotal: 18m 51s\tremaining: 31m 47s\n",
      "3723:\tlearn: 5.3596935\ttotal: 18m 51s\tremaining: 31m 47s\n",
      "3724:\tlearn: 5.3589475\ttotal: 18m 52s\tremaining: 31m 47s\n",
      "3725:\tlearn: 5.3577619\ttotal: 18m 52s\tremaining: 31m 46s\n",
      "3726:\tlearn: 5.3572853\ttotal: 18m 52s\tremaining: 31m 46s\n",
      "3727:\tlearn: 5.3567549\ttotal: 18m 53s\tremaining: 31m 46s\n",
      "3728:\tlearn: 5.3562253\ttotal: 18m 53s\tremaining: 31m 45s\n",
      "3729:\tlearn: 5.3559238\ttotal: 18m 53s\tremaining: 31m 45s\n",
      "3730:\tlearn: 5.3553425\ttotal: 18m 53s\tremaining: 31m 45s\n",
      "3731:\tlearn: 5.3548013\ttotal: 18m 54s\tremaining: 31m 44s\n",
      "3732:\tlearn: 5.3540956\ttotal: 18m 54s\tremaining: 31m 44s\n",
      "3733:\tlearn: 5.3538202\ttotal: 18m 54s\tremaining: 31m 44s\n",
      "3734:\tlearn: 5.3533482\ttotal: 18m 55s\tremaining: 31m 43s\n",
      "3735:\tlearn: 5.3525807\ttotal: 18m 55s\tremaining: 31m 43s\n",
      "3736:\tlearn: 5.3520555\ttotal: 18m 55s\tremaining: 31m 43s\n",
      "3737:\tlearn: 5.3516703\ttotal: 18m 55s\tremaining: 31m 42s\n",
      "3738:\tlearn: 5.3509863\ttotal: 18m 56s\tremaining: 31m 42s\n",
      "3739:\tlearn: 5.3505132\ttotal: 18m 56s\tremaining: 31m 42s\n",
      "3740:\tlearn: 5.3498529\ttotal: 18m 56s\tremaining: 31m 41s\n",
      "3741:\tlearn: 5.3490407\ttotal: 18m 57s\tremaining: 31m 41s\n",
      "3742:\tlearn: 5.3484745\ttotal: 18m 57s\tremaining: 31m 41s\n",
      "3743:\tlearn: 5.3477144\ttotal: 18m 57s\tremaining: 31m 40s\n",
      "3744:\tlearn: 5.3469866\ttotal: 18m 57s\tremaining: 31m 40s\n",
      "3745:\tlearn: 5.3459836\ttotal: 18m 58s\tremaining: 31m 40s\n",
      "3746:\tlearn: 5.3454657\ttotal: 18m 58s\tremaining: 31m 39s\n",
      "3747:\tlearn: 5.3450430\ttotal: 18m 58s\tremaining: 31m 39s\n",
      "3748:\tlearn: 5.3441332\ttotal: 18m 59s\tremaining: 31m 39s\n",
      "3749:\tlearn: 5.3432464\ttotal: 18m 59s\tremaining: 31m 39s\n",
      "3750:\tlearn: 5.3427510\ttotal: 18m 59s\tremaining: 31m 38s\n",
      "3751:\tlearn: 5.3422346\ttotal: 19m\tremaining: 31m 38s\n",
      "3752:\tlearn: 5.3418217\ttotal: 19m\tremaining: 31m 38s\n",
      "3753:\tlearn: 5.3413695\ttotal: 19m\tremaining: 31m 37s\n",
      "3754:\tlearn: 5.3411346\ttotal: 19m\tremaining: 31m 37s\n",
      "3755:\tlearn: 5.3401686\ttotal: 19m 1s\tremaining: 31m 36s\n",
      "3756:\tlearn: 5.3398970\ttotal: 19m 1s\tremaining: 31m 36s\n",
      "3757:\tlearn: 5.3392730\ttotal: 19m 1s\tremaining: 31m 36s\n",
      "3758:\tlearn: 5.3387710\ttotal: 19m 2s\tremaining: 31m 36s\n",
      "3759:\tlearn: 5.3384023\ttotal: 19m 2s\tremaining: 31m 35s\n",
      "3760:\tlearn: 5.3379880\ttotal: 19m 2s\tremaining: 31m 35s\n",
      "3761:\tlearn: 5.3375934\ttotal: 19m 2s\tremaining: 31m 35s\n",
      "3762:\tlearn: 5.3370899\ttotal: 19m 3s\tremaining: 31m 34s\n",
      "3763:\tlearn: 5.3365535\ttotal: 19m 3s\tremaining: 31m 34s\n",
      "3764:\tlearn: 5.3357388\ttotal: 19m 3s\tremaining: 31m 34s\n",
      "3765:\tlearn: 5.3350107\ttotal: 19m 4s\tremaining: 31m 33s\n",
      "3766:\tlearn: 5.3344101\ttotal: 19m 4s\tremaining: 31m 33s\n",
      "3767:\tlearn: 5.3335756\ttotal: 19m 4s\tremaining: 31m 33s\n",
      "3768:\tlearn: 5.3328472\ttotal: 19m 5s\tremaining: 31m 33s\n",
      "3769:\tlearn: 5.3322006\ttotal: 19m 5s\tremaining: 31m 33s\n",
      "3770:\tlearn: 5.3315752\ttotal: 19m 5s\tremaining: 31m 32s\n",
      "3771:\tlearn: 5.3309390\ttotal: 19m 6s\tremaining: 31m 32s\n",
      "3772:\tlearn: 5.3300669\ttotal: 19m 6s\tremaining: 31m 32s\n",
      "3773:\tlearn: 5.3291075\ttotal: 19m 6s\tremaining: 31m 32s\n",
      "3774:\tlearn: 5.3287215\ttotal: 19m 7s\tremaining: 31m 31s\n",
      "3775:\tlearn: 5.3280995\ttotal: 19m 7s\tremaining: 31m 31s\n",
      "3776:\tlearn: 5.3276028\ttotal: 19m 7s\tremaining: 31m 31s\n",
      "3777:\tlearn: 5.3266180\ttotal: 19m 8s\tremaining: 31m 31s\n",
      "3778:\tlearn: 5.3261547\ttotal: 19m 8s\tremaining: 31m 30s\n",
      "3779:\tlearn: 5.3254810\ttotal: 19m 8s\tremaining: 31m 30s\n",
      "3780:\tlearn: 5.3247823\ttotal: 19m 9s\tremaining: 31m 30s\n",
      "3781:\tlearn: 5.3241669\ttotal: 19m 9s\tremaining: 31m 29s\n",
      "3782:\tlearn: 5.3236643\ttotal: 19m 9s\tremaining: 31m 29s\n",
      "3783:\tlearn: 5.3228409\ttotal: 19m 9s\tremaining: 31m 29s\n",
      "3784:\tlearn: 5.3224153\ttotal: 19m 10s\tremaining: 31m 28s\n",
      "3785:\tlearn: 5.3219241\ttotal: 19m 10s\tremaining: 31m 28s\n",
      "3786:\tlearn: 5.3215068\ttotal: 19m 10s\tremaining: 31m 27s\n",
      "3787:\tlearn: 5.3208572\ttotal: 19m 11s\tremaining: 31m 27s\n",
      "3788:\tlearn: 5.3203226\ttotal: 19m 11s\tremaining: 31m 27s\n",
      "3789:\tlearn: 5.3196760\ttotal: 19m 11s\tremaining: 31m 26s\n",
      "3790:\tlearn: 5.3191141\ttotal: 19m 11s\tremaining: 31m 26s\n",
      "3791:\tlearn: 5.3179674\ttotal: 19m 12s\tremaining: 31m 26s\n",
      "3792:\tlearn: 5.3168624\ttotal: 19m 12s\tremaining: 31m 26s\n",
      "3793:\tlearn: 5.3160199\ttotal: 19m 12s\tremaining: 31m 25s\n",
      "3794:\tlearn: 5.3153031\ttotal: 19m 13s\tremaining: 31m 25s\n",
      "3795:\tlearn: 5.3146811\ttotal: 19m 13s\tremaining: 31m 25s\n",
      "3796:\tlearn: 5.3138607\ttotal: 19m 13s\tremaining: 31m 24s\n",
      "3797:\tlearn: 5.3134621\ttotal: 19m 14s\tremaining: 31m 24s\n",
      "3798:\tlearn: 5.3131284\ttotal: 19m 14s\tremaining: 31m 24s\n",
      "3799:\tlearn: 5.3124695\ttotal: 19m 14s\tremaining: 31m 24s\n",
      "3800:\tlearn: 5.3113187\ttotal: 19m 15s\tremaining: 31m 23s\n",
      "3801:\tlearn: 5.3109319\ttotal: 19m 15s\tremaining: 31m 23s\n",
      "3802:\tlearn: 5.3101630\ttotal: 19m 15s\tremaining: 31m 23s\n",
      "3803:\tlearn: 5.3096836\ttotal: 19m 15s\tremaining: 31m 22s\n",
      "3804:\tlearn: 5.3086836\ttotal: 19m 16s\tremaining: 31m 22s\n",
      "3805:\tlearn: 5.3078507\ttotal: 19m 16s\tremaining: 31m 22s\n",
      "3806:\tlearn: 5.3071801\ttotal: 19m 17s\tremaining: 31m 22s\n",
      "3807:\tlearn: 5.3067385\ttotal: 19m 17s\tremaining: 31m 21s\n",
      "3808:\tlearn: 5.3061118\ttotal: 19m 17s\tremaining: 31m 21s\n",
      "3809:\tlearn: 5.3049993\ttotal: 19m 18s\tremaining: 31m 21s\n",
      "3810:\tlearn: 5.3041169\ttotal: 19m 18s\tremaining: 31m 21s\n",
      "3811:\tlearn: 5.3032809\ttotal: 19m 18s\tremaining: 31m 21s\n",
      "3812:\tlearn: 5.3027981\ttotal: 19m 19s\tremaining: 31m 20s\n",
      "3813:\tlearn: 5.3021319\ttotal: 19m 19s\tremaining: 31m 20s\n",
      "3814:\tlearn: 5.3011035\ttotal: 19m 19s\tremaining: 31m 20s\n",
      "3815:\tlearn: 5.3006390\ttotal: 19m 20s\tremaining: 31m 20s\n",
      "3816:\tlearn: 5.3001234\ttotal: 19m 20s\tremaining: 31m 19s\n",
      "3817:\tlearn: 5.2991926\ttotal: 19m 20s\tremaining: 31m 19s\n",
      "3818:\tlearn: 5.2986715\ttotal: 19m 21s\tremaining: 31m 19s\n",
      "3819:\tlearn: 5.2977369\ttotal: 19m 21s\tremaining: 31m 19s\n",
      "3820:\tlearn: 5.2973387\ttotal: 19m 21s\tremaining: 31m 18s\n",
      "3821:\tlearn: 5.2967885\ttotal: 19m 22s\tremaining: 31m 18s\n",
      "3822:\tlearn: 5.2962337\ttotal: 19m 22s\tremaining: 31m 18s\n",
      "3823:\tlearn: 5.2956011\ttotal: 19m 22s\tremaining: 31m 17s\n",
      "3824:\tlearn: 5.2949660\ttotal: 19m 23s\tremaining: 31m 17s\n",
      "3825:\tlearn: 5.2943356\ttotal: 19m 23s\tremaining: 31m 17s\n",
      "3826:\tlearn: 5.2938297\ttotal: 19m 23s\tremaining: 31m 17s\n",
      "3827:\tlearn: 5.2933560\ttotal: 19m 23s\tremaining: 31m 16s\n",
      "3828:\tlearn: 5.2926656\ttotal: 19m 24s\tremaining: 31m 16s\n",
      "3829:\tlearn: 5.2917660\ttotal: 19m 24s\tremaining: 31m 16s\n",
      "3830:\tlearn: 5.2907990\ttotal: 19m 25s\tremaining: 31m 16s\n",
      "3831:\tlearn: 5.2900866\ttotal: 19m 25s\tremaining: 31m 15s\n",
      "3832:\tlearn: 5.2894295\ttotal: 19m 25s\tremaining: 31m 15s\n",
      "3833:\tlearn: 5.2888023\ttotal: 19m 26s\tremaining: 31m 15s\n",
      "3834:\tlearn: 5.2882057\ttotal: 19m 26s\tremaining: 31m 14s\n",
      "3835:\tlearn: 5.2876822\ttotal: 19m 26s\tremaining: 31m 14s\n",
      "3836:\tlearn: 5.2873569\ttotal: 19m 26s\tremaining: 31m 14s\n",
      "3837:\tlearn: 5.2867396\ttotal: 19m 27s\tremaining: 31m 13s\n",
      "3838:\tlearn: 5.2861289\ttotal: 19m 27s\tremaining: 31m 13s\n",
      "3839:\tlearn: 5.2853045\ttotal: 19m 27s\tremaining: 31m 13s\n",
      "3840:\tlearn: 5.2845644\ttotal: 19m 28s\tremaining: 31m 13s\n",
      "3841:\tlearn: 5.2837070\ttotal: 19m 28s\tremaining: 31m 13s\n",
      "3842:\tlearn: 5.2832524\ttotal: 19m 28s\tremaining: 31m 12s\n",
      "3843:\tlearn: 5.2828062\ttotal: 19m 29s\tremaining: 31m 12s\n",
      "3844:\tlearn: 5.2821947\ttotal: 19m 29s\tremaining: 31m 12s\n",
      "3845:\tlearn: 5.2818702\ttotal: 19m 29s\tremaining: 31m 11s\n",
      "3846:\tlearn: 5.2813733\ttotal: 19m 30s\tremaining: 31m 11s\n",
      "3847:\tlearn: 5.2807100\ttotal: 19m 30s\tremaining: 31m 11s\n",
      "3848:\tlearn: 5.2801829\ttotal: 19m 30s\tremaining: 31m 10s\n",
      "3849:\tlearn: 5.2789974\ttotal: 19m 30s\tremaining: 31m 10s\n",
      "3850:\tlearn: 5.2783601\ttotal: 19m 31s\tremaining: 31m 10s\n",
      "3851:\tlearn: 5.2777902\ttotal: 19m 31s\tremaining: 31m 9s\n",
      "3852:\tlearn: 5.2769767\ttotal: 19m 31s\tremaining: 31m 9s\n",
      "3853:\tlearn: 5.2764769\ttotal: 19m 32s\tremaining: 31m 9s\n",
      "3854:\tlearn: 5.2759302\ttotal: 19m 32s\tremaining: 31m 8s\n",
      "3855:\tlearn: 5.2757020\ttotal: 19m 32s\tremaining: 31m 8s\n",
      "3856:\tlearn: 5.2749562\ttotal: 19m 33s\tremaining: 31m 8s\n",
      "3857:\tlearn: 5.2743150\ttotal: 19m 33s\tremaining: 31m 8s\n",
      "3858:\tlearn: 5.2736729\ttotal: 19m 33s\tremaining: 31m 7s\n",
      "3859:\tlearn: 5.2731930\ttotal: 19m 34s\tremaining: 31m 7s\n",
      "3860:\tlearn: 5.2728234\ttotal: 19m 34s\tremaining: 31m 7s\n",
      "3861:\tlearn: 5.2720162\ttotal: 19m 34s\tremaining: 31m 6s\n",
      "3862:\tlearn: 5.2713498\ttotal: 19m 35s\tremaining: 31m 6s\n",
      "3863:\tlearn: 5.2704570\ttotal: 19m 35s\tremaining: 31m 6s\n",
      "3864:\tlearn: 5.2699873\ttotal: 19m 35s\tremaining: 31m 6s\n",
      "3865:\tlearn: 5.2695647\ttotal: 19m 36s\tremaining: 31m 6s\n",
      "3866:\tlearn: 5.2685589\ttotal: 19m 36s\tremaining: 31m 5s\n",
      "3867:\tlearn: 5.2681161\ttotal: 19m 36s\tremaining: 31m 5s\n",
      "3868:\tlearn: 5.2677535\ttotal: 19m 37s\tremaining: 31m 5s\n",
      "3869:\tlearn: 5.2669688\ttotal: 19m 37s\tremaining: 31m 5s\n",
      "3870:\tlearn: 5.2665088\ttotal: 19m 37s\tremaining: 31m 4s\n",
      "3871:\tlearn: 5.2655812\ttotal: 19m 38s\tremaining: 31m 4s\n",
      "3872:\tlearn: 5.2648138\ttotal: 19m 38s\tremaining: 31m 4s\n",
      "3873:\tlearn: 5.2642575\ttotal: 19m 38s\tremaining: 31m 4s\n",
      "3874:\tlearn: 5.2630745\ttotal: 19m 39s\tremaining: 31m 4s\n",
      "3875:\tlearn: 5.2620692\ttotal: 19m 39s\tremaining: 31m 3s\n",
      "3876:\tlearn: 5.2616253\ttotal: 19m 39s\tremaining: 31m 3s\n",
      "3877:\tlearn: 5.2606875\ttotal: 19m 40s\tremaining: 31m 3s\n",
      "3878:\tlearn: 5.2603250\ttotal: 19m 40s\tremaining: 31m 2s\n",
      "3879:\tlearn: 5.2595544\ttotal: 19m 40s\tremaining: 31m 2s\n",
      "3880:\tlearn: 5.2588351\ttotal: 19m 41s\tremaining: 31m 2s\n",
      "3881:\tlearn: 5.2581229\ttotal: 19m 41s\tremaining: 31m 2s\n",
      "3882:\tlearn: 5.2574112\ttotal: 19m 42s\tremaining: 31m 2s\n",
      "3883:\tlearn: 5.2564353\ttotal: 19m 42s\tremaining: 31m 2s\n",
      "3884:\tlearn: 5.2561159\ttotal: 19m 42s\tremaining: 31m 1s\n",
      "3885:\tlearn: 5.2552859\ttotal: 19m 43s\tremaining: 31m 1s\n",
      "3886:\tlearn: 5.2548438\ttotal: 19m 43s\tremaining: 31m 1s\n",
      "3887:\tlearn: 5.2540261\ttotal: 19m 43s\tremaining: 31m\n",
      "3888:\tlearn: 5.2536321\ttotal: 19m 44s\tremaining: 31m\n",
      "3889:\tlearn: 5.2531784\ttotal: 19m 44s\tremaining: 31m\n",
      "3890:\tlearn: 5.2528563\ttotal: 19m 44s\tremaining: 31m\n",
      "3891:\tlearn: 5.2522931\ttotal: 19m 45s\tremaining: 30m 59s\n",
      "3892:\tlearn: 5.2515656\ttotal: 19m 45s\tremaining: 30m 59s\n",
      "3893:\tlearn: 5.2506478\ttotal: 19m 45s\tremaining: 30m 59s\n",
      "3894:\tlearn: 5.2502816\ttotal: 19m 45s\tremaining: 30m 58s\n",
      "3895:\tlearn: 5.2493028\ttotal: 19m 46s\tremaining: 30m 58s\n",
      "3896:\tlearn: 5.2484382\ttotal: 19m 46s\tremaining: 30m 58s\n",
      "3897:\tlearn: 5.2479662\ttotal: 19m 47s\tremaining: 30m 58s\n",
      "3898:\tlearn: 5.2475100\ttotal: 19m 47s\tremaining: 30m 58s\n",
      "3899:\tlearn: 5.2465952\ttotal: 19m 47s\tremaining: 30m 57s\n",
      "3900:\tlearn: 5.2458115\ttotal: 19m 48s\tremaining: 30m 57s\n",
      "3901:\tlearn: 5.2450298\ttotal: 19m 48s\tremaining: 30m 57s\n",
      "3902:\tlearn: 5.2446024\ttotal: 19m 48s\tremaining: 30m 57s\n",
      "3903:\tlearn: 5.2437617\ttotal: 19m 49s\tremaining: 30m 56s\n",
      "3904:\tlearn: 5.2433823\ttotal: 19m 49s\tremaining: 30m 56s\n",
      "3905:\tlearn: 5.2426355\ttotal: 19m 49s\tremaining: 30m 56s\n",
      "3906:\tlearn: 5.2421789\ttotal: 19m 50s\tremaining: 30m 55s\n",
      "3907:\tlearn: 5.2416603\ttotal: 19m 50s\tremaining: 30m 55s\n",
      "3908:\tlearn: 5.2413787\ttotal: 19m 50s\tremaining: 30m 55s\n",
      "3909:\tlearn: 5.2406048\ttotal: 19m 50s\tremaining: 30m 54s\n",
      "3910:\tlearn: 5.2397990\ttotal: 19m 51s\tremaining: 30m 54s\n",
      "3911:\tlearn: 5.2390720\ttotal: 19m 51s\tremaining: 30m 54s\n",
      "3912:\tlearn: 5.2386984\ttotal: 19m 51s\tremaining: 30m 54s\n",
      "3913:\tlearn: 5.2380933\ttotal: 19m 52s\tremaining: 30m 53s\n",
      "3914:\tlearn: 5.2372076\ttotal: 19m 52s\tremaining: 30m 53s\n",
      "3915:\tlearn: 5.2367820\ttotal: 19m 52s\tremaining: 30m 53s\n",
      "3916:\tlearn: 5.2358580\ttotal: 19m 53s\tremaining: 30m 53s\n",
      "3917:\tlearn: 5.2353497\ttotal: 19m 53s\tremaining: 30m 52s\n",
      "3918:\tlearn: 5.2344486\ttotal: 19m 53s\tremaining: 30m 52s\n",
      "3919:\tlearn: 5.2340976\ttotal: 19m 54s\tremaining: 30m 52s\n",
      "3920:\tlearn: 5.2334456\ttotal: 19m 54s\tremaining: 30m 52s\n",
      "3921:\tlearn: 5.2327697\ttotal: 19m 54s\tremaining: 30m 51s\n",
      "3922:\tlearn: 5.2318978\ttotal: 19m 55s\tremaining: 30m 51s\n",
      "3923:\tlearn: 5.2310336\ttotal: 19m 55s\tremaining: 30m 51s\n",
      "3924:\tlearn: 5.2304955\ttotal: 19m 55s\tremaining: 30m 50s\n",
      "3925:\tlearn: 5.2297136\ttotal: 19m 56s\tremaining: 30m 50s\n",
      "3926:\tlearn: 5.2285828\ttotal: 19m 56s\tremaining: 30m 50s\n",
      "3927:\tlearn: 5.2279943\ttotal: 19m 56s\tremaining: 30m 50s\n",
      "3928:\tlearn: 5.2273467\ttotal: 19m 57s\tremaining: 30m 49s\n",
      "3929:\tlearn: 5.2270163\ttotal: 19m 57s\tremaining: 30m 49s\n",
      "3930:\tlearn: 5.2265731\ttotal: 19m 57s\tremaining: 30m 49s\n",
      "3931:\tlearn: 5.2260568\ttotal: 19m 58s\tremaining: 30m 49s\n",
      "3932:\tlearn: 5.2255739\ttotal: 19m 58s\tremaining: 30m 48s\n",
      "3933:\tlearn: 5.2244900\ttotal: 19m 58s\tremaining: 30m 48s\n",
      "3934:\tlearn: 5.2236240\ttotal: 19m 59s\tremaining: 30m 48s\n",
      "3935:\tlearn: 5.2226952\ttotal: 19m 59s\tremaining: 30m 47s\n",
      "3936:\tlearn: 5.2218669\ttotal: 19m 59s\tremaining: 30m 47s\n",
      "3937:\tlearn: 5.2211151\ttotal: 20m\tremaining: 30m 47s\n",
      "3938:\tlearn: 5.2207391\ttotal: 20m\tremaining: 30m 47s\n",
      "3939:\tlearn: 5.2203579\ttotal: 20m\tremaining: 30m 46s\n",
      "3940:\tlearn: 5.2196880\ttotal: 20m\tremaining: 30m 46s\n",
      "3941:\tlearn: 5.2192945\ttotal: 20m 1s\tremaining: 30m 46s\n",
      "3942:\tlearn: 5.2188618\ttotal: 20m 1s\tremaining: 30m 45s\n",
      "3943:\tlearn: 5.2182010\ttotal: 20m 1s\tremaining: 30m 45s\n",
      "3944:\tlearn: 5.2178129\ttotal: 20m 2s\tremaining: 30m 45s\n",
      "3945:\tlearn: 5.2172198\ttotal: 20m 2s\tremaining: 30m 44s\n",
      "3946:\tlearn: 5.2167647\ttotal: 20m 2s\tremaining: 30m 44s\n",
      "3947:\tlearn: 5.2160753\ttotal: 20m 3s\tremaining: 30m 44s\n",
      "3948:\tlearn: 5.2155941\ttotal: 20m 3s\tremaining: 30m 44s\n",
      "3949:\tlearn: 5.2152535\ttotal: 20m 3s\tremaining: 30m 43s\n",
      "3950:\tlearn: 5.2143990\ttotal: 20m 4s\tremaining: 30m 43s\n",
      "3951:\tlearn: 5.2135960\ttotal: 20m 4s\tremaining: 30m 43s\n",
      "3952:\tlearn: 5.2130962\ttotal: 20m 4s\tremaining: 30m 43s\n",
      "3953:\tlearn: 5.2127021\ttotal: 20m 5s\tremaining: 30m 42s\n",
      "3954:\tlearn: 5.2116895\ttotal: 20m 5s\tremaining: 30m 42s\n",
      "3955:\tlearn: 5.2113758\ttotal: 20m 5s\tremaining: 30m 42s\n",
      "3956:\tlearn: 5.2109464\ttotal: 20m 6s\tremaining: 30m 41s\n",
      "3957:\tlearn: 5.2104327\ttotal: 20m 6s\tremaining: 30m 41s\n",
      "3958:\tlearn: 5.2092081\ttotal: 20m 6s\tremaining: 30m 41s\n",
      "3959:\tlearn: 5.2087647\ttotal: 20m 7s\tremaining: 30m 41s\n",
      "3960:\tlearn: 5.2082428\ttotal: 20m 7s\tremaining: 30m 40s\n",
      "3961:\tlearn: 5.2073115\ttotal: 20m 7s\tremaining: 30m 40s\n",
      "3962:\tlearn: 5.2064930\ttotal: 20m 8s\tremaining: 30m 40s\n",
      "3963:\tlearn: 5.2058164\ttotal: 20m 8s\tremaining: 30m 40s\n",
      "3964:\tlearn: 5.2050848\ttotal: 20m 8s\tremaining: 30m 39s\n",
      "3965:\tlearn: 5.2047380\ttotal: 20m 9s\tremaining: 30m 39s\n",
      "3966:\tlearn: 5.2037700\ttotal: 20m 9s\tremaining: 30m 39s\n",
      "3967:\tlearn: 5.2032906\ttotal: 20m 9s\tremaining: 30m 38s\n",
      "3968:\tlearn: 5.2021427\ttotal: 20m 10s\tremaining: 30m 38s\n",
      "3969:\tlearn: 5.2015906\ttotal: 20m 10s\tremaining: 30m 38s\n",
      "3970:\tlearn: 5.2010599\ttotal: 20m 10s\tremaining: 30m 37s\n",
      "3971:\tlearn: 5.2002720\ttotal: 20m 10s\tremaining: 30m 37s\n",
      "3972:\tlearn: 5.1995285\ttotal: 20m 11s\tremaining: 30m 37s\n",
      "3973:\tlearn: 5.1988883\ttotal: 20m 11s\tremaining: 30m 37s\n",
      "3974:\tlearn: 5.1984424\ttotal: 20m 11s\tremaining: 30m 36s\n",
      "3975:\tlearn: 5.1975618\ttotal: 20m 12s\tremaining: 30m 36s\n",
      "3976:\tlearn: 5.1970037\ttotal: 20m 12s\tremaining: 30m 36s\n",
      "3977:\tlearn: 5.1963263\ttotal: 20m 12s\tremaining: 30m 36s\n",
      "3978:\tlearn: 5.1957512\ttotal: 20m 13s\tremaining: 30m 35s\n",
      "3979:\tlearn: 5.1952219\ttotal: 20m 13s\tremaining: 30m 35s\n",
      "3980:\tlearn: 5.1948070\ttotal: 20m 13s\tremaining: 30m 35s\n",
      "3981:\tlearn: 5.1944335\ttotal: 20m 14s\tremaining: 30m 34s\n",
      "3982:\tlearn: 5.1939903\ttotal: 20m 14s\tremaining: 30m 34s\n",
      "3983:\tlearn: 5.1933227\ttotal: 20m 14s\tremaining: 30m 34s\n",
      "3984:\tlearn: 5.1928960\ttotal: 20m 14s\tremaining: 30m 33s\n",
      "3985:\tlearn: 5.1922293\ttotal: 20m 15s\tremaining: 30m 33s\n",
      "3986:\tlearn: 5.1916726\ttotal: 20m 15s\tremaining: 30m 33s\n",
      "3987:\tlearn: 5.1912322\ttotal: 20m 15s\tremaining: 30m 32s\n",
      "3988:\tlearn: 5.1906094\ttotal: 20m 16s\tremaining: 30m 32s\n",
      "3989:\tlearn: 5.1901201\ttotal: 20m 16s\tremaining: 30m 32s\n",
      "3990:\tlearn: 5.1892286\ttotal: 20m 16s\tremaining: 30m 31s\n",
      "3991:\tlearn: 5.1883972\ttotal: 20m 17s\tremaining: 30m 31s\n",
      "3992:\tlearn: 5.1878256\ttotal: 20m 17s\tremaining: 30m 31s\n",
      "3993:\tlearn: 5.1873526\ttotal: 20m 17s\tremaining: 30m 30s\n",
      "3994:\tlearn: 5.1866764\ttotal: 20m 17s\tremaining: 30m 30s\n",
      "3995:\tlearn: 5.1860783\ttotal: 20m 18s\tremaining: 30m 30s\n",
      "3996:\tlearn: 5.1853636\ttotal: 20m 18s\tremaining: 30m 30s\n",
      "3997:\tlearn: 5.1847282\ttotal: 20m 18s\tremaining: 30m 29s\n",
      "3998:\tlearn: 5.1839757\ttotal: 20m 19s\tremaining: 30m 29s\n",
      "3999:\tlearn: 5.1835010\ttotal: 20m 19s\tremaining: 30m 29s\n",
      "4000:\tlearn: 5.1827606\ttotal: 20m 19s\tremaining: 30m 28s\n",
      "4001:\tlearn: 5.1823108\ttotal: 20m 19s\tremaining: 30m 28s\n",
      "4002:\tlearn: 5.1818249\ttotal: 20m 20s\tremaining: 30m 28s\n",
      "4003:\tlearn: 5.1814423\ttotal: 20m 20s\tremaining: 30m 27s\n",
      "4004:\tlearn: 5.1811631\ttotal: 20m 20s\tremaining: 30m 27s\n",
      "4005:\tlearn: 5.1802576\ttotal: 20m 21s\tremaining: 30m 27s\n",
      "4006:\tlearn: 5.1798390\ttotal: 20m 21s\tremaining: 30m 26s\n",
      "4007:\tlearn: 5.1791808\ttotal: 20m 21s\tremaining: 30m 26s\n",
      "4008:\tlearn: 5.1788290\ttotal: 20m 22s\tremaining: 30m 26s\n",
      "4009:\tlearn: 5.1783007\ttotal: 20m 22s\tremaining: 30m 25s\n",
      "4010:\tlearn: 5.1778630\ttotal: 20m 22s\tremaining: 30m 25s\n",
      "4011:\tlearn: 5.1771218\ttotal: 20m 23s\tremaining: 30m 25s\n",
      "4012:\tlearn: 5.1763426\ttotal: 20m 23s\tremaining: 30m 25s\n",
      "4013:\tlearn: 5.1757657\ttotal: 20m 23s\tremaining: 30m 24s\n",
      "4014:\tlearn: 5.1752432\ttotal: 20m 23s\tremaining: 30m 24s\n",
      "4015:\tlearn: 5.1744793\ttotal: 20m 24s\tremaining: 30m 24s\n",
      "4016:\tlearn: 5.1737498\ttotal: 20m 24s\tremaining: 30m 24s\n",
      "4017:\tlearn: 5.1729783\ttotal: 20m 24s\tremaining: 30m 23s\n",
      "4018:\tlearn: 5.1726165\ttotal: 20m 25s\tremaining: 30m 23s\n",
      "4019:\tlearn: 5.1721765\ttotal: 20m 25s\tremaining: 30m 22s\n",
      "4020:\tlearn: 5.1718118\ttotal: 20m 25s\tremaining: 30m 22s\n",
      "4021:\tlearn: 5.1709257\ttotal: 20m 26s\tremaining: 30m 22s\n",
      "4022:\tlearn: 5.1702221\ttotal: 20m 26s\tremaining: 30m 22s\n",
      "4023:\tlearn: 5.1697466\ttotal: 20m 26s\tremaining: 30m 21s\n",
      "4024:\tlearn: 5.1691888\ttotal: 20m 27s\tremaining: 30m 21s\n",
      "4025:\tlearn: 5.1682936\ttotal: 20m 27s\tremaining: 30m 21s\n",
      "4026:\tlearn: 5.1677338\ttotal: 20m 27s\tremaining: 30m 20s\n",
      "4027:\tlearn: 5.1672923\ttotal: 20m 27s\tremaining: 30m 20s\n",
      "4028:\tlearn: 5.1669525\ttotal: 20m 28s\tremaining: 30m 20s\n",
      "4029:\tlearn: 5.1663130\ttotal: 20m 28s\tremaining: 30m 19s\n",
      "4030:\tlearn: 5.1656907\ttotal: 20m 28s\tremaining: 30m 19s\n",
      "4031:\tlearn: 5.1649276\ttotal: 20m 29s\tremaining: 30m 19s\n",
      "4032:\tlearn: 5.1645496\ttotal: 20m 29s\tremaining: 30m 19s\n",
      "4033:\tlearn: 5.1640515\ttotal: 20m 29s\tremaining: 30m 18s\n",
      "4034:\tlearn: 5.1634391\ttotal: 20m 29s\tremaining: 30m 18s\n",
      "4035:\tlearn: 5.1629512\ttotal: 20m 30s\tremaining: 30m 17s\n",
      "4036:\tlearn: 5.1625471\ttotal: 20m 30s\tremaining: 30m 17s\n",
      "4037:\tlearn: 5.1620666\ttotal: 20m 30s\tremaining: 30m 17s\n",
      "4038:\tlearn: 5.1616415\ttotal: 20m 31s\tremaining: 30m 16s\n",
      "4039:\tlearn: 5.1609971\ttotal: 20m 31s\tremaining: 30m 16s\n",
      "4040:\tlearn: 5.1604575\ttotal: 20m 31s\tremaining: 30m 16s\n",
      "4041:\tlearn: 5.1596263\ttotal: 20m 31s\tremaining: 30m 15s\n",
      "4042:\tlearn: 5.1589502\ttotal: 20m 32s\tremaining: 30m 15s\n",
      "4043:\tlearn: 5.1585081\ttotal: 20m 32s\tremaining: 30m 15s\n",
      "4044:\tlearn: 5.1579371\ttotal: 20m 32s\tremaining: 30m 14s\n",
      "4045:\tlearn: 5.1571619\ttotal: 20m 33s\tremaining: 30m 14s\n",
      "4046:\tlearn: 5.1565937\ttotal: 20m 33s\tremaining: 30m 14s\n",
      "4047:\tlearn: 5.1563992\ttotal: 20m 33s\tremaining: 30m 13s\n",
      "4048:\tlearn: 5.1557057\ttotal: 20m 33s\tremaining: 30m 13s\n",
      "4049:\tlearn: 5.1549605\ttotal: 20m 34s\tremaining: 30m 13s\n",
      "4050:\tlearn: 5.1546345\ttotal: 20m 34s\tremaining: 30m 13s\n",
      "4051:\tlearn: 5.1540235\ttotal: 20m 34s\tremaining: 30m 12s\n",
      "4052:\tlearn: 5.1533533\ttotal: 20m 35s\tremaining: 30m 12s\n",
      "4053:\tlearn: 5.1529433\ttotal: 20m 35s\tremaining: 30m 12s\n",
      "4054:\tlearn: 5.1522314\ttotal: 20m 35s\tremaining: 30m 12s\n",
      "4055:\tlearn: 5.1515730\ttotal: 20m 36s\tremaining: 30m 11s\n",
      "4056:\tlearn: 5.1510511\ttotal: 20m 36s\tremaining: 30m 11s\n",
      "4057:\tlearn: 5.1506860\ttotal: 20m 36s\tremaining: 30m 11s\n",
      "4058:\tlearn: 5.1499021\ttotal: 20m 37s\tremaining: 30m 10s\n",
      "4059:\tlearn: 5.1493134\ttotal: 20m 37s\tremaining: 30m 10s\n",
      "4060:\tlearn: 5.1486450\ttotal: 20m 37s\tremaining: 30m 10s\n",
      "4061:\tlearn: 5.1481245\ttotal: 20m 37s\tremaining: 30m 9s\n",
      "4062:\tlearn: 5.1473920\ttotal: 20m 38s\tremaining: 30m 9s\n",
      "4063:\tlearn: 5.1469605\ttotal: 20m 38s\tremaining: 30m 9s\n",
      "4064:\tlearn: 5.1462571\ttotal: 20m 39s\tremaining: 30m 8s\n",
      "4065:\tlearn: 5.1453462\ttotal: 20m 39s\tremaining: 30m 8s\n",
      "4066:\tlearn: 5.1446530\ttotal: 20m 39s\tremaining: 30m 8s\n",
      "4067:\tlearn: 5.1441042\ttotal: 20m 39s\tremaining: 30m 8s\n",
      "4068:\tlearn: 5.1433432\ttotal: 20m 40s\tremaining: 30m 7s\n",
      "4069:\tlearn: 5.1430160\ttotal: 20m 40s\tremaining: 30m 7s\n",
      "4070:\tlearn: 5.1424453\ttotal: 20m 41s\tremaining: 30m 7s\n",
      "4071:\tlearn: 5.1414724\ttotal: 20m 41s\tremaining: 30m 7s\n",
      "4072:\tlearn: 5.1405423\ttotal: 20m 41s\tremaining: 30m 7s\n",
      "4073:\tlearn: 5.1400130\ttotal: 20m 42s\tremaining: 30m 6s\n",
      "4074:\tlearn: 5.1396892\ttotal: 20m 42s\tremaining: 30m 6s\n",
      "4075:\tlearn: 5.1390934\ttotal: 20m 42s\tremaining: 30m 6s\n",
      "4076:\tlearn: 5.1385408\ttotal: 20m 43s\tremaining: 30m 5s\n",
      "4077:\tlearn: 5.1378774\ttotal: 20m 43s\tremaining: 30m 5s\n",
      "4078:\tlearn: 5.1369584\ttotal: 20m 43s\tremaining: 30m 5s\n",
      "4079:\tlearn: 5.1363228\ttotal: 20m 44s\tremaining: 30m 5s\n",
      "4080:\tlearn: 5.1358860\ttotal: 20m 44s\tremaining: 30m 5s\n",
      "4081:\tlearn: 5.1350188\ttotal: 20m 45s\tremaining: 30m 5s\n",
      "4082:\tlearn: 5.1342452\ttotal: 20m 45s\tremaining: 30m 4s\n",
      "4083:\tlearn: 5.1334911\ttotal: 20m 45s\tremaining: 30m 4s\n",
      "4084:\tlearn: 5.1329548\ttotal: 20m 46s\tremaining: 30m 4s\n",
      "4085:\tlearn: 5.1324858\ttotal: 20m 46s\tremaining: 30m 3s\n",
      "4086:\tlearn: 5.1319792\ttotal: 20m 46s\tremaining: 30m 3s\n",
      "4087:\tlearn: 5.1313392\ttotal: 20m 47s\tremaining: 30m 3s\n",
      "4088:\tlearn: 5.1305389\ttotal: 20m 47s\tremaining: 30m 3s\n",
      "4089:\tlearn: 5.1299530\ttotal: 20m 47s\tremaining: 30m 2s\n",
      "4090:\tlearn: 5.1293253\ttotal: 20m 47s\tremaining: 30m 2s\n",
      "4091:\tlearn: 5.1284136\ttotal: 20m 48s\tremaining: 30m 2s\n",
      "4092:\tlearn: 5.1279132\ttotal: 20m 48s\tremaining: 30m 2s\n",
      "4093:\tlearn: 5.1271522\ttotal: 20m 49s\tremaining: 30m 1s\n",
      "4094:\tlearn: 5.1261722\ttotal: 20m 49s\tremaining: 30m 1s\n",
      "4095:\tlearn: 5.1254562\ttotal: 20m 49s\tremaining: 30m 1s\n",
      "4096:\tlearn: 5.1247426\ttotal: 20m 50s\tremaining: 30m 1s\n",
      "4097:\tlearn: 5.1241833\ttotal: 20m 50s\tremaining: 30m 1s\n",
      "4098:\tlearn: 5.1236889\ttotal: 20m 50s\tremaining: 30m\n",
      "4099:\tlearn: 5.1230744\ttotal: 20m 51s\tremaining: 30m\n",
      "4100:\tlearn: 5.1223729\ttotal: 20m 51s\tremaining: 30m\n",
      "4101:\tlearn: 5.1216895\ttotal: 20m 51s\tremaining: 30m\n",
      "4102:\tlearn: 5.1210018\ttotal: 20m 52s\tremaining: 29m 59s\n",
      "4103:\tlearn: 5.1203637\ttotal: 20m 52s\tremaining: 29m 59s\n",
      "4104:\tlearn: 5.1196060\ttotal: 20m 52s\tremaining: 29m 59s\n",
      "4105:\tlearn: 5.1192046\ttotal: 20m 53s\tremaining: 29m 59s\n",
      "4106:\tlearn: 5.1181952\ttotal: 20m 53s\tremaining: 29m 58s\n",
      "4107:\tlearn: 5.1177470\ttotal: 20m 54s\tremaining: 29m 58s\n",
      "4108:\tlearn: 5.1173155\ttotal: 20m 54s\tremaining: 29m 58s\n",
      "4109:\tlearn: 5.1168542\ttotal: 20m 54s\tremaining: 29m 58s\n",
      "4110:\tlearn: 5.1157434\ttotal: 20m 54s\tremaining: 29m 57s\n",
      "4111:\tlearn: 5.1148919\ttotal: 20m 55s\tremaining: 29m 57s\n",
      "4112:\tlearn: 5.1140187\ttotal: 20m 55s\tremaining: 29m 57s\n",
      "4113:\tlearn: 5.1135375\ttotal: 20m 56s\tremaining: 29m 57s\n",
      "4114:\tlearn: 5.1129224\ttotal: 20m 56s\tremaining: 29m 57s\n",
      "4115:\tlearn: 5.1124868\ttotal: 20m 56s\tremaining: 29m 56s\n",
      "4116:\tlearn: 5.1119784\ttotal: 20m 57s\tremaining: 29m 56s\n",
      "4117:\tlearn: 5.1113877\ttotal: 20m 57s\tremaining: 29m 56s\n",
      "4118:\tlearn: 5.1109636\ttotal: 20m 57s\tremaining: 29m 55s\n",
      "4119:\tlearn: 5.1104902\ttotal: 20m 58s\tremaining: 29m 55s\n",
      "4120:\tlearn: 5.1099614\ttotal: 20m 58s\tremaining: 29m 55s\n",
      "4121:\tlearn: 5.1090250\ttotal: 20m 58s\tremaining: 29m 55s\n",
      "4122:\tlearn: 5.1083275\ttotal: 20m 59s\tremaining: 29m 54s\n",
      "4123:\tlearn: 5.1079114\ttotal: 20m 59s\tremaining: 29m 54s\n",
      "4124:\tlearn: 5.1070131\ttotal: 20m 59s\tremaining: 29m 54s\n",
      "4125:\tlearn: 5.1066151\ttotal: 21m\tremaining: 29m 53s\n",
      "4126:\tlearn: 5.1061286\ttotal: 21m\tremaining: 29m 53s\n",
      "4127:\tlearn: 5.1057248\ttotal: 21m\tremaining: 29m 53s\n",
      "4128:\tlearn: 5.1051756\ttotal: 21m\tremaining: 29m 53s\n",
      "4129:\tlearn: 5.1047291\ttotal: 21m 1s\tremaining: 29m 52s\n",
      "4130:\tlearn: 5.1037815\ttotal: 21m 1s\tremaining: 29m 52s\n",
      "4131:\tlearn: 5.1032682\ttotal: 21m 1s\tremaining: 29m 52s\n",
      "4132:\tlearn: 5.1027525\ttotal: 21m 2s\tremaining: 29m 51s\n",
      "4133:\tlearn: 5.1021223\ttotal: 21m 2s\tremaining: 29m 51s\n",
      "4134:\tlearn: 5.1014115\ttotal: 21m 2s\tremaining: 29m 51s\n",
      "4135:\tlearn: 5.1009703\ttotal: 21m 3s\tremaining: 29m 50s\n",
      "4136:\tlearn: 5.1005980\ttotal: 21m 3s\tremaining: 29m 50s\n",
      "4137:\tlearn: 5.1002114\ttotal: 21m 3s\tremaining: 29m 49s\n",
      "4138:\tlearn: 5.0999269\ttotal: 21m 3s\tremaining: 29m 49s\n",
      "4139:\tlearn: 5.0994436\ttotal: 21m 4s\tremaining: 29m 49s\n",
      "4140:\tlearn: 5.0989249\ttotal: 21m 4s\tremaining: 29m 48s\n",
      "4141:\tlearn: 5.0983625\ttotal: 21m 4s\tremaining: 29m 48s\n",
      "4142:\tlearn: 5.0978685\ttotal: 21m 4s\tremaining: 29m 48s\n",
      "4143:\tlearn: 5.0970758\ttotal: 21m 5s\tremaining: 29m 47s\n",
      "4144:\tlearn: 5.0966932\ttotal: 21m 5s\tremaining: 29m 47s\n",
      "4145:\tlearn: 5.0962629\ttotal: 21m 5s\tremaining: 29m 47s\n",
      "4146:\tlearn: 5.0959794\ttotal: 21m 6s\tremaining: 29m 46s\n",
      "4147:\tlearn: 5.0954139\ttotal: 21m 6s\tremaining: 29m 46s\n",
      "4148:\tlearn: 5.0948520\ttotal: 21m 6s\tremaining: 29m 46s\n",
      "4149:\tlearn: 5.0942707\ttotal: 21m 6s\tremaining: 29m 45s\n",
      "4150:\tlearn: 5.0935385\ttotal: 21m 7s\tremaining: 29m 45s\n",
      "4151:\tlearn: 5.0931365\ttotal: 21m 7s\tremaining: 29m 45s\n",
      "4152:\tlearn: 5.0922377\ttotal: 21m 7s\tremaining: 29m 44s\n",
      "4153:\tlearn: 5.0917009\ttotal: 21m 7s\tremaining: 29m 44s\n",
      "4154:\tlearn: 5.0909746\ttotal: 21m 8s\tremaining: 29m 44s\n",
      "4155:\tlearn: 5.0906568\ttotal: 21m 8s\tremaining: 29m 43s\n",
      "4156:\tlearn: 5.0901002\ttotal: 21m 8s\tremaining: 29m 43s\n",
      "4157:\tlearn: 5.0895596\ttotal: 21m 9s\tremaining: 29m 42s\n",
      "4158:\tlearn: 5.0890688\ttotal: 21m 9s\tremaining: 29m 42s\n",
      "4159:\tlearn: 5.0885928\ttotal: 21m 9s\tremaining: 29m 42s\n",
      "4160:\tlearn: 5.0881477\ttotal: 21m 9s\tremaining: 29m 41s\n",
      "4161:\tlearn: 5.0877140\ttotal: 21m 10s\tremaining: 29m 41s\n",
      "4162:\tlearn: 5.0871655\ttotal: 21m 10s\tremaining: 29m 41s\n",
      "4163:\tlearn: 5.0868111\ttotal: 21m 10s\tremaining: 29m 40s\n",
      "4164:\tlearn: 5.0861236\ttotal: 21m 11s\tremaining: 29m 40s\n",
      "4165:\tlearn: 5.0853927\ttotal: 21m 11s\tremaining: 29m 40s\n",
      "4166:\tlearn: 5.0849827\ttotal: 21m 11s\tremaining: 29m 40s\n",
      "4167:\tlearn: 5.0843655\ttotal: 21m 11s\tremaining: 29m 39s\n",
      "4168:\tlearn: 5.0836636\ttotal: 21m 12s\tremaining: 29m 39s\n",
      "4169:\tlearn: 5.0830330\ttotal: 21m 12s\tremaining: 29m 39s\n",
      "4170:\tlearn: 5.0825229\ttotal: 21m 12s\tremaining: 29m 38s\n",
      "4171:\tlearn: 5.0819369\ttotal: 21m 13s\tremaining: 29m 38s\n",
      "4172:\tlearn: 5.0814362\ttotal: 21m 13s\tremaining: 29m 38s\n",
      "4173:\tlearn: 5.0810929\ttotal: 21m 13s\tremaining: 29m 37s\n",
      "4174:\tlearn: 5.0805489\ttotal: 21m 13s\tremaining: 29m 37s\n",
      "4175:\tlearn: 5.0799511\ttotal: 21m 14s\tremaining: 29m 36s\n",
      "4176:\tlearn: 5.0795904\ttotal: 21m 14s\tremaining: 29m 36s\n",
      "4177:\tlearn: 5.0788628\ttotal: 21m 14s\tremaining: 29m 36s\n",
      "4178:\tlearn: 5.0780474\ttotal: 21m 15s\tremaining: 29m 36s\n",
      "4179:\tlearn: 5.0774819\ttotal: 21m 15s\tremaining: 29m 35s\n",
      "4180:\tlearn: 5.0769673\ttotal: 21m 15s\tremaining: 29m 35s\n",
      "4181:\tlearn: 5.0762611\ttotal: 21m 15s\tremaining: 29m 35s\n",
      "4182:\tlearn: 5.0756908\ttotal: 21m 16s\tremaining: 29m 34s\n",
      "4183:\tlearn: 5.0749633\ttotal: 21m 16s\tremaining: 29m 34s\n",
      "4184:\tlearn: 5.0743841\ttotal: 21m 16s\tremaining: 29m 34s\n",
      "4185:\tlearn: 5.0735905\ttotal: 21m 17s\tremaining: 29m 33s\n",
      "4186:\tlearn: 5.0727156\ttotal: 21m 17s\tremaining: 29m 33s\n",
      "4187:\tlearn: 5.0719260\ttotal: 21m 17s\tremaining: 29m 33s\n",
      "4188:\tlearn: 5.0714607\ttotal: 21m 18s\tremaining: 29m 33s\n",
      "4189:\tlearn: 5.0708726\ttotal: 21m 18s\tremaining: 29m 32s\n",
      "4190:\tlearn: 5.0702247\ttotal: 21m 18s\tremaining: 29m 32s\n",
      "4191:\tlearn: 5.0697831\ttotal: 21m 19s\tremaining: 29m 32s\n",
      "4192:\tlearn: 5.0691011\ttotal: 21m 19s\tremaining: 29m 32s\n",
      "4193:\tlearn: 5.0686393\ttotal: 21m 20s\tremaining: 29m 32s\n",
      "4194:\tlearn: 5.0680316\ttotal: 21m 20s\tremaining: 29m 31s\n",
      "4195:\tlearn: 5.0675345\ttotal: 21m 20s\tremaining: 29m 31s\n",
      "4196:\tlearn: 5.0666590\ttotal: 21m 21s\tremaining: 29m 31s\n",
      "4197:\tlearn: 5.0661169\ttotal: 21m 21s\tremaining: 29m 31s\n",
      "4198:\tlearn: 5.0657626\ttotal: 21m 21s\tremaining: 29m 30s\n",
      "4199:\tlearn: 5.0651717\ttotal: 21m 22s\tremaining: 29m 30s\n",
      "4200:\tlearn: 5.0643178\ttotal: 21m 22s\tremaining: 29m 30s\n",
      "4201:\tlearn: 5.0637871\ttotal: 21m 22s\tremaining: 29m 30s\n",
      "4202:\tlearn: 5.0630378\ttotal: 21m 23s\tremaining: 29m 29s\n",
      "4203:\tlearn: 5.0627013\ttotal: 21m 23s\tremaining: 29m 29s\n",
      "4204:\tlearn: 5.0622947\ttotal: 21m 23s\tremaining: 29m 29s\n",
      "4205:\tlearn: 5.0616652\ttotal: 21m 24s\tremaining: 29m 29s\n",
      "4206:\tlearn: 5.0611820\ttotal: 21m 24s\tremaining: 29m 28s\n",
      "4207:\tlearn: 5.0608094\ttotal: 21m 24s\tremaining: 29m 28s\n",
      "4208:\tlearn: 5.0602205\ttotal: 21m 25s\tremaining: 29m 28s\n",
      "4209:\tlearn: 5.0598573\ttotal: 21m 25s\tremaining: 29m 27s\n",
      "4210:\tlearn: 5.0592789\ttotal: 21m 25s\tremaining: 29m 27s\n",
      "4211:\tlearn: 5.0588155\ttotal: 21m 25s\tremaining: 29m 27s\n",
      "4212:\tlearn: 5.0584595\ttotal: 21m 26s\tremaining: 29m 26s\n",
      "4213:\tlearn: 5.0576730\ttotal: 21m 26s\tremaining: 29m 26s\n",
      "4214:\tlearn: 5.0573631\ttotal: 21m 26s\tremaining: 29m 26s\n",
      "4215:\tlearn: 5.0571140\ttotal: 21m 27s\tremaining: 29m 25s\n",
      "4216:\tlearn: 5.0563112\ttotal: 21m 27s\tremaining: 29m 25s\n",
      "4217:\tlearn: 5.0560380\ttotal: 21m 27s\tremaining: 29m 25s\n",
      "4218:\tlearn: 5.0556635\ttotal: 21m 28s\tremaining: 29m 24s\n",
      "4219:\tlearn: 5.0552834\ttotal: 21m 28s\tremaining: 29m 24s\n",
      "4220:\tlearn: 5.0547905\ttotal: 21m 28s\tremaining: 29m 24s\n",
      "4221:\tlearn: 5.0542960\ttotal: 21m 29s\tremaining: 29m 24s\n",
      "4222:\tlearn: 5.0538238\ttotal: 21m 29s\tremaining: 29m 23s\n",
      "4223:\tlearn: 5.0529440\ttotal: 21m 29s\tremaining: 29m 23s\n",
      "4224:\tlearn: 5.0524250\ttotal: 21m 30s\tremaining: 29m 23s\n",
      "4225:\tlearn: 5.0517524\ttotal: 21m 30s\tremaining: 29m 22s\n",
      "4226:\tlearn: 5.0510509\ttotal: 21m 30s\tremaining: 29m 22s\n",
      "4227:\tlearn: 5.0503445\ttotal: 21m 30s\tremaining: 29m 22s\n",
      "4228:\tlearn: 5.0497932\ttotal: 21m 31s\tremaining: 29m 22s\n",
      "4229:\tlearn: 5.0492080\ttotal: 21m 31s\tremaining: 29m 21s\n",
      "4230:\tlearn: 5.0488288\ttotal: 21m 31s\tremaining: 29m 21s\n",
      "4231:\tlearn: 5.0483759\ttotal: 21m 32s\tremaining: 29m 21s\n",
      "4232:\tlearn: 5.0478312\ttotal: 21m 32s\tremaining: 29m 20s\n",
      "4233:\tlearn: 5.0469889\ttotal: 21m 32s\tremaining: 29m 20s\n",
      "4234:\tlearn: 5.0460030\ttotal: 21m 33s\tremaining: 29m 20s\n",
      "4235:\tlearn: 5.0453848\ttotal: 21m 33s\tremaining: 29m 20s\n",
      "4236:\tlearn: 5.0450328\ttotal: 21m 33s\tremaining: 29m 19s\n",
      "4237:\tlearn: 5.0444296\ttotal: 21m 34s\tremaining: 29m 19s\n",
      "4238:\tlearn: 5.0439440\ttotal: 21m 34s\tremaining: 29m 19s\n",
      "4239:\tlearn: 5.0432602\ttotal: 21m 34s\tremaining: 29m 19s\n",
      "4240:\tlearn: 5.0427070\ttotal: 21m 35s\tremaining: 29m 18s\n",
      "4241:\tlearn: 5.0420990\ttotal: 21m 35s\tremaining: 29m 18s\n",
      "4242:\tlearn: 5.0413845\ttotal: 21m 35s\tremaining: 29m 18s\n",
      "4243:\tlearn: 5.0408347\ttotal: 21m 36s\tremaining: 29m 17s\n",
      "4244:\tlearn: 5.0402990\ttotal: 21m 36s\tremaining: 29m 17s\n",
      "4245:\tlearn: 5.0396384\ttotal: 21m 36s\tremaining: 29m 17s\n",
      "4246:\tlearn: 5.0390890\ttotal: 21m 36s\tremaining: 29m 16s\n",
      "4247:\tlearn: 5.0383622\ttotal: 21m 37s\tremaining: 29m 16s\n",
      "4248:\tlearn: 5.0374049\ttotal: 21m 37s\tremaining: 29m 16s\n",
      "4249:\tlearn: 5.0369977\ttotal: 21m 37s\tremaining: 29m 15s\n",
      "4250:\tlearn: 5.0364820\ttotal: 21m 38s\tremaining: 29m 15s\n",
      "4251:\tlearn: 5.0360494\ttotal: 21m 38s\tremaining: 29m 15s\n",
      "4252:\tlearn: 5.0353534\ttotal: 21m 38s\tremaining: 29m 14s\n",
      "4253:\tlearn: 5.0348503\ttotal: 21m 39s\tremaining: 29m 14s\n",
      "4254:\tlearn: 5.0339312\ttotal: 21m 39s\tremaining: 29m 14s\n",
      "4255:\tlearn: 5.0336295\ttotal: 21m 39s\tremaining: 29m 14s\n",
      "4256:\tlearn: 5.0331482\ttotal: 21m 40s\tremaining: 29m 13s\n",
      "4257:\tlearn: 5.0325838\ttotal: 21m 40s\tremaining: 29m 13s\n",
      "4258:\tlearn: 5.0317720\ttotal: 21m 40s\tremaining: 29m 13s\n",
      "4259:\tlearn: 5.0311074\ttotal: 21m 41s\tremaining: 29m 13s\n",
      "4260:\tlearn: 5.0306858\ttotal: 21m 41s\tremaining: 29m 12s\n",
      "4261:\tlearn: 5.0301524\ttotal: 21m 41s\tremaining: 29m 12s\n",
      "4262:\tlearn: 5.0297104\ttotal: 21m 42s\tremaining: 29m 12s\n",
      "4263:\tlearn: 5.0293260\ttotal: 21m 42s\tremaining: 29m 11s\n",
      "4264:\tlearn: 5.0288339\ttotal: 21m 42s\tremaining: 29m 11s\n",
      "4265:\tlearn: 5.0281340\ttotal: 21m 42s\tremaining: 29m 11s\n",
      "4266:\tlearn: 5.0277274\ttotal: 21m 43s\tremaining: 29m 11s\n",
      "4267:\tlearn: 5.0273799\ttotal: 21m 43s\tremaining: 29m 10s\n",
      "4268:\tlearn: 5.0267953\ttotal: 21m 43s\tremaining: 29m 10s\n",
      "4269:\tlearn: 5.0263868\ttotal: 21m 44s\tremaining: 29m 10s\n",
      "4270:\tlearn: 5.0259687\ttotal: 21m 44s\tremaining: 29m 9s\n",
      "4271:\tlearn: 5.0257128\ttotal: 21m 44s\tremaining: 29m 9s\n",
      "4272:\tlearn: 5.0253033\ttotal: 21m 45s\tremaining: 29m 9s\n",
      "4273:\tlearn: 5.0244654\ttotal: 21m 45s\tremaining: 29m 8s\n",
      "4274:\tlearn: 5.0239180\ttotal: 21m 45s\tremaining: 29m 8s\n",
      "4275:\tlearn: 5.0231281\ttotal: 21m 46s\tremaining: 29m 8s\n",
      "4276:\tlearn: 5.0224568\ttotal: 21m 46s\tremaining: 29m 8s\n",
      "4277:\tlearn: 5.0216077\ttotal: 21m 46s\tremaining: 29m 7s\n",
      "4278:\tlearn: 5.0209341\ttotal: 21m 47s\tremaining: 29m 7s\n",
      "4279:\tlearn: 5.0203842\ttotal: 21m 47s\tremaining: 29m 7s\n",
      "4280:\tlearn: 5.0200008\ttotal: 21m 47s\tremaining: 29m 7s\n",
      "4281:\tlearn: 5.0191912\ttotal: 21m 48s\tremaining: 29m 6s\n",
      "4282:\tlearn: 5.0187797\ttotal: 21m 48s\tremaining: 29m 6s\n",
      "4283:\tlearn: 5.0181919\ttotal: 21m 48s\tremaining: 29m 6s\n",
      "4284:\tlearn: 5.0178054\ttotal: 21m 49s\tremaining: 29m 6s\n",
      "4285:\tlearn: 5.0174451\ttotal: 21m 49s\tremaining: 29m 5s\n",
      "4286:\tlearn: 5.0171251\ttotal: 21m 49s\tremaining: 29m 5s\n",
      "4287:\tlearn: 5.0164001\ttotal: 21m 50s\tremaining: 29m 5s\n",
      "4288:\tlearn: 5.0157382\ttotal: 21m 50s\tremaining: 29m 4s\n",
      "4289:\tlearn: 5.0153130\ttotal: 21m 50s\tremaining: 29m 4s\n",
      "4290:\tlearn: 5.0147726\ttotal: 21m 51s\tremaining: 29m 4s\n",
      "4291:\tlearn: 5.0144285\ttotal: 21m 51s\tremaining: 29m 4s\n",
      "4292:\tlearn: 5.0139036\ttotal: 21m 51s\tremaining: 29m 4s\n",
      "4293:\tlearn: 5.0134687\ttotal: 21m 52s\tremaining: 29m 3s\n",
      "4294:\tlearn: 5.0131425\ttotal: 21m 52s\tremaining: 29m 3s\n",
      "4295:\tlearn: 5.0127591\ttotal: 21m 52s\tremaining: 29m 3s\n",
      "4296:\tlearn: 5.0119537\ttotal: 21m 53s\tremaining: 29m 2s\n",
      "4297:\tlearn: 5.0115832\ttotal: 21m 53s\tremaining: 29m 2s\n",
      "4298:\tlearn: 5.0108225\ttotal: 21m 53s\tremaining: 29m 2s\n",
      "4299:\tlearn: 5.0102071\ttotal: 21m 54s\tremaining: 29m 1s\n",
      "4300:\tlearn: 5.0097223\ttotal: 21m 54s\tremaining: 29m 1s\n",
      "4301:\tlearn: 5.0092056\ttotal: 21m 54s\tremaining: 29m 1s\n",
      "4302:\tlearn: 5.0085428\ttotal: 21m 55s\tremaining: 29m 1s\n",
      "4303:\tlearn: 5.0080423\ttotal: 21m 55s\tremaining: 29m\n",
      "4304:\tlearn: 5.0076191\ttotal: 21m 55s\tremaining: 29m\n",
      "4305:\tlearn: 5.0073000\ttotal: 21m 56s\tremaining: 29m\n",
      "4306:\tlearn: 5.0070112\ttotal: 21m 56s\tremaining: 28m 59s\n",
      "4307:\tlearn: 5.0061575\ttotal: 21m 56s\tremaining: 28m 59s\n",
      "4308:\tlearn: 5.0057718\ttotal: 21m 56s\tremaining: 28m 59s\n",
      "4309:\tlearn: 5.0050535\ttotal: 21m 57s\tremaining: 28m 59s\n",
      "4310:\tlearn: 5.0045243\ttotal: 21m 57s\tremaining: 28m 58s\n",
      "4311:\tlearn: 5.0038392\ttotal: 21m 57s\tremaining: 28m 58s\n",
      "4312:\tlearn: 5.0035629\ttotal: 21m 58s\tremaining: 28m 58s\n",
      "4313:\tlearn: 5.0031475\ttotal: 21m 58s\tremaining: 28m 57s\n",
      "4314:\tlearn: 5.0027643\ttotal: 21m 58s\tremaining: 28m 57s\n",
      "4315:\tlearn: 5.0017085\ttotal: 21m 59s\tremaining: 28m 57s\n",
      "4316:\tlearn: 5.0011773\ttotal: 21m 59s\tremaining: 28m 57s\n",
      "4317:\tlearn: 5.0007742\ttotal: 21m 59s\tremaining: 28m 56s\n",
      "4318:\tlearn: 5.0001914\ttotal: 22m\tremaining: 28m 56s\n",
      "4319:\tlearn: 4.9997982\ttotal: 22m\tremaining: 28m 56s\n",
      "4320:\tlearn: 4.9987856\ttotal: 22m\tremaining: 28m 56s\n",
      "4321:\tlearn: 4.9980141\ttotal: 22m 1s\tremaining: 28m 55s\n",
      "4322:\tlearn: 4.9977556\ttotal: 22m 1s\tremaining: 28m 55s\n",
      "4323:\tlearn: 4.9970399\ttotal: 22m 1s\tremaining: 28m 55s\n",
      "4324:\tlearn: 4.9961353\ttotal: 22m 2s\tremaining: 28m 55s\n",
      "4325:\tlearn: 4.9958085\ttotal: 22m 2s\tremaining: 28m 54s\n",
      "4326:\tlearn: 4.9951674\ttotal: 22m 2s\tremaining: 28m 54s\n",
      "4327:\tlearn: 4.9943883\ttotal: 22m 3s\tremaining: 28m 54s\n",
      "4328:\tlearn: 4.9938530\ttotal: 22m 3s\tremaining: 28m 53s\n",
      "4329:\tlearn: 4.9932644\ttotal: 22m 3s\tremaining: 28m 53s\n",
      "4330:\tlearn: 4.9927105\ttotal: 22m 4s\tremaining: 28m 53s\n",
      "4331:\tlearn: 4.9921768\ttotal: 22m 4s\tremaining: 28m 52s\n",
      "4332:\tlearn: 4.9915462\ttotal: 22m 4s\tremaining: 28m 52s\n",
      "4333:\tlearn: 4.9910587\ttotal: 22m 5s\tremaining: 28m 52s\n",
      "4334:\tlearn: 4.9906438\ttotal: 22m 5s\tremaining: 28m 52s\n",
      "4335:\tlearn: 4.9901170\ttotal: 22m 5s\tremaining: 28m 51s\n",
      "4336:\tlearn: 4.9897720\ttotal: 22m 6s\tremaining: 28m 51s\n",
      "4337:\tlearn: 4.9893832\ttotal: 22m 6s\tremaining: 28m 51s\n",
      "4338:\tlearn: 4.9888434\ttotal: 22m 6s\tremaining: 28m 50s\n",
      "4339:\tlearn: 4.9883743\ttotal: 22m 7s\tremaining: 28m 50s\n",
      "4340:\tlearn: 4.9880568\ttotal: 22m 7s\tremaining: 28m 50s\n",
      "4341:\tlearn: 4.9872649\ttotal: 22m 7s\tremaining: 28m 50s\n",
      "4342:\tlearn: 4.9865967\ttotal: 22m 8s\tremaining: 28m 49s\n",
      "4343:\tlearn: 4.9860148\ttotal: 22m 8s\tremaining: 28m 49s\n",
      "4344:\tlearn: 4.9856701\ttotal: 22m 8s\tremaining: 28m 49s\n",
      "4345:\tlearn: 4.9851662\ttotal: 22m 8s\tremaining: 28m 48s\n",
      "4346:\tlearn: 4.9848085\ttotal: 22m 9s\tremaining: 28m 48s\n",
      "4347:\tlearn: 4.9838944\ttotal: 22m 9s\tremaining: 28m 48s\n",
      "4348:\tlearn: 4.9834743\ttotal: 22m 9s\tremaining: 28m 48s\n",
      "4349:\tlearn: 4.9827765\ttotal: 22m 10s\tremaining: 28m 47s\n",
      "4350:\tlearn: 4.9820837\ttotal: 22m 10s\tremaining: 28m 47s\n",
      "4351:\tlearn: 4.9816443\ttotal: 22m 10s\tremaining: 28m 47s\n",
      "4352:\tlearn: 4.9811449\ttotal: 22m 11s\tremaining: 28m 46s\n",
      "4353:\tlearn: 4.9805117\ttotal: 22m 11s\tremaining: 28m 46s\n",
      "4354:\tlearn: 4.9800595\ttotal: 22m 11s\tremaining: 28m 46s\n",
      "4355:\tlearn: 4.9793948\ttotal: 22m 12s\tremaining: 28m 45s\n",
      "4356:\tlearn: 4.9784939\ttotal: 22m 12s\tremaining: 28m 45s\n",
      "4357:\tlearn: 4.9780085\ttotal: 22m 12s\tremaining: 28m 45s\n",
      "4358:\tlearn: 4.9776092\ttotal: 22m 12s\tremaining: 28m 44s\n",
      "4359:\tlearn: 4.9771587\ttotal: 22m 13s\tremaining: 28m 44s\n",
      "4360:\tlearn: 4.9766784\ttotal: 22m 13s\tremaining: 28m 44s\n",
      "4361:\tlearn: 4.9755751\ttotal: 22m 13s\tremaining: 28m 44s\n",
      "4362:\tlearn: 4.9752884\ttotal: 22m 14s\tremaining: 28m 43s\n",
      "4363:\tlearn: 4.9745829\ttotal: 22m 14s\tremaining: 28m 43s\n",
      "4364:\tlearn: 4.9738932\ttotal: 22m 14s\tremaining: 28m 43s\n",
      "4365:\tlearn: 4.9732645\ttotal: 22m 15s\tremaining: 28m 43s\n",
      "4366:\tlearn: 4.9727952\ttotal: 22m 15s\tremaining: 28m 42s\n",
      "4367:\tlearn: 4.9723237\ttotal: 22m 15s\tremaining: 28m 42s\n",
      "4368:\tlearn: 4.9720183\ttotal: 22m 16s\tremaining: 28m 42s\n",
      "4369:\tlearn: 4.9717590\ttotal: 22m 16s\tremaining: 28m 41s\n",
      "4370:\tlearn: 4.9710989\ttotal: 22m 16s\tremaining: 28m 41s\n",
      "4371:\tlearn: 4.9705739\ttotal: 22m 17s\tremaining: 28m 41s\n",
      "4372:\tlearn: 4.9702099\ttotal: 22m 17s\tremaining: 28m 40s\n",
      "4373:\tlearn: 4.9695958\ttotal: 22m 17s\tremaining: 28m 40s\n",
      "4374:\tlearn: 4.9691195\ttotal: 22m 17s\tremaining: 28m 40s\n",
      "4375:\tlearn: 4.9684002\ttotal: 22m 18s\tremaining: 28m 40s\n",
      "4376:\tlearn: 4.9675046\ttotal: 22m 18s\tremaining: 28m 39s\n",
      "4377:\tlearn: 4.9669471\ttotal: 22m 19s\tremaining: 28m 39s\n",
      "4378:\tlearn: 4.9662739\ttotal: 22m 19s\tremaining: 28m 39s\n",
      "4379:\tlearn: 4.9656205\ttotal: 22m 19s\tremaining: 28m 39s\n",
      "4380:\tlearn: 4.9653096\ttotal: 22m 20s\tremaining: 28m 38s\n",
      "4381:\tlearn: 4.9649197\ttotal: 22m 20s\tremaining: 28m 38s\n",
      "4382:\tlearn: 4.9643138\ttotal: 22m 20s\tremaining: 28m 38s\n",
      "4383:\tlearn: 4.9638106\ttotal: 22m 21s\tremaining: 28m 38s\n",
      "4384:\tlearn: 4.9633207\ttotal: 22m 21s\tremaining: 28m 37s\n",
      "4385:\tlearn: 4.9629230\ttotal: 22m 21s\tremaining: 28m 37s\n",
      "4386:\tlearn: 4.9620735\ttotal: 22m 22s\tremaining: 28m 37s\n",
      "4387:\tlearn: 4.9614995\ttotal: 22m 22s\tremaining: 28m 36s\n",
      "4388:\tlearn: 4.9607842\ttotal: 22m 22s\tremaining: 28m 36s\n",
      "4389:\tlearn: 4.9602480\ttotal: 22m 23s\tremaining: 28m 36s\n",
      "4390:\tlearn: 4.9597193\ttotal: 22m 23s\tremaining: 28m 36s\n",
      "4391:\tlearn: 4.9594201\ttotal: 22m 23s\tremaining: 28m 35s\n",
      "4392:\tlearn: 4.9584970\ttotal: 22m 24s\tremaining: 28m 35s\n",
      "4393:\tlearn: 4.9581901\ttotal: 22m 24s\tremaining: 28m 35s\n",
      "4394:\tlearn: 4.9575529\ttotal: 22m 24s\tremaining: 28m 34s\n",
      "4395:\tlearn: 4.9568704\ttotal: 22m 24s\tremaining: 28m 34s\n",
      "4396:\tlearn: 4.9561815\ttotal: 22m 25s\tremaining: 28m 34s\n",
      "4397:\tlearn: 4.9558550\ttotal: 22m 25s\tremaining: 28m 33s\n",
      "4398:\tlearn: 4.9553298\ttotal: 22m 25s\tremaining: 28m 33s\n",
      "4399:\tlearn: 4.9548435\ttotal: 22m 25s\tremaining: 28m 33s\n",
      "4400:\tlearn: 4.9542610\ttotal: 22m 26s\tremaining: 28m 32s\n",
      "4401:\tlearn: 4.9533102\ttotal: 22m 26s\tremaining: 28m 32s\n",
      "4402:\tlearn: 4.9528461\ttotal: 22m 26s\tremaining: 28m 32s\n",
      "4403:\tlearn: 4.9522483\ttotal: 22m 27s\tremaining: 28m 31s\n",
      "4404:\tlearn: 4.9515002\ttotal: 22m 27s\tremaining: 28m 31s\n",
      "4405:\tlearn: 4.9508618\ttotal: 22m 27s\tremaining: 28m 31s\n",
      "4406:\tlearn: 4.9503488\ttotal: 22m 28s\tremaining: 28m 30s\n",
      "4407:\tlearn: 4.9499831\ttotal: 22m 28s\tremaining: 28m 30s\n",
      "4408:\tlearn: 4.9493524\ttotal: 22m 28s\tremaining: 28m 30s\n",
      "4409:\tlearn: 4.9488052\ttotal: 22m 28s\tremaining: 28m 29s\n",
      "4410:\tlearn: 4.9481782\ttotal: 22m 29s\tremaining: 28m 29s\n",
      "4411:\tlearn: 4.9476702\ttotal: 22m 29s\tremaining: 28m 29s\n",
      "4412:\tlearn: 4.9470450\ttotal: 22m 29s\tremaining: 28m 28s\n",
      "4413:\tlearn: 4.9465113\ttotal: 22m 30s\tremaining: 28m 28s\n",
      "4414:\tlearn: 4.9460431\ttotal: 22m 30s\tremaining: 28m 28s\n",
      "4415:\tlearn: 4.9453211\ttotal: 22m 30s\tremaining: 28m 28s\n",
      "4416:\tlearn: 4.9445654\ttotal: 22m 31s\tremaining: 28m 27s\n",
      "4417:\tlearn: 4.9439320\ttotal: 22m 31s\tremaining: 28m 27s\n",
      "4418:\tlearn: 4.9435206\ttotal: 22m 31s\tremaining: 28m 27s\n",
      "4419:\tlearn: 4.9430141\ttotal: 22m 32s\tremaining: 28m 26s\n",
      "4420:\tlearn: 4.9423002\ttotal: 22m 32s\tremaining: 28m 26s\n",
      "4421:\tlearn: 4.9415900\ttotal: 22m 32s\tremaining: 28m 26s\n",
      "4422:\tlearn: 4.9410135\ttotal: 22m 32s\tremaining: 28m 25s\n",
      "4423:\tlearn: 4.9405027\ttotal: 22m 33s\tremaining: 28m 25s\n",
      "4424:\tlearn: 4.9401080\ttotal: 22m 33s\tremaining: 28m 25s\n",
      "4425:\tlearn: 4.9396189\ttotal: 22m 33s\tremaining: 28m 24s\n",
      "4426:\tlearn: 4.9389553\ttotal: 22m 34s\tremaining: 28m 24s\n",
      "4427:\tlearn: 4.9384677\ttotal: 22m 34s\tremaining: 28m 24s\n",
      "4428:\tlearn: 4.9380325\ttotal: 22m 34s\tremaining: 28m 23s\n",
      "4429:\tlearn: 4.9374752\ttotal: 22m 34s\tremaining: 28m 23s\n",
      "4430:\tlearn: 4.9364922\ttotal: 22m 35s\tremaining: 28m 23s\n",
      "4431:\tlearn: 4.9359527\ttotal: 22m 35s\tremaining: 28m 22s\n",
      "4432:\tlearn: 4.9355349\ttotal: 22m 35s\tremaining: 28m 22s\n",
      "4433:\tlearn: 4.9348989\ttotal: 22m 36s\tremaining: 28m 22s\n",
      "4434:\tlearn: 4.9345826\ttotal: 22m 36s\tremaining: 28m 21s\n",
      "4435:\tlearn: 4.9343130\ttotal: 22m 36s\tremaining: 28m 21s\n",
      "4436:\tlearn: 4.9338014\ttotal: 22m 36s\tremaining: 28m 21s\n",
      "4437:\tlearn: 4.9332109\ttotal: 22m 37s\tremaining: 28m 20s\n",
      "4438:\tlearn: 4.9324358\ttotal: 22m 37s\tremaining: 28m 20s\n",
      "4439:\tlearn: 4.9320681\ttotal: 22m 37s\tremaining: 28m 20s\n",
      "4440:\tlearn: 4.9314214\ttotal: 22m 38s\tremaining: 28m 19s\n",
      "4441:\tlearn: 4.9309867\ttotal: 22m 38s\tremaining: 28m 19s\n",
      "4442:\tlearn: 4.9306092\ttotal: 22m 38s\tremaining: 28m 19s\n",
      "4443:\tlearn: 4.9303202\ttotal: 22m 38s\tremaining: 28m 18s\n",
      "4444:\tlearn: 4.9298634\ttotal: 22m 39s\tremaining: 28m 18s\n",
      "4445:\tlearn: 4.9294295\ttotal: 22m 39s\tremaining: 28m 18s\n",
      "4446:\tlearn: 4.9287938\ttotal: 22m 39s\tremaining: 28m 17s\n",
      "4447:\tlearn: 4.9281472\ttotal: 22m 39s\tremaining: 28m 17s\n",
      "4448:\tlearn: 4.9276334\ttotal: 22m 40s\tremaining: 28m 17s\n",
      "4449:\tlearn: 4.9268437\ttotal: 22m 40s\tremaining: 28m 16s\n",
      "4450:\tlearn: 4.9261331\ttotal: 22m 40s\tremaining: 28m 16s\n",
      "4451:\tlearn: 4.9256529\ttotal: 22m 41s\tremaining: 28m 16s\n",
      "4452:\tlearn: 4.9254091\ttotal: 22m 41s\tremaining: 28m 15s\n",
      "4453:\tlearn: 4.9246917\ttotal: 22m 41s\tremaining: 28m 15s\n",
      "4454:\tlearn: 4.9237883\ttotal: 22m 41s\tremaining: 28m 15s\n",
      "4455:\tlearn: 4.9234330\ttotal: 22m 42s\tremaining: 28m 14s\n",
      "4456:\tlearn: 4.9227060\ttotal: 22m 42s\tremaining: 28m 14s\n",
      "4457:\tlearn: 4.9220815\ttotal: 22m 42s\tremaining: 28m 14s\n",
      "4458:\tlearn: 4.9217059\ttotal: 22m 43s\tremaining: 28m 13s\n",
      "4459:\tlearn: 4.9209487\ttotal: 22m 43s\tremaining: 28m 13s\n",
      "4460:\tlearn: 4.9206055\ttotal: 22m 43s\tremaining: 28m 13s\n",
      "4461:\tlearn: 4.9201066\ttotal: 22m 43s\tremaining: 28m 12s\n",
      "4462:\tlearn: 4.9193804\ttotal: 22m 44s\tremaining: 28m 12s\n",
      "4463:\tlearn: 4.9189305\ttotal: 22m 44s\tremaining: 28m 12s\n",
      "4464:\tlearn: 4.9181853\ttotal: 22m 44s\tremaining: 28m 11s\n",
      "4465:\tlearn: 4.9177388\ttotal: 22m 45s\tremaining: 28m 11s\n",
      "4466:\tlearn: 4.9173764\ttotal: 22m 45s\tremaining: 28m 11s\n",
      "4467:\tlearn: 4.9169453\ttotal: 22m 45s\tremaining: 28m 10s\n",
      "4468:\tlearn: 4.9164853\ttotal: 22m 45s\tremaining: 28m 10s\n",
      "4469:\tlearn: 4.9160077\ttotal: 22m 46s\tremaining: 28m 9s\n",
      "4470:\tlearn: 4.9154940\ttotal: 22m 46s\tremaining: 28m 9s\n",
      "4471:\tlearn: 4.9152283\ttotal: 22m 46s\tremaining: 28m 9s\n",
      "4472:\tlearn: 4.9147655\ttotal: 22m 46s\tremaining: 28m 9s\n",
      "4473:\tlearn: 4.9141050\ttotal: 22m 47s\tremaining: 28m 8s\n",
      "4474:\tlearn: 4.9134160\ttotal: 22m 47s\tremaining: 28m 8s\n",
      "4475:\tlearn: 4.9131368\ttotal: 22m 47s\tremaining: 28m 8s\n",
      "4476:\tlearn: 4.9125097\ttotal: 22m 48s\tremaining: 28m 8s\n",
      "4477:\tlearn: 4.9117570\ttotal: 22m 48s\tremaining: 28m 7s\n",
      "4478:\tlearn: 4.9111955\ttotal: 22m 49s\tremaining: 28m 7s\n",
      "4479:\tlearn: 4.9108048\ttotal: 22m 49s\tremaining: 28m 7s\n",
      "4480:\tlearn: 4.9100255\ttotal: 22m 49s\tremaining: 28m 7s\n",
      "4481:\tlearn: 4.9093832\ttotal: 22m 50s\tremaining: 28m 6s\n",
      "4482:\tlearn: 4.9087578\ttotal: 22m 50s\tremaining: 28m 6s\n",
      "4483:\tlearn: 4.9081223\ttotal: 22m 50s\tremaining: 28m 6s\n",
      "4484:\tlearn: 4.9075819\ttotal: 22m 51s\tremaining: 28m 5s\n",
      "4485:\tlearn: 4.9072619\ttotal: 22m 51s\tremaining: 28m 5s\n",
      "4486:\tlearn: 4.9065716\ttotal: 22m 51s\tremaining: 28m 5s\n",
      "4487:\tlearn: 4.9057942\ttotal: 22m 51s\tremaining: 28m 4s\n",
      "4488:\tlearn: 4.9053344\ttotal: 22m 52s\tremaining: 28m 4s\n",
      "4489:\tlearn: 4.9050208\ttotal: 22m 52s\tremaining: 28m 4s\n",
      "4490:\tlearn: 4.9043742\ttotal: 22m 52s\tremaining: 28m 4s\n",
      "4491:\tlearn: 4.9036898\ttotal: 22m 53s\tremaining: 28m 3s\n",
      "4492:\tlearn: 4.9031682\ttotal: 22m 53s\tremaining: 28m 3s\n",
      "4493:\tlearn: 4.9024124\ttotal: 22m 53s\tremaining: 28m 3s\n",
      "4494:\tlearn: 4.9021136\ttotal: 22m 54s\tremaining: 28m 2s\n",
      "4495:\tlearn: 4.9012649\ttotal: 22m 54s\tremaining: 28m 2s\n",
      "4496:\tlearn: 4.9006413\ttotal: 22m 54s\tremaining: 28m 2s\n",
      "4497:\tlearn: 4.9002796\ttotal: 22m 55s\tremaining: 28m 2s\n",
      "4498:\tlearn: 4.8998081\ttotal: 22m 55s\tremaining: 28m 1s\n",
      "4499:\tlearn: 4.8992003\ttotal: 22m 55s\tremaining: 28m 1s\n",
      "4500:\tlearn: 4.8983964\ttotal: 22m 55s\tremaining: 28m 1s\n",
      "4501:\tlearn: 4.8977475\ttotal: 22m 56s\tremaining: 28m\n",
      "4502:\tlearn: 4.8972367\ttotal: 22m 56s\tremaining: 28m\n",
      "4503:\tlearn: 4.8964219\ttotal: 22m 57s\tremaining: 28m\n",
      "4504:\tlearn: 4.8956469\ttotal: 22m 57s\tremaining: 28m\n",
      "4505:\tlearn: 4.8949675\ttotal: 22m 57s\tremaining: 27m 59s\n",
      "4506:\tlearn: 4.8944229\ttotal: 22m 58s\tremaining: 27m 59s\n",
      "4507:\tlearn: 4.8940782\ttotal: 22m 58s\tremaining: 27m 59s\n",
      "4508:\tlearn: 4.8933421\ttotal: 22m 58s\tremaining: 27m 58s\n",
      "4509:\tlearn: 4.8928876\ttotal: 22m 59s\tremaining: 27m 58s\n",
      "4510:\tlearn: 4.8920016\ttotal: 22m 59s\tremaining: 27m 58s\n",
      "4511:\tlearn: 4.8915444\ttotal: 22m 59s\tremaining: 27m 58s\n",
      "4512:\tlearn: 4.8911512\ttotal: 23m\tremaining: 27m 57s\n",
      "4513:\tlearn: 4.8909133\ttotal: 23m\tremaining: 27m 57s\n",
      "4514:\tlearn: 4.8903717\ttotal: 23m\tremaining: 27m 57s\n",
      "4515:\tlearn: 4.8897274\ttotal: 23m 1s\tremaining: 27m 57s\n",
      "4516:\tlearn: 4.8891352\ttotal: 23m 1s\tremaining: 27m 56s\n",
      "4517:\tlearn: 4.8886277\ttotal: 23m 1s\tremaining: 27m 56s\n",
      "4518:\tlearn: 4.8882995\ttotal: 23m 2s\tremaining: 27m 56s\n",
      "4519:\tlearn: 4.8875944\ttotal: 23m 2s\tremaining: 27m 56s\n",
      "4520:\tlearn: 4.8872650\ttotal: 23m 2s\tremaining: 27m 55s\n",
      "4521:\tlearn: 4.8867589\ttotal: 23m 3s\tremaining: 27m 55s\n",
      "4522:\tlearn: 4.8861514\ttotal: 23m 3s\tremaining: 27m 55s\n",
      "4523:\tlearn: 4.8853060\ttotal: 23m 3s\tremaining: 27m 55s\n",
      "4524:\tlearn: 4.8847172\ttotal: 23m 4s\tremaining: 27m 54s\n",
      "4525:\tlearn: 4.8843261\ttotal: 23m 4s\tremaining: 27m 54s\n",
      "4526:\tlearn: 4.8839766\ttotal: 23m 4s\tremaining: 27m 54s\n",
      "4527:\tlearn: 4.8836195\ttotal: 23m 5s\tremaining: 27m 54s\n",
      "4528:\tlearn: 4.8831927\ttotal: 23m 5s\tremaining: 27m 53s\n",
      "4529:\tlearn: 4.8827233\ttotal: 23m 5s\tremaining: 27m 53s\n",
      "4530:\tlearn: 4.8823476\ttotal: 23m 6s\tremaining: 27m 53s\n",
      "4531:\tlearn: 4.8817051\ttotal: 23m 6s\tremaining: 27m 52s\n",
      "4532:\tlearn: 4.8813042\ttotal: 23m 6s\tremaining: 27m 52s\n",
      "4533:\tlearn: 4.8807864\ttotal: 23m 7s\tremaining: 27m 52s\n",
      "4534:\tlearn: 4.8801260\ttotal: 23m 7s\tremaining: 27m 51s\n",
      "4535:\tlearn: 4.8797056\ttotal: 23m 7s\tremaining: 27m 51s\n",
      "4536:\tlearn: 4.8792605\ttotal: 23m 7s\tremaining: 27m 51s\n",
      "4537:\tlearn: 4.8786897\ttotal: 23m 8s\tremaining: 27m 50s\n",
      "4538:\tlearn: 4.8784470\ttotal: 23m 8s\tremaining: 27m 50s\n",
      "4539:\tlearn: 4.8781701\ttotal: 23m 8s\tremaining: 27m 50s\n",
      "4540:\tlearn: 4.8777135\ttotal: 23m 9s\tremaining: 27m 50s\n",
      "4541:\tlearn: 4.8773186\ttotal: 23m 9s\tremaining: 27m 49s\n",
      "4542:\tlearn: 4.8770074\ttotal: 23m 9s\tremaining: 27m 49s\n",
      "4543:\tlearn: 4.8765094\ttotal: 23m 10s\tremaining: 27m 49s\n",
      "4544:\tlearn: 4.8758101\ttotal: 23m 10s\tremaining: 27m 48s\n",
      "4545:\tlearn: 4.8751670\ttotal: 23m 10s\tremaining: 27m 48s\n",
      "4546:\tlearn: 4.8746659\ttotal: 23m 11s\tremaining: 27m 48s\n",
      "4547:\tlearn: 4.8739709\ttotal: 23m 11s\tremaining: 27m 48s\n",
      "4548:\tlearn: 4.8731430\ttotal: 23m 11s\tremaining: 27m 47s\n",
      "4549:\tlearn: 4.8726598\ttotal: 23m 12s\tremaining: 27m 47s\n",
      "4550:\tlearn: 4.8718605\ttotal: 23m 12s\tremaining: 27m 47s\n",
      "4551:\tlearn: 4.8714017\ttotal: 23m 12s\tremaining: 27m 46s\n",
      "4552:\tlearn: 4.8709173\ttotal: 23m 13s\tremaining: 27m 46s\n",
      "4553:\tlearn: 4.8700496\ttotal: 23m 13s\tremaining: 27m 46s\n",
      "4554:\tlearn: 4.8695050\ttotal: 23m 13s\tremaining: 27m 46s\n",
      "4555:\tlearn: 4.8690962\ttotal: 23m 14s\tremaining: 27m 45s\n",
      "4556:\tlearn: 4.8686394\ttotal: 23m 14s\tremaining: 27m 45s\n",
      "4557:\tlearn: 4.8679770\ttotal: 23m 14s\tremaining: 27m 45s\n",
      "4558:\tlearn: 4.8672062\ttotal: 23m 15s\tremaining: 27m 45s\n",
      "4559:\tlearn: 4.8665601\ttotal: 23m 15s\tremaining: 27m 45s\n",
      "4560:\tlearn: 4.8661870\ttotal: 23m 16s\tremaining: 27m 44s\n",
      "4561:\tlearn: 4.8655824\ttotal: 23m 16s\tremaining: 27m 44s\n",
      "4562:\tlearn: 4.8649097\ttotal: 23m 16s\tremaining: 27m 44s\n",
      "4563:\tlearn: 4.8642691\ttotal: 23m 17s\tremaining: 27m 44s\n",
      "4564:\tlearn: 4.8638672\ttotal: 23m 17s\tremaining: 27m 43s\n",
      "4565:\tlearn: 4.8635347\ttotal: 23m 17s\tremaining: 27m 43s\n",
      "4566:\tlearn: 4.8627183\ttotal: 23m 18s\tremaining: 27m 43s\n",
      "4567:\tlearn: 4.8624983\ttotal: 23m 18s\tremaining: 27m 42s\n",
      "4568:\tlearn: 4.8620857\ttotal: 23m 18s\tremaining: 27m 42s\n",
      "4569:\tlearn: 4.8612283\ttotal: 23m 18s\tremaining: 27m 42s\n",
      "4570:\tlearn: 4.8608289\ttotal: 23m 19s\tremaining: 27m 41s\n",
      "4571:\tlearn: 4.8604029\ttotal: 23m 19s\tremaining: 27m 41s\n",
      "4572:\tlearn: 4.8597202\ttotal: 23m 19s\tremaining: 27m 41s\n",
      "4573:\tlearn: 4.8593945\ttotal: 23m 20s\tremaining: 27m 41s\n",
      "4574:\tlearn: 4.8590044\ttotal: 23m 20s\tremaining: 27m 40s\n",
      "4575:\tlearn: 4.8586139\ttotal: 23m 20s\tremaining: 27m 40s\n",
      "4576:\tlearn: 4.8582502\ttotal: 23m 21s\tremaining: 27m 40s\n",
      "4577:\tlearn: 4.8575876\ttotal: 23m 21s\tremaining: 27m 39s\n",
      "4578:\tlearn: 4.8567346\ttotal: 23m 21s\tremaining: 27m 39s\n",
      "4579:\tlearn: 4.8563149\ttotal: 23m 22s\tremaining: 27m 39s\n",
      "4580:\tlearn: 4.8560831\ttotal: 23m 22s\tremaining: 27m 39s\n",
      "4581:\tlearn: 4.8554278\ttotal: 23m 22s\tremaining: 27m 38s\n",
      "4582:\tlearn: 4.8548238\ttotal: 23m 23s\tremaining: 27m 38s\n",
      "4583:\tlearn: 4.8544704\ttotal: 23m 23s\tremaining: 27m 38s\n",
      "4584:\tlearn: 4.8541015\ttotal: 23m 23s\tremaining: 27m 37s\n",
      "4585:\tlearn: 4.8538294\ttotal: 23m 24s\tremaining: 27m 37s\n",
      "4586:\tlearn: 4.8532309\ttotal: 23m 24s\tremaining: 27m 37s\n",
      "4587:\tlearn: 4.8526767\ttotal: 23m 24s\tremaining: 27m 37s\n",
      "4588:\tlearn: 4.8522503\ttotal: 23m 25s\tremaining: 27m 36s\n",
      "4589:\tlearn: 4.8518159\ttotal: 23m 25s\tremaining: 27m 36s\n",
      "4590:\tlearn: 4.8514395\ttotal: 23m 25s\tremaining: 27m 36s\n",
      "4591:\tlearn: 4.8508492\ttotal: 23m 26s\tremaining: 27m 35s\n",
      "4592:\tlearn: 4.8500950\ttotal: 23m 26s\tremaining: 27m 35s\n",
      "4593:\tlearn: 4.8494954\ttotal: 23m 26s\tremaining: 27m 35s\n",
      "4594:\tlearn: 4.8492771\ttotal: 23m 27s\tremaining: 27m 35s\n",
      "4595:\tlearn: 4.8485413\ttotal: 23m 27s\tremaining: 27m 34s\n",
      "4596:\tlearn: 4.8480591\ttotal: 23m 27s\tremaining: 27m 34s\n",
      "4597:\tlearn: 4.8475544\ttotal: 23m 28s\tremaining: 27m 34s\n",
      "4598:\tlearn: 4.8469530\ttotal: 23m 28s\tremaining: 27m 34s\n",
      "4599:\tlearn: 4.8463466\ttotal: 23m 28s\tremaining: 27m 33s\n",
      "4600:\tlearn: 4.8460358\ttotal: 23m 29s\tremaining: 27m 33s\n",
      "4601:\tlearn: 4.8455269\ttotal: 23m 29s\tremaining: 27m 33s\n",
      "4602:\tlearn: 4.8444423\ttotal: 23m 29s\tremaining: 27m 32s\n",
      "4603:\tlearn: 4.8439741\ttotal: 23m 30s\tremaining: 27m 32s\n",
      "4604:\tlearn: 4.8434599\ttotal: 23m 30s\tremaining: 27m 32s\n",
      "4605:\tlearn: 4.8430618\ttotal: 23m 30s\tremaining: 27m 32s\n",
      "4606:\tlearn: 4.8425954\ttotal: 23m 31s\tremaining: 27m 31s\n",
      "4607:\tlearn: 4.8421578\ttotal: 23m 31s\tremaining: 27m 31s\n",
      "4608:\tlearn: 4.8414602\ttotal: 23m 31s\tremaining: 27m 31s\n",
      "4609:\tlearn: 4.8411713\ttotal: 23m 32s\tremaining: 27m 30s\n",
      "4610:\tlearn: 4.8405771\ttotal: 23m 32s\tremaining: 27m 30s\n",
      "4611:\tlearn: 4.8400141\ttotal: 23m 32s\tremaining: 27m 30s\n",
      "4612:\tlearn: 4.8396311\ttotal: 23m 32s\tremaining: 27m 29s\n",
      "4613:\tlearn: 4.8390585\ttotal: 23m 33s\tremaining: 27m 29s\n",
      "4614:\tlearn: 4.8387365\ttotal: 23m 33s\tremaining: 27m 29s\n",
      "4615:\tlearn: 4.8381167\ttotal: 23m 33s\tremaining: 27m 29s\n",
      "4616:\tlearn: 4.8374487\ttotal: 23m 34s\tremaining: 27m 28s\n",
      "4617:\tlearn: 4.8370999\ttotal: 23m 34s\tremaining: 27m 28s\n",
      "4618:\tlearn: 4.8364843\ttotal: 23m 34s\tremaining: 27m 28s\n",
      "4619:\tlearn: 4.8361243\ttotal: 23m 35s\tremaining: 27m 28s\n",
      "4620:\tlearn: 4.8357694\ttotal: 23m 35s\tremaining: 27m 27s\n",
      "4621:\tlearn: 4.8353878\ttotal: 23m 35s\tremaining: 27m 27s\n",
      "4622:\tlearn: 4.8350245\ttotal: 23m 36s\tremaining: 27m 27s\n",
      "4623:\tlearn: 4.8347398\ttotal: 23m 36s\tremaining: 27m 26s\n",
      "4624:\tlearn: 4.8340856\ttotal: 23m 36s\tremaining: 27m 26s\n",
      "4625:\tlearn: 4.8336857\ttotal: 23m 37s\tremaining: 27m 26s\n",
      "4626:\tlearn: 4.8329704\ttotal: 23m 37s\tremaining: 27m 25s\n",
      "4627:\tlearn: 4.8323214\ttotal: 23m 37s\tremaining: 27m 25s\n",
      "4628:\tlearn: 4.8317896\ttotal: 23m 38s\tremaining: 27m 25s\n",
      "4629:\tlearn: 4.8311478\ttotal: 23m 38s\tremaining: 27m 25s\n",
      "4630:\tlearn: 4.8306995\ttotal: 23m 38s\tremaining: 27m 24s\n",
      "4631:\tlearn: 4.8302758\ttotal: 23m 39s\tremaining: 27m 24s\n",
      "4632:\tlearn: 4.8297825\ttotal: 23m 39s\tremaining: 27m 24s\n",
      "4633:\tlearn: 4.8291944\ttotal: 23m 39s\tremaining: 27m 24s\n",
      "4634:\tlearn: 4.8286827\ttotal: 23m 40s\tremaining: 27m 23s\n",
      "4635:\tlearn: 4.8282281\ttotal: 23m 40s\tremaining: 27m 23s\n",
      "4636:\tlearn: 4.8278280\ttotal: 23m 40s\tremaining: 27m 23s\n",
      "4637:\tlearn: 4.8274339\ttotal: 23m 41s\tremaining: 27m 23s\n",
      "4638:\tlearn: 4.8269498\ttotal: 23m 41s\tremaining: 27m 22s\n",
      "4639:\tlearn: 4.8257022\ttotal: 23m 41s\tremaining: 27m 22s\n",
      "4640:\tlearn: 4.8252374\ttotal: 23m 42s\tremaining: 27m 22s\n",
      "4641:\tlearn: 4.8247739\ttotal: 23m 42s\tremaining: 27m 21s\n",
      "4642:\tlearn: 4.8239517\ttotal: 23m 42s\tremaining: 27m 21s\n",
      "4643:\tlearn: 4.8235264\ttotal: 23m 43s\tremaining: 27m 21s\n",
      "4644:\tlearn: 4.8231332\ttotal: 23m 43s\tremaining: 27m 21s\n",
      "4645:\tlearn: 4.8223481\ttotal: 23m 43s\tremaining: 27m 20s\n",
      "4646:\tlearn: 4.8215839\ttotal: 23m 44s\tremaining: 27m 20s\n",
      "4647:\tlearn: 4.8212301\ttotal: 23m 44s\tremaining: 27m 20s\n",
      "4648:\tlearn: 4.8209062\ttotal: 23m 44s\tremaining: 27m 19s\n",
      "4649:\tlearn: 4.8202956\ttotal: 23m 45s\tremaining: 27m 19s\n",
      "4650:\tlearn: 4.8198963\ttotal: 23m 45s\tremaining: 27m 19s\n",
      "4651:\tlearn: 4.8195935\ttotal: 23m 45s\tremaining: 27m 18s\n",
      "4652:\tlearn: 4.8191209\ttotal: 23m 46s\tremaining: 27m 18s\n",
      "4653:\tlearn: 4.8183369\ttotal: 23m 46s\tremaining: 27m 18s\n",
      "4654:\tlearn: 4.8173981\ttotal: 23m 46s\tremaining: 27m 18s\n",
      "4655:\tlearn: 4.8170995\ttotal: 23m 47s\tremaining: 27m 17s\n",
      "4656:\tlearn: 4.8164372\ttotal: 23m 47s\tremaining: 27m 17s\n",
      "4657:\tlearn: 4.8157730\ttotal: 23m 47s\tremaining: 27m 17s\n",
      "4658:\tlearn: 4.8153329\ttotal: 23m 48s\tremaining: 27m 17s\n",
      "4659:\tlearn: 4.8147661\ttotal: 23m 48s\tremaining: 27m 16s\n",
      "4660:\tlearn: 4.8143378\ttotal: 23m 48s\tremaining: 27m 16s\n",
      "4661:\tlearn: 4.8135307\ttotal: 23m 49s\tremaining: 27m 16s\n",
      "4662:\tlearn: 4.8123534\ttotal: 23m 49s\tremaining: 27m 16s\n",
      "4663:\tlearn: 4.8115751\ttotal: 23m 49s\tremaining: 27m 15s\n",
      "4664:\tlearn: 4.8111041\ttotal: 23m 50s\tremaining: 27m 15s\n",
      "4665:\tlearn: 4.8102795\ttotal: 23m 50s\tremaining: 27m 15s\n",
      "4666:\tlearn: 4.8100151\ttotal: 23m 51s\tremaining: 27m 15s\n",
      "4667:\tlearn: 4.8096701\ttotal: 23m 51s\tremaining: 27m 14s\n",
      "4668:\tlearn: 4.8091785\ttotal: 23m 51s\tremaining: 27m 14s\n",
      "4669:\tlearn: 4.8086811\ttotal: 23m 51s\tremaining: 27m 14s\n",
      "4670:\tlearn: 4.8080891\ttotal: 23m 52s\tremaining: 27m 14s\n",
      "4671:\tlearn: 4.8075173\ttotal: 23m 52s\tremaining: 27m 13s\n",
      "4672:\tlearn: 4.8070376\ttotal: 23m 52s\tremaining: 27m 13s\n",
      "4673:\tlearn: 4.8066488\ttotal: 23m 53s\tremaining: 27m 13s\n",
      "4674:\tlearn: 4.8062941\ttotal: 23m 53s\tremaining: 27m 12s\n",
      "4675:\tlearn: 4.8058323\ttotal: 23m 53s\tremaining: 27m 12s\n",
      "4676:\tlearn: 4.8049822\ttotal: 23m 54s\tremaining: 27m 12s\n",
      "4677:\tlearn: 4.8043902\ttotal: 23m 54s\tremaining: 27m 11s\n",
      "4678:\tlearn: 4.8036865\ttotal: 23m 54s\tremaining: 27m 11s\n",
      "4679:\tlearn: 4.8031683\ttotal: 23m 55s\tremaining: 27m 11s\n",
      "4680:\tlearn: 4.8028624\ttotal: 23m 55s\tremaining: 27m 10s\n",
      "4681:\tlearn: 4.8024132\ttotal: 23m 55s\tremaining: 27m 10s\n",
      "4682:\tlearn: 4.8017722\ttotal: 23m 56s\tremaining: 27m 10s\n",
      "4683:\tlearn: 4.8014638\ttotal: 23m 56s\tremaining: 27m 10s\n",
      "4684:\tlearn: 4.8008865\ttotal: 23m 56s\tremaining: 27m 9s\n",
      "4685:\tlearn: 4.8004507\ttotal: 23m 56s\tremaining: 27m 9s\n",
      "4686:\tlearn: 4.7999465\ttotal: 23m 57s\tremaining: 27m 9s\n",
      "4687:\tlearn: 4.7995673\ttotal: 23m 57s\tremaining: 27m 8s\n",
      "4688:\tlearn: 4.7990726\ttotal: 23m 57s\tremaining: 27m 8s\n",
      "4689:\tlearn: 4.7987367\ttotal: 23m 58s\tremaining: 27m 8s\n",
      "4690:\tlearn: 4.7982740\ttotal: 23m 58s\tremaining: 27m 8s\n",
      "4691:\tlearn: 4.7976466\ttotal: 23m 58s\tremaining: 27m 7s\n",
      "4692:\tlearn: 4.7971047\ttotal: 23m 59s\tremaining: 27m 7s\n",
      "4693:\tlearn: 4.7964328\ttotal: 23m 59s\tremaining: 27m 7s\n",
      "4694:\tlearn: 4.7956204\ttotal: 24m\tremaining: 27m 7s\n",
      "4695:\tlearn: 4.7953147\ttotal: 24m\tremaining: 27m 6s\n",
      "4696:\tlearn: 4.7948691\ttotal: 24m\tremaining: 27m 6s\n",
      "4697:\tlearn: 4.7943001\ttotal: 24m 1s\tremaining: 27m 6s\n",
      "4698:\tlearn: 4.7940586\ttotal: 24m 1s\tremaining: 27m 5s\n",
      "4699:\tlearn: 4.7935872\ttotal: 24m 1s\tremaining: 27m 5s\n",
      "4700:\tlearn: 4.7928302\ttotal: 24m 1s\tremaining: 27m 5s\n",
      "4701:\tlearn: 4.7923652\ttotal: 24m 2s\tremaining: 27m 5s\n",
      "4702:\tlearn: 4.7918794\ttotal: 24m 2s\tremaining: 27m 4s\n",
      "4703:\tlearn: 4.7915845\ttotal: 24m 2s\tremaining: 27m 4s\n",
      "4704:\tlearn: 4.7909432\ttotal: 24m 3s\tremaining: 27m 4s\n",
      "4705:\tlearn: 4.7904075\ttotal: 24m 3s\tremaining: 27m 3s\n",
      "4706:\tlearn: 4.7897165\ttotal: 24m 3s\tremaining: 27m 3s\n",
      "4707:\tlearn: 4.7890092\ttotal: 24m 4s\tremaining: 27m 3s\n",
      "4708:\tlearn: 4.7886245\ttotal: 24m 4s\tremaining: 27m 3s\n",
      "4709:\tlearn: 4.7882728\ttotal: 24m 4s\tremaining: 27m 2s\n",
      "4710:\tlearn: 4.7879754\ttotal: 24m 5s\tremaining: 27m 2s\n",
      "4711:\tlearn: 4.7872948\ttotal: 24m 5s\tremaining: 27m 2s\n",
      "4712:\tlearn: 4.7867853\ttotal: 24m 5s\tremaining: 27m 1s\n",
      "4713:\tlearn: 4.7865534\ttotal: 24m 5s\tremaining: 27m 1s\n",
      "4714:\tlearn: 4.7861653\ttotal: 24m 6s\tremaining: 27m 1s\n",
      "4715:\tlearn: 4.7854646\ttotal: 24m 6s\tremaining: 27m\n",
      "4716:\tlearn: 4.7850339\ttotal: 24m 6s\tremaining: 27m\n",
      "4717:\tlearn: 4.7844763\ttotal: 24m 7s\tremaining: 27m\n",
      "4718:\tlearn: 4.7837720\ttotal: 24m 7s\tremaining: 26m 59s\n",
      "4719:\tlearn: 4.7831694\ttotal: 24m 7s\tremaining: 26m 59s\n",
      "4720:\tlearn: 4.7828199\ttotal: 24m 8s\tremaining: 26m 59s\n",
      "4721:\tlearn: 4.7825021\ttotal: 24m 8s\tremaining: 26m 59s\n",
      "4722:\tlearn: 4.7822604\ttotal: 24m 8s\tremaining: 26m 58s\n",
      "4723:\tlearn: 4.7816460\ttotal: 24m 9s\tremaining: 26m 58s\n",
      "4724:\tlearn: 4.7808524\ttotal: 24m 9s\tremaining: 26m 58s\n",
      "4725:\tlearn: 4.7804683\ttotal: 24m 9s\tremaining: 26m 57s\n",
      "4726:\tlearn: 4.7801476\ttotal: 24m 10s\tremaining: 26m 57s\n",
      "4727:\tlearn: 4.7795817\ttotal: 24m 10s\tremaining: 26m 57s\n",
      "4728:\tlearn: 4.7790954\ttotal: 24m 10s\tremaining: 26m 57s\n",
      "4729:\tlearn: 4.7783900\ttotal: 24m 11s\tremaining: 26m 56s\n",
      "4730:\tlearn: 4.7781829\ttotal: 24m 11s\tremaining: 26m 56s\n",
      "4731:\tlearn: 4.7779308\ttotal: 24m 11s\tremaining: 26m 56s\n",
      "4732:\tlearn: 4.7772666\ttotal: 24m 12s\tremaining: 26m 55s\n",
      "4733:\tlearn: 4.7763343\ttotal: 24m 12s\tremaining: 26m 55s\n",
      "4734:\tlearn: 4.7759392\ttotal: 24m 12s\tremaining: 26m 55s\n",
      "4735:\tlearn: 4.7755044\ttotal: 24m 13s\tremaining: 26m 55s\n",
      "4736:\tlearn: 4.7748102\ttotal: 24m 13s\tremaining: 26m 54s\n",
      "4737:\tlearn: 4.7743061\ttotal: 24m 13s\tremaining: 26m 54s\n",
      "4738:\tlearn: 4.7737137\ttotal: 24m 14s\tremaining: 26m 54s\n",
      "4739:\tlearn: 4.7735210\ttotal: 24m 14s\tremaining: 26m 53s\n",
      "4740:\tlearn: 4.7730372\ttotal: 24m 14s\tremaining: 26m 53s\n",
      "4741:\tlearn: 4.7726496\ttotal: 24m 14s\tremaining: 26m 53s\n",
      "4742:\tlearn: 4.7720437\ttotal: 24m 15s\tremaining: 26m 53s\n",
      "4743:\tlearn: 4.7715408\ttotal: 24m 15s\tremaining: 26m 52s\n",
      "4744:\tlearn: 4.7710965\ttotal: 24m 16s\tremaining: 26m 52s\n",
      "4745:\tlearn: 4.7703226\ttotal: 24m 16s\tremaining: 26m 52s\n",
      "4746:\tlearn: 4.7699448\ttotal: 24m 16s\tremaining: 26m 52s\n",
      "4747:\tlearn: 4.7696382\ttotal: 24m 17s\tremaining: 26m 51s\n",
      "4748:\tlearn: 4.7687580\ttotal: 24m 17s\tremaining: 26m 51s\n",
      "4749:\tlearn: 4.7683402\ttotal: 24m 17s\tremaining: 26m 51s\n",
      "4750:\tlearn: 4.7679103\ttotal: 24m 18s\tremaining: 26m 50s\n",
      "4751:\tlearn: 4.7672962\ttotal: 24m 18s\tremaining: 26m 50s\n",
      "4752:\tlearn: 4.7665766\ttotal: 24m 18s\tremaining: 26m 50s\n",
      "4753:\tlearn: 4.7660140\ttotal: 24m 19s\tremaining: 26m 50s\n",
      "4754:\tlearn: 4.7655508\ttotal: 24m 19s\tremaining: 26m 49s\n",
      "4755:\tlearn: 4.7650966\ttotal: 24m 19s\tremaining: 26m 49s\n",
      "4756:\tlearn: 4.7647099\ttotal: 24m 20s\tremaining: 26m 49s\n",
      "4757:\tlearn: 4.7642903\ttotal: 24m 20s\tremaining: 26m 49s\n",
      "4758:\tlearn: 4.7639691\ttotal: 24m 20s\tremaining: 26m 48s\n",
      "4759:\tlearn: 4.7633692\ttotal: 24m 21s\tremaining: 26m 48s\n",
      "4760:\tlearn: 4.7627794\ttotal: 24m 21s\tremaining: 26m 48s\n",
      "4761:\tlearn: 4.7625011\ttotal: 24m 21s\tremaining: 26m 47s\n",
      "4762:\tlearn: 4.7617372\ttotal: 24m 22s\tremaining: 26m 47s\n",
      "4763:\tlearn: 4.7613796\ttotal: 24m 22s\tremaining: 26m 47s\n",
      "4764:\tlearn: 4.7610021\ttotal: 24m 22s\tremaining: 26m 46s\n",
      "4765:\tlearn: 4.7601131\ttotal: 24m 23s\tremaining: 26m 46s\n",
      "4766:\tlearn: 4.7594787\ttotal: 24m 23s\tremaining: 26m 46s\n",
      "4767:\tlearn: 4.7587743\ttotal: 24m 23s\tremaining: 26m 46s\n",
      "4768:\tlearn: 4.7582271\ttotal: 24m 24s\tremaining: 26m 45s\n",
      "4769:\tlearn: 4.7575498\ttotal: 24m 24s\tremaining: 26m 45s\n",
      "4770:\tlearn: 4.7572097\ttotal: 24m 24s\tremaining: 26m 45s\n",
      "4771:\tlearn: 4.7567505\ttotal: 24m 25s\tremaining: 26m 45s\n",
      "4772:\tlearn: 4.7563507\ttotal: 24m 25s\tremaining: 26m 44s\n",
      "4773:\tlearn: 4.7557510\ttotal: 24m 25s\tremaining: 26m 44s\n",
      "4774:\tlearn: 4.7553300\ttotal: 24m 26s\tremaining: 26m 44s\n",
      "4775:\tlearn: 4.7544852\ttotal: 24m 26s\tremaining: 26m 43s\n",
      "4776:\tlearn: 4.7539036\ttotal: 24m 26s\tremaining: 26m 43s\n",
      "4777:\tlearn: 4.7532608\ttotal: 24m 27s\tremaining: 26m 43s\n",
      "4778:\tlearn: 4.7527141\ttotal: 24m 27s\tremaining: 26m 43s\n",
      "4779:\tlearn: 4.7523482\ttotal: 24m 27s\tremaining: 26m 42s\n",
      "4780:\tlearn: 4.7520029\ttotal: 24m 28s\tremaining: 26m 42s\n",
      "4781:\tlearn: 4.7514379\ttotal: 24m 28s\tremaining: 26m 42s\n",
      "4782:\tlearn: 4.7508646\ttotal: 24m 28s\tremaining: 26m 41s\n",
      "4783:\tlearn: 4.7504779\ttotal: 24m 28s\tremaining: 26m 41s\n",
      "4784:\tlearn: 4.7501843\ttotal: 24m 29s\tremaining: 26m 41s\n",
      "4785:\tlearn: 4.7496281\ttotal: 24m 29s\tremaining: 26m 40s\n",
      "4786:\tlearn: 4.7488524\ttotal: 24m 29s\tremaining: 26m 40s\n",
      "4787:\tlearn: 4.7483045\ttotal: 24m 30s\tremaining: 26m 40s\n",
      "4788:\tlearn: 4.7478677\ttotal: 24m 30s\tremaining: 26m 40s\n",
      "4789:\tlearn: 4.7476002\ttotal: 24m 30s\tremaining: 26m 39s\n",
      "4790:\tlearn: 4.7469725\ttotal: 24m 31s\tremaining: 26m 39s\n",
      "4791:\tlearn: 4.7462674\ttotal: 24m 31s\tremaining: 26m 39s\n",
      "4792:\tlearn: 4.7459433\ttotal: 24m 31s\tremaining: 26m 38s\n",
      "4793:\tlearn: 4.7454737\ttotal: 24m 32s\tremaining: 26m 38s\n",
      "4794:\tlearn: 4.7451271\ttotal: 24m 32s\tremaining: 26m 38s\n",
      "4795:\tlearn: 4.7447536\ttotal: 24m 32s\tremaining: 26m 38s\n",
      "4796:\tlearn: 4.7444768\ttotal: 24m 32s\tremaining: 26m 37s\n",
      "4797:\tlearn: 4.7439920\ttotal: 24m 33s\tremaining: 26m 37s\n",
      "4798:\tlearn: 4.7435522\ttotal: 24m 33s\tremaining: 26m 37s\n",
      "4799:\tlearn: 4.7432627\ttotal: 24m 33s\tremaining: 26m 36s\n",
      "4800:\tlearn: 4.7425970\ttotal: 24m 34s\tremaining: 26m 36s\n",
      "4801:\tlearn: 4.7416235\ttotal: 24m 34s\tremaining: 26m 36s\n",
      "4802:\tlearn: 4.7409974\ttotal: 24m 35s\tremaining: 26m 36s\n",
      "4803:\tlearn: 4.7404037\ttotal: 24m 35s\tremaining: 26m 35s\n",
      "4804:\tlearn: 4.7399043\ttotal: 24m 35s\tremaining: 26m 35s\n",
      "4805:\tlearn: 4.7391345\ttotal: 24m 36s\tremaining: 26m 35s\n",
      "4806:\tlearn: 4.7387268\ttotal: 24m 36s\tremaining: 26m 35s\n",
      "4807:\tlearn: 4.7381546\ttotal: 24m 36s\tremaining: 26m 34s\n",
      "4808:\tlearn: 4.7379288\ttotal: 24m 37s\tremaining: 26m 34s\n",
      "4809:\tlearn: 4.7375484\ttotal: 24m 37s\tremaining: 26m 34s\n",
      "4810:\tlearn: 4.7372957\ttotal: 24m 37s\tremaining: 26m 33s\n",
      "4811:\tlearn: 4.7368053\ttotal: 24m 37s\tremaining: 26m 33s\n",
      "4812:\tlearn: 4.7361826\ttotal: 24m 38s\tremaining: 26m 33s\n",
      "4813:\tlearn: 4.7358062\ttotal: 24m 38s\tremaining: 26m 32s\n",
      "4814:\tlearn: 4.7349160\ttotal: 24m 38s\tremaining: 26m 32s\n",
      "4815:\tlearn: 4.7345999\ttotal: 24m 39s\tremaining: 26m 32s\n",
      "4816:\tlearn: 4.7342581\ttotal: 24m 39s\tremaining: 26m 31s\n",
      "4817:\tlearn: 4.7336078\ttotal: 24m 39s\tremaining: 26m 31s\n",
      "4818:\tlearn: 4.7332770\ttotal: 24m 40s\tremaining: 26m 31s\n",
      "4819:\tlearn: 4.7323994\ttotal: 24m 40s\tremaining: 26m 31s\n",
      "4820:\tlearn: 4.7319906\ttotal: 24m 40s\tremaining: 26m 30s\n",
      "4821:\tlearn: 4.7317449\ttotal: 24m 41s\tremaining: 26m 30s\n",
      "4822:\tlearn: 4.7311785\ttotal: 24m 41s\tremaining: 26m 30s\n",
      "4823:\tlearn: 4.7305565\ttotal: 24m 41s\tremaining: 26m 29s\n",
      "4824:\tlearn: 4.7298671\ttotal: 24m 42s\tremaining: 26m 29s\n",
      "4825:\tlearn: 4.7294233\ttotal: 24m 42s\tremaining: 26m 29s\n",
      "4826:\tlearn: 4.7289236\ttotal: 24m 42s\tremaining: 26m 28s\n",
      "4827:\tlearn: 4.7284858\ttotal: 24m 42s\tremaining: 26m 28s\n",
      "4828:\tlearn: 4.7278599\ttotal: 24m 43s\tremaining: 26m 28s\n",
      "4829:\tlearn: 4.7273290\ttotal: 24m 43s\tremaining: 26m 28s\n",
      "4830:\tlearn: 4.7266926\ttotal: 24m 43s\tremaining: 26m 27s\n",
      "4831:\tlearn: 4.7262652\ttotal: 24m 44s\tremaining: 26m 27s\n",
      "4832:\tlearn: 4.7256275\ttotal: 24m 44s\tremaining: 26m 27s\n",
      "4833:\tlearn: 4.7249906\ttotal: 24m 44s\tremaining: 26m 26s\n",
      "4834:\tlearn: 4.7243767\ttotal: 24m 45s\tremaining: 26m 26s\n",
      "4835:\tlearn: 4.7237609\ttotal: 24m 45s\tremaining: 26m 26s\n",
      "4836:\tlearn: 4.7230311\ttotal: 24m 45s\tremaining: 26m 26s\n",
      "4837:\tlearn: 4.7225449\ttotal: 24m 46s\tremaining: 26m 25s\n",
      "4838:\tlearn: 4.7216574\ttotal: 24m 46s\tremaining: 26m 25s\n",
      "4839:\tlearn: 4.7211295\ttotal: 24m 46s\tremaining: 26m 25s\n",
      "4840:\tlearn: 4.7203745\ttotal: 24m 47s\tremaining: 26m 24s\n",
      "4841:\tlearn: 4.7201437\ttotal: 24m 47s\tremaining: 26m 24s\n",
      "4842:\tlearn: 4.7197463\ttotal: 24m 47s\tremaining: 26m 24s\n",
      "4843:\tlearn: 4.7190772\ttotal: 24m 48s\tremaining: 26m 23s\n",
      "4844:\tlearn: 4.7187115\ttotal: 24m 48s\tremaining: 26m 23s\n",
      "4845:\tlearn: 4.7181573\ttotal: 24m 48s\tremaining: 26m 23s\n",
      "4846:\tlearn: 4.7177500\ttotal: 24m 48s\tremaining: 26m 22s\n",
      "4847:\tlearn: 4.7172366\ttotal: 24m 49s\tremaining: 26m 22s\n",
      "4848:\tlearn: 4.7166510\ttotal: 24m 49s\tremaining: 26m 22s\n",
      "4849:\tlearn: 4.7158082\ttotal: 24m 50s\tremaining: 26m 22s\n",
      "4850:\tlearn: 4.7155082\ttotal: 24m 50s\tremaining: 26m 21s\n",
      "4851:\tlearn: 4.7147284\ttotal: 24m 50s\tremaining: 26m 21s\n",
      "4852:\tlearn: 4.7143716\ttotal: 24m 51s\tremaining: 26m 21s\n",
      "4853:\tlearn: 4.7139805\ttotal: 24m 51s\tremaining: 26m 21s\n",
      "4854:\tlearn: 4.7133498\ttotal: 24m 51s\tremaining: 26m 20s\n",
      "4855:\tlearn: 4.7130499\ttotal: 24m 51s\tremaining: 26m 20s\n",
      "4856:\tlearn: 4.7127246\ttotal: 24m 52s\tremaining: 26m 20s\n",
      "4857:\tlearn: 4.7119641\ttotal: 24m 52s\tremaining: 26m 19s\n",
      "4858:\tlearn: 4.7115117\ttotal: 24m 52s\tremaining: 26m 19s\n",
      "4859:\tlearn: 4.7107765\ttotal: 24m 53s\tremaining: 26m 19s\n",
      "4860:\tlearn: 4.7099598\ttotal: 24m 53s\tremaining: 26m 19s\n",
      "4861:\tlearn: 4.7095208\ttotal: 24m 54s\tremaining: 26m 18s\n",
      "4862:\tlearn: 4.7092369\ttotal: 24m 54s\tremaining: 26m 18s\n",
      "4863:\tlearn: 4.7084821\ttotal: 24m 54s\tremaining: 26m 18s\n",
      "4864:\tlearn: 4.7080178\ttotal: 24m 55s\tremaining: 26m 17s\n",
      "4865:\tlearn: 4.7075371\ttotal: 24m 55s\tremaining: 26m 17s\n",
      "4866:\tlearn: 4.7070442\ttotal: 24m 55s\tremaining: 26m 17s\n",
      "4867:\tlearn: 4.7066220\ttotal: 24m 56s\tremaining: 26m 17s\n",
      "4868:\tlearn: 4.7060048\ttotal: 24m 56s\tremaining: 26m 16s\n",
      "4869:\tlearn: 4.7056462\ttotal: 24m 56s\tremaining: 26m 16s\n",
      "4870:\tlearn: 4.7051607\ttotal: 24m 57s\tremaining: 26m 16s\n",
      "4871:\tlearn: 4.7046759\ttotal: 24m 57s\tremaining: 26m 16s\n",
      "4872:\tlearn: 4.7039200\ttotal: 24m 57s\tremaining: 26m 15s\n",
      "4873:\tlearn: 4.7033524\ttotal: 24m 58s\tremaining: 26m 15s\n",
      "4874:\tlearn: 4.7029766\ttotal: 24m 58s\tremaining: 26m 15s\n",
      "4875:\tlearn: 4.7025450\ttotal: 24m 58s\tremaining: 26m 14s\n",
      "4876:\tlearn: 4.7021210\ttotal: 24m 59s\tremaining: 26m 14s\n",
      "4877:\tlearn: 4.7015727\ttotal: 24m 59s\tremaining: 26m 14s\n",
      "4878:\tlearn: 4.7013239\ttotal: 24m 59s\tremaining: 26m 14s\n",
      "4879:\tlearn: 4.7009120\ttotal: 24m 59s\tremaining: 26m 13s\n",
      "4880:\tlearn: 4.7005114\ttotal: 25m\tremaining: 26m 13s\n",
      "4881:\tlearn: 4.7000865\ttotal: 25m\tremaining: 26m 13s\n",
      "4882:\tlearn: 4.6997625\ttotal: 25m\tremaining: 26m 12s\n",
      "4883:\tlearn: 4.6988830\ttotal: 25m 1s\tremaining: 26m 12s\n",
      "4884:\tlearn: 4.6985682\ttotal: 25m 1s\tremaining: 26m 12s\n",
      "4885:\tlearn: 4.6979270\ttotal: 25m 1s\tremaining: 26m 12s\n",
      "4886:\tlearn: 4.6976238\ttotal: 25m 2s\tremaining: 26m 11s\n",
      "4887:\tlearn: 4.6972853\ttotal: 25m 2s\tremaining: 26m 11s\n",
      "4888:\tlearn: 4.6968813\ttotal: 25m 2s\tremaining: 26m 11s\n",
      "4889:\tlearn: 4.6965015\ttotal: 25m 3s\tremaining: 26m 10s\n",
      "4890:\tlearn: 4.6960624\ttotal: 25m 3s\tremaining: 26m 10s\n",
      "4891:\tlearn: 4.6954921\ttotal: 25m 3s\tremaining: 26m 10s\n",
      "4892:\tlearn: 4.6950967\ttotal: 25m 4s\tremaining: 26m 9s\n",
      "4893:\tlearn: 4.6945072\ttotal: 25m 4s\tremaining: 26m 9s\n",
      "4894:\tlearn: 4.6941924\ttotal: 25m 4s\tremaining: 26m 9s\n",
      "4895:\tlearn: 4.6936298\ttotal: 25m 5s\tremaining: 26m 8s\n",
      "4896:\tlearn: 4.6932211\ttotal: 25m 5s\tremaining: 26m 8s\n",
      "4897:\tlearn: 4.6926367\ttotal: 25m 5s\tremaining: 26m 8s\n",
      "4898:\tlearn: 4.6923995\ttotal: 25m 6s\tremaining: 26m 8s\n",
      "4899:\tlearn: 4.6919917\ttotal: 25m 6s\tremaining: 26m 7s\n",
      "4900:\tlearn: 4.6913484\ttotal: 25m 6s\tremaining: 26m 7s\n",
      "4901:\tlearn: 4.6909077\ttotal: 25m 7s\tremaining: 26m 7s\n",
      "4902:\tlearn: 4.6903516\ttotal: 25m 7s\tremaining: 26m 7s\n",
      "4903:\tlearn: 4.6899959\ttotal: 25m 7s\tremaining: 26m 6s\n",
      "4904:\tlearn: 4.6894747\ttotal: 25m 8s\tremaining: 26m 6s\n",
      "4905:\tlearn: 4.6889353\ttotal: 25m 8s\tremaining: 26m 6s\n",
      "4906:\tlearn: 4.6884369\ttotal: 25m 8s\tremaining: 26m 5s\n",
      "4907:\tlearn: 4.6881218\ttotal: 25m 8s\tremaining: 26m 5s\n",
      "4908:\tlearn: 4.6876885\ttotal: 25m 9s\tremaining: 26m 5s\n",
      "4909:\tlearn: 4.6873565\ttotal: 25m 9s\tremaining: 26m 4s\n",
      "4910:\tlearn: 4.6867653\ttotal: 25m 9s\tremaining: 26m 4s\n",
      "4911:\tlearn: 4.6865383\ttotal: 25m 10s\tremaining: 26m 4s\n",
      "4912:\tlearn: 4.6858454\ttotal: 25m 10s\tremaining: 26m 4s\n",
      "4913:\tlearn: 4.6850926\ttotal: 25m 10s\tremaining: 26m 3s\n",
      "4914:\tlearn: 4.6846645\ttotal: 25m 11s\tremaining: 26m 3s\n",
      "4915:\tlearn: 4.6840823\ttotal: 25m 11s\tremaining: 26m 3s\n",
      "4916:\tlearn: 4.6837088\ttotal: 25m 11s\tremaining: 26m 2s\n",
      "4917:\tlearn: 4.6830665\ttotal: 25m 12s\tremaining: 26m 2s\n",
      "4918:\tlearn: 4.6823397\ttotal: 25m 12s\tremaining: 26m 2s\n",
      "4919:\tlearn: 4.6817874\ttotal: 25m 12s\tremaining: 26m 2s\n",
      "4920:\tlearn: 4.6814393\ttotal: 25m 13s\tremaining: 26m 1s\n",
      "4921:\tlearn: 4.6809854\ttotal: 25m 13s\tremaining: 26m 1s\n",
      "4922:\tlearn: 4.6803959\ttotal: 25m 13s\tremaining: 26m 1s\n",
      "4923:\tlearn: 4.6799973\ttotal: 25m 14s\tremaining: 26m\n",
      "4924:\tlearn: 4.6792657\ttotal: 25m 14s\tremaining: 26m\n",
      "4925:\tlearn: 4.6785263\ttotal: 25m 14s\tremaining: 26m\n",
      "4926:\tlearn: 4.6780261\ttotal: 25m 15s\tremaining: 26m\n",
      "4927:\tlearn: 4.6772793\ttotal: 25m 15s\tremaining: 25m 59s\n",
      "4928:\tlearn: 4.6768596\ttotal: 25m 16s\tremaining: 25m 59s\n",
      "4929:\tlearn: 4.6764637\ttotal: 25m 16s\tremaining: 25m 59s\n",
      "4930:\tlearn: 4.6758242\ttotal: 25m 16s\tremaining: 25m 59s\n",
      "4931:\tlearn: 4.6754239\ttotal: 25m 16s\tremaining: 25m 58s\n",
      "4932:\tlearn: 4.6751204\ttotal: 25m 17s\tremaining: 25m 58s\n",
      "4933:\tlearn: 4.6743703\ttotal: 25m 17s\tremaining: 25m 58s\n",
      "4934:\tlearn: 4.6736449\ttotal: 25m 18s\tremaining: 25m 58s\n",
      "4935:\tlearn: 4.6730410\ttotal: 25m 18s\tremaining: 25m 57s\n",
      "4936:\tlearn: 4.6724948\ttotal: 25m 18s\tremaining: 25m 57s\n",
      "4937:\tlearn: 4.6720939\ttotal: 25m 19s\tremaining: 25m 57s\n",
      "4938:\tlearn: 4.6713893\ttotal: 25m 19s\tremaining: 25m 56s\n",
      "4939:\tlearn: 4.6710944\ttotal: 25m 19s\tremaining: 25m 56s\n",
      "4940:\tlearn: 4.6704575\ttotal: 25m 20s\tremaining: 25m 56s\n",
      "4941:\tlearn: 4.6699801\ttotal: 25m 20s\tremaining: 25m 56s\n",
      "4942:\tlearn: 4.6694284\ttotal: 25m 20s\tremaining: 25m 55s\n",
      "4943:\tlearn: 4.6688962\ttotal: 25m 21s\tremaining: 25m 55s\n",
      "4944:\tlearn: 4.6680790\ttotal: 25m 21s\tremaining: 25m 55s\n",
      "4945:\tlearn: 4.6675448\ttotal: 25m 21s\tremaining: 25m 55s\n",
      "4946:\tlearn: 4.6671612\ttotal: 25m 22s\tremaining: 25m 54s\n",
      "4947:\tlearn: 4.6665161\ttotal: 25m 22s\tremaining: 25m 54s\n",
      "4948:\tlearn: 4.6662431\ttotal: 25m 22s\tremaining: 25m 54s\n",
      "4949:\tlearn: 4.6655094\ttotal: 25m 23s\tremaining: 25m 53s\n",
      "4950:\tlearn: 4.6650373\ttotal: 25m 23s\tremaining: 25m 53s\n",
      "4951:\tlearn: 4.6646457\ttotal: 25m 23s\tremaining: 25m 53s\n",
      "4952:\tlearn: 4.6641359\ttotal: 25m 24s\tremaining: 25m 53s\n",
      "4953:\tlearn: 4.6635754\ttotal: 25m 24s\tremaining: 25m 52s\n",
      "4954:\tlearn: 4.6632673\ttotal: 25m 24s\tremaining: 25m 52s\n",
      "4955:\tlearn: 4.6629365\ttotal: 25m 25s\tremaining: 25m 52s\n",
      "4956:\tlearn: 4.6626323\ttotal: 25m 25s\tremaining: 25m 51s\n",
      "4957:\tlearn: 4.6618861\ttotal: 25m 25s\tremaining: 25m 51s\n",
      "4958:\tlearn: 4.6614456\ttotal: 25m 25s\tremaining: 25m 51s\n",
      "4959:\tlearn: 4.6610157\ttotal: 25m 26s\tremaining: 25m 50s\n",
      "4960:\tlearn: 4.6607741\ttotal: 25m 26s\tremaining: 25m 50s\n",
      "4961:\tlearn: 4.6602831\ttotal: 25m 26s\tremaining: 25m 50s\n",
      "4962:\tlearn: 4.6597556\ttotal: 25m 27s\tremaining: 25m 49s\n",
      "4963:\tlearn: 4.6592131\ttotal: 25m 27s\tremaining: 25m 49s\n",
      "4964:\tlearn: 4.6589358\ttotal: 25m 27s\tremaining: 25m 49s\n",
      "4965:\tlearn: 4.6584856\ttotal: 25m 28s\tremaining: 25m 49s\n",
      "4966:\tlearn: 4.6577911\ttotal: 25m 28s\tremaining: 25m 48s\n",
      "4967:\tlearn: 4.6572075\ttotal: 25m 28s\tremaining: 25m 48s\n",
      "4968:\tlearn: 4.6564708\ttotal: 25m 29s\tremaining: 25m 48s\n",
      "4969:\tlearn: 4.6560356\ttotal: 25m 29s\tremaining: 25m 47s\n",
      "4970:\tlearn: 4.6555051\ttotal: 25m 29s\tremaining: 25m 47s\n",
      "4971:\tlearn: 4.6550279\ttotal: 25m 30s\tremaining: 25m 47s\n",
      "4972:\tlearn: 4.6543538\ttotal: 25m 30s\tremaining: 25m 47s\n",
      "4973:\tlearn: 4.6538771\ttotal: 25m 30s\tremaining: 25m 46s\n",
      "4974:\tlearn: 4.6533974\ttotal: 25m 31s\tremaining: 25m 46s\n",
      "4975:\tlearn: 4.6528916\ttotal: 25m 31s\tremaining: 25m 46s\n",
      "4976:\tlearn: 4.6522274\ttotal: 25m 31s\tremaining: 25m 46s\n",
      "4977:\tlearn: 4.6517958\ttotal: 25m 32s\tremaining: 25m 45s\n",
      "4978:\tlearn: 4.6513706\ttotal: 25m 32s\tremaining: 25m 45s\n",
      "4979:\tlearn: 4.6507177\ttotal: 25m 32s\tremaining: 25m 45s\n",
      "4980:\tlearn: 4.6503267\ttotal: 25m 33s\tremaining: 25m 44s\n",
      "4981:\tlearn: 4.6498918\ttotal: 25m 33s\tremaining: 25m 44s\n",
      "4982:\tlearn: 4.6494263\ttotal: 25m 33s\tremaining: 25m 44s\n",
      "4983:\tlearn: 4.6490212\ttotal: 25m 34s\tremaining: 25m 43s\n",
      "4984:\tlearn: 4.6486523\ttotal: 25m 34s\tremaining: 25m 43s\n",
      "4985:\tlearn: 4.6482454\ttotal: 25m 34s\tremaining: 25m 43s\n",
      "4986:\tlearn: 4.6477439\ttotal: 25m 34s\tremaining: 25m 42s\n",
      "4987:\tlearn: 4.6472794\ttotal: 25m 35s\tremaining: 25m 42s\n",
      "4988:\tlearn: 4.6469241\ttotal: 25m 35s\tremaining: 25m 42s\n",
      "4989:\tlearn: 4.6464892\ttotal: 25m 35s\tremaining: 25m 42s\n",
      "4990:\tlearn: 4.6459074\ttotal: 25m 36s\tremaining: 25m 41s\n",
      "4991:\tlearn: 4.6453672\ttotal: 25m 36s\tremaining: 25m 41s\n",
      "4992:\tlearn: 4.6450244\ttotal: 25m 36s\tremaining: 25m 41s\n",
      "4993:\tlearn: 4.6445862\ttotal: 25m 37s\tremaining: 25m 40s\n",
      "4994:\tlearn: 4.6441522\ttotal: 25m 37s\tremaining: 25m 40s\n",
      "4995:\tlearn: 4.6436360\ttotal: 25m 37s\tremaining: 25m 40s\n",
      "4996:\tlearn: 4.6430757\ttotal: 25m 38s\tremaining: 25m 39s\n",
      "4997:\tlearn: 4.6428868\ttotal: 25m 38s\tremaining: 25m 39s\n",
      "4998:\tlearn: 4.6424279\ttotal: 25m 38s\tremaining: 25m 39s\n",
      "4999:\tlearn: 4.6418650\ttotal: 25m 38s\tremaining: 25m 38s\n",
      "5000:\tlearn: 4.6413690\ttotal: 25m 39s\tremaining: 25m 38s\n",
      "5001:\tlearn: 4.6409396\ttotal: 25m 39s\tremaining: 25m 38s\n",
      "5002:\tlearn: 4.6404877\ttotal: 25m 39s\tremaining: 25m 37s\n",
      "5003:\tlearn: 4.6403140\ttotal: 25m 39s\tremaining: 25m 37s\n",
      "5004:\tlearn: 4.6399761\ttotal: 25m 40s\tremaining: 25m 37s\n",
      "5005:\tlearn: 4.6395025\ttotal: 25m 40s\tremaining: 25m 36s\n",
      "5006:\tlearn: 4.6388326\ttotal: 25m 40s\tremaining: 25m 36s\n",
      "5007:\tlearn: 4.6380877\ttotal: 25m 41s\tremaining: 25m 36s\n",
      "5008:\tlearn: 4.6377585\ttotal: 25m 41s\tremaining: 25m 36s\n",
      "5009:\tlearn: 4.6371216\ttotal: 25m 41s\tremaining: 25m 35s\n",
      "5010:\tlearn: 4.6367578\ttotal: 25m 42s\tremaining: 25m 35s\n",
      "5011:\tlearn: 4.6361686\ttotal: 25m 42s\tremaining: 25m 35s\n",
      "5012:\tlearn: 4.6356056\ttotal: 25m 42s\tremaining: 25m 34s\n",
      "5013:\tlearn: 4.6350465\ttotal: 25m 43s\tremaining: 25m 34s\n",
      "5014:\tlearn: 4.6343563\ttotal: 25m 43s\tremaining: 25m 34s\n",
      "5015:\tlearn: 4.6338467\ttotal: 25m 43s\tremaining: 25m 34s\n",
      "5016:\tlearn: 4.6334387\ttotal: 25m 44s\tremaining: 25m 33s\n",
      "5017:\tlearn: 4.6329578\ttotal: 25m 44s\tremaining: 25m 33s\n",
      "5018:\tlearn: 4.6325579\ttotal: 25m 44s\tremaining: 25m 33s\n",
      "5019:\tlearn: 4.6321667\ttotal: 25m 45s\tremaining: 25m 32s\n",
      "5020:\tlearn: 4.6316597\ttotal: 25m 45s\tremaining: 25m 32s\n",
      "5021:\tlearn: 4.6307387\ttotal: 25m 45s\tremaining: 25m 32s\n",
      "5022:\tlearn: 4.6303461\ttotal: 25m 46s\tremaining: 25m 32s\n",
      "5023:\tlearn: 4.6299834\ttotal: 25m 46s\tremaining: 25m 31s\n",
      "5024:\tlearn: 4.6294527\ttotal: 25m 46s\tremaining: 25m 31s\n",
      "5025:\tlearn: 4.6289464\ttotal: 25m 47s\tremaining: 25m 31s\n",
      "5026:\tlearn: 4.6285982\ttotal: 25m 47s\tremaining: 25m 30s\n",
      "5027:\tlearn: 4.6283887\ttotal: 25m 47s\tremaining: 25m 30s\n",
      "5028:\tlearn: 4.6277743\ttotal: 25m 48s\tremaining: 25m 30s\n",
      "5029:\tlearn: 4.6266612\ttotal: 25m 48s\tremaining: 25m 29s\n",
      "5030:\tlearn: 4.6263477\ttotal: 25m 48s\tremaining: 25m 29s\n",
      "5031:\tlearn: 4.6258404\ttotal: 25m 49s\tremaining: 25m 29s\n",
      "5032:\tlearn: 4.6253423\ttotal: 25m 49s\tremaining: 25m 28s\n",
      "5033:\tlearn: 4.6250164\ttotal: 25m 49s\tremaining: 25m 28s\n",
      "5034:\tlearn: 4.6244788\ttotal: 25m 49s\tremaining: 25m 28s\n",
      "5035:\tlearn: 4.6241049\ttotal: 25m 50s\tremaining: 25m 27s\n",
      "5036:\tlearn: 4.6234888\ttotal: 25m 50s\tremaining: 25m 27s\n",
      "5037:\tlearn: 4.6227980\ttotal: 25m 50s\tremaining: 25m 27s\n",
      "5038:\tlearn: 4.6222838\ttotal: 25m 51s\tremaining: 25m 27s\n",
      "5039:\tlearn: 4.6216630\ttotal: 25m 51s\tremaining: 25m 26s\n",
      "5040:\tlearn: 4.6213324\ttotal: 25m 51s\tremaining: 25m 26s\n",
      "5041:\tlearn: 4.6208825\ttotal: 25m 52s\tremaining: 25m 26s\n",
      "5042:\tlearn: 4.6205900\ttotal: 25m 52s\tremaining: 25m 25s\n",
      "5043:\tlearn: 4.6199880\ttotal: 25m 52s\tremaining: 25m 25s\n",
      "5044:\tlearn: 4.6196045\ttotal: 25m 52s\tremaining: 25m 25s\n",
      "5045:\tlearn: 4.6190990\ttotal: 25m 53s\tremaining: 25m 24s\n",
      "5046:\tlearn: 4.6186742\ttotal: 25m 53s\tremaining: 25m 24s\n",
      "5047:\tlearn: 4.6181596\ttotal: 25m 53s\tremaining: 25m 24s\n",
      "5048:\tlearn: 4.6178388\ttotal: 25m 54s\tremaining: 25m 23s\n",
      "5049:\tlearn: 4.6172932\ttotal: 25m 54s\tremaining: 25m 23s\n",
      "5050:\tlearn: 4.6169279\ttotal: 25m 54s\tremaining: 25m 23s\n",
      "5051:\tlearn: 4.6163702\ttotal: 25m 54s\tremaining: 25m 22s\n",
      "5052:\tlearn: 4.6160228\ttotal: 25m 55s\tremaining: 25m 22s\n",
      "5053:\tlearn: 4.6157727\ttotal: 25m 55s\tremaining: 25m 22s\n",
      "5054:\tlearn: 4.6153196\ttotal: 25m 55s\tremaining: 25m 21s\n",
      "5055:\tlearn: 4.6149941\ttotal: 25m 56s\tremaining: 25m 21s\n",
      "5056:\tlearn: 4.6144528\ttotal: 25m 56s\tremaining: 25m 21s\n",
      "5057:\tlearn: 4.6139567\ttotal: 25m 56s\tremaining: 25m 21s\n",
      "5058:\tlearn: 4.6135665\ttotal: 25m 57s\tremaining: 25m 20s\n",
      "5059:\tlearn: 4.6130168\ttotal: 25m 57s\tremaining: 25m 20s\n",
      "5060:\tlearn: 4.6125328\ttotal: 25m 57s\tremaining: 25m 20s\n",
      "5061:\tlearn: 4.6121215\ttotal: 25m 58s\tremaining: 25m 19s\n",
      "5062:\tlearn: 4.6117974\ttotal: 25m 58s\tremaining: 25m 19s\n",
      "5063:\tlearn: 4.6113486\ttotal: 25m 58s\tremaining: 25m 19s\n",
      "5064:\tlearn: 4.6107602\ttotal: 25m 59s\tremaining: 25m 19s\n",
      "5065:\tlearn: 4.6103789\ttotal: 25m 59s\tremaining: 25m 18s\n",
      "5066:\tlearn: 4.6099857\ttotal: 25m 59s\tremaining: 25m 18s\n",
      "5067:\tlearn: 4.6095086\ttotal: 25m 59s\tremaining: 25m 17s\n",
      "5068:\tlearn: 4.6088826\ttotal: 26m\tremaining: 25m 17s\n",
      "5069:\tlearn: 4.6085408\ttotal: 26m\tremaining: 25m 17s\n",
      "5070:\tlearn: 4.6081940\ttotal: 26m\tremaining: 25m 17s\n",
      "5071:\tlearn: 4.6077774\ttotal: 26m 1s\tremaining: 25m 16s\n",
      "5072:\tlearn: 4.6074078\ttotal: 26m 1s\tremaining: 25m 16s\n",
      "5073:\tlearn: 4.6070034\ttotal: 26m 1s\tremaining: 25m 16s\n",
      "5074:\tlearn: 4.6063803\ttotal: 26m 2s\tremaining: 25m 15s\n",
      "5075:\tlearn: 4.6059642\ttotal: 26m 2s\tremaining: 25m 15s\n",
      "5076:\tlearn: 4.6056958\ttotal: 26m 2s\tremaining: 25m 15s\n",
      "5077:\tlearn: 4.6053630\ttotal: 26m 2s\tremaining: 25m 14s\n",
      "5078:\tlearn: 4.6048185\ttotal: 26m 3s\tremaining: 25m 14s\n",
      "5079:\tlearn: 4.6044814\ttotal: 26m 3s\tremaining: 25m 14s\n",
      "5080:\tlearn: 4.6040461\ttotal: 26m 3s\tremaining: 25m 13s\n",
      "5081:\tlearn: 4.6036896\ttotal: 26m 4s\tremaining: 25m 13s\n",
      "5082:\tlearn: 4.6034145\ttotal: 26m 4s\tremaining: 25m 13s\n",
      "5083:\tlearn: 4.6030454\ttotal: 26m 4s\tremaining: 25m 12s\n",
      "5084:\tlearn: 4.6027050\ttotal: 26m 4s\tremaining: 25m 12s\n",
      "5085:\tlearn: 4.6022231\ttotal: 26m 5s\tremaining: 25m 12s\n",
      "5086:\tlearn: 4.6016336\ttotal: 26m 5s\tremaining: 25m 12s\n",
      "5087:\tlearn: 4.6011122\ttotal: 26m 5s\tremaining: 25m 11s\n",
      "5088:\tlearn: 4.6006442\ttotal: 26m 6s\tremaining: 25m 11s\n",
      "5089:\tlearn: 4.6002529\ttotal: 26m 6s\tremaining: 25m 11s\n",
      "5090:\tlearn: 4.5998177\ttotal: 26m 6s\tremaining: 25m 10s\n",
      "5091:\tlearn: 4.5996166\ttotal: 26m 6s\tremaining: 25m 10s\n",
      "5092:\tlearn: 4.5992730\ttotal: 26m 7s\tremaining: 25m 9s\n",
      "5093:\tlearn: 4.5987314\ttotal: 26m 7s\tremaining: 25m 9s\n",
      "5094:\tlearn: 4.5982802\ttotal: 26m 7s\tremaining: 25m 9s\n",
      "5095:\tlearn: 4.5979980\ttotal: 26m 8s\tremaining: 25m 8s\n",
      "5096:\tlearn: 4.5975101\ttotal: 26m 8s\tremaining: 25m 8s\n",
      "5097:\tlearn: 4.5971254\ttotal: 26m 8s\tremaining: 25m 8s\n",
      "5098:\tlearn: 4.5965222\ttotal: 26m 8s\tremaining: 25m 8s\n",
      "5099:\tlearn: 4.5961460\ttotal: 26m 9s\tremaining: 25m 7s\n",
      "5100:\tlearn: 4.5956950\ttotal: 26m 9s\tremaining: 25m 7s\n",
      "5101:\tlearn: 4.5950949\ttotal: 26m 9s\tremaining: 25m 7s\n",
      "5102:\tlearn: 4.5946974\ttotal: 26m 10s\tremaining: 25m 6s\n",
      "5103:\tlearn: 4.5944190\ttotal: 26m 10s\tremaining: 25m 6s\n",
      "5104:\tlearn: 4.5938700\ttotal: 26m 10s\tremaining: 25m 6s\n",
      "5105:\tlearn: 4.5933329\ttotal: 26m 11s\tremaining: 25m 5s\n",
      "5106:\tlearn: 4.5930466\ttotal: 26m 11s\tremaining: 25m 5s\n",
      "5107:\tlearn: 4.5926183\ttotal: 26m 11s\tremaining: 25m 5s\n",
      "5108:\tlearn: 4.5922204\ttotal: 26m 12s\tremaining: 25m 4s\n",
      "5109:\tlearn: 4.5917866\ttotal: 26m 12s\tremaining: 25m 4s\n",
      "5110:\tlearn: 4.5914205\ttotal: 26m 12s\tremaining: 25m 4s\n",
      "5111:\tlearn: 4.5910091\ttotal: 26m 12s\tremaining: 25m 4s\n",
      "5112:\tlearn: 4.5906529\ttotal: 26m 13s\tremaining: 25m 3s\n",
      "5113:\tlearn: 4.5902855\ttotal: 26m 13s\tremaining: 25m 3s\n",
      "5114:\tlearn: 4.5899676\ttotal: 26m 13s\tremaining: 25m 3s\n",
      "5115:\tlearn: 4.5897378\ttotal: 26m 14s\tremaining: 25m 2s\n",
      "5116:\tlearn: 4.5893578\ttotal: 26m 14s\tremaining: 25m 2s\n",
      "5117:\tlearn: 4.5888701\ttotal: 26m 14s\tremaining: 25m 2s\n",
      "5118:\tlearn: 4.5883173\ttotal: 26m 15s\tremaining: 25m 1s\n",
      "5119:\tlearn: 4.5876653\ttotal: 26m 15s\tremaining: 25m 1s\n",
      "5120:\tlearn: 4.5872625\ttotal: 26m 15s\tremaining: 25m 1s\n",
      "5121:\tlearn: 4.5868547\ttotal: 26m 16s\tremaining: 25m 1s\n",
      "5122:\tlearn: 4.5866022\ttotal: 26m 16s\tremaining: 25m\n",
      "5123:\tlearn: 4.5862526\ttotal: 26m 16s\tremaining: 25m\n",
      "5124:\tlearn: 4.5858782\ttotal: 26m 16s\tremaining: 25m\n",
      "5125:\tlearn: 4.5852868\ttotal: 26m 17s\tremaining: 24m 59s\n",
      "5126:\tlearn: 4.5848819\ttotal: 26m 17s\tremaining: 24m 59s\n",
      "5127:\tlearn: 4.5845375\ttotal: 26m 17s\tremaining: 24m 59s\n",
      "5128:\tlearn: 4.5842631\ttotal: 26m 18s\tremaining: 24m 58s\n",
      "5129:\tlearn: 4.5836397\ttotal: 26m 18s\tremaining: 24m 58s\n",
      "5130:\tlearn: 4.5831592\ttotal: 26m 18s\tremaining: 24m 58s\n",
      "5131:\tlearn: 4.5829315\ttotal: 26m 19s\tremaining: 24m 58s\n",
      "5132:\tlearn: 4.5822847\ttotal: 26m 19s\tremaining: 24m 57s\n",
      "5133:\tlearn: 4.5819112\ttotal: 26m 19s\tremaining: 24m 57s\n",
      "5134:\tlearn: 4.5815463\ttotal: 26m 20s\tremaining: 24m 57s\n",
      "5135:\tlearn: 4.5813921\ttotal: 26m 20s\tremaining: 24m 56s\n",
      "5136:\tlearn: 4.5806122\ttotal: 26m 20s\tremaining: 24m 56s\n",
      "5137:\tlearn: 4.5800477\ttotal: 26m 21s\tremaining: 24m 56s\n",
      "5138:\tlearn: 4.5796761\ttotal: 26m 21s\tremaining: 24m 55s\n",
      "5139:\tlearn: 4.5788202\ttotal: 26m 21s\tremaining: 24m 55s\n",
      "5140:\tlearn: 4.5783097\ttotal: 26m 22s\tremaining: 24m 55s\n",
      "5141:\tlearn: 4.5776682\ttotal: 26m 22s\tremaining: 24m 55s\n",
      "5142:\tlearn: 4.5773354\ttotal: 26m 22s\tremaining: 24m 54s\n",
      "5143:\tlearn: 4.5767344\ttotal: 26m 23s\tremaining: 24m 54s\n",
      "5144:\tlearn: 4.5760201\ttotal: 26m 23s\tremaining: 24m 54s\n",
      "5145:\tlearn: 4.5757149\ttotal: 26m 23s\tremaining: 24m 54s\n",
      "5146:\tlearn: 4.5753899\ttotal: 26m 24s\tremaining: 24m 53s\n",
      "5147:\tlearn: 4.5747465\ttotal: 26m 24s\tremaining: 24m 53s\n",
      "5148:\tlearn: 4.5743454\ttotal: 26m 24s\tremaining: 24m 53s\n",
      "5149:\tlearn: 4.5736526\ttotal: 26m 25s\tremaining: 24m 52s\n",
      "5150:\tlearn: 4.5731705\ttotal: 26m 25s\tremaining: 24m 52s\n",
      "5151:\tlearn: 4.5729202\ttotal: 26m 25s\tremaining: 24m 52s\n",
      "5152:\tlearn: 4.5725840\ttotal: 26m 26s\tremaining: 24m 52s\n",
      "5153:\tlearn: 4.5721898\ttotal: 26m 26s\tremaining: 24m 51s\n",
      "5154:\tlearn: 4.5716151\ttotal: 26m 26s\tremaining: 24m 51s\n",
      "5155:\tlearn: 4.5712071\ttotal: 26m 27s\tremaining: 24m 51s\n",
      "5156:\tlearn: 4.5706148\ttotal: 26m 27s\tremaining: 24m 50s\n",
      "5157:\tlearn: 4.5701824\ttotal: 26m 27s\tremaining: 24m 50s\n",
      "5158:\tlearn: 4.5698289\ttotal: 26m 28s\tremaining: 24m 50s\n",
      "5159:\tlearn: 4.5695209\ttotal: 26m 28s\tremaining: 24m 49s\n",
      "5160:\tlearn: 4.5690484\ttotal: 26m 28s\tremaining: 24m 49s\n",
      "5161:\tlearn: 4.5686742\ttotal: 26m 28s\tremaining: 24m 49s\n",
      "5162:\tlearn: 4.5680572\ttotal: 26m 29s\tremaining: 24m 48s\n",
      "5163:\tlearn: 4.5673729\ttotal: 26m 29s\tremaining: 24m 48s\n",
      "5164:\tlearn: 4.5670929\ttotal: 26m 29s\tremaining: 24m 48s\n",
      "5165:\tlearn: 4.5667009\ttotal: 26m 30s\tremaining: 24m 48s\n",
      "5166:\tlearn: 4.5665220\ttotal: 26m 30s\tremaining: 24m 47s\n",
      "5167:\tlearn: 4.5659834\ttotal: 26m 30s\tremaining: 24m 47s\n",
      "5168:\tlearn: 4.5655581\ttotal: 26m 31s\tremaining: 24m 47s\n",
      "5169:\tlearn: 4.5648698\ttotal: 26m 31s\tremaining: 24m 46s\n",
      "5170:\tlearn: 4.5643733\ttotal: 26m 31s\tremaining: 24m 46s\n",
      "5171:\tlearn: 4.5640197\ttotal: 26m 32s\tremaining: 24m 46s\n",
      "5172:\tlearn: 4.5634078\ttotal: 26m 32s\tremaining: 24m 45s\n",
      "5173:\tlearn: 4.5629188\ttotal: 26m 32s\tremaining: 24m 45s\n",
      "5174:\tlearn: 4.5626011\ttotal: 26m 33s\tremaining: 24m 45s\n",
      "5175:\tlearn: 4.5622227\ttotal: 26m 33s\tremaining: 24m 44s\n",
      "5176:\tlearn: 4.5618471\ttotal: 26m 33s\tremaining: 24m 44s\n",
      "5177:\tlearn: 4.5609940\ttotal: 26m 33s\tremaining: 24m 44s\n",
      "5178:\tlearn: 4.5606287\ttotal: 26m 34s\tremaining: 24m 44s\n",
      "5179:\tlearn: 4.5602378\ttotal: 26m 34s\tremaining: 24m 43s\n",
      "5180:\tlearn: 4.5596269\ttotal: 26m 34s\tremaining: 24m 43s\n",
      "5181:\tlearn: 4.5591221\ttotal: 26m 35s\tremaining: 24m 43s\n",
      "5182:\tlearn: 4.5587441\ttotal: 26m 35s\tremaining: 24m 42s\n",
      "5183:\tlearn: 4.5583982\ttotal: 26m 35s\tremaining: 24m 42s\n",
      "5184:\tlearn: 4.5578558\ttotal: 26m 36s\tremaining: 24m 42s\n",
      "5185:\tlearn: 4.5573970\ttotal: 26m 36s\tremaining: 24m 41s\n",
      "5186:\tlearn: 4.5567306\ttotal: 26m 36s\tremaining: 24m 41s\n",
      "5187:\tlearn: 4.5564468\ttotal: 26m 37s\tremaining: 24m 41s\n",
      "5188:\tlearn: 4.5559261\ttotal: 26m 37s\tremaining: 24m 41s\n",
      "5189:\tlearn: 4.5555083\ttotal: 26m 37s\tremaining: 24m 40s\n",
      "5190:\tlearn: 4.5548306\ttotal: 26m 38s\tremaining: 24m 40s\n",
      "5191:\tlearn: 4.5542334\ttotal: 26m 38s\tremaining: 24m 40s\n",
      "5192:\tlearn: 4.5538502\ttotal: 26m 38s\tremaining: 24m 39s\n",
      "5193:\tlearn: 4.5535322\ttotal: 26m 39s\tremaining: 24m 39s\n",
      "5194:\tlearn: 4.5531487\ttotal: 26m 39s\tremaining: 24m 39s\n",
      "5195:\tlearn: 4.5526939\ttotal: 26m 39s\tremaining: 24m 38s\n",
      "5196:\tlearn: 4.5521863\ttotal: 26m 39s\tremaining: 24m 38s\n",
      "5197:\tlearn: 4.5518306\ttotal: 26m 40s\tremaining: 24m 38s\n",
      "5198:\tlearn: 4.5513750\ttotal: 26m 40s\tremaining: 24m 37s\n",
      "5199:\tlearn: 4.5509483\ttotal: 26m 40s\tremaining: 24m 37s\n",
      "5200:\tlearn: 4.5507329\ttotal: 26m 41s\tremaining: 24m 37s\n",
      "5201:\tlearn: 4.5503939\ttotal: 26m 41s\tremaining: 24m 36s\n",
      "5202:\tlearn: 4.5498813\ttotal: 26m 41s\tremaining: 24m 36s\n",
      "5203:\tlearn: 4.5496258\ttotal: 26m 41s\tremaining: 24m 36s\n",
      "5204:\tlearn: 4.5494523\ttotal: 26m 42s\tremaining: 24m 35s\n",
      "5205:\tlearn: 4.5489828\ttotal: 26m 42s\tremaining: 24m 35s\n",
      "5206:\tlearn: 4.5485059\ttotal: 26m 42s\tremaining: 24m 35s\n",
      "5207:\tlearn: 4.5480112\ttotal: 26m 42s\tremaining: 24m 34s\n",
      "5208:\tlearn: 4.5473804\ttotal: 26m 43s\tremaining: 24m 34s\n",
      "5209:\tlearn: 4.5469168\ttotal: 26m 43s\tremaining: 24m 34s\n",
      "5210:\tlearn: 4.5464453\ttotal: 26m 43s\tremaining: 24m 34s\n",
      "5211:\tlearn: 4.5457223\ttotal: 26m 44s\tremaining: 24m 33s\n",
      "5212:\tlearn: 4.5453937\ttotal: 26m 44s\tremaining: 24m 33s\n",
      "5213:\tlearn: 4.5446871\ttotal: 26m 44s\tremaining: 24m 33s\n",
      "5214:\tlearn: 4.5443016\ttotal: 26m 45s\tremaining: 24m 32s\n",
      "5215:\tlearn: 4.5437524\ttotal: 26m 45s\tremaining: 24m 32s\n",
      "5216:\tlearn: 4.5434740\ttotal: 26m 45s\tremaining: 24m 32s\n",
      "5217:\tlearn: 4.5430662\ttotal: 26m 46s\tremaining: 24m 31s\n",
      "5218:\tlearn: 4.5425492\ttotal: 26m 46s\tremaining: 24m 31s\n",
      "5219:\tlearn: 4.5421894\ttotal: 26m 46s\tremaining: 24m 31s\n",
      "5220:\tlearn: 4.5415569\ttotal: 26m 46s\tremaining: 24m 30s\n",
      "5221:\tlearn: 4.5412437\ttotal: 26m 47s\tremaining: 24m 30s\n",
      "5222:\tlearn: 4.5406947\ttotal: 26m 47s\tremaining: 24m 30s\n",
      "5223:\tlearn: 4.5400279\ttotal: 26m 47s\tremaining: 24m 30s\n",
      "5224:\tlearn: 4.5395814\ttotal: 26m 48s\tremaining: 24m 29s\n",
      "5225:\tlearn: 4.5392791\ttotal: 26m 48s\tremaining: 24m 29s\n",
      "5226:\tlearn: 4.5386821\ttotal: 26m 48s\tremaining: 24m 29s\n",
      "5227:\tlearn: 4.5383014\ttotal: 26m 49s\tremaining: 24m 28s\n",
      "5228:\tlearn: 4.5378377\ttotal: 26m 49s\tremaining: 24m 28s\n",
      "5229:\tlearn: 4.5374233\ttotal: 26m 49s\tremaining: 24m 28s\n",
      "5230:\tlearn: 4.5368235\ttotal: 26m 50s\tremaining: 24m 28s\n",
      "5231:\tlearn: 4.5362208\ttotal: 26m 50s\tremaining: 24m 27s\n",
      "5232:\tlearn: 4.5358576\ttotal: 26m 50s\tremaining: 24m 27s\n",
      "5233:\tlearn: 4.5353830\ttotal: 26m 51s\tremaining: 24m 27s\n",
      "5234:\tlearn: 4.5350709\ttotal: 26m 51s\tremaining: 24m 26s\n",
      "5235:\tlearn: 4.5346716\ttotal: 26m 51s\tremaining: 24m 26s\n",
      "5236:\tlearn: 4.5341833\ttotal: 26m 52s\tremaining: 24m 26s\n",
      "5237:\tlearn: 4.5337016\ttotal: 26m 52s\tremaining: 24m 25s\n",
      "5238:\tlearn: 4.5331872\ttotal: 26m 52s\tremaining: 24m 25s\n",
      "5239:\tlearn: 4.5326850\ttotal: 26m 53s\tremaining: 24m 25s\n",
      "5240:\tlearn: 4.5321826\ttotal: 26m 53s\tremaining: 24m 24s\n",
      "5241:\tlearn: 4.5318925\ttotal: 26m 53s\tremaining: 24m 24s\n",
      "5242:\tlearn: 4.5314511\ttotal: 26m 53s\tremaining: 24m 24s\n",
      "5243:\tlearn: 4.5311758\ttotal: 26m 54s\tremaining: 24m 23s\n",
      "5244:\tlearn: 4.5308281\ttotal: 26m 54s\tremaining: 24m 23s\n",
      "5245:\tlearn: 4.5303054\ttotal: 26m 54s\tremaining: 24m 23s\n",
      "5246:\tlearn: 4.5296572\ttotal: 26m 55s\tremaining: 24m 23s\n",
      "5247:\tlearn: 4.5293007\ttotal: 26m 55s\tremaining: 24m 22s\n",
      "5248:\tlearn: 4.5289259\ttotal: 26m 55s\tremaining: 24m 22s\n",
      "5249:\tlearn: 4.5286386\ttotal: 26m 55s\tremaining: 24m 21s\n",
      "5250:\tlearn: 4.5282189\ttotal: 26m 56s\tremaining: 24m 21s\n",
      "5251:\tlearn: 4.5278594\ttotal: 26m 56s\tremaining: 24m 21s\n",
      "5252:\tlearn: 4.5272322\ttotal: 26m 56s\tremaining: 24m 20s\n",
      "5253:\tlearn: 4.5269736\ttotal: 26m 56s\tremaining: 24m 20s\n",
      "5254:\tlearn: 4.5263254\ttotal: 26m 57s\tremaining: 24m 20s\n",
      "5255:\tlearn: 4.5259056\ttotal: 26m 57s\tremaining: 24m 19s\n",
      "5256:\tlearn: 4.5256702\ttotal: 26m 57s\tremaining: 24m 19s\n",
      "5257:\tlearn: 4.5251504\ttotal: 26m 58s\tremaining: 24m 19s\n",
      "5258:\tlearn: 4.5247510\ttotal: 26m 58s\tremaining: 24m 18s\n",
      "5259:\tlearn: 4.5244973\ttotal: 26m 58s\tremaining: 24m 18s\n",
      "5260:\tlearn: 4.5242581\ttotal: 26m 58s\tremaining: 24m 18s\n",
      "5261:\tlearn: 4.5238460\ttotal: 26m 59s\tremaining: 24m 17s\n",
      "5262:\tlearn: 4.5229223\ttotal: 26m 59s\tremaining: 24m 17s\n",
      "5263:\tlearn: 4.5224111\ttotal: 26m 59s\tremaining: 24m 17s\n",
      "5264:\tlearn: 4.5219049\ttotal: 27m\tremaining: 24m 16s\n",
      "5265:\tlearn: 4.5215364\ttotal: 27m\tremaining: 24m 16s\n",
      "5266:\tlearn: 4.5211559\ttotal: 27m\tremaining: 24m 16s\n",
      "5267:\tlearn: 4.5208196\ttotal: 27m\tremaining: 24m 15s\n",
      "5268:\tlearn: 4.5202268\ttotal: 27m 1s\tremaining: 24m 15s\n",
      "5269:\tlearn: 4.5198481\ttotal: 27m 1s\tremaining: 24m 15s\n",
      "5270:\tlearn: 4.5195969\ttotal: 27m 1s\tremaining: 24m 14s\n",
      "5271:\tlearn: 4.5193504\ttotal: 27m 1s\tremaining: 24m 14s\n",
      "5272:\tlearn: 4.5190016\ttotal: 27m 2s\tremaining: 24m 14s\n",
      "5273:\tlearn: 4.5186356\ttotal: 27m 2s\tremaining: 24m 13s\n",
      "5274:\tlearn: 4.5183182\ttotal: 27m 2s\tremaining: 24m 13s\n",
      "5275:\tlearn: 4.5178579\ttotal: 27m 3s\tremaining: 24m 13s\n",
      "5276:\tlearn: 4.5174283\ttotal: 27m 3s\tremaining: 24m 12s\n",
      "5277:\tlearn: 4.5167670\ttotal: 27m 3s\tremaining: 24m 12s\n",
      "5278:\tlearn: 4.5162125\ttotal: 27m 3s\tremaining: 24m 12s\n",
      "5279:\tlearn: 4.5158925\ttotal: 27m 4s\tremaining: 24m 11s\n",
      "5280:\tlearn: 4.5153420\ttotal: 27m 4s\tremaining: 24m 11s\n",
      "5281:\tlearn: 4.5147307\ttotal: 27m 4s\tremaining: 24m 11s\n",
      "5282:\tlearn: 4.5142602\ttotal: 27m 5s\tremaining: 24m 11s\n",
      "5283:\tlearn: 4.5139607\ttotal: 27m 5s\tremaining: 24m 10s\n",
      "5284:\tlearn: 4.5134683\ttotal: 27m 5s\tremaining: 24m 10s\n",
      "5285:\tlearn: 4.5131148\ttotal: 27m 6s\tremaining: 24m 10s\n",
      "5286:\tlearn: 4.5122783\ttotal: 27m 6s\tremaining: 24m 9s\n",
      "5287:\tlearn: 4.5117303\ttotal: 27m 6s\tremaining: 24m 9s\n",
      "5288:\tlearn: 4.5110932\ttotal: 27m 7s\tremaining: 24m 9s\n",
      "5289:\tlearn: 4.5104326\ttotal: 27m 7s\tremaining: 24m 9s\n",
      "5290:\tlearn: 4.5098204\ttotal: 27m 7s\tremaining: 24m 8s\n",
      "5291:\tlearn: 4.5094243\ttotal: 27m 8s\tremaining: 24m 8s\n",
      "5292:\tlearn: 4.5090630\ttotal: 27m 8s\tremaining: 24m 8s\n",
      "5293:\tlearn: 4.5087052\ttotal: 27m 8s\tremaining: 24m 7s\n",
      "5294:\tlearn: 4.5081123\ttotal: 27m 8s\tremaining: 24m 7s\n",
      "5295:\tlearn: 4.5076710\ttotal: 27m 9s\tremaining: 24m 7s\n",
      "5296:\tlearn: 4.5070281\ttotal: 27m 9s\tremaining: 24m 6s\n",
      "5297:\tlearn: 4.5066724\ttotal: 27m 9s\tremaining: 24m 6s\n",
      "5298:\tlearn: 4.5063436\ttotal: 27m 10s\tremaining: 24m 6s\n",
      "5299:\tlearn: 4.5059526\ttotal: 27m 10s\tremaining: 24m 5s\n",
      "5300:\tlearn: 4.5056862\ttotal: 27m 10s\tremaining: 24m 5s\n",
      "5301:\tlearn: 4.5055140\ttotal: 27m 10s\tremaining: 24m 4s\n",
      "5302:\tlearn: 4.5049947\ttotal: 27m 10s\tremaining: 24m 4s\n",
      "5303:\tlearn: 4.5044945\ttotal: 27m 11s\tremaining: 24m 4s\n",
      "5304:\tlearn: 4.5042682\ttotal: 27m 11s\tremaining: 24m 3s\n",
      "5305:\tlearn: 4.5035900\ttotal: 27m 11s\tremaining: 24m 3s\n",
      "5306:\tlearn: 4.5029621\ttotal: 27m 12s\tremaining: 24m 3s\n",
      "5307:\tlearn: 4.5023573\ttotal: 27m 12s\tremaining: 24m 2s\n",
      "5308:\tlearn: 4.5019729\ttotal: 27m 12s\tremaining: 24m 2s\n",
      "5309:\tlearn: 4.5015535\ttotal: 27m 12s\tremaining: 24m 2s\n",
      "5310:\tlearn: 4.5010066\ttotal: 27m 13s\tremaining: 24m 2s\n",
      "5311:\tlearn: 4.5005499\ttotal: 27m 13s\tremaining: 24m 1s\n",
      "5312:\tlearn: 4.5000992\ttotal: 27m 13s\tremaining: 24m 1s\n",
      "5313:\tlearn: 4.4995057\ttotal: 27m 14s\tremaining: 24m 1s\n",
      "5314:\tlearn: 4.4990461\ttotal: 27m 14s\tremaining: 24m\n",
      "5315:\tlearn: 4.4985750\ttotal: 27m 14s\tremaining: 24m\n",
      "5316:\tlearn: 4.4978542\ttotal: 27m 15s\tremaining: 24m\n",
      "5317:\tlearn: 4.4971029\ttotal: 27m 15s\tremaining: 23m 59s\n",
      "5318:\tlearn: 4.4968000\ttotal: 27m 15s\tremaining: 23m 59s\n",
      "5319:\tlearn: 4.4962166\ttotal: 27m 16s\tremaining: 23m 59s\n",
      "5320:\tlearn: 4.4958148\ttotal: 27m 16s\tremaining: 23m 58s\n",
      "5321:\tlearn: 4.4954364\ttotal: 27m 16s\tremaining: 23m 58s\n",
      "5322:\tlearn: 4.4949374\ttotal: 27m 16s\tremaining: 23m 58s\n",
      "5323:\tlearn: 4.4945688\ttotal: 27m 17s\tremaining: 23m 57s\n",
      "5324:\tlearn: 4.4939121\ttotal: 27m 17s\tremaining: 23m 57s\n",
      "5325:\tlearn: 4.4934833\ttotal: 27m 17s\tremaining: 23m 57s\n",
      "5326:\tlearn: 4.4932715\ttotal: 27m 18s\tremaining: 23m 56s\n",
      "5327:\tlearn: 4.4924697\ttotal: 27m 18s\tremaining: 23m 56s\n",
      "5328:\tlearn: 4.4920295\ttotal: 27m 18s\tremaining: 23m 56s\n",
      "5329:\tlearn: 4.4915658\ttotal: 27m 18s\tremaining: 23m 55s\n",
      "5330:\tlearn: 4.4909796\ttotal: 27m 19s\tremaining: 23m 55s\n",
      "5331:\tlearn: 4.4904043\ttotal: 27m 19s\tremaining: 23m 55s\n",
      "5332:\tlearn: 4.4900029\ttotal: 27m 19s\tremaining: 23m 55s\n",
      "5333:\tlearn: 4.4894817\ttotal: 27m 20s\tremaining: 23m 54s\n",
      "5334:\tlearn: 4.4891394\ttotal: 27m 20s\tremaining: 23m 54s\n",
      "5335:\tlearn: 4.4888624\ttotal: 27m 20s\tremaining: 23m 54s\n",
      "5336:\tlearn: 4.4880883\ttotal: 27m 21s\tremaining: 23m 53s\n",
      "5337:\tlearn: 4.4874464\ttotal: 27m 21s\tremaining: 23m 53s\n",
      "5338:\tlearn: 4.4872552\ttotal: 27m 21s\tremaining: 23m 53s\n",
      "5339:\tlearn: 4.4865331\ttotal: 27m 22s\tremaining: 23m 52s\n",
      "5340:\tlearn: 4.4861728\ttotal: 27m 22s\tremaining: 23m 52s\n",
      "5341:\tlearn: 4.4856302\ttotal: 27m 22s\tremaining: 23m 52s\n",
      "5342:\tlearn: 4.4851246\ttotal: 27m 23s\tremaining: 23m 52s\n",
      "5343:\tlearn: 4.4847916\ttotal: 27m 23s\tremaining: 23m 51s\n",
      "5344:\tlearn: 4.4842317\ttotal: 27m 23s\tremaining: 23m 51s\n",
      "5345:\tlearn: 4.4838682\ttotal: 27m 23s\tremaining: 23m 51s\n",
      "5346:\tlearn: 4.4836162\ttotal: 27m 24s\tremaining: 23m 50s\n",
      "5347:\tlearn: 4.4833267\ttotal: 27m 24s\tremaining: 23m 50s\n",
      "5348:\tlearn: 4.4827616\ttotal: 27m 24s\tremaining: 23m 50s\n",
      "5349:\tlearn: 4.4822431\ttotal: 27m 25s\tremaining: 23m 49s\n",
      "5350:\tlearn: 4.4817138\ttotal: 27m 25s\tremaining: 23m 49s\n",
      "5351:\tlearn: 4.4811278\ttotal: 27m 25s\tremaining: 23m 49s\n",
      "5352:\tlearn: 4.4804666\ttotal: 27m 26s\tremaining: 23m 48s\n",
      "5353:\tlearn: 4.4801298\ttotal: 27m 26s\tremaining: 23m 48s\n",
      "5354:\tlearn: 4.4797761\ttotal: 27m 26s\tremaining: 23m 48s\n",
      "5355:\tlearn: 4.4793660\ttotal: 27m 26s\tremaining: 23m 47s\n",
      "5356:\tlearn: 4.4790949\ttotal: 27m 27s\tremaining: 23m 47s\n",
      "5357:\tlearn: 4.4786446\ttotal: 27m 27s\tremaining: 23m 47s\n",
      "5358:\tlearn: 4.4782896\ttotal: 27m 27s\tremaining: 23m 46s\n",
      "5359:\tlearn: 4.4775298\ttotal: 27m 28s\tremaining: 23m 46s\n",
      "5360:\tlearn: 4.4773551\ttotal: 27m 28s\tremaining: 23m 46s\n",
      "5361:\tlearn: 4.4767150\ttotal: 27m 28s\tremaining: 23m 45s\n",
      "5362:\tlearn: 4.4763442\ttotal: 27m 28s\tremaining: 23m 45s\n",
      "5363:\tlearn: 4.4760252\ttotal: 27m 29s\tremaining: 23m 45s\n",
      "5364:\tlearn: 4.4755470\ttotal: 27m 29s\tremaining: 23m 44s\n",
      "5365:\tlearn: 4.4750070\ttotal: 27m 29s\tremaining: 23m 44s\n",
      "5366:\tlearn: 4.4746531\ttotal: 27m 29s\tremaining: 23m 44s\n",
      "5367:\tlearn: 4.4741793\ttotal: 27m 30s\tremaining: 23m 43s\n",
      "5368:\tlearn: 4.4738767\ttotal: 27m 30s\tremaining: 23m 43s\n",
      "5369:\tlearn: 4.4734229\ttotal: 27m 30s\tremaining: 23m 43s\n",
      "5370:\tlearn: 4.4732476\ttotal: 27m 31s\tremaining: 23m 43s\n",
      "5371:\tlearn: 4.4727532\ttotal: 27m 31s\tremaining: 23m 42s\n",
      "5372:\tlearn: 4.4722504\ttotal: 27m 31s\tremaining: 23m 42s\n",
      "5373:\tlearn: 4.4717143\ttotal: 27m 32s\tremaining: 23m 42s\n",
      "5374:\tlearn: 4.4713848\ttotal: 27m 32s\tremaining: 23m 41s\n",
      "5375:\tlearn: 4.4710152\ttotal: 27m 32s\tremaining: 23m 41s\n",
      "5376:\tlearn: 4.4705525\ttotal: 27m 33s\tremaining: 23m 41s\n",
      "5377:\tlearn: 4.4702041\ttotal: 27m 33s\tremaining: 23m 40s\n",
      "5378:\tlearn: 4.4699437\ttotal: 27m 33s\tremaining: 23m 40s\n",
      "5379:\tlearn: 4.4695357\ttotal: 27m 33s\tremaining: 23m 40s\n",
      "5380:\tlearn: 4.4691492\ttotal: 27m 34s\tremaining: 23m 39s\n",
      "5381:\tlearn: 4.4685972\ttotal: 27m 34s\tremaining: 23m 39s\n",
      "5382:\tlearn: 4.4683589\ttotal: 27m 34s\tremaining: 23m 39s\n",
      "5383:\tlearn: 4.4676120\ttotal: 27m 35s\tremaining: 23m 38s\n",
      "5384:\tlearn: 4.4671532\ttotal: 27m 35s\tremaining: 23m 38s\n",
      "5385:\tlearn: 4.4668042\ttotal: 27m 35s\tremaining: 23m 38s\n",
      "5386:\tlearn: 4.4664386\ttotal: 27m 36s\tremaining: 23m 38s\n",
      "5387:\tlearn: 4.4658591\ttotal: 27m 36s\tremaining: 23m 37s\n",
      "5388:\tlearn: 4.4655325\ttotal: 27m 36s\tremaining: 23m 37s\n",
      "5389:\tlearn: 4.4651014\ttotal: 27m 37s\tremaining: 23m 37s\n",
      "5390:\tlearn: 4.4648613\ttotal: 27m 37s\tremaining: 23m 36s\n",
      "5391:\tlearn: 4.4643006\ttotal: 27m 37s\tremaining: 23m 36s\n",
      "5392:\tlearn: 4.4637028\ttotal: 27m 37s\tremaining: 23m 36s\n",
      "5393:\tlearn: 4.4634696\ttotal: 27m 38s\tremaining: 23m 35s\n",
      "5394:\tlearn: 4.4632214\ttotal: 27m 38s\tremaining: 23m 35s\n",
      "5395:\tlearn: 4.4627277\ttotal: 27m 38s\tremaining: 23m 35s\n",
      "5396:\tlearn: 4.4621190\ttotal: 27m 39s\tremaining: 23m 35s\n",
      "5397:\tlearn: 4.4617398\ttotal: 27m 39s\tremaining: 23m 34s\n",
      "5398:\tlearn: 4.4613968\ttotal: 27m 39s\tremaining: 23m 34s\n",
      "5399:\tlearn: 4.4606524\ttotal: 27m 40s\tremaining: 23m 34s\n",
      "5400:\tlearn: 4.4602143\ttotal: 27m 40s\tremaining: 23m 33s\n",
      "5401:\tlearn: 4.4596762\ttotal: 27m 40s\tremaining: 23m 33s\n",
      "5402:\tlearn: 4.4591903\ttotal: 27m 41s\tremaining: 23m 33s\n",
      "5403:\tlearn: 4.4586176\ttotal: 27m 41s\tremaining: 23m 33s\n",
      "5404:\tlearn: 4.4582806\ttotal: 27m 41s\tremaining: 23m 32s\n",
      "5405:\tlearn: 4.4579149\ttotal: 27m 41s\tremaining: 23m 32s\n",
      "5406:\tlearn: 4.4575098\ttotal: 27m 42s\tremaining: 23m 32s\n",
      "5407:\tlearn: 4.4571733\ttotal: 27m 42s\tremaining: 23m 31s\n",
      "5408:\tlearn: 4.4567147\ttotal: 27m 42s\tremaining: 23m 31s\n",
      "5409:\tlearn: 4.4561414\ttotal: 27m 43s\tremaining: 23m 31s\n",
      "5410:\tlearn: 4.4555208\ttotal: 27m 43s\tremaining: 23m 30s\n",
      "5411:\tlearn: 4.4550948\ttotal: 27m 43s\tremaining: 23m 30s\n",
      "5412:\tlearn: 4.4549613\ttotal: 27m 44s\tremaining: 23m 30s\n",
      "5413:\tlearn: 4.4543871\ttotal: 27m 44s\tremaining: 23m 29s\n",
      "5414:\tlearn: 4.4539891\ttotal: 27m 44s\tremaining: 23m 29s\n",
      "5415:\tlearn: 4.4534262\ttotal: 27m 45s\tremaining: 23m 29s\n",
      "5416:\tlearn: 4.4528684\ttotal: 27m 45s\tremaining: 23m 28s\n",
      "5417:\tlearn: 4.4525308\ttotal: 27m 45s\tremaining: 23m 28s\n",
      "5418:\tlearn: 4.4522371\ttotal: 27m 45s\tremaining: 23m 28s\n",
      "5419:\tlearn: 4.4517715\ttotal: 27m 46s\tremaining: 23m 27s\n",
      "5420:\tlearn: 4.4514466\ttotal: 27m 46s\tremaining: 23m 27s\n",
      "5421:\tlearn: 4.4509551\ttotal: 27m 46s\tremaining: 23m 27s\n",
      "5422:\tlearn: 4.4506555\ttotal: 27m 46s\tremaining: 23m 26s\n",
      "5423:\tlearn: 4.4502910\ttotal: 27m 47s\tremaining: 23m 26s\n",
      "5424:\tlearn: 4.4495877\ttotal: 27m 47s\tremaining: 23m 26s\n",
      "5425:\tlearn: 4.4489082\ttotal: 27m 47s\tremaining: 23m 25s\n",
      "5426:\tlearn: 4.4485514\ttotal: 27m 48s\tremaining: 23m 25s\n",
      "5427:\tlearn: 4.4481316\ttotal: 27m 48s\tremaining: 23m 25s\n",
      "5428:\tlearn: 4.4477584\ttotal: 27m 48s\tremaining: 23m 25s\n",
      "5429:\tlearn: 4.4475935\ttotal: 27m 49s\tremaining: 23m 24s\n",
      "5430:\tlearn: 4.4471612\ttotal: 27m 49s\tremaining: 23m 24s\n",
      "5431:\tlearn: 4.4467335\ttotal: 27m 49s\tremaining: 23m 24s\n",
      "5432:\tlearn: 4.4464748\ttotal: 27m 49s\tremaining: 23m 23s\n",
      "5433:\tlearn: 4.4460054\ttotal: 27m 50s\tremaining: 23m 23s\n",
      "5434:\tlearn: 4.4452934\ttotal: 27m 50s\tremaining: 23m 23s\n",
      "5435:\tlearn: 4.4447148\ttotal: 27m 50s\tremaining: 23m 22s\n",
      "5436:\tlearn: 4.4442258\ttotal: 27m 51s\tremaining: 23m 22s\n",
      "5437:\tlearn: 4.4436981\ttotal: 27m 51s\tremaining: 23m 22s\n",
      "5438:\tlearn: 4.4435072\ttotal: 27m 51s\tremaining: 23m 21s\n",
      "5439:\tlearn: 4.4431950\ttotal: 27m 52s\tremaining: 23m 21s\n",
      "5440:\tlearn: 4.4427660\ttotal: 27m 52s\tremaining: 23m 21s\n",
      "5441:\tlearn: 4.4423614\ttotal: 27m 52s\tremaining: 23m 21s\n",
      "5442:\tlearn: 4.4418695\ttotal: 27m 53s\tremaining: 23m 20s\n",
      "5443:\tlearn: 4.4413202\ttotal: 27m 53s\tremaining: 23m 20s\n",
      "5444:\tlearn: 4.4407017\ttotal: 27m 53s\tremaining: 23m 20s\n",
      "5445:\tlearn: 4.4402838\ttotal: 27m 54s\tremaining: 23m 19s\n",
      "5446:\tlearn: 4.4397392\ttotal: 27m 54s\tremaining: 23m 19s\n",
      "5447:\tlearn: 4.4394696\ttotal: 27m 54s\tremaining: 23m 19s\n",
      "5448:\tlearn: 4.4391322\ttotal: 27m 55s\tremaining: 23m 19s\n",
      "5449:\tlearn: 4.4388269\ttotal: 27m 55s\tremaining: 23m 18s\n",
      "5450:\tlearn: 4.4384290\ttotal: 27m 55s\tremaining: 23m 18s\n",
      "5451:\tlearn: 4.4380115\ttotal: 27m 55s\tremaining: 23m 18s\n",
      "5452:\tlearn: 4.4376559\ttotal: 27m 56s\tremaining: 23m 17s\n",
      "5453:\tlearn: 4.4369157\ttotal: 27m 56s\tremaining: 23m 17s\n",
      "5454:\tlearn: 4.4364854\ttotal: 27m 56s\tremaining: 23m 17s\n",
      "5455:\tlearn: 4.4362200\ttotal: 27m 57s\tremaining: 23m 16s\n",
      "5456:\tlearn: 4.4359413\ttotal: 27m 57s\tremaining: 23m 16s\n",
      "5457:\tlearn: 4.4357315\ttotal: 27m 57s\tremaining: 23m 16s\n",
      "5458:\tlearn: 4.4351475\ttotal: 27m 58s\tremaining: 23m 15s\n",
      "5459:\tlearn: 4.4347717\ttotal: 27m 58s\tremaining: 23m 15s\n",
      "5460:\tlearn: 4.4344768\ttotal: 27m 58s\tremaining: 23m 15s\n",
      "5461:\tlearn: 4.4341175\ttotal: 27m 59s\tremaining: 23m 14s\n",
      "5462:\tlearn: 4.4337760\ttotal: 27m 59s\tremaining: 23m 14s\n",
      "5463:\tlearn: 4.4333553\ttotal: 27m 59s\tremaining: 23m 14s\n",
      "5464:\tlearn: 4.4330866\ttotal: 27m 59s\tremaining: 23m 13s\n",
      "5465:\tlearn: 4.4327733\ttotal: 28m\tremaining: 23m 13s\n",
      "5466:\tlearn: 4.4322346\ttotal: 28m\tremaining: 23m 13s\n",
      "5467:\tlearn: 4.4318870\ttotal: 28m\tremaining: 23m 12s\n",
      "5468:\tlearn: 4.4315745\ttotal: 28m\tremaining: 23m 12s\n",
      "5469:\tlearn: 4.4310000\ttotal: 28m 1s\tremaining: 23m 12s\n",
      "5470:\tlearn: 4.4307641\ttotal: 28m 1s\tremaining: 23m 12s\n",
      "5471:\tlearn: 4.4305055\ttotal: 28m 1s\tremaining: 23m 11s\n",
      "5472:\tlearn: 4.4297489\ttotal: 28m 2s\tremaining: 23m 11s\n",
      "5473:\tlearn: 4.4293767\ttotal: 28m 2s\tremaining: 23m 11s\n",
      "5474:\tlearn: 4.4286990\ttotal: 28m 2s\tremaining: 23m 10s\n",
      "5475:\tlearn: 4.4283575\ttotal: 28m 3s\tremaining: 23m 10s\n",
      "5476:\tlearn: 4.4279195\ttotal: 28m 3s\tremaining: 23m 10s\n",
      "5477:\tlearn: 4.4274888\ttotal: 28m 3s\tremaining: 23m 9s\n",
      "5478:\tlearn: 4.4272902\ttotal: 28m 3s\tremaining: 23m 9s\n",
      "5479:\tlearn: 4.4268664\ttotal: 28m 4s\tremaining: 23m 9s\n",
      "5480:\tlearn: 4.4264946\ttotal: 28m 4s\tremaining: 23m 8s\n",
      "5481:\tlearn: 4.4261405\ttotal: 28m 4s\tremaining: 23m 8s\n",
      "5482:\tlearn: 4.4258604\ttotal: 28m 5s\tremaining: 23m 8s\n",
      "5483:\tlearn: 4.4255409\ttotal: 28m 5s\tremaining: 23m 8s\n",
      "5484:\tlearn: 4.4252096\ttotal: 28m 5s\tremaining: 23m 7s\n",
      "5485:\tlearn: 4.4247380\ttotal: 28m 6s\tremaining: 23m 7s\n",
      "5486:\tlearn: 4.4241286\ttotal: 28m 6s\tremaining: 23m 7s\n",
      "5487:\tlearn: 4.4238589\ttotal: 28m 6s\tremaining: 23m 6s\n",
      "5488:\tlearn: 4.4234373\ttotal: 28m 7s\tremaining: 23m 6s\n",
      "5489:\tlearn: 4.4231464\ttotal: 28m 7s\tremaining: 23m 6s\n",
      "5490:\tlearn: 4.4225400\ttotal: 28m 7s\tremaining: 23m 5s\n",
      "5491:\tlearn: 4.4218537\ttotal: 28m 8s\tremaining: 23m 5s\n",
      "5492:\tlearn: 4.4214130\ttotal: 28m 8s\tremaining: 23m 5s\n",
      "5493:\tlearn: 4.4210618\ttotal: 28m 8s\tremaining: 23m 4s\n",
      "5494:\tlearn: 4.4205492\ttotal: 28m 8s\tremaining: 23m 4s\n",
      "5495:\tlearn: 4.4202227\ttotal: 28m 9s\tremaining: 23m 4s\n",
      "5496:\tlearn: 4.4194803\ttotal: 28m 9s\tremaining: 23m 4s\n",
      "5497:\tlearn: 4.4191578\ttotal: 28m 9s\tremaining: 23m 3s\n",
      "5498:\tlearn: 4.4184558\ttotal: 28m 10s\tremaining: 23m 3s\n",
      "5499:\tlearn: 4.4182495\ttotal: 28m 10s\tremaining: 23m 3s\n",
      "5500:\tlearn: 4.4179727\ttotal: 28m 10s\tremaining: 23m 2s\n",
      "5501:\tlearn: 4.4174861\ttotal: 28m 11s\tremaining: 23m 2s\n",
      "5502:\tlearn: 4.4172651\ttotal: 28m 11s\tremaining: 23m 2s\n",
      "5503:\tlearn: 4.4169817\ttotal: 28m 11s\tremaining: 23m 1s\n",
      "5504:\tlearn: 4.4161634\ttotal: 28m 11s\tremaining: 23m 1s\n",
      "5505:\tlearn: 4.4157428\ttotal: 28m 12s\tremaining: 23m 1s\n",
      "5506:\tlearn: 4.4153221\ttotal: 28m 12s\tremaining: 23m\n",
      "5507:\tlearn: 4.4151360\ttotal: 28m 12s\tremaining: 23m\n",
      "5508:\tlearn: 4.4148965\ttotal: 28m 12s\tremaining: 23m\n",
      "5509:\tlearn: 4.4144919\ttotal: 28m 13s\tremaining: 22m 59s\n",
      "5510:\tlearn: 4.4140120\ttotal: 28m 13s\tremaining: 22m 59s\n",
      "5511:\tlearn: 4.4134834\ttotal: 28m 13s\tremaining: 22m 59s\n",
      "5512:\tlearn: 4.4132963\ttotal: 28m 14s\tremaining: 22m 58s\n",
      "5513:\tlearn: 4.4130505\ttotal: 28m 14s\tremaining: 22m 58s\n",
      "5514:\tlearn: 4.4127285\ttotal: 28m 14s\tremaining: 22m 58s\n",
      "5515:\tlearn: 4.4122675\ttotal: 28m 14s\tremaining: 22m 57s\n",
      "5516:\tlearn: 4.4117142\ttotal: 28m 15s\tremaining: 22m 57s\n",
      "5517:\tlearn: 4.4111304\ttotal: 28m 15s\tremaining: 22m 57s\n",
      "5518:\tlearn: 4.4107377\ttotal: 28m 15s\tremaining: 22m 56s\n",
      "5519:\tlearn: 4.4104680\ttotal: 28m 16s\tremaining: 22m 56s\n",
      "5520:\tlearn: 4.4101385\ttotal: 28m 16s\tremaining: 22m 56s\n",
      "5521:\tlearn: 4.4096013\ttotal: 28m 16s\tremaining: 22m 55s\n",
      "5522:\tlearn: 4.4090642\ttotal: 28m 16s\tremaining: 22m 55s\n",
      "5523:\tlearn: 4.4086794\ttotal: 28m 17s\tremaining: 22m 55s\n",
      "5524:\tlearn: 4.4080210\ttotal: 28m 17s\tremaining: 22m 54s\n",
      "5525:\tlearn: 4.4077373\ttotal: 28m 17s\tremaining: 22m 54s\n",
      "5526:\tlearn: 4.4072013\ttotal: 28m 18s\tremaining: 22m 54s\n",
      "5527:\tlearn: 4.4066840\ttotal: 28m 18s\tremaining: 22m 53s\n",
      "5528:\tlearn: 4.4061278\ttotal: 28m 18s\tremaining: 22m 53s\n",
      "5529:\tlearn: 4.4058109\ttotal: 28m 19s\tremaining: 22m 53s\n",
      "5530:\tlearn: 4.4054974\ttotal: 28m 19s\tremaining: 22m 52s\n",
      "5531:\tlearn: 4.4051895\ttotal: 28m 19s\tremaining: 22m 52s\n",
      "5532:\tlearn: 4.4048667\ttotal: 28m 19s\tremaining: 22m 52s\n",
      "5533:\tlearn: 4.4044529\ttotal: 28m 20s\tremaining: 22m 52s\n",
      "5534:\tlearn: 4.4040483\ttotal: 28m 20s\tremaining: 22m 51s\n",
      "5535:\tlearn: 4.4033730\ttotal: 28m 20s\tremaining: 22m 51s\n",
      "5536:\tlearn: 4.4031276\ttotal: 28m 21s\tremaining: 22m 51s\n",
      "5537:\tlearn: 4.4025952\ttotal: 28m 21s\tremaining: 22m 50s\n",
      "5538:\tlearn: 4.4020715\ttotal: 28m 21s\tremaining: 22m 50s\n",
      "5539:\tlearn: 4.4018074\ttotal: 28m 21s\tremaining: 22m 50s\n",
      "5540:\tlearn: 4.4016390\ttotal: 28m 22s\tremaining: 22m 49s\n",
      "5541:\tlearn: 4.4012801\ttotal: 28m 22s\tremaining: 22m 49s\n",
      "5542:\tlearn: 4.4010288\ttotal: 28m 22s\tremaining: 22m 49s\n",
      "5543:\tlearn: 4.4006279\ttotal: 28m 22s\tremaining: 22m 48s\n",
      "5544:\tlearn: 4.3999894\ttotal: 28m 23s\tremaining: 22m 48s\n",
      "5545:\tlearn: 4.3996645\ttotal: 28m 23s\tremaining: 22m 48s\n",
      "5546:\tlearn: 4.3991044\ttotal: 28m 23s\tremaining: 22m 47s\n",
      "5547:\tlearn: 4.3987216\ttotal: 28m 24s\tremaining: 22m 47s\n",
      "5548:\tlearn: 4.3983527\ttotal: 28m 24s\tremaining: 22m 47s\n",
      "5549:\tlearn: 4.3976784\ttotal: 28m 24s\tremaining: 22m 46s\n",
      "5550:\tlearn: 4.3973678\ttotal: 28m 25s\tremaining: 22m 46s\n",
      "5551:\tlearn: 4.3968324\ttotal: 28m 25s\tremaining: 22m 46s\n",
      "5552:\tlearn: 4.3964284\ttotal: 28m 25s\tremaining: 22m 46s\n",
      "5553:\tlearn: 4.3959816\ttotal: 28m 26s\tremaining: 22m 45s\n",
      "5554:\tlearn: 4.3955692\ttotal: 28m 26s\tremaining: 22m 45s\n",
      "5555:\tlearn: 4.3953293\ttotal: 28m 26s\tremaining: 22m 45s\n",
      "5556:\tlearn: 4.3950404\ttotal: 28m 26s\tremaining: 22m 44s\n",
      "5557:\tlearn: 4.3944117\ttotal: 28m 27s\tremaining: 22m 44s\n",
      "5558:\tlearn: 4.3937996\ttotal: 28m 27s\tremaining: 22m 44s\n",
      "5559:\tlearn: 4.3933222\ttotal: 28m 27s\tremaining: 22m 43s\n",
      "5560:\tlearn: 4.3930205\ttotal: 28m 28s\tremaining: 22m 43s\n",
      "5561:\tlearn: 4.3924664\ttotal: 28m 28s\tremaining: 22m 43s\n",
      "5562:\tlearn: 4.3922607\ttotal: 28m 28s\tremaining: 22m 42s\n",
      "5563:\tlearn: 4.3917047\ttotal: 28m 28s\tremaining: 22m 42s\n",
      "5564:\tlearn: 4.3914706\ttotal: 28m 29s\tremaining: 22m 42s\n",
      "5565:\tlearn: 4.3909269\ttotal: 28m 29s\tremaining: 22m 41s\n",
      "5566:\tlearn: 4.3904041\ttotal: 28m 29s\tremaining: 22m 41s\n",
      "5567:\tlearn: 4.3899573\ttotal: 28m 30s\tremaining: 22m 41s\n",
      "5568:\tlearn: 4.3894920\ttotal: 28m 30s\tremaining: 22m 40s\n",
      "5569:\tlearn: 4.3890163\ttotal: 28m 30s\tremaining: 22m 40s\n",
      "5570:\tlearn: 4.3887376\ttotal: 28m 30s\tremaining: 22m 40s\n",
      "5571:\tlearn: 4.3882216\ttotal: 28m 31s\tremaining: 22m 39s\n",
      "5572:\tlearn: 4.3878856\ttotal: 28m 31s\tremaining: 22m 39s\n",
      "5573:\tlearn: 4.3875702\ttotal: 28m 31s\tremaining: 22m 39s\n",
      "5574:\tlearn: 4.3872875\ttotal: 28m 31s\tremaining: 22m 38s\n",
      "5575:\tlearn: 4.3870016\ttotal: 28m 32s\tremaining: 22m 38s\n",
      "5576:\tlearn: 4.3864853\ttotal: 28m 32s\tremaining: 22m 38s\n",
      "5577:\tlearn: 4.3861911\ttotal: 28m 32s\tremaining: 22m 37s\n",
      "5578:\tlearn: 4.3859079\ttotal: 28m 33s\tremaining: 22m 37s\n",
      "5579:\tlearn: 4.3854589\ttotal: 28m 33s\tremaining: 22m 37s\n",
      "5580:\tlearn: 4.3849753\ttotal: 28m 33s\tremaining: 22m 36s\n",
      "5581:\tlearn: 4.3846887\ttotal: 28m 33s\tremaining: 22m 36s\n",
      "5582:\tlearn: 4.3841591\ttotal: 28m 34s\tremaining: 22m 36s\n",
      "5583:\tlearn: 4.3837105\ttotal: 28m 34s\tremaining: 22m 35s\n",
      "5584:\tlearn: 4.3834848\ttotal: 28m 34s\tremaining: 22m 35s\n",
      "5585:\tlearn: 4.3831340\ttotal: 28m 34s\tremaining: 22m 35s\n",
      "5586:\tlearn: 4.3827351\ttotal: 28m 35s\tremaining: 22m 34s\n",
      "5587:\tlearn: 4.3821682\ttotal: 28m 35s\tremaining: 22m 34s\n",
      "5588:\tlearn: 4.3818005\ttotal: 28m 35s\tremaining: 22m 34s\n",
      "5589:\tlearn: 4.3811224\ttotal: 28m 36s\tremaining: 22m 33s\n",
      "5590:\tlearn: 4.3806925\ttotal: 28m 36s\tremaining: 22m 33s\n",
      "5591:\tlearn: 4.3800176\ttotal: 28m 36s\tremaining: 22m 33s\n",
      "5592:\tlearn: 4.3795368\ttotal: 28m 37s\tremaining: 22m 32s\n",
      "5593:\tlearn: 4.3788297\ttotal: 28m 37s\tremaining: 22m 32s\n",
      "5594:\tlearn: 4.3781674\ttotal: 28m 37s\tremaining: 22m 32s\n",
      "5595:\tlearn: 4.3778124\ttotal: 28m 37s\tremaining: 22m 31s\n",
      "5596:\tlearn: 4.3775684\ttotal: 28m 38s\tremaining: 22m 31s\n",
      "5597:\tlearn: 4.3771989\ttotal: 28m 38s\tremaining: 22m 31s\n",
      "5598:\tlearn: 4.3766212\ttotal: 28m 38s\tremaining: 22m 30s\n",
      "5599:\tlearn: 4.3758227\ttotal: 28m 39s\tremaining: 22m 30s\n",
      "5600:\tlearn: 4.3755938\ttotal: 28m 39s\tremaining: 22m 30s\n",
      "5601:\tlearn: 4.3752866\ttotal: 28m 39s\tremaining: 22m 29s\n",
      "5602:\tlearn: 4.3748305\ttotal: 28m 39s\tremaining: 22m 29s\n",
      "5603:\tlearn: 4.3743757\ttotal: 28m 40s\tremaining: 22m 29s\n",
      "5604:\tlearn: 4.3737600\ttotal: 28m 40s\tremaining: 22m 29s\n",
      "5605:\tlearn: 4.3733859\ttotal: 28m 40s\tremaining: 22m 28s\n",
      "5606:\tlearn: 4.3728992\ttotal: 28m 41s\tremaining: 22m 28s\n",
      "5607:\tlearn: 4.3725871\ttotal: 28m 41s\tremaining: 22m 28s\n",
      "5608:\tlearn: 4.3718841\ttotal: 28m 41s\tremaining: 22m 27s\n",
      "5609:\tlearn: 4.3716190\ttotal: 28m 42s\tremaining: 22m 27s\n",
      "5610:\tlearn: 4.3712910\ttotal: 28m 42s\tremaining: 22m 27s\n",
      "5611:\tlearn: 4.3709376\ttotal: 28m 42s\tremaining: 22m 26s\n",
      "5612:\tlearn: 4.3708041\ttotal: 28m 42s\tremaining: 22m 26s\n",
      "5613:\tlearn: 4.3701727\ttotal: 28m 43s\tremaining: 22m 26s\n",
      "5614:\tlearn: 4.3698482\ttotal: 28m 43s\tremaining: 22m 25s\n",
      "5615:\tlearn: 4.3693168\ttotal: 28m 43s\tremaining: 22m 25s\n",
      "5616:\tlearn: 4.3690471\ttotal: 28m 44s\tremaining: 22m 25s\n",
      "5617:\tlearn: 4.3684390\ttotal: 28m 44s\tremaining: 22m 24s\n",
      "5618:\tlearn: 4.3680207\ttotal: 28m 44s\tremaining: 22m 24s\n",
      "5619:\tlearn: 4.3676585\ttotal: 28m 44s\tremaining: 22m 24s\n",
      "5620:\tlearn: 4.3671108\ttotal: 28m 45s\tremaining: 22m 24s\n",
      "5621:\tlearn: 4.3668424\ttotal: 28m 45s\tremaining: 22m 23s\n",
      "5622:\tlearn: 4.3662524\ttotal: 28m 45s\tremaining: 22m 23s\n",
      "5623:\tlearn: 4.3660587\ttotal: 28m 45s\tremaining: 22m 22s\n",
      "5624:\tlearn: 4.3656461\ttotal: 28m 46s\tremaining: 22m 22s\n",
      "5625:\tlearn: 4.3651949\ttotal: 28m 46s\tremaining: 22m 22s\n",
      "5626:\tlearn: 4.3648980\ttotal: 28m 46s\tremaining: 22m 21s\n",
      "5627:\tlearn: 4.3647024\ttotal: 28m 46s\tremaining: 22m 21s\n",
      "5628:\tlearn: 4.3644482\ttotal: 28m 47s\tremaining: 22m 21s\n",
      "5629:\tlearn: 4.3640412\ttotal: 28m 47s\tremaining: 22m 20s\n",
      "5630:\tlearn: 4.3636350\ttotal: 28m 47s\tremaining: 22m 20s\n",
      "5631:\tlearn: 4.3631873\ttotal: 28m 48s\tremaining: 22m 20s\n",
      "5632:\tlearn: 4.3629201\ttotal: 28m 48s\tremaining: 22m 19s\n",
      "5633:\tlearn: 4.3623796\ttotal: 28m 48s\tremaining: 22m 19s\n",
      "5634:\tlearn: 4.3618632\ttotal: 28m 48s\tremaining: 22m 19s\n",
      "5635:\tlearn: 4.3617515\ttotal: 28m 49s\tremaining: 22m 18s\n",
      "5636:\tlearn: 4.3613935\ttotal: 28m 49s\tremaining: 22m 18s\n",
      "5637:\tlearn: 4.3607447\ttotal: 28m 49s\tremaining: 22m 18s\n",
      "5638:\tlearn: 4.3603811\ttotal: 28m 49s\tremaining: 22m 17s\n",
      "5639:\tlearn: 4.3601091\ttotal: 28m 50s\tremaining: 22m 17s\n",
      "5640:\tlearn: 4.3597456\ttotal: 28m 50s\tremaining: 22m 17s\n",
      "5641:\tlearn: 4.3593565\ttotal: 28m 50s\tremaining: 22m 16s\n",
      "5642:\tlearn: 4.3590782\ttotal: 28m 51s\tremaining: 22m 16s\n",
      "5643:\tlearn: 4.3587787\ttotal: 28m 51s\tremaining: 22m 16s\n",
      "5644:\tlearn: 4.3582522\ttotal: 28m 51s\tremaining: 22m 15s\n",
      "5645:\tlearn: 4.3579520\ttotal: 28m 51s\tremaining: 22m 15s\n",
      "5646:\tlearn: 4.3574193\ttotal: 28m 52s\tremaining: 22m 15s\n",
      "5647:\tlearn: 4.3570790\ttotal: 28m 52s\tremaining: 22m 14s\n",
      "5648:\tlearn: 4.3565111\ttotal: 28m 52s\tremaining: 22m 14s\n",
      "5649:\tlearn: 4.3561596\ttotal: 28m 53s\tremaining: 22m 14s\n",
      "5650:\tlearn: 4.3555322\ttotal: 28m 53s\tremaining: 22m 14s\n",
      "5651:\tlearn: 4.3551564\ttotal: 28m 53s\tremaining: 22m 13s\n",
      "5652:\tlearn: 4.3545296\ttotal: 28m 54s\tremaining: 22m 13s\n",
      "5653:\tlearn: 4.3540307\ttotal: 28m 54s\tremaining: 22m 13s\n",
      "5654:\tlearn: 4.3537005\ttotal: 28m 54s\tremaining: 22m 12s\n",
      "5655:\tlearn: 4.3532391\ttotal: 28m 55s\tremaining: 22m 12s\n",
      "5656:\tlearn: 4.3529321\ttotal: 28m 55s\tremaining: 22m 12s\n",
      "5657:\tlearn: 4.3522512\ttotal: 28m 55s\tremaining: 22m 11s\n",
      "5658:\tlearn: 4.3515119\ttotal: 28m 56s\tremaining: 22m 11s\n",
      "5659:\tlearn: 4.3509247\ttotal: 28m 56s\tremaining: 22m 11s\n",
      "5660:\tlearn: 4.3505376\ttotal: 28m 56s\tremaining: 22m 11s\n",
      "5661:\tlearn: 4.3497732\ttotal: 28m 57s\tremaining: 22m 10s\n",
      "5662:\tlearn: 4.3493686\ttotal: 28m 57s\tremaining: 22m 10s\n",
      "5663:\tlearn: 4.3488903\ttotal: 28m 57s\tremaining: 22m 10s\n",
      "5664:\tlearn: 4.3486843\ttotal: 28m 57s\tremaining: 22m 9s\n",
      "5665:\tlearn: 4.3481871\ttotal: 28m 58s\tremaining: 22m 9s\n",
      "5666:\tlearn: 4.3478170\ttotal: 28m 58s\tremaining: 22m 9s\n",
      "5667:\tlearn: 4.3472743\ttotal: 28m 58s\tremaining: 22m 8s\n",
      "5668:\tlearn: 4.3467900\ttotal: 28m 58s\tremaining: 22m 8s\n",
      "5669:\tlearn: 4.3463412\ttotal: 28m 59s\tremaining: 22m 8s\n",
      "5670:\tlearn: 4.3459831\ttotal: 28m 59s\tremaining: 22m 7s\n",
      "5671:\tlearn: 4.3456613\ttotal: 28m 59s\tremaining: 22m 7s\n",
      "5672:\tlearn: 4.3453166\ttotal: 28m 59s\tremaining: 22m 7s\n",
      "5673:\tlearn: 4.3448055\ttotal: 29m\tremaining: 22m 6s\n",
      "5674:\tlearn: 4.3445267\ttotal: 29m\tremaining: 22m 6s\n",
      "5675:\tlearn: 4.3442336\ttotal: 29m\tremaining: 22m 6s\n",
      "5676:\tlearn: 4.3437396\ttotal: 29m 1s\tremaining: 22m 5s\n",
      "5677:\tlearn: 4.3434002\ttotal: 29m 1s\tremaining: 22m 5s\n",
      "5678:\tlearn: 4.3431622\ttotal: 29m 1s\tremaining: 22m 5s\n",
      "5679:\tlearn: 4.3426843\ttotal: 29m 2s\tremaining: 22m 5s\n",
      "5680:\tlearn: 4.3422514\ttotal: 29m 2s\tremaining: 22m 4s\n",
      "5681:\tlearn: 4.3418025\ttotal: 29m 2s\tremaining: 22m 4s\n",
      "5682:\tlearn: 4.3412016\ttotal: 29m 3s\tremaining: 22m 4s\n",
      "5683:\tlearn: 4.3408958\ttotal: 29m 3s\tremaining: 22m 3s\n",
      "5684:\tlearn: 4.3404160\ttotal: 29m 3s\tremaining: 22m 3s\n",
      "5685:\tlearn: 4.3399130\ttotal: 29m 3s\tremaining: 22m 3s\n",
      "5686:\tlearn: 4.3392847\ttotal: 29m 4s\tremaining: 22m 2s\n",
      "5687:\tlearn: 4.3389817\ttotal: 29m 4s\tremaining: 22m 2s\n",
      "5688:\tlearn: 4.3386507\ttotal: 29m 4s\tremaining: 22m 2s\n",
      "5689:\tlearn: 4.3383011\ttotal: 29m 5s\tremaining: 22m 1s\n",
      "5690:\tlearn: 4.3378692\ttotal: 29m 5s\tremaining: 22m 1s\n",
      "5691:\tlearn: 4.3375490\ttotal: 29m 5s\tremaining: 22m 1s\n",
      "5692:\tlearn: 4.3372203\ttotal: 29m 6s\tremaining: 22m 1s\n",
      "5693:\tlearn: 4.3368740\ttotal: 29m 6s\tremaining: 22m\n",
      "5694:\tlearn: 4.3361105\ttotal: 29m 6s\tremaining: 22m\n",
      "5695:\tlearn: 4.3356980\ttotal: 29m 7s\tremaining: 22m\n",
      "5696:\tlearn: 4.3349794\ttotal: 29m 7s\tremaining: 21m 59s\n",
      "5697:\tlearn: 4.3346232\ttotal: 29m 7s\tremaining: 21m 59s\n",
      "5698:\tlearn: 4.3340414\ttotal: 29m 7s\tremaining: 21m 59s\n",
      "5699:\tlearn: 4.3335575\ttotal: 29m 8s\tremaining: 21m 58s\n",
      "5700:\tlearn: 4.3328339\ttotal: 29m 8s\tremaining: 21m 58s\n",
      "5701:\tlearn: 4.3324229\ttotal: 29m 8s\tremaining: 21m 58s\n",
      "5702:\tlearn: 4.3320095\ttotal: 29m 9s\tremaining: 21m 57s\n",
      "5703:\tlearn: 4.3314745\ttotal: 29m 9s\tremaining: 21m 57s\n",
      "5704:\tlearn: 4.3308242\ttotal: 29m 9s\tremaining: 21m 57s\n",
      "5705:\tlearn: 4.3304249\ttotal: 29m 10s\tremaining: 21m 57s\n",
      "5706:\tlearn: 4.3301791\ttotal: 29m 10s\tremaining: 21m 56s\n",
      "5707:\tlearn: 4.3299377\ttotal: 29m 10s\tremaining: 21m 56s\n",
      "5708:\tlearn: 4.3294809\ttotal: 29m 10s\tremaining: 21m 56s\n",
      "5709:\tlearn: 4.3291979\ttotal: 29m 11s\tremaining: 21m 55s\n",
      "5710:\tlearn: 4.3287556\ttotal: 29m 11s\tremaining: 21m 55s\n",
      "5711:\tlearn: 4.3282547\ttotal: 29m 11s\tremaining: 21m 55s\n",
      "5712:\tlearn: 4.3278522\ttotal: 29m 12s\tremaining: 21m 54s\n",
      "5713:\tlearn: 4.3275628\ttotal: 29m 12s\tremaining: 21m 54s\n",
      "5714:\tlearn: 4.3270722\ttotal: 29m 12s\tremaining: 21m 54s\n",
      "5715:\tlearn: 4.3266561\ttotal: 29m 13s\tremaining: 21m 54s\n",
      "5716:\tlearn: 4.3260744\ttotal: 29m 13s\tremaining: 21m 53s\n",
      "5717:\tlearn: 4.3256424\ttotal: 29m 13s\tremaining: 21m 53s\n",
      "5718:\tlearn: 4.3254681\ttotal: 29m 14s\tremaining: 21m 53s\n",
      "5719:\tlearn: 4.3247775\ttotal: 29m 14s\tremaining: 21m 52s\n",
      "5720:\tlearn: 4.3244149\ttotal: 29m 14s\tremaining: 21m 52s\n",
      "5721:\tlearn: 4.3240687\ttotal: 29m 15s\tremaining: 21m 52s\n",
      "5722:\tlearn: 4.3235639\ttotal: 29m 15s\tremaining: 21m 51s\n",
      "5723:\tlearn: 4.3230047\ttotal: 29m 15s\tremaining: 21m 51s\n",
      "5724:\tlearn: 4.3226818\ttotal: 29m 16s\tremaining: 21m 51s\n",
      "5725:\tlearn: 4.3222786\ttotal: 29m 16s\tremaining: 21m 50s\n",
      "5726:\tlearn: 4.3220410\ttotal: 29m 16s\tremaining: 21m 50s\n",
      "5727:\tlearn: 4.3218704\ttotal: 29m 16s\tremaining: 21m 50s\n",
      "5728:\tlearn: 4.3212191\ttotal: 29m 17s\tremaining: 21m 50s\n",
      "5729:\tlearn: 4.3208796\ttotal: 29m 17s\tremaining: 21m 49s\n",
      "5730:\tlearn: 4.3206483\ttotal: 29m 17s\tremaining: 21m 49s\n",
      "5731:\tlearn: 4.3203561\ttotal: 29m 18s\tremaining: 21m 48s\n",
      "5732:\tlearn: 4.3199676\ttotal: 29m 18s\tremaining: 21m 48s\n",
      "5733:\tlearn: 4.3195493\ttotal: 29m 18s\tremaining: 21m 48s\n",
      "5734:\tlearn: 4.3190894\ttotal: 29m 18s\tremaining: 21m 48s\n",
      "5735:\tlearn: 4.3186862\ttotal: 29m 19s\tremaining: 21m 47s\n",
      "5736:\tlearn: 4.3186169\ttotal: 29m 19s\tremaining: 21m 47s\n",
      "5737:\tlearn: 4.3181959\ttotal: 29m 19s\tremaining: 21m 47s\n",
      "5738:\tlearn: 4.3178443\ttotal: 29m 20s\tremaining: 21m 46s\n",
      "5739:\tlearn: 4.3176631\ttotal: 29m 20s\tremaining: 21m 46s\n",
      "5740:\tlearn: 4.3171049\ttotal: 29m 20s\tremaining: 21m 46s\n",
      "5741:\tlearn: 4.3165898\ttotal: 29m 21s\tremaining: 21m 45s\n",
      "5742:\tlearn: 4.3163548\ttotal: 29m 21s\tremaining: 21m 45s\n",
      "5743:\tlearn: 4.3159039\ttotal: 29m 21s\tremaining: 21m 45s\n",
      "5744:\tlearn: 4.3153598\ttotal: 29m 21s\tremaining: 21m 44s\n",
      "5745:\tlearn: 4.3148204\ttotal: 29m 22s\tremaining: 21m 44s\n",
      "5746:\tlearn: 4.3145494\ttotal: 29m 22s\tremaining: 21m 44s\n",
      "5747:\tlearn: 4.3139765\ttotal: 29m 22s\tremaining: 21m 44s\n",
      "5748:\tlearn: 4.3135478\ttotal: 29m 23s\tremaining: 21m 43s\n",
      "5749:\tlearn: 4.3131616\ttotal: 29m 23s\tremaining: 21m 43s\n",
      "5750:\tlearn: 4.3128672\ttotal: 29m 23s\tremaining: 21m 43s\n",
      "5751:\tlearn: 4.3122358\ttotal: 29m 24s\tremaining: 21m 43s\n",
      "5752:\tlearn: 4.3118209\ttotal: 29m 24s\tremaining: 21m 42s\n",
      "5753:\tlearn: 4.3114960\ttotal: 29m 24s\tremaining: 21m 42s\n",
      "5754:\tlearn: 4.3109133\ttotal: 29m 25s\tremaining: 21m 42s\n",
      "5755:\tlearn: 4.3104971\ttotal: 29m 25s\tremaining: 21m 41s\n",
      "5756:\tlearn: 4.3101406\ttotal: 29m 25s\tremaining: 21m 41s\n",
      "5757:\tlearn: 4.3095050\ttotal: 29m 26s\tremaining: 21m 41s\n",
      "5758:\tlearn: 4.3090475\ttotal: 29m 26s\tremaining: 21m 40s\n",
      "5759:\tlearn: 4.3087962\ttotal: 29m 26s\tremaining: 21m 40s\n",
      "5760:\tlearn: 4.3084668\ttotal: 29m 27s\tremaining: 21m 40s\n",
      "5761:\tlearn: 4.3078687\ttotal: 29m 27s\tremaining: 21m 39s\n",
      "5762:\tlearn: 4.3076160\ttotal: 29m 27s\tremaining: 21m 39s\n",
      "5763:\tlearn: 4.3070504\ttotal: 29m 27s\tremaining: 21m 39s\n",
      "5764:\tlearn: 4.3067498\ttotal: 29m 28s\tremaining: 21m 38s\n",
      "5765:\tlearn: 4.3062788\ttotal: 29m 28s\tremaining: 21m 38s\n",
      "5766:\tlearn: 4.3058865\ttotal: 29m 28s\tremaining: 21m 38s\n",
      "5767:\tlearn: 4.3055214\ttotal: 29m 29s\tremaining: 21m 37s\n",
      "5768:\tlearn: 4.3052938\ttotal: 29m 29s\tremaining: 21m 37s\n",
      "5769:\tlearn: 4.3048098\ttotal: 29m 29s\tremaining: 21m 37s\n",
      "5770:\tlearn: 4.3045615\ttotal: 29m 29s\tremaining: 21m 36s\n",
      "5771:\tlearn: 4.3040663\ttotal: 29m 30s\tremaining: 21m 36s\n",
      "5772:\tlearn: 4.3037624\ttotal: 29m 30s\tremaining: 21m 36s\n",
      "5773:\tlearn: 4.3035303\ttotal: 29m 30s\tremaining: 21m 36s\n",
      "5774:\tlearn: 4.3031984\ttotal: 29m 31s\tremaining: 21m 35s\n",
      "5775:\tlearn: 4.3029207\ttotal: 29m 31s\tremaining: 21m 35s\n",
      "5776:\tlearn: 4.3025658\ttotal: 29m 31s\tremaining: 21m 34s\n",
      "5777:\tlearn: 4.3021632\ttotal: 29m 31s\tremaining: 21m 34s\n",
      "5778:\tlearn: 4.3016824\ttotal: 29m 32s\tremaining: 21m 34s\n",
      "5779:\tlearn: 4.3010511\ttotal: 29m 32s\tremaining: 21m 34s\n",
      "5780:\tlearn: 4.3006821\ttotal: 29m 32s\tremaining: 21m 33s\n",
      "5781:\tlearn: 4.3003463\ttotal: 29m 32s\tremaining: 21m 33s\n",
      "5782:\tlearn: 4.3000180\ttotal: 29m 33s\tremaining: 21m 33s\n",
      "5783:\tlearn: 4.2996328\ttotal: 29m 33s\tremaining: 21m 32s\n",
      "5784:\tlearn: 4.2993828\ttotal: 29m 33s\tremaining: 21m 32s\n",
      "5785:\tlearn: 4.2990645\ttotal: 29m 34s\tremaining: 21m 32s\n",
      "5786:\tlearn: 4.2987510\ttotal: 29m 34s\tremaining: 21m 31s\n",
      "5787:\tlearn: 4.2984198\ttotal: 29m 34s\tremaining: 21m 31s\n",
      "5788:\tlearn: 4.2980287\ttotal: 29m 34s\tremaining: 21m 31s\n",
      "5789:\tlearn: 4.2977530\ttotal: 29m 35s\tremaining: 21m 30s\n",
      "5790:\tlearn: 4.2972538\ttotal: 29m 35s\tremaining: 21m 30s\n",
      "5791:\tlearn: 4.2965238\ttotal: 29m 35s\tremaining: 21m 30s\n",
      "5792:\tlearn: 4.2961010\ttotal: 29m 36s\tremaining: 21m 29s\n",
      "5793:\tlearn: 4.2954795\ttotal: 29m 36s\tremaining: 21m 29s\n",
      "5794:\tlearn: 4.2952485\ttotal: 29m 36s\tremaining: 21m 29s\n",
      "5795:\tlearn: 4.2946763\ttotal: 29m 37s\tremaining: 21m 28s\n",
      "5796:\tlearn: 4.2941524\ttotal: 29m 37s\tremaining: 21m 28s\n",
      "5797:\tlearn: 4.2939209\ttotal: 29m 37s\tremaining: 21m 28s\n",
      "5798:\tlearn: 4.2935993\ttotal: 29m 37s\tremaining: 21m 27s\n",
      "5799:\tlearn: 4.2929622\ttotal: 29m 38s\tremaining: 21m 27s\n",
      "5800:\tlearn: 4.2923466\ttotal: 29m 38s\tremaining: 21m 27s\n",
      "5801:\tlearn: 4.2920249\ttotal: 29m 38s\tremaining: 21m 27s\n",
      "5802:\tlearn: 4.2914890\ttotal: 29m 39s\tremaining: 21m 26s\n",
      "5803:\tlearn: 4.2913010\ttotal: 29m 39s\tremaining: 21m 26s\n",
      "5804:\tlearn: 4.2906563\ttotal: 29m 39s\tremaining: 21m 26s\n",
      "5805:\tlearn: 4.2902726\ttotal: 29m 40s\tremaining: 21m 25s\n",
      "5806:\tlearn: 4.2898023\ttotal: 29m 40s\tremaining: 21m 25s\n",
      "5807:\tlearn: 4.2894258\ttotal: 29m 40s\tremaining: 21m 25s\n",
      "5808:\tlearn: 4.2891289\ttotal: 29m 41s\tremaining: 21m 24s\n",
      "5809:\tlearn: 4.2889142\ttotal: 29m 41s\tremaining: 21m 24s\n",
      "5810:\tlearn: 4.2883224\ttotal: 29m 41s\tremaining: 21m 24s\n",
      "5811:\tlearn: 4.2879850\ttotal: 29m 41s\tremaining: 21m 23s\n",
      "5812:\tlearn: 4.2875248\ttotal: 29m 42s\tremaining: 21m 23s\n",
      "5813:\tlearn: 4.2872046\ttotal: 29m 42s\tremaining: 21m 23s\n",
      "5814:\tlearn: 4.2867709\ttotal: 29m 42s\tremaining: 21m 23s\n",
      "5815:\tlearn: 4.2864300\ttotal: 29m 43s\tremaining: 21m 22s\n",
      "5816:\tlearn: 4.2860989\ttotal: 29m 43s\tremaining: 21m 22s\n",
      "5817:\tlearn: 4.2854174\ttotal: 29m 43s\tremaining: 21m 22s\n",
      "5818:\tlearn: 4.2850623\ttotal: 29m 44s\tremaining: 21m 22s\n",
      "5819:\tlearn: 4.2847485\ttotal: 29m 44s\tremaining: 21m 21s\n",
      "5820:\tlearn: 4.2844366\ttotal: 29m 44s\tremaining: 21m 21s\n",
      "5821:\tlearn: 4.2841103\ttotal: 29m 45s\tremaining: 21m 21s\n",
      "5822:\tlearn: 4.2835280\ttotal: 29m 45s\tremaining: 21m 20s\n",
      "5823:\tlearn: 4.2831594\ttotal: 29m 45s\tremaining: 21m 20s\n",
      "5824:\tlearn: 4.2824777\ttotal: 29m 46s\tremaining: 21m 20s\n",
      "5825:\tlearn: 4.2817669\ttotal: 29m 46s\tremaining: 21m 19s\n",
      "5826:\tlearn: 4.2813254\ttotal: 29m 46s\tremaining: 21m 19s\n",
      "5827:\tlearn: 4.2808100\ttotal: 29m 47s\tremaining: 21m 19s\n",
      "5828:\tlearn: 4.2804324\ttotal: 29m 47s\tremaining: 21m 19s\n",
      "5829:\tlearn: 4.2799111\ttotal: 29m 47s\tremaining: 21m 18s\n",
      "5830:\tlearn: 4.2794065\ttotal: 29m 48s\tremaining: 21m 18s\n",
      "5831:\tlearn: 4.2790123\ttotal: 29m 48s\tremaining: 21m 18s\n",
      "5832:\tlearn: 4.2784143\ttotal: 29m 48s\tremaining: 21m 17s\n",
      "5833:\tlearn: 4.2782200\ttotal: 29m 49s\tremaining: 21m 17s\n",
      "5834:\tlearn: 4.2777518\ttotal: 29m 49s\tremaining: 21m 17s\n",
      "5835:\tlearn: 4.2774146\ttotal: 29m 49s\tremaining: 21m 17s\n",
      "5836:\tlearn: 4.2769635\ttotal: 29m 50s\tremaining: 21m 16s\n",
      "5837:\tlearn: 4.2763584\ttotal: 29m 50s\tremaining: 21m 16s\n",
      "5838:\tlearn: 4.2761304\ttotal: 29m 50s\tremaining: 21m 16s\n",
      "5839:\tlearn: 4.2756557\ttotal: 29m 51s\tremaining: 21m 15s\n",
      "5840:\tlearn: 4.2752570\ttotal: 29m 51s\tremaining: 21m 15s\n",
      "5841:\tlearn: 4.2747863\ttotal: 29m 51s\tremaining: 21m 15s\n",
      "5842:\tlearn: 4.2744784\ttotal: 29m 52s\tremaining: 21m 14s\n",
      "5843:\tlearn: 4.2740437\ttotal: 29m 52s\tremaining: 21m 14s\n",
      "5844:\tlearn: 4.2737172\ttotal: 29m 52s\tremaining: 21m 14s\n",
      "5845:\tlearn: 4.2734446\ttotal: 29m 52s\tremaining: 21m 14s\n",
      "5846:\tlearn: 4.2730017\ttotal: 29m 53s\tremaining: 21m 13s\n",
      "5847:\tlearn: 4.2728613\ttotal: 29m 53s\tremaining: 21m 13s\n",
      "5848:\tlearn: 4.2724052\ttotal: 29m 53s\tremaining: 21m 13s\n",
      "5849:\tlearn: 4.2720872\ttotal: 29m 54s\tremaining: 21m 12s\n",
      "5850:\tlearn: 4.2716964\ttotal: 29m 54s\tremaining: 21m 12s\n",
      "5851:\tlearn: 4.2711079\ttotal: 29m 54s\tremaining: 21m 12s\n",
      "5852:\tlearn: 4.2707428\ttotal: 29m 55s\tremaining: 21m 11s\n",
      "5853:\tlearn: 4.2704320\ttotal: 29m 55s\tremaining: 21m 11s\n",
      "5854:\tlearn: 4.2699804\ttotal: 29m 56s\tremaining: 21m 11s\n",
      "5855:\tlearn: 4.2694779\ttotal: 29m 56s\tremaining: 21m 11s\n",
      "5856:\tlearn: 4.2688764\ttotal: 29m 56s\tremaining: 21m 10s\n",
      "5857:\tlearn: 4.2683366\ttotal: 29m 57s\tremaining: 21m 10s\n",
      "5858:\tlearn: 4.2678075\ttotal: 29m 57s\tremaining: 21m 10s\n",
      "5859:\tlearn: 4.2673962\ttotal: 29m 57s\tremaining: 21m 10s\n",
      "5860:\tlearn: 4.2669624\ttotal: 29m 58s\tremaining: 21m 9s\n",
      "5861:\tlearn: 4.2666916\ttotal: 29m 58s\tremaining: 21m 9s\n",
      "5862:\tlearn: 4.2663732\ttotal: 29m 58s\tremaining: 21m 9s\n",
      "5863:\tlearn: 4.2661158\ttotal: 29m 59s\tremaining: 21m 8s\n",
      "5864:\tlearn: 4.2657868\ttotal: 29m 59s\tremaining: 21m 8s\n",
      "5865:\tlearn: 4.2654617\ttotal: 29m 59s\tremaining: 21m 8s\n",
      "5866:\tlearn: 4.2648669\ttotal: 30m\tremaining: 21m 8s\n",
      "5867:\tlearn: 4.2644992\ttotal: 30m\tremaining: 21m 7s\n",
      "5868:\tlearn: 4.2641881\ttotal: 30m\tremaining: 21m 7s\n",
      "5869:\tlearn: 4.2638567\ttotal: 30m 1s\tremaining: 21m 7s\n",
      "5870:\tlearn: 4.2633830\ttotal: 30m 1s\tremaining: 21m 6s\n",
      "5871:\tlearn: 4.2627542\ttotal: 30m 1s\tremaining: 21m 6s\n",
      "5872:\tlearn: 4.2621903\ttotal: 30m 2s\tremaining: 21m 6s\n",
      "5873:\tlearn: 4.2615236\ttotal: 30m 2s\tremaining: 21m 6s\n",
      "5874:\tlearn: 4.2611254\ttotal: 30m 2s\tremaining: 21m 5s\n",
      "5875:\tlearn: 4.2608127\ttotal: 30m 3s\tremaining: 21m 5s\n",
      "5876:\tlearn: 4.2604037\ttotal: 30m 3s\tremaining: 21m 5s\n",
      "5877:\tlearn: 4.2602148\ttotal: 30m 3s\tremaining: 21m 4s\n",
      "5878:\tlearn: 4.2598689\ttotal: 30m 4s\tremaining: 21m 4s\n",
      "5879:\tlearn: 4.2592869\ttotal: 30m 4s\tremaining: 21m 4s\n",
      "5880:\tlearn: 4.2588384\ttotal: 30m 4s\tremaining: 21m 3s\n",
      "5881:\tlearn: 4.2582934\ttotal: 30m 4s\tremaining: 21m 3s\n",
      "5882:\tlearn: 4.2579267\ttotal: 30m 5s\tremaining: 21m 3s\n",
      "5883:\tlearn: 4.2575957\ttotal: 30m 5s\tremaining: 21m 2s\n",
      "5884:\tlearn: 4.2573005\ttotal: 30m 5s\tremaining: 21m 2s\n",
      "5885:\tlearn: 4.2569684\ttotal: 30m 6s\tremaining: 21m 2s\n",
      "5886:\tlearn: 4.2566481\ttotal: 30m 6s\tremaining: 21m 1s\n",
      "5887:\tlearn: 4.2563189\ttotal: 30m 6s\tremaining: 21m 1s\n",
      "5888:\tlearn: 4.2559782\ttotal: 30m 6s\tremaining: 21m 1s\n",
      "5889:\tlearn: 4.2557454\ttotal: 30m 7s\tremaining: 21m\n",
      "5890:\tlearn: 4.2554465\ttotal: 30m 7s\tremaining: 21m\n",
      "5891:\tlearn: 4.2551543\ttotal: 30m 7s\tremaining: 21m\n",
      "5892:\tlearn: 4.2548477\ttotal: 30m 7s\tremaining: 20m 59s\n",
      "5893:\tlearn: 4.2547757\ttotal: 30m 8s\tremaining: 20m 59s\n",
      "5894:\tlearn: 4.2544111\ttotal: 30m 8s\tremaining: 20m 59s\n",
      "5895:\tlearn: 4.2539309\ttotal: 30m 8s\tremaining: 20m 59s\n",
      "5896:\tlearn: 4.2536343\ttotal: 30m 9s\tremaining: 20m 58s\n",
      "5897:\tlearn: 4.2531974\ttotal: 30m 9s\tremaining: 20m 58s\n",
      "5898:\tlearn: 4.2526957\ttotal: 30m 9s\tremaining: 20m 58s\n",
      "5899:\tlearn: 4.2522665\ttotal: 30m 9s\tremaining: 20m 57s\n",
      "5900:\tlearn: 4.2520101\ttotal: 30m 10s\tremaining: 20m 57s\n",
      "5901:\tlearn: 4.2516402\ttotal: 30m 10s\tremaining: 20m 57s\n",
      "5902:\tlearn: 4.2514209\ttotal: 30m 10s\tremaining: 20m 56s\n",
      "5903:\tlearn: 4.2510560\ttotal: 30m 11s\tremaining: 20m 56s\n",
      "5904:\tlearn: 4.2508583\ttotal: 30m 11s\tremaining: 20m 56s\n",
      "5905:\tlearn: 4.2503236\ttotal: 30m 11s\tremaining: 20m 55s\n",
      "5906:\tlearn: 4.2500488\ttotal: 30m 11s\tremaining: 20m 55s\n",
      "5907:\tlearn: 4.2497551\ttotal: 30m 12s\tremaining: 20m 55s\n",
      "5908:\tlearn: 4.2493837\ttotal: 30m 12s\tremaining: 20m 54s\n",
      "5909:\tlearn: 4.2489598\ttotal: 30m 12s\tremaining: 20m 54s\n",
      "5910:\tlearn: 4.2483839\ttotal: 30m 13s\tremaining: 20m 54s\n",
      "5911:\tlearn: 4.2481767\ttotal: 30m 13s\tremaining: 20m 53s\n",
      "5912:\tlearn: 4.2476248\ttotal: 30m 13s\tremaining: 20m 53s\n",
      "5913:\tlearn: 4.2473216\ttotal: 30m 13s\tremaining: 20m 53s\n",
      "5914:\tlearn: 4.2469532\ttotal: 30m 14s\tremaining: 20m 52s\n",
      "5915:\tlearn: 4.2464081\ttotal: 30m 14s\tremaining: 20m 52s\n",
      "5916:\tlearn: 4.2460225\ttotal: 30m 14s\tremaining: 20m 52s\n",
      "5917:\tlearn: 4.2454979\ttotal: 30m 15s\tremaining: 20m 51s\n",
      "5918:\tlearn: 4.2450139\ttotal: 30m 15s\tremaining: 20m 51s\n",
      "5919:\tlearn: 4.2445256\ttotal: 30m 15s\tremaining: 20m 51s\n",
      "5920:\tlearn: 4.2441820\ttotal: 30m 16s\tremaining: 20m 51s\n",
      "5921:\tlearn: 4.2437637\ttotal: 30m 16s\tremaining: 20m 50s\n",
      "5922:\tlearn: 4.2433123\ttotal: 30m 16s\tremaining: 20m 50s\n",
      "5923:\tlearn: 4.2427551\ttotal: 30m 17s\tremaining: 20m 50s\n",
      "5924:\tlearn: 4.2422210\ttotal: 30m 17s\tremaining: 20m 49s\n",
      "5925:\tlearn: 4.2417938\ttotal: 30m 17s\tremaining: 20m 49s\n",
      "5926:\tlearn: 4.2415280\ttotal: 30m 17s\tremaining: 20m 49s\n",
      "5927:\tlearn: 4.2410335\ttotal: 30m 18s\tremaining: 20m 49s\n",
      "5928:\tlearn: 4.2407395\ttotal: 30m 18s\tremaining: 20m 48s\n",
      "5929:\tlearn: 4.2403658\ttotal: 30m 18s\tremaining: 20m 48s\n",
      "5930:\tlearn: 4.2401474\ttotal: 30m 19s\tremaining: 20m 48s\n",
      "5931:\tlearn: 4.2398302\ttotal: 30m 19s\tremaining: 20m 47s\n",
      "5932:\tlearn: 4.2393669\ttotal: 30m 19s\tremaining: 20m 47s\n",
      "5933:\tlearn: 4.2390715\ttotal: 30m 20s\tremaining: 20m 47s\n",
      "5934:\tlearn: 4.2387324\ttotal: 30m 20s\tremaining: 20m 46s\n",
      "5935:\tlearn: 4.2383151\ttotal: 30m 20s\tremaining: 20m 46s\n",
      "5936:\tlearn: 4.2377793\ttotal: 30m 21s\tremaining: 20m 46s\n",
      "5937:\tlearn: 4.2375932\ttotal: 30m 21s\tremaining: 20m 45s\n",
      "5938:\tlearn: 4.2370339\ttotal: 30m 21s\tremaining: 20m 45s\n",
      "5939:\tlearn: 4.2366508\ttotal: 30m 22s\tremaining: 20m 45s\n",
      "5940:\tlearn: 4.2362428\ttotal: 30m 22s\tremaining: 20m 45s\n",
      "5941:\tlearn: 4.2357225\ttotal: 30m 22s\tremaining: 20m 44s\n",
      "5942:\tlearn: 4.2352962\ttotal: 30m 22s\tremaining: 20m 44s\n",
      "5943:\tlearn: 4.2349426\ttotal: 30m 23s\tremaining: 20m 44s\n",
      "5944:\tlearn: 4.2345596\ttotal: 30m 23s\tremaining: 20m 43s\n",
      "5945:\tlearn: 4.2342902\ttotal: 30m 23s\tremaining: 20m 43s\n",
      "5946:\tlearn: 4.2336542\ttotal: 30m 24s\tremaining: 20m 43s\n",
      "5947:\tlearn: 4.2333056\ttotal: 30m 24s\tremaining: 20m 42s\n",
      "5948:\tlearn: 4.2329908\ttotal: 30m 24s\tremaining: 20m 42s\n",
      "5949:\tlearn: 4.2326320\ttotal: 30m 25s\tremaining: 20m 42s\n",
      "5950:\tlearn: 4.2321201\ttotal: 30m 25s\tremaining: 20m 42s\n",
      "5951:\tlearn: 4.2317570\ttotal: 30m 25s\tremaining: 20m 41s\n",
      "5952:\tlearn: 4.2313904\ttotal: 30m 26s\tremaining: 20m 41s\n",
      "5953:\tlearn: 4.2309760\ttotal: 30m 26s\tremaining: 20m 41s\n",
      "5954:\tlearn: 4.2305437\ttotal: 30m 26s\tremaining: 20m 40s\n",
      "5955:\tlearn: 4.2300652\ttotal: 30m 26s\tremaining: 20m 40s\n",
      "5956:\tlearn: 4.2297286\ttotal: 30m 27s\tremaining: 20m 40s\n",
      "5957:\tlearn: 4.2293475\ttotal: 30m 27s\tremaining: 20m 39s\n",
      "5958:\tlearn: 4.2289766\ttotal: 30m 27s\tremaining: 20m 39s\n",
      "5959:\tlearn: 4.2286704\ttotal: 30m 28s\tremaining: 20m 39s\n",
      "5960:\tlearn: 4.2283637\ttotal: 30m 28s\tremaining: 20m 38s\n",
      "5961:\tlearn: 4.2279848\ttotal: 30m 28s\tremaining: 20m 38s\n",
      "5962:\tlearn: 4.2277111\ttotal: 30m 28s\tremaining: 20m 38s\n",
      "5963:\tlearn: 4.2271270\ttotal: 30m 29s\tremaining: 20m 37s\n",
      "5964:\tlearn: 4.2267053\ttotal: 30m 29s\tremaining: 20m 37s\n",
      "5965:\tlearn: 4.2262988\ttotal: 30m 29s\tremaining: 20m 37s\n",
      "5966:\tlearn: 4.2259657\ttotal: 30m 30s\tremaining: 20m 37s\n",
      "5967:\tlearn: 4.2254222\ttotal: 30m 30s\tremaining: 20m 36s\n",
      "5968:\tlearn: 4.2249169\ttotal: 30m 30s\tremaining: 20m 36s\n",
      "5969:\tlearn: 4.2246181\ttotal: 30m 31s\tremaining: 20m 36s\n",
      "5970:\tlearn: 4.2240944\ttotal: 30m 31s\tremaining: 20m 35s\n",
      "5971:\tlearn: 4.2236616\ttotal: 30m 32s\tremaining: 20m 35s\n",
      "5972:\tlearn: 4.2233294\ttotal: 30m 32s\tremaining: 20m 35s\n",
      "5973:\tlearn: 4.2228589\ttotal: 30m 32s\tremaining: 20m 35s\n",
      "5974:\tlearn: 4.2224256\ttotal: 30m 33s\tremaining: 20m 34s\n",
      "5975:\tlearn: 4.2218141\ttotal: 30m 33s\tremaining: 20m 34s\n",
      "5976:\tlearn: 4.2214443\ttotal: 30m 33s\tremaining: 20m 34s\n",
      "5977:\tlearn: 4.2211081\ttotal: 30m 34s\tremaining: 20m 33s\n",
      "5978:\tlearn: 4.2207545\ttotal: 30m 34s\tremaining: 20m 33s\n",
      "5979:\tlearn: 4.2205369\ttotal: 30m 34s\tremaining: 20m 33s\n",
      "5980:\tlearn: 4.2203668\ttotal: 30m 34s\tremaining: 20m 32s\n",
      "5981:\tlearn: 4.2200101\ttotal: 30m 35s\tremaining: 20m 32s\n",
      "5982:\tlearn: 4.2195226\ttotal: 30m 35s\tremaining: 20m 32s\n",
      "5983:\tlearn: 4.2191388\ttotal: 30m 35s\tremaining: 20m 32s\n",
      "5984:\tlearn: 4.2186331\ttotal: 30m 36s\tremaining: 20m 31s\n",
      "5985:\tlearn: 4.2182657\ttotal: 30m 36s\tremaining: 20m 31s\n",
      "5986:\tlearn: 4.2177932\ttotal: 30m 36s\tremaining: 20m 31s\n",
      "5987:\tlearn: 4.2172519\ttotal: 30m 37s\tremaining: 20m 30s\n",
      "5988:\tlearn: 4.2166681\ttotal: 30m 37s\tremaining: 20m 30s\n",
      "5989:\tlearn: 4.2163932\ttotal: 30m 37s\tremaining: 20m 30s\n",
      "5990:\tlearn: 4.2160398\ttotal: 30m 37s\tremaining: 20m 29s\n",
      "5991:\tlearn: 4.2157488\ttotal: 30m 38s\tremaining: 20m 29s\n",
      "5992:\tlearn: 4.2153608\ttotal: 30m 38s\tremaining: 20m 29s\n",
      "5993:\tlearn: 4.2148635\ttotal: 30m 38s\tremaining: 20m 28s\n",
      "5994:\tlearn: 4.2143337\ttotal: 30m 39s\tremaining: 20m 28s\n",
      "5995:\tlearn: 4.2138535\ttotal: 30m 39s\tremaining: 20m 28s\n",
      "5996:\tlearn: 4.2135033\ttotal: 30m 39s\tremaining: 20m 28s\n",
      "5997:\tlearn: 4.2130645\ttotal: 30m 40s\tremaining: 20m 27s\n",
      "5998:\tlearn: 4.2127170\ttotal: 30m 40s\tremaining: 20m 27s\n",
      "5999:\tlearn: 4.2124528\ttotal: 30m 40s\tremaining: 20m 27s\n",
      "6000:\tlearn: 4.2119234\ttotal: 30m 41s\tremaining: 20m 26s\n",
      "6001:\tlearn: 4.2115577\ttotal: 30m 41s\tremaining: 20m 26s\n",
      "6002:\tlearn: 4.2113337\ttotal: 30m 41s\tremaining: 20m 26s\n",
      "6003:\tlearn: 4.2110083\ttotal: 30m 41s\tremaining: 20m 25s\n",
      "6004:\tlearn: 4.2107044\ttotal: 30m 42s\tremaining: 20m 25s\n",
      "6005:\tlearn: 4.2102597\ttotal: 30m 42s\tremaining: 20m 25s\n",
      "6006:\tlearn: 4.2100334\ttotal: 30m 42s\tremaining: 20m 24s\n",
      "6007:\tlearn: 4.2097492\ttotal: 30m 42s\tremaining: 20m 24s\n",
      "6008:\tlearn: 4.2094158\ttotal: 30m 43s\tremaining: 20m 24s\n",
      "6009:\tlearn: 4.2091400\ttotal: 30m 43s\tremaining: 20m 23s\n",
      "6010:\tlearn: 4.2085996\ttotal: 30m 43s\tremaining: 20m 23s\n",
      "6011:\tlearn: 4.2083235\ttotal: 30m 44s\tremaining: 20m 23s\n",
      "6012:\tlearn: 4.2080531\ttotal: 30m 44s\tremaining: 20m 22s\n",
      "6013:\tlearn: 4.2077246\ttotal: 30m 44s\tremaining: 20m 22s\n",
      "6014:\tlearn: 4.2074156\ttotal: 30m 44s\tremaining: 20m 22s\n",
      "6015:\tlearn: 4.2068779\ttotal: 30m 45s\tremaining: 20m 21s\n",
      "6016:\tlearn: 4.2065021\ttotal: 30m 45s\tremaining: 20m 21s\n",
      "6017:\tlearn: 4.2061853\ttotal: 30m 45s\tremaining: 20m 21s\n",
      "6018:\tlearn: 4.2058399\ttotal: 30m 46s\tremaining: 20m 21s\n",
      "6019:\tlearn: 4.2055308\ttotal: 30m 46s\tremaining: 20m 20s\n",
      "6020:\tlearn: 4.2050993\ttotal: 30m 46s\tremaining: 20m 20s\n",
      "6021:\tlearn: 4.2046316\ttotal: 30m 46s\tremaining: 20m 20s\n",
      "6022:\tlearn: 4.2043272\ttotal: 30m 47s\tremaining: 20m 19s\n",
      "6023:\tlearn: 4.2039287\ttotal: 30m 47s\tremaining: 20m 19s\n",
      "6024:\tlearn: 4.2036074\ttotal: 30m 47s\tremaining: 20m 19s\n",
      "6025:\tlearn: 4.2032317\ttotal: 30m 48s\tremaining: 20m 18s\n",
      "6026:\tlearn: 4.2030607\ttotal: 30m 48s\tremaining: 20m 18s\n",
      "6027:\tlearn: 4.2027042\ttotal: 30m 48s\tremaining: 20m 18s\n",
      "6028:\tlearn: 4.2021997\ttotal: 30m 49s\tremaining: 20m 18s\n",
      "6029:\tlearn: 4.2020367\ttotal: 30m 49s\tremaining: 20m 17s\n",
      "6030:\tlearn: 4.2016589\ttotal: 30m 49s\tremaining: 20m 17s\n",
      "6031:\tlearn: 4.2011029\ttotal: 30m 50s\tremaining: 20m 17s\n",
      "6032:\tlearn: 4.2008317\ttotal: 30m 50s\tremaining: 20m 16s\n",
      "6033:\tlearn: 4.2004726\ttotal: 30m 50s\tremaining: 20m 16s\n",
      "6034:\tlearn: 4.2000850\ttotal: 30m 51s\tremaining: 20m 16s\n",
      "6035:\tlearn: 4.1996668\ttotal: 30m 51s\tremaining: 20m 15s\n",
      "6036:\tlearn: 4.1992860\ttotal: 30m 51s\tremaining: 20m 15s\n",
      "6037:\tlearn: 4.1987510\ttotal: 30m 51s\tremaining: 20m 15s\n",
      "6038:\tlearn: 4.1984043\ttotal: 30m 52s\tremaining: 20m 14s\n",
      "6039:\tlearn: 4.1981553\ttotal: 30m 52s\tremaining: 20m 14s\n",
      "6040:\tlearn: 4.1975964\ttotal: 30m 52s\tremaining: 20m 14s\n",
      "6041:\tlearn: 4.1973465\ttotal: 30m 53s\tremaining: 20m 13s\n",
      "6042:\tlearn: 4.1970800\ttotal: 30m 53s\tremaining: 20m 13s\n",
      "6043:\tlearn: 4.1966504\ttotal: 30m 53s\tremaining: 20m 13s\n",
      "6044:\tlearn: 4.1961730\ttotal: 30m 53s\tremaining: 20m 12s\n",
      "6045:\tlearn: 4.1956590\ttotal: 30m 54s\tremaining: 20m 12s\n",
      "6046:\tlearn: 4.1953210\ttotal: 30m 54s\tremaining: 20m 12s\n",
      "6047:\tlearn: 4.1950410\ttotal: 30m 54s\tremaining: 20m 11s\n",
      "6048:\tlearn: 4.1948329\ttotal: 30m 54s\tremaining: 20m 11s\n",
      "6049:\tlearn: 4.1942686\ttotal: 30m 55s\tremaining: 20m 11s\n",
      "6050:\tlearn: 4.1938437\ttotal: 30m 55s\tremaining: 20m 10s\n",
      "6051:\tlearn: 4.1935471\ttotal: 30m 55s\tremaining: 20m 10s\n",
      "6052:\tlearn: 4.1931574\ttotal: 30m 56s\tremaining: 20m 10s\n",
      "6053:\tlearn: 4.1927703\ttotal: 30m 56s\tremaining: 20m 9s\n",
      "6054:\tlearn: 4.1923877\ttotal: 30m 56s\tremaining: 20m 9s\n",
      "6055:\tlearn: 4.1920448\ttotal: 30m 56s\tremaining: 20m 9s\n",
      "6056:\tlearn: 4.1917891\ttotal: 30m 57s\tremaining: 20m 9s\n",
      "6057:\tlearn: 4.1914655\ttotal: 30m 57s\tremaining: 20m 8s\n",
      "6058:\tlearn: 4.1912683\ttotal: 30m 57s\tremaining: 20m 8s\n",
      "6059:\tlearn: 4.1906546\ttotal: 30m 58s\tremaining: 20m 8s\n",
      "6060:\tlearn: 4.1902515\ttotal: 30m 58s\tremaining: 20m 7s\n",
      "6061:\tlearn: 4.1896900\ttotal: 30m 58s\tremaining: 20m 7s\n",
      "6062:\tlearn: 4.1893458\ttotal: 30m 59s\tremaining: 20m 7s\n",
      "6063:\tlearn: 4.1887647\ttotal: 30m 59s\tremaining: 20m 7s\n",
      "6064:\tlearn: 4.1883904\ttotal: 30m 59s\tremaining: 20m 6s\n",
      "6065:\tlearn: 4.1878168\ttotal: 31m\tremaining: 20m 6s\n",
      "6066:\tlearn: 4.1875282\ttotal: 31m\tremaining: 20m 6s\n",
      "6067:\tlearn: 4.1872000\ttotal: 31m\tremaining: 20m 5s\n",
      "6068:\tlearn: 4.1869335\ttotal: 31m 1s\tremaining: 20m 5s\n",
      "6069:\tlearn: 4.1864638\ttotal: 31m 1s\tremaining: 20m 5s\n",
      "6070:\tlearn: 4.1860027\ttotal: 31m 1s\tremaining: 20m 4s\n",
      "6071:\tlearn: 4.1856839\ttotal: 31m 1s\tremaining: 20m 4s\n",
      "6072:\tlearn: 4.1853941\ttotal: 31m 2s\tremaining: 20m 4s\n",
      "6073:\tlearn: 4.1850141\ttotal: 31m 2s\tremaining: 20m 3s\n",
      "6074:\tlearn: 4.1846919\ttotal: 31m 2s\tremaining: 20m 3s\n",
      "6075:\tlearn: 4.1843630\ttotal: 31m 3s\tremaining: 20m 3s\n",
      "6076:\tlearn: 4.1837302\ttotal: 31m 3s\tremaining: 20m 3s\n",
      "6077:\tlearn: 4.1834614\ttotal: 31m 3s\tremaining: 20m 2s\n",
      "6078:\tlearn: 4.1831233\ttotal: 31m 4s\tremaining: 20m 2s\n",
      "6079:\tlearn: 4.1828437\ttotal: 31m 4s\tremaining: 20m 2s\n",
      "6080:\tlearn: 4.1825000\ttotal: 31m 4s\tremaining: 20m 1s\n",
      "6081:\tlearn: 4.1821932\ttotal: 31m 4s\tremaining: 20m 1s\n",
      "6082:\tlearn: 4.1818692\ttotal: 31m 5s\tremaining: 20m 1s\n",
      "6083:\tlearn: 4.1814376\ttotal: 31m 5s\tremaining: 20m\n",
      "6084:\tlearn: 4.1811200\ttotal: 31m 5s\tremaining: 20m\n",
      "6085:\tlearn: 4.1804022\ttotal: 31m 6s\tremaining: 20m\n",
      "6086:\tlearn: 4.1798949\ttotal: 31m 6s\tremaining: 19m 59s\n",
      "6087:\tlearn: 4.1795799\ttotal: 31m 6s\tremaining: 19m 59s\n",
      "6088:\tlearn: 4.1790953\ttotal: 31m 7s\tremaining: 19m 59s\n",
      "6089:\tlearn: 4.1787872\ttotal: 31m 7s\tremaining: 19m 58s\n",
      "6090:\tlearn: 4.1783732\ttotal: 31m 7s\tremaining: 19m 58s\n",
      "6091:\tlearn: 4.1778136\ttotal: 31m 7s\tremaining: 19m 58s\n",
      "6092:\tlearn: 4.1776253\ttotal: 31m 7s\tremaining: 19m 57s\n",
      "6093:\tlearn: 4.1771642\ttotal: 31m 8s\tremaining: 19m 57s\n",
      "6094:\tlearn: 4.1768483\ttotal: 31m 8s\tremaining: 19m 57s\n",
      "6095:\tlearn: 4.1763771\ttotal: 31m 8s\tremaining: 19m 56s\n",
      "6096:\tlearn: 4.1759065\ttotal: 31m 9s\tremaining: 19m 56s\n",
      "6097:\tlearn: 4.1755226\ttotal: 31m 9s\tremaining: 19m 56s\n",
      "6098:\tlearn: 4.1749846\ttotal: 31m 9s\tremaining: 19m 56s\n",
      "6099:\tlearn: 4.1745305\ttotal: 31m 10s\tremaining: 19m 55s\n",
      "6100:\tlearn: 4.1740438\ttotal: 31m 10s\tremaining: 19m 55s\n",
      "6101:\tlearn: 4.1738045\ttotal: 31m 10s\tremaining: 19m 55s\n",
      "6102:\tlearn: 4.1733474\ttotal: 31m 11s\tremaining: 19m 54s\n",
      "6103:\tlearn: 4.1728682\ttotal: 31m 11s\tremaining: 19m 54s\n",
      "6104:\tlearn: 4.1725872\ttotal: 31m 11s\tremaining: 19m 54s\n",
      "6105:\tlearn: 4.1722960\ttotal: 31m 12s\tremaining: 19m 54s\n",
      "6106:\tlearn: 4.1719750\ttotal: 31m 12s\tremaining: 19m 53s\n",
      "6107:\tlearn: 4.1716955\ttotal: 31m 12s\tremaining: 19m 53s\n",
      "6108:\tlearn: 4.1714331\ttotal: 31m 13s\tremaining: 19m 53s\n",
      "6109:\tlearn: 4.1710690\ttotal: 31m 13s\tremaining: 19m 52s\n",
      "6110:\tlearn: 4.1705836\ttotal: 31m 13s\tremaining: 19m 52s\n",
      "6111:\tlearn: 4.1701202\ttotal: 31m 14s\tremaining: 19m 52s\n",
      "6112:\tlearn: 4.1698311\ttotal: 31m 14s\tremaining: 19m 51s\n",
      "6113:\tlearn: 4.1694349\ttotal: 31m 14s\tremaining: 19m 51s\n",
      "6114:\tlearn: 4.1691281\ttotal: 31m 14s\tremaining: 19m 51s\n",
      "6115:\tlearn: 4.1685982\ttotal: 31m 15s\tremaining: 19m 50s\n",
      "6116:\tlearn: 4.1682954\ttotal: 31m 15s\tremaining: 19m 50s\n",
      "6117:\tlearn: 4.1679483\ttotal: 31m 15s\tremaining: 19m 50s\n",
      "6118:\tlearn: 4.1674059\ttotal: 31m 16s\tremaining: 19m 49s\n",
      "6119:\tlearn: 4.1668903\ttotal: 31m 16s\tremaining: 19m 49s\n",
      "6120:\tlearn: 4.1662071\ttotal: 31m 16s\tremaining: 19m 49s\n",
      "6121:\tlearn: 4.1659681\ttotal: 31m 17s\tremaining: 19m 49s\n",
      "6122:\tlearn: 4.1656303\ttotal: 31m 17s\tremaining: 19m 48s\n",
      "6123:\tlearn: 4.1654050\ttotal: 31m 17s\tremaining: 19m 48s\n",
      "6124:\tlearn: 4.1648762\ttotal: 31m 18s\tremaining: 19m 48s\n",
      "6125:\tlearn: 4.1644552\ttotal: 31m 18s\tremaining: 19m 47s\n",
      "6126:\tlearn: 4.1639725\ttotal: 31m 18s\tremaining: 19m 47s\n",
      "6127:\tlearn: 4.1636162\ttotal: 31m 18s\tremaining: 19m 47s\n",
      "6128:\tlearn: 4.1633353\ttotal: 31m 19s\tremaining: 19m 46s\n",
      "6129:\tlearn: 4.1630775\ttotal: 31m 19s\tremaining: 19m 46s\n",
      "6130:\tlearn: 4.1627087\ttotal: 31m 19s\tremaining: 19m 46s\n",
      "6131:\tlearn: 4.1622877\ttotal: 31m 20s\tremaining: 19m 45s\n",
      "6132:\tlearn: 4.1620501\ttotal: 31m 20s\tremaining: 19m 45s\n",
      "6133:\tlearn: 4.1616540\ttotal: 31m 20s\tremaining: 19m 45s\n",
      "6134:\tlearn: 4.1610633\ttotal: 31m 21s\tremaining: 19m 45s\n",
      "6135:\tlearn: 4.1606859\ttotal: 31m 21s\tremaining: 19m 44s\n",
      "6136:\tlearn: 4.1604451\ttotal: 31m 21s\tremaining: 19m 44s\n",
      "6137:\tlearn: 4.1601228\ttotal: 31m 22s\tremaining: 19m 44s\n",
      "6138:\tlearn: 4.1597206\ttotal: 31m 22s\tremaining: 19m 44s\n",
      "6139:\tlearn: 4.1591675\ttotal: 31m 22s\tremaining: 19m 43s\n",
      "6140:\tlearn: 4.1586011\ttotal: 31m 23s\tremaining: 19m 43s\n",
      "6141:\tlearn: 4.1581704\ttotal: 31m 23s\tremaining: 19m 43s\n",
      "6142:\tlearn: 4.1578852\ttotal: 31m 23s\tremaining: 19m 42s\n",
      "6143:\tlearn: 4.1575533\ttotal: 31m 24s\tremaining: 19m 42s\n",
      "6144:\tlearn: 4.1571544\ttotal: 31m 24s\tremaining: 19m 42s\n",
      "6145:\tlearn: 4.1569769\ttotal: 31m 24s\tremaining: 19m 41s\n",
      "6146:\tlearn: 4.1566269\ttotal: 31m 25s\tremaining: 19m 41s\n",
      "6147:\tlearn: 4.1564342\ttotal: 31m 25s\tremaining: 19m 41s\n",
      "6148:\tlearn: 4.1562648\ttotal: 31m 25s\tremaining: 19m 40s\n",
      "6149:\tlearn: 4.1559471\ttotal: 31m 26s\tremaining: 19m 40s\n",
      "6150:\tlearn: 4.1555512\ttotal: 31m 26s\tremaining: 19m 40s\n",
      "6151:\tlearn: 4.1552909\ttotal: 31m 26s\tremaining: 19m 40s\n",
      "6152:\tlearn: 4.1546478\ttotal: 31m 26s\tremaining: 19m 39s\n",
      "6153:\tlearn: 4.1544409\ttotal: 31m 27s\tremaining: 19m 39s\n",
      "6154:\tlearn: 4.1541798\ttotal: 31m 27s\tremaining: 19m 39s\n",
      "6155:\tlearn: 4.1538957\ttotal: 31m 27s\tremaining: 19m 38s\n",
      "6156:\tlearn: 4.1537049\ttotal: 31m 28s\tremaining: 19m 38s\n",
      "6157:\tlearn: 4.1532755\ttotal: 31m 28s\tremaining: 19m 38s\n",
      "6158:\tlearn: 4.1528318\ttotal: 31m 28s\tremaining: 19m 37s\n",
      "6159:\tlearn: 4.1526244\ttotal: 31m 29s\tremaining: 19m 37s\n",
      "6160:\tlearn: 4.1523149\ttotal: 31m 29s\tremaining: 19m 37s\n",
      "6161:\tlearn: 4.1518917\ttotal: 31m 29s\tremaining: 19m 36s\n",
      "6162:\tlearn: 4.1514780\ttotal: 31m 30s\tremaining: 19m 36s\n",
      "6163:\tlearn: 4.1508738\ttotal: 31m 30s\tremaining: 19m 36s\n",
      "6164:\tlearn: 4.1504114\ttotal: 31m 30s\tremaining: 19m 36s\n",
      "6165:\tlearn: 4.1499305\ttotal: 31m 31s\tremaining: 19m 35s\n",
      "6166:\tlearn: 4.1496481\ttotal: 31m 31s\tremaining: 19m 35s\n",
      "6167:\tlearn: 4.1493255\ttotal: 31m 31s\tremaining: 19m 35s\n",
      "6168:\tlearn: 4.1491536\ttotal: 31m 31s\tremaining: 19m 34s\n",
      "6169:\tlearn: 4.1487822\ttotal: 31m 32s\tremaining: 19m 34s\n",
      "6170:\tlearn: 4.1485155\ttotal: 31m 32s\tremaining: 19m 34s\n",
      "6171:\tlearn: 4.1480720\ttotal: 31m 32s\tremaining: 19m 34s\n",
      "6172:\tlearn: 4.1475333\ttotal: 31m 33s\tremaining: 19m 33s\n",
      "6173:\tlearn: 4.1472330\ttotal: 31m 33s\tremaining: 19m 33s\n",
      "6174:\tlearn: 4.1469509\ttotal: 31m 33s\tremaining: 19m 33s\n",
      "6175:\tlearn: 4.1466714\ttotal: 31m 34s\tremaining: 19m 32s\n",
      "6176:\tlearn: 4.1461591\ttotal: 31m 34s\tremaining: 19m 32s\n",
      "6177:\tlearn: 4.1459010\ttotal: 31m 34s\tremaining: 19m 32s\n",
      "6178:\tlearn: 4.1456202\ttotal: 31m 35s\tremaining: 19m 31s\n",
      "6179:\tlearn: 4.1451334\ttotal: 31m 35s\tremaining: 19m 31s\n",
      "6180:\tlearn: 4.1448512\ttotal: 31m 35s\tremaining: 19m 31s\n",
      "6181:\tlearn: 4.1447191\ttotal: 31m 36s\tremaining: 19m 30s\n",
      "6182:\tlearn: 4.1442034\ttotal: 31m 36s\tremaining: 19m 30s\n",
      "6183:\tlearn: 4.1438657\ttotal: 31m 36s\tremaining: 19m 30s\n",
      "6184:\tlearn: 4.1435524\ttotal: 31m 36s\tremaining: 19m 30s\n",
      "6185:\tlearn: 4.1432343\ttotal: 31m 37s\tremaining: 19m 29s\n",
      "6186:\tlearn: 4.1429535\ttotal: 31m 37s\tremaining: 19m 29s\n",
      "6187:\tlearn: 4.1423095\ttotal: 31m 37s\tremaining: 19m 29s\n",
      "6188:\tlearn: 4.1420043\ttotal: 31m 38s\tremaining: 19m 28s\n",
      "6189:\tlearn: 4.1416514\ttotal: 31m 38s\tremaining: 19m 28s\n",
      "6190:\tlearn: 4.1413643\ttotal: 31m 38s\tremaining: 19m 28s\n",
      "6191:\tlearn: 4.1410778\ttotal: 31m 38s\tremaining: 19m 27s\n",
      "6192:\tlearn: 4.1405119\ttotal: 31m 39s\tremaining: 19m 27s\n",
      "6193:\tlearn: 4.1401777\ttotal: 31m 39s\tremaining: 19m 27s\n",
      "6194:\tlearn: 4.1397694\ttotal: 31m 39s\tremaining: 19m 26s\n",
      "6195:\tlearn: 4.1394338\ttotal: 31m 40s\tremaining: 19m 26s\n",
      "6196:\tlearn: 4.1387690\ttotal: 31m 40s\tremaining: 19m 26s\n",
      "6197:\tlearn: 4.1384297\ttotal: 31m 40s\tremaining: 19m 26s\n",
      "6198:\tlearn: 4.1378960\ttotal: 31m 41s\tremaining: 19m 25s\n",
      "6199:\tlearn: 4.1373810\ttotal: 31m 41s\tremaining: 19m 25s\n",
      "6200:\tlearn: 4.1370059\ttotal: 31m 41s\tremaining: 19m 25s\n",
      "6201:\tlearn: 4.1365990\ttotal: 31m 42s\tremaining: 19m 24s\n",
      "6202:\tlearn: 4.1361389\ttotal: 31m 42s\tremaining: 19m 24s\n",
      "6203:\tlearn: 4.1358061\ttotal: 31m 42s\tremaining: 19m 24s\n",
      "6204:\tlearn: 4.1353445\ttotal: 31m 43s\tremaining: 19m 23s\n",
      "6205:\tlearn: 4.1347307\ttotal: 31m 43s\tremaining: 19m 23s\n",
      "6206:\tlearn: 4.1342436\ttotal: 31m 43s\tremaining: 19m 23s\n",
      "6207:\tlearn: 4.1338114\ttotal: 31m 44s\tremaining: 19m 23s\n",
      "6208:\tlearn: 4.1333899\ttotal: 31m 44s\tremaining: 19m 22s\n",
      "6209:\tlearn: 4.1331911\ttotal: 31m 44s\tremaining: 19m 22s\n",
      "6210:\tlearn: 4.1328157\ttotal: 31m 44s\tremaining: 19m 22s\n",
      "6211:\tlearn: 4.1325211\ttotal: 31m 45s\tremaining: 19m 21s\n",
      "6212:\tlearn: 4.1323387\ttotal: 31m 45s\tremaining: 19m 21s\n",
      "6213:\tlearn: 4.1318778\ttotal: 31m 45s\tremaining: 19m 21s\n",
      "6214:\tlearn: 4.1314275\ttotal: 31m 46s\tremaining: 19m 20s\n",
      "6215:\tlearn: 4.1310543\ttotal: 31m 46s\tremaining: 19m 20s\n",
      "6216:\tlearn: 4.1304886\ttotal: 31m 46s\tremaining: 19m 20s\n",
      "6217:\tlearn: 4.1300336\ttotal: 31m 47s\tremaining: 19m 19s\n",
      "6218:\tlearn: 4.1294412\ttotal: 31m 47s\tremaining: 19m 19s\n",
      "6219:\tlearn: 4.1291927\ttotal: 31m 47s\tremaining: 19m 19s\n",
      "6220:\tlearn: 4.1290120\ttotal: 31m 47s\tremaining: 19m 18s\n",
      "6221:\tlearn: 4.1287296\ttotal: 31m 48s\tremaining: 19m 18s\n",
      "6222:\tlearn: 4.1284627\ttotal: 31m 48s\tremaining: 19m 18s\n",
      "6223:\tlearn: 4.1282624\ttotal: 31m 48s\tremaining: 19m 18s\n",
      "6224:\tlearn: 4.1279988\ttotal: 31m 48s\tremaining: 19m 17s\n",
      "6225:\tlearn: 4.1276604\ttotal: 31m 49s\tremaining: 19m 17s\n",
      "6226:\tlearn: 4.1273226\ttotal: 31m 49s\tremaining: 19m 17s\n",
      "6227:\tlearn: 4.1269536\ttotal: 31m 49s\tremaining: 19m 16s\n",
      "6228:\tlearn: 4.1263484\ttotal: 31m 50s\tremaining: 19m 16s\n",
      "6229:\tlearn: 4.1259641\ttotal: 31m 50s\tremaining: 19m 16s\n",
      "6230:\tlearn: 4.1253734\ttotal: 31m 50s\tremaining: 19m 15s\n",
      "6231:\tlearn: 4.1249150\ttotal: 31m 50s\tremaining: 19m 15s\n",
      "6232:\tlearn: 4.1243742\ttotal: 31m 51s\tremaining: 19m 15s\n",
      "6233:\tlearn: 4.1241481\ttotal: 31m 51s\tremaining: 19m 14s\n",
      "6234:\tlearn: 4.1236788\ttotal: 31m 51s\tremaining: 19m 14s\n",
      "6235:\tlearn: 4.1231799\ttotal: 31m 52s\tremaining: 19m 14s\n",
      "6236:\tlearn: 4.1228790\ttotal: 31m 52s\tremaining: 19m 13s\n",
      "6237:\tlearn: 4.1226788\ttotal: 31m 52s\tremaining: 19m 13s\n",
      "6238:\tlearn: 4.1221465\ttotal: 31m 52s\tremaining: 19m 13s\n",
      "6239:\tlearn: 4.1219103\ttotal: 31m 53s\tremaining: 19m 12s\n",
      "6240:\tlearn: 4.1215685\ttotal: 31m 53s\tremaining: 19m 12s\n",
      "6241:\tlearn: 4.1210139\ttotal: 31m 53s\tremaining: 19m 12s\n",
      "6242:\tlearn: 4.1204667\ttotal: 31m 53s\tremaining: 19m 11s\n",
      "6243:\tlearn: 4.1202302\ttotal: 31m 54s\tremaining: 19m 11s\n",
      "6244:\tlearn: 4.1198661\ttotal: 31m 54s\tremaining: 19m 11s\n",
      "6245:\tlearn: 4.1195483\ttotal: 31m 54s\tremaining: 19m 10s\n",
      "6246:\tlearn: 4.1192458\ttotal: 31m 55s\tremaining: 19m 10s\n",
      "6247:\tlearn: 4.1190005\ttotal: 31m 55s\tremaining: 19m 10s\n",
      "6248:\tlearn: 4.1187206\ttotal: 31m 55s\tremaining: 19m 9s\n",
      "6249:\tlearn: 4.1182659\ttotal: 31m 55s\tremaining: 19m 9s\n",
      "6250:\tlearn: 4.1179396\ttotal: 31m 56s\tremaining: 19m 9s\n",
      "6251:\tlearn: 4.1174895\ttotal: 31m 56s\tremaining: 19m 8s\n",
      "6252:\tlearn: 4.1171303\ttotal: 31m 56s\tremaining: 19m 8s\n",
      "6253:\tlearn: 4.1165562\ttotal: 31m 57s\tremaining: 19m 8s\n",
      "6254:\tlearn: 4.1161942\ttotal: 31m 57s\tremaining: 19m 8s\n",
      "6255:\tlearn: 4.1159193\ttotal: 31m 57s\tremaining: 19m 7s\n",
      "6256:\tlearn: 4.1154645\ttotal: 31m 58s\tremaining: 19m 7s\n",
      "6257:\tlearn: 4.1151001\ttotal: 31m 58s\tremaining: 19m 7s\n",
      "6258:\tlearn: 4.1143978\ttotal: 31m 58s\tremaining: 19m 6s\n",
      "6259:\tlearn: 4.1140781\ttotal: 31m 58s\tremaining: 19m 6s\n",
      "6260:\tlearn: 4.1137862\ttotal: 31m 59s\tremaining: 19m 6s\n",
      "6261:\tlearn: 4.1136405\ttotal: 31m 59s\tremaining: 19m 5s\n",
      "6262:\tlearn: 4.1133510\ttotal: 31m 59s\tremaining: 19m 5s\n",
      "6263:\tlearn: 4.1131647\ttotal: 31m 59s\tremaining: 19m 5s\n",
      "6264:\tlearn: 4.1128772\ttotal: 32m\tremaining: 19m 4s\n",
      "6265:\tlearn: 4.1126033\ttotal: 32m\tremaining: 19m 4s\n",
      "6266:\tlearn: 4.1124871\ttotal: 32m\tremaining: 19m 3s\n",
      "6267:\tlearn: 4.1121511\ttotal: 32m\tremaining: 19m 3s\n",
      "6268:\tlearn: 4.1119650\ttotal: 32m 1s\tremaining: 19m 3s\n",
      "6269:\tlearn: 4.1116752\ttotal: 32m 1s\tremaining: 19m 2s\n",
      "6270:\tlearn: 4.1113521\ttotal: 32m 1s\tremaining: 19m 2s\n",
      "6271:\tlearn: 4.1110964\ttotal: 32m 1s\tremaining: 19m 2s\n",
      "6272:\tlearn: 4.1106956\ttotal: 32m 2s\tremaining: 19m 1s\n",
      "6273:\tlearn: 4.1102215\ttotal: 32m 2s\tremaining: 19m 1s\n",
      "6274:\tlearn: 4.1096839\ttotal: 32m 2s\tremaining: 19m 1s\n",
      "6275:\tlearn: 4.1093565\ttotal: 32m 2s\tremaining: 19m\n",
      "6276:\tlearn: 4.1089751\ttotal: 32m 3s\tremaining: 19m\n",
      "6277:\tlearn: 4.1086192\ttotal: 32m 3s\tremaining: 19m\n",
      "6278:\tlearn: 4.1082684\ttotal: 32m 3s\tremaining: 19m\n",
      "6279:\tlearn: 4.1078695\ttotal: 32m 4s\tremaining: 18m 59s\n",
      "6280:\tlearn: 4.1073980\ttotal: 32m 4s\tremaining: 18m 59s\n",
      "6281:\tlearn: 4.1070179\ttotal: 32m 4s\tremaining: 18m 59s\n",
      "6282:\tlearn: 4.1067544\ttotal: 32m 4s\tremaining: 18m 58s\n",
      "6283:\tlearn: 4.1064638\ttotal: 32m 5s\tremaining: 18m 58s\n",
      "6284:\tlearn: 4.1060646\ttotal: 32m 5s\tremaining: 18m 58s\n",
      "6285:\tlearn: 4.1056882\ttotal: 32m 5s\tremaining: 18m 57s\n",
      "6286:\tlearn: 4.1052413\ttotal: 32m 5s\tremaining: 18m 57s\n",
      "6287:\tlearn: 4.1047426\ttotal: 32m 6s\tremaining: 18m 57s\n",
      "6288:\tlearn: 4.1044593\ttotal: 32m 6s\tremaining: 18m 56s\n",
      "6289:\tlearn: 4.1041578\ttotal: 32m 6s\tremaining: 18m 56s\n",
      "6290:\tlearn: 4.1039041\ttotal: 32m 6s\tremaining: 18m 56s\n",
      "6291:\tlearn: 4.1034859\ttotal: 32m 7s\tremaining: 18m 55s\n",
      "6292:\tlearn: 4.1029590\ttotal: 32m 7s\tremaining: 18m 55s\n",
      "6293:\tlearn: 4.1026555\ttotal: 32m 7s\tremaining: 18m 55s\n",
      "6294:\tlearn: 4.1021057\ttotal: 32m 8s\tremaining: 18m 54s\n",
      "6295:\tlearn: 4.1014867\ttotal: 32m 8s\tremaining: 18m 54s\n",
      "6296:\tlearn: 4.1011090\ttotal: 32m 8s\tremaining: 18m 54s\n",
      "6297:\tlearn: 4.1007736\ttotal: 32m 9s\tremaining: 18m 53s\n",
      "6298:\tlearn: 4.1004314\ttotal: 32m 9s\tremaining: 18m 53s\n",
      "6299:\tlearn: 4.1000856\ttotal: 32m 9s\tremaining: 18m 53s\n",
      "6300:\tlearn: 4.0996498\ttotal: 32m 9s\tremaining: 18m 52s\n",
      "6301:\tlearn: 4.0994787\ttotal: 32m 10s\tremaining: 18m 52s\n",
      "6302:\tlearn: 4.0990984\ttotal: 32m 10s\tremaining: 18m 52s\n",
      "6303:\tlearn: 4.0987226\ttotal: 32m 10s\tremaining: 18m 51s\n",
      "6304:\tlearn: 4.0983834\ttotal: 32m 10s\tremaining: 18m 51s\n",
      "6305:\tlearn: 4.0981262\ttotal: 32m 11s\tremaining: 18m 51s\n",
      "6306:\tlearn: 4.0978670\ttotal: 32m 11s\tremaining: 18m 50s\n",
      "6307:\tlearn: 4.0973434\ttotal: 32m 11s\tremaining: 18m 50s\n",
      "6308:\tlearn: 4.0969486\ttotal: 32m 12s\tremaining: 18m 50s\n",
      "6309:\tlearn: 4.0966128\ttotal: 32m 12s\tremaining: 18m 50s\n",
      "6310:\tlearn: 4.0963350\ttotal: 32m 12s\tremaining: 18m 49s\n",
      "6311:\tlearn: 4.0958872\ttotal: 32m 12s\tremaining: 18m 49s\n",
      "6312:\tlearn: 4.0955132\ttotal: 32m 13s\tremaining: 18m 48s\n",
      "6313:\tlearn: 4.0952393\ttotal: 32m 13s\tremaining: 18m 48s\n",
      "6314:\tlearn: 4.0948997\ttotal: 32m 13s\tremaining: 18m 48s\n",
      "6315:\tlearn: 4.0945190\ttotal: 32m 14s\tremaining: 18m 48s\n",
      "6316:\tlearn: 4.0943209\ttotal: 32m 14s\tremaining: 18m 47s\n",
      "6317:\tlearn: 4.0938144\ttotal: 32m 14s\tremaining: 18m 47s\n",
      "6318:\tlearn: 4.0934758\ttotal: 32m 15s\tremaining: 18m 47s\n",
      "6319:\tlearn: 4.0929699\ttotal: 32m 15s\tremaining: 18m 46s\n",
      "6320:\tlearn: 4.0924741\ttotal: 32m 15s\tremaining: 18m 46s\n",
      "6321:\tlearn: 4.0920816\ttotal: 32m 16s\tremaining: 18m 46s\n",
      "6322:\tlearn: 4.0917437\ttotal: 32m 16s\tremaining: 18m 46s\n",
      "6323:\tlearn: 4.0914679\ttotal: 32m 16s\tremaining: 18m 45s\n",
      "6324:\tlearn: 4.0912256\ttotal: 32m 16s\tremaining: 18m 45s\n",
      "6325:\tlearn: 4.0905774\ttotal: 32m 17s\tremaining: 18m 45s\n",
      "6326:\tlearn: 4.0900420\ttotal: 32m 17s\tremaining: 18m 44s\n",
      "6327:\tlearn: 4.0894045\ttotal: 32m 17s\tremaining: 18m 44s\n",
      "6328:\tlearn: 4.0889356\ttotal: 32m 18s\tremaining: 18m 44s\n",
      "6329:\tlearn: 4.0885793\ttotal: 32m 18s\tremaining: 18m 43s\n",
      "6330:\tlearn: 4.0880412\ttotal: 32m 18s\tremaining: 18m 43s\n",
      "6331:\tlearn: 4.0877764\ttotal: 32m 19s\tremaining: 18m 43s\n",
      "6332:\tlearn: 4.0875772\ttotal: 32m 19s\tremaining: 18m 43s\n",
      "6333:\tlearn: 4.0870583\ttotal: 32m 19s\tremaining: 18m 42s\n",
      "6334:\tlearn: 4.0866036\ttotal: 32m 20s\tremaining: 18m 42s\n",
      "6335:\tlearn: 4.0862807\ttotal: 32m 20s\tremaining: 18m 42s\n",
      "6336:\tlearn: 4.0858984\ttotal: 32m 20s\tremaining: 18m 41s\n",
      "6337:\tlearn: 4.0853552\ttotal: 32m 21s\tremaining: 18m 41s\n",
      "6338:\tlearn: 4.0851194\ttotal: 32m 21s\tremaining: 18m 41s\n",
      "6339:\tlearn: 4.0848282\ttotal: 32m 21s\tremaining: 18m 40s\n",
      "6340:\tlearn: 4.0845387\ttotal: 32m 21s\tremaining: 18m 40s\n",
      "6341:\tlearn: 4.0841374\ttotal: 32m 22s\tremaining: 18m 40s\n",
      "6342:\tlearn: 4.0838120\ttotal: 32m 22s\tremaining: 18m 39s\n",
      "6343:\tlearn: 4.0833206\ttotal: 32m 22s\tremaining: 18m 39s\n",
      "6344:\tlearn: 4.0829612\ttotal: 32m 23s\tremaining: 18m 39s\n",
      "6345:\tlearn: 4.0824195\ttotal: 32m 23s\tremaining: 18m 39s\n",
      "6346:\tlearn: 4.0820062\ttotal: 32m 23s\tremaining: 18m 38s\n",
      "6347:\tlearn: 4.0816274\ttotal: 32m 24s\tremaining: 18m 38s\n",
      "6348:\tlearn: 4.0813074\ttotal: 32m 24s\tremaining: 18m 38s\n",
      "6349:\tlearn: 4.0809811\ttotal: 32m 25s\tremaining: 18m 37s\n",
      "6350:\tlearn: 4.0807015\ttotal: 32m 25s\tremaining: 18m 37s\n",
      "6351:\tlearn: 4.0803224\ttotal: 32m 25s\tremaining: 18m 37s\n",
      "6352:\tlearn: 4.0801117\ttotal: 32m 25s\tremaining: 18m 37s\n",
      "6353:\tlearn: 4.0799110\ttotal: 32m 26s\tremaining: 18m 36s\n",
      "6354:\tlearn: 4.0794931\ttotal: 32m 26s\tremaining: 18m 36s\n",
      "6355:\tlearn: 4.0792344\ttotal: 32m 26s\tremaining: 18m 36s\n",
      "6356:\tlearn: 4.0786956\ttotal: 32m 27s\tremaining: 18m 35s\n",
      "6357:\tlearn: 4.0784213\ttotal: 32m 27s\tremaining: 18m 35s\n",
      "6358:\tlearn: 4.0780346\ttotal: 32m 27s\tremaining: 18m 35s\n",
      "6359:\tlearn: 4.0777389\ttotal: 32m 27s\tremaining: 18m 34s\n",
      "6360:\tlearn: 4.0772607\ttotal: 32m 28s\tremaining: 18m 34s\n",
      "6361:\tlearn: 4.0767928\ttotal: 32m 28s\tremaining: 18m 34s\n",
      "6362:\tlearn: 4.0764875\ttotal: 32m 28s\tremaining: 18m 33s\n",
      "6363:\tlearn: 4.0760163\ttotal: 32m 29s\tremaining: 18m 33s\n",
      "6364:\tlearn: 4.0757199\ttotal: 32m 29s\tremaining: 18m 33s\n",
      "6365:\tlearn: 4.0753353\ttotal: 32m 29s\tremaining: 18m 32s\n",
      "6366:\tlearn: 4.0749109\ttotal: 32m 30s\tremaining: 18m 32s\n",
      "6367:\tlearn: 4.0746223\ttotal: 32m 30s\tremaining: 18m 32s\n",
      "6368:\tlearn: 4.0742601\ttotal: 32m 30s\tremaining: 18m 32s\n",
      "6369:\tlearn: 4.0737159\ttotal: 32m 31s\tremaining: 18m 31s\n",
      "6370:\tlearn: 4.0733533\ttotal: 32m 31s\tremaining: 18m 31s\n",
      "6371:\tlearn: 4.0728891\ttotal: 32m 31s\tremaining: 18m 31s\n",
      "6372:\tlearn: 4.0724697\ttotal: 32m 32s\tremaining: 18m 30s\n",
      "6373:\tlearn: 4.0722740\ttotal: 32m 32s\tremaining: 18m 30s\n",
      "6374:\tlearn: 4.0721846\ttotal: 32m 32s\tremaining: 18m 30s\n",
      "6375:\tlearn: 4.0720446\ttotal: 32m 32s\tremaining: 18m 29s\n",
      "6376:\tlearn: 4.0718812\ttotal: 32m 33s\tremaining: 18m 29s\n",
      "6377:\tlearn: 4.0715281\ttotal: 32m 33s\tremaining: 18m 29s\n",
      "6378:\tlearn: 4.0710123\ttotal: 32m 33s\tremaining: 18m 29s\n",
      "6379:\tlearn: 4.0705202\ttotal: 32m 34s\tremaining: 18m 28s\n",
      "6380:\tlearn: 4.0701600\ttotal: 32m 34s\tremaining: 18m 28s\n",
      "6381:\tlearn: 4.0697636\ttotal: 32m 34s\tremaining: 18m 28s\n",
      "6382:\tlearn: 4.0695290\ttotal: 32m 35s\tremaining: 18m 27s\n",
      "6383:\tlearn: 4.0692299\ttotal: 32m 35s\tremaining: 18m 27s\n",
      "6384:\tlearn: 4.0689557\ttotal: 32m 35s\tremaining: 18m 27s\n",
      "6385:\tlearn: 4.0685363\ttotal: 32m 35s\tremaining: 18m 26s\n",
      "6386:\tlearn: 4.0681621\ttotal: 32m 36s\tremaining: 18m 26s\n",
      "6387:\tlearn: 4.0678127\ttotal: 32m 36s\tremaining: 18m 26s\n",
      "6388:\tlearn: 4.0675100\ttotal: 32m 36s\tremaining: 18m 25s\n",
      "6389:\tlearn: 4.0670720\ttotal: 32m 36s\tremaining: 18m 25s\n",
      "6390:\tlearn: 4.0666817\ttotal: 32m 37s\tremaining: 18m 25s\n",
      "6391:\tlearn: 4.0662663\ttotal: 32m 37s\tremaining: 18m 25s\n",
      "6392:\tlearn: 4.0659593\ttotal: 32m 37s\tremaining: 18m 24s\n",
      "6393:\tlearn: 4.0654485\ttotal: 32m 38s\tremaining: 18m 24s\n",
      "6394:\tlearn: 4.0650661\ttotal: 32m 38s\tremaining: 18m 24s\n",
      "6395:\tlearn: 4.0647150\ttotal: 32m 38s\tremaining: 18m 23s\n",
      "6396:\tlearn: 4.0642307\ttotal: 32m 39s\tremaining: 18m 23s\n",
      "6397:\tlearn: 4.0640979\ttotal: 32m 39s\tremaining: 18m 23s\n",
      "6398:\tlearn: 4.0638595\ttotal: 32m 39s\tremaining: 18m 22s\n",
      "6399:\tlearn: 4.0635246\ttotal: 32m 40s\tremaining: 18m 22s\n",
      "6400:\tlearn: 4.0631008\ttotal: 32m 40s\tremaining: 18m 22s\n",
      "6401:\tlearn: 4.0627228\ttotal: 32m 40s\tremaining: 18m 21s\n",
      "6402:\tlearn: 4.0624712\ttotal: 32m 41s\tremaining: 18m 21s\n",
      "6403:\tlearn: 4.0621281\ttotal: 32m 41s\tremaining: 18m 21s\n",
      "6404:\tlearn: 4.0618005\ttotal: 32m 41s\tremaining: 18m 20s\n",
      "6405:\tlearn: 4.0614353\ttotal: 32m 41s\tremaining: 18m 20s\n",
      "6406:\tlearn: 4.0611435\ttotal: 32m 42s\tremaining: 18m 20s\n",
      "6407:\tlearn: 4.0608236\ttotal: 32m 42s\tremaining: 18m 20s\n",
      "6408:\tlearn: 4.0604844\ttotal: 32m 42s\tremaining: 18m 19s\n",
      "6409:\tlearn: 4.0601951\ttotal: 32m 42s\tremaining: 18m 19s\n",
      "6410:\tlearn: 4.0598818\ttotal: 32m 43s\tremaining: 18m 19s\n",
      "6411:\tlearn: 4.0595247\ttotal: 32m 43s\tremaining: 18m 18s\n",
      "6412:\tlearn: 4.0593661\ttotal: 32m 43s\tremaining: 18m 18s\n",
      "6413:\tlearn: 4.0590780\ttotal: 32m 44s\tremaining: 18m 18s\n",
      "6414:\tlearn: 4.0587478\ttotal: 32m 44s\tremaining: 18m 17s\n",
      "6415:\tlearn: 4.0583410\ttotal: 32m 44s\tremaining: 18m 17s\n",
      "6416:\tlearn: 4.0578474\ttotal: 32m 44s\tremaining: 18m 17s\n",
      "6417:\tlearn: 4.0574801\ttotal: 32m 45s\tremaining: 18m 16s\n",
      "6418:\tlearn: 4.0568663\ttotal: 32m 45s\tremaining: 18m 16s\n",
      "6419:\tlearn: 4.0564519\ttotal: 32m 45s\tremaining: 18m 16s\n",
      "6420:\tlearn: 4.0562566\ttotal: 32m 46s\tremaining: 18m 15s\n",
      "6421:\tlearn: 4.0558746\ttotal: 32m 46s\tremaining: 18m 15s\n",
      "6422:\tlearn: 4.0553018\ttotal: 32m 46s\tremaining: 18m 15s\n",
      "6423:\tlearn: 4.0546601\ttotal: 32m 47s\tremaining: 18m 15s\n",
      "6424:\tlearn: 4.0545157\ttotal: 32m 47s\tremaining: 18m 14s\n",
      "6425:\tlearn: 4.0541993\ttotal: 32m 47s\tremaining: 18m 14s\n",
      "6426:\tlearn: 4.0538939\ttotal: 32m 47s\tremaining: 18m 14s\n",
      "6427:\tlearn: 4.0535477\ttotal: 32m 48s\tremaining: 18m 13s\n",
      "6428:\tlearn: 4.0532387\ttotal: 32m 48s\tremaining: 18m 13s\n",
      "6429:\tlearn: 4.0528524\ttotal: 32m 48s\tremaining: 18m 13s\n",
      "6430:\tlearn: 4.0524531\ttotal: 32m 49s\tremaining: 18m 12s\n",
      "6431:\tlearn: 4.0521608\ttotal: 32m 49s\tremaining: 18m 12s\n",
      "6432:\tlearn: 4.0518762\ttotal: 32m 49s\tremaining: 18m 12s\n",
      "6433:\tlearn: 4.0515849\ttotal: 32m 50s\tremaining: 18m 11s\n",
      "6434:\tlearn: 4.0513066\ttotal: 32m 50s\tremaining: 18m 11s\n",
      "6435:\tlearn: 4.0510300\ttotal: 32m 50s\tremaining: 18m 11s\n",
      "6436:\tlearn: 4.0507381\ttotal: 32m 50s\tremaining: 18m 10s\n",
      "6437:\tlearn: 4.0504886\ttotal: 32m 51s\tremaining: 18m 10s\n",
      "6438:\tlearn: 4.0501069\ttotal: 32m 51s\tremaining: 18m 10s\n",
      "6439:\tlearn: 4.0496214\ttotal: 32m 51s\tremaining: 18m 9s\n",
      "6440:\tlearn: 4.0492739\ttotal: 32m 52s\tremaining: 18m 9s\n",
      "6441:\tlearn: 4.0490052\ttotal: 32m 52s\tremaining: 18m 9s\n",
      "6442:\tlearn: 4.0487634\ttotal: 32m 52s\tremaining: 18m 9s\n",
      "6443:\tlearn: 4.0484913\ttotal: 32m 52s\tremaining: 18m 8s\n",
      "6444:\tlearn: 4.0480622\ttotal: 32m 53s\tremaining: 18m 8s\n",
      "6445:\tlearn: 4.0476880\ttotal: 32m 53s\tremaining: 18m 8s\n",
      "6446:\tlearn: 4.0471295\ttotal: 32m 53s\tremaining: 18m 7s\n",
      "6447:\tlearn: 4.0468223\ttotal: 32m 54s\tremaining: 18m 7s\n",
      "6448:\tlearn: 4.0465989\ttotal: 32m 54s\tremaining: 18m 7s\n",
      "6449:\tlearn: 4.0464399\ttotal: 32m 54s\tremaining: 18m 6s\n",
      "6450:\tlearn: 4.0460233\ttotal: 32m 55s\tremaining: 18m 6s\n",
      "6451:\tlearn: 4.0458058\ttotal: 32m 55s\tremaining: 18m 6s\n",
      "6452:\tlearn: 4.0456614\ttotal: 32m 55s\tremaining: 18m 5s\n",
      "6453:\tlearn: 4.0453568\ttotal: 32m 55s\tremaining: 18m 5s\n",
      "6454:\tlearn: 4.0451302\ttotal: 32m 56s\tremaining: 18m 5s\n",
      "6455:\tlearn: 4.0447104\ttotal: 32m 56s\tremaining: 18m 4s\n",
      "6456:\tlearn: 4.0445101\ttotal: 32m 56s\tremaining: 18m 4s\n",
      "6457:\tlearn: 4.0441889\ttotal: 32m 56s\tremaining: 18m 4s\n",
      "6458:\tlearn: 4.0439692\ttotal: 32m 57s\tremaining: 18m 3s\n",
      "6459:\tlearn: 4.0437049\ttotal: 32m 57s\tremaining: 18m 3s\n",
      "6460:\tlearn: 4.0432374\ttotal: 32m 57s\tremaining: 18m 3s\n",
      "6461:\tlearn: 4.0428608\ttotal: 32m 58s\tremaining: 18m 2s\n",
      "6462:\tlearn: 4.0424687\ttotal: 32m 58s\tremaining: 18m 2s\n",
      "6463:\tlearn: 4.0421004\ttotal: 32m 58s\tremaining: 18m 2s\n",
      "6464:\tlearn: 4.0418596\ttotal: 32m 58s\tremaining: 18m 2s\n",
      "6465:\tlearn: 4.0415806\ttotal: 32m 59s\tremaining: 18m 1s\n",
      "6466:\tlearn: 4.0413086\ttotal: 32m 59s\tremaining: 18m 1s\n",
      "6467:\tlearn: 4.0411461\ttotal: 32m 59s\tremaining: 18m 1s\n",
      "6468:\tlearn: 4.0406338\ttotal: 33m\tremaining: 18m\n",
      "6469:\tlearn: 4.0401109\ttotal: 33m\tremaining: 18m\n",
      "6470:\tlearn: 4.0399315\ttotal: 33m\tremaining: 18m\n",
      "6471:\tlearn: 4.0395651\ttotal: 33m\tremaining: 17m 59s\n",
      "6472:\tlearn: 4.0394669\ttotal: 33m 1s\tremaining: 17m 59s\n",
      "6473:\tlearn: 4.0388819\ttotal: 33m 1s\tremaining: 17m 59s\n",
      "6474:\tlearn: 4.0385325\ttotal: 33m 1s\tremaining: 17m 58s\n",
      "6475:\tlearn: 4.0382019\ttotal: 33m 2s\tremaining: 17m 58s\n",
      "6476:\tlearn: 4.0379016\ttotal: 33m 2s\tremaining: 17m 58s\n",
      "6477:\tlearn: 4.0376272\ttotal: 33m 2s\tremaining: 17m 57s\n",
      "6478:\tlearn: 4.0372740\ttotal: 33m 2s\tremaining: 17m 57s\n",
      "6479:\tlearn: 4.0369992\ttotal: 33m 3s\tremaining: 17m 57s\n",
      "6480:\tlearn: 4.0367203\ttotal: 33m 3s\tremaining: 17m 57s\n",
      "6481:\tlearn: 4.0363214\ttotal: 33m 3s\tremaining: 17m 56s\n",
      "6482:\tlearn: 4.0359849\ttotal: 33m 4s\tremaining: 17m 56s\n",
      "6483:\tlearn: 4.0356562\ttotal: 33m 4s\tremaining: 17m 56s\n",
      "6484:\tlearn: 4.0352016\ttotal: 33m 4s\tremaining: 17m 55s\n",
      "6485:\tlearn: 4.0347586\ttotal: 33m 5s\tremaining: 17m 55s\n",
      "6486:\tlearn: 4.0344770\ttotal: 33m 5s\tremaining: 17m 55s\n",
      "6487:\tlearn: 4.0339256\ttotal: 33m 5s\tremaining: 17m 54s\n",
      "6488:\tlearn: 4.0335195\ttotal: 33m 6s\tremaining: 17m 54s\n",
      "6489:\tlearn: 4.0331594\ttotal: 33m 6s\tremaining: 17m 54s\n",
      "6490:\tlearn: 4.0327571\ttotal: 33m 6s\tremaining: 17m 54s\n",
      "6491:\tlearn: 4.0323721\ttotal: 33m 7s\tremaining: 17m 53s\n",
      "6492:\tlearn: 4.0318984\ttotal: 33m 7s\tremaining: 17m 53s\n",
      "6493:\tlearn: 4.0314732\ttotal: 33m 7s\tremaining: 17m 53s\n",
      "6494:\tlearn: 4.0311460\ttotal: 33m 7s\tremaining: 17m 52s\n",
      "6495:\tlearn: 4.0307687\ttotal: 33m 8s\tremaining: 17m 52s\n",
      "6496:\tlearn: 4.0305574\ttotal: 33m 8s\tremaining: 17m 52s\n",
      "6497:\tlearn: 4.0300809\ttotal: 33m 8s\tremaining: 17m 51s\n",
      "6498:\tlearn: 4.0296236\ttotal: 33m 9s\tremaining: 17m 51s\n",
      "6499:\tlearn: 4.0294371\ttotal: 33m 9s\tremaining: 17m 51s\n",
      "6500:\tlearn: 4.0289275\ttotal: 33m 9s\tremaining: 17m 51s\n",
      "6501:\tlearn: 4.0285093\ttotal: 33m 10s\tremaining: 17m 50s\n",
      "6502:\tlearn: 4.0281635\ttotal: 33m 10s\tremaining: 17m 50s\n",
      "6503:\tlearn: 4.0277267\ttotal: 33m 10s\tremaining: 17m 50s\n",
      "6504:\tlearn: 4.0272963\ttotal: 33m 11s\tremaining: 17m 49s\n",
      "6505:\tlearn: 4.0269469\ttotal: 33m 11s\tremaining: 17m 49s\n",
      "6506:\tlearn: 4.0265895\ttotal: 33m 11s\tremaining: 17m 49s\n",
      "6507:\tlearn: 4.0264275\ttotal: 33m 11s\tremaining: 17m 48s\n",
      "6508:\tlearn: 4.0261114\ttotal: 33m 12s\tremaining: 17m 48s\n",
      "6509:\tlearn: 4.0257501\ttotal: 33m 12s\tremaining: 17m 48s\n",
      "6510:\tlearn: 4.0253087\ttotal: 33m 12s\tremaining: 17m 47s\n",
      "6511:\tlearn: 4.0250396\ttotal: 33m 13s\tremaining: 17m 47s\n",
      "6512:\tlearn: 4.0247576\ttotal: 33m 13s\tremaining: 17m 47s\n",
      "6513:\tlearn: 4.0243905\ttotal: 33m 13s\tremaining: 17m 46s\n",
      "6514:\tlearn: 4.0240976\ttotal: 33m 13s\tremaining: 17m 46s\n",
      "6515:\tlearn: 4.0238104\ttotal: 33m 14s\tremaining: 17m 46s\n",
      "6516:\tlearn: 4.0234206\ttotal: 33m 14s\tremaining: 17m 45s\n",
      "6517:\tlearn: 4.0231220\ttotal: 33m 14s\tremaining: 17m 45s\n",
      "6518:\tlearn: 4.0228523\ttotal: 33m 15s\tremaining: 17m 45s\n",
      "6519:\tlearn: 4.0224834\ttotal: 33m 15s\tremaining: 17m 44s\n",
      "6520:\tlearn: 4.0222580\ttotal: 33m 15s\tremaining: 17m 44s\n",
      "6521:\tlearn: 4.0215784\ttotal: 33m 15s\tremaining: 17m 44s\n",
      "6522:\tlearn: 4.0211830\ttotal: 33m 16s\tremaining: 17m 44s\n",
      "6523:\tlearn: 4.0209320\ttotal: 33m 16s\tremaining: 17m 43s\n",
      "6524:\tlearn: 4.0205864\ttotal: 33m 16s\tremaining: 17m 43s\n",
      "6525:\tlearn: 4.0201580\ttotal: 33m 17s\tremaining: 17m 43s\n",
      "6526:\tlearn: 4.0196096\ttotal: 33m 17s\tremaining: 17m 42s\n",
      "6527:\tlearn: 4.0192844\ttotal: 33m 17s\tremaining: 17m 42s\n",
      "6528:\tlearn: 4.0189369\ttotal: 33m 18s\tremaining: 17m 42s\n",
      "6529:\tlearn: 4.0183895\ttotal: 33m 18s\tremaining: 17m 41s\n",
      "6530:\tlearn: 4.0180913\ttotal: 33m 18s\tremaining: 17m 41s\n",
      "6531:\tlearn: 4.0178306\ttotal: 33m 18s\tremaining: 17m 41s\n",
      "6532:\tlearn: 4.0175360\ttotal: 33m 19s\tremaining: 17m 41s\n",
      "6533:\tlearn: 4.0173082\ttotal: 33m 19s\tremaining: 17m 40s\n",
      "6534:\tlearn: 4.0171572\ttotal: 33m 19s\tremaining: 17m 40s\n",
      "6535:\tlearn: 4.0168354\ttotal: 33m 20s\tremaining: 17m 40s\n",
      "6536:\tlearn: 4.0165000\ttotal: 33m 20s\tremaining: 17m 39s\n",
      "6537:\tlearn: 4.0162083\ttotal: 33m 20s\tremaining: 17m 39s\n",
      "6538:\tlearn: 4.0156340\ttotal: 33m 21s\tremaining: 17m 39s\n",
      "6539:\tlearn: 4.0150764\ttotal: 33m 21s\tremaining: 17m 38s\n",
      "6540:\tlearn: 4.0147680\ttotal: 33m 21s\tremaining: 17m 38s\n",
      "6541:\tlearn: 4.0142914\ttotal: 33m 22s\tremaining: 17m 38s\n",
      "6542:\tlearn: 4.0139860\ttotal: 33m 22s\tremaining: 17m 38s\n",
      "6543:\tlearn: 4.0135490\ttotal: 33m 22s\tremaining: 17m 37s\n",
      "6544:\tlearn: 4.0130989\ttotal: 33m 23s\tremaining: 17m 37s\n",
      "6545:\tlearn: 4.0127763\ttotal: 33m 23s\tremaining: 17m 37s\n",
      "6546:\tlearn: 4.0124784\ttotal: 33m 23s\tremaining: 17m 36s\n",
      "6547:\tlearn: 4.0120671\ttotal: 33m 24s\tremaining: 17m 36s\n",
      "6548:\tlearn: 4.0117487\ttotal: 33m 24s\tremaining: 17m 36s\n",
      "6549:\tlearn: 4.0113824\ttotal: 33m 24s\tremaining: 17m 35s\n",
      "6550:\tlearn: 4.0108506\ttotal: 33m 25s\tremaining: 17m 35s\n",
      "6551:\tlearn: 4.0106233\ttotal: 33m 25s\tremaining: 17m 35s\n",
      "6552:\tlearn: 4.0103741\ttotal: 33m 25s\tremaining: 17m 35s\n",
      "6553:\tlearn: 4.0098848\ttotal: 33m 26s\tremaining: 17m 34s\n",
      "6554:\tlearn: 4.0095798\ttotal: 33m 26s\tremaining: 17m 34s\n",
      "6555:\tlearn: 4.0093288\ttotal: 33m 26s\tremaining: 17m 34s\n",
      "6556:\tlearn: 4.0090485\ttotal: 33m 26s\tremaining: 17m 33s\n",
      "6557:\tlearn: 4.0086449\ttotal: 33m 27s\tremaining: 17m 33s\n",
      "6558:\tlearn: 4.0084141\ttotal: 33m 27s\tremaining: 17m 33s\n",
      "6559:\tlearn: 4.0081683\ttotal: 33m 27s\tremaining: 17m 32s\n",
      "6560:\tlearn: 4.0078190\ttotal: 33m 28s\tremaining: 17m 32s\n",
      "6561:\tlearn: 4.0073466\ttotal: 33m 28s\tremaining: 17m 32s\n",
      "6562:\tlearn: 4.0068829\ttotal: 33m 28s\tremaining: 17m 31s\n",
      "6563:\tlearn: 4.0065841\ttotal: 33m 28s\tremaining: 17m 31s\n",
      "6564:\tlearn: 4.0060646\ttotal: 33m 29s\tremaining: 17m 31s\n",
      "6565:\tlearn: 4.0057279\ttotal: 33m 29s\tremaining: 17m 30s\n",
      "6566:\tlearn: 4.0054013\ttotal: 33m 29s\tremaining: 17m 30s\n",
      "6567:\tlearn: 4.0046764\ttotal: 33m 30s\tremaining: 17m 30s\n",
      "6568:\tlearn: 4.0043637\ttotal: 33m 30s\tremaining: 17m 30s\n",
      "6569:\tlearn: 4.0041710\ttotal: 33m 30s\tremaining: 17m 29s\n",
      "6570:\tlearn: 4.0038270\ttotal: 33m 30s\tremaining: 17m 29s\n",
      "6571:\tlearn: 4.0035559\ttotal: 33m 31s\tremaining: 17m 29s\n",
      "6572:\tlearn: 4.0030473\ttotal: 33m 31s\tremaining: 17m 28s\n",
      "6573:\tlearn: 4.0027075\ttotal: 33m 31s\tremaining: 17m 28s\n",
      "6574:\tlearn: 4.0024272\ttotal: 33m 31s\tremaining: 17m 28s\n",
      "6575:\tlearn: 4.0018704\ttotal: 33m 32s\tremaining: 17m 27s\n",
      "6576:\tlearn: 4.0017195\ttotal: 33m 32s\tremaining: 17m 27s\n",
      "6577:\tlearn: 4.0015741\ttotal: 33m 32s\tremaining: 17m 27s\n",
      "6578:\tlearn: 4.0014219\ttotal: 33m 32s\tremaining: 17m 26s\n",
      "6579:\tlearn: 4.0011050\ttotal: 33m 33s\tremaining: 17m 26s\n",
      "6580:\tlearn: 4.0008672\ttotal: 33m 33s\tremaining: 17m 26s\n",
      "6581:\tlearn: 4.0005328\ttotal: 33m 33s\tremaining: 17m 25s\n",
      "6582:\tlearn: 4.0001025\ttotal: 33m 34s\tremaining: 17m 25s\n",
      "6583:\tlearn: 3.9998663\ttotal: 33m 34s\tremaining: 17m 25s\n",
      "6584:\tlearn: 3.9996450\ttotal: 33m 34s\tremaining: 17m 24s\n",
      "6585:\tlearn: 3.9993148\ttotal: 33m 34s\tremaining: 17m 24s\n",
      "6586:\tlearn: 3.9989818\ttotal: 33m 35s\tremaining: 17m 24s\n",
      "6587:\tlearn: 3.9986647\ttotal: 33m 35s\tremaining: 17m 23s\n",
      "6588:\tlearn: 3.9984082\ttotal: 33m 35s\tremaining: 17m 23s\n",
      "6589:\tlearn: 3.9981824\ttotal: 33m 35s\tremaining: 17m 23s\n",
      "6590:\tlearn: 3.9978297\ttotal: 33m 36s\tremaining: 17m 22s\n",
      "6591:\tlearn: 3.9973755\ttotal: 33m 36s\tremaining: 17m 22s\n",
      "6592:\tlearn: 3.9970477\ttotal: 33m 36s\tremaining: 17m 22s\n",
      "6593:\tlearn: 3.9968171\ttotal: 33m 36s\tremaining: 17m 21s\n",
      "6594:\tlearn: 3.9964688\ttotal: 33m 37s\tremaining: 17m 21s\n",
      "6595:\tlearn: 3.9961737\ttotal: 33m 37s\tremaining: 17m 21s\n",
      "6596:\tlearn: 3.9957253\ttotal: 33m 37s\tremaining: 17m 20s\n",
      "6597:\tlearn: 3.9953293\ttotal: 33m 37s\tremaining: 17m 20s\n",
      "6598:\tlearn: 3.9948273\ttotal: 33m 38s\tremaining: 17m 20s\n",
      "6599:\tlearn: 3.9943665\ttotal: 33m 38s\tremaining: 17m 19s\n",
      "6600:\tlearn: 3.9939800\ttotal: 33m 38s\tremaining: 17m 19s\n",
      "6601:\tlearn: 3.9937696\ttotal: 33m 38s\tremaining: 17m 19s\n",
      "6602:\tlearn: 3.9933569\ttotal: 33m 39s\tremaining: 17m 18s\n",
      "6603:\tlearn: 3.9930218\ttotal: 33m 39s\tremaining: 17m 18s\n",
      "6604:\tlearn: 3.9926810\ttotal: 33m 39s\tremaining: 17m 18s\n",
      "6605:\tlearn: 3.9922909\ttotal: 33m 40s\tremaining: 17m 17s\n",
      "6606:\tlearn: 3.9919023\ttotal: 33m 40s\tremaining: 17m 17s\n",
      "6607:\tlearn: 3.9915160\ttotal: 33m 40s\tremaining: 17m 17s\n",
      "6608:\tlearn: 3.9911777\ttotal: 33m 40s\tremaining: 17m 16s\n",
      "6609:\tlearn: 3.9906582\ttotal: 33m 41s\tremaining: 17m 16s\n",
      "6610:\tlearn: 3.9903220\ttotal: 33m 41s\tremaining: 17m 16s\n",
      "6611:\tlearn: 3.9901089\ttotal: 33m 41s\tremaining: 17m 15s\n",
      "6612:\tlearn: 3.9897553\ttotal: 33m 41s\tremaining: 17m 15s\n",
      "6613:\tlearn: 3.9894711\ttotal: 33m 42s\tremaining: 17m 15s\n",
      "6614:\tlearn: 3.9891598\ttotal: 33m 42s\tremaining: 17m 14s\n",
      "6615:\tlearn: 3.9886033\ttotal: 33m 42s\tremaining: 17m 14s\n",
      "6616:\tlearn: 3.9880523\ttotal: 33m 43s\tremaining: 17m 14s\n",
      "6617:\tlearn: 3.9877165\ttotal: 33m 43s\tremaining: 17m 13s\n",
      "6618:\tlearn: 3.9872709\ttotal: 33m 43s\tremaining: 17m 13s\n",
      "6619:\tlearn: 3.9867798\ttotal: 33m 43s\tremaining: 17m 13s\n",
      "6620:\tlearn: 3.9865442\ttotal: 33m 44s\tremaining: 17m 13s\n",
      "6621:\tlearn: 3.9862307\ttotal: 33m 44s\tremaining: 17m 12s\n",
      "6622:\tlearn: 3.9857567\ttotal: 33m 44s\tremaining: 17m 12s\n",
      "6623:\tlearn: 3.9853162\ttotal: 33m 44s\tremaining: 17m 12s\n",
      "6624:\tlearn: 3.9849905\ttotal: 33m 45s\tremaining: 17m 11s\n",
      "6625:\tlearn: 3.9847598\ttotal: 33m 45s\tremaining: 17m 11s\n",
      "6626:\tlearn: 3.9844509\ttotal: 33m 45s\tremaining: 17m 11s\n",
      "6627:\tlearn: 3.9841690\ttotal: 33m 45s\tremaining: 17m 10s\n",
      "6628:\tlearn: 3.9837172\ttotal: 33m 46s\tremaining: 17m 10s\n",
      "6629:\tlearn: 3.9833216\ttotal: 33m 46s\tremaining: 17m 10s\n",
      "6630:\tlearn: 3.9830021\ttotal: 33m 46s\tremaining: 17m 9s\n",
      "6631:\tlearn: 3.9827038\ttotal: 33m 46s\tremaining: 17m 9s\n",
      "6632:\tlearn: 3.9825645\ttotal: 33m 47s\tremaining: 17m 8s\n",
      "6633:\tlearn: 3.9824495\ttotal: 33m 47s\tremaining: 17m 8s\n",
      "6634:\tlearn: 3.9821264\ttotal: 33m 47s\tremaining: 17m 8s\n",
      "6635:\tlearn: 3.9818392\ttotal: 33m 47s\tremaining: 17m 8s\n",
      "6636:\tlearn: 3.9815577\ttotal: 33m 48s\tremaining: 17m 7s\n",
      "6637:\tlearn: 3.9813036\ttotal: 33m 48s\tremaining: 17m 7s\n",
      "6638:\tlearn: 3.9808791\ttotal: 33m 48s\tremaining: 17m 7s\n",
      "6639:\tlearn: 3.9805458\ttotal: 33m 49s\tremaining: 17m 6s\n",
      "6640:\tlearn: 3.9801992\ttotal: 33m 49s\tremaining: 17m 6s\n",
      "6641:\tlearn: 3.9799628\ttotal: 33m 49s\tremaining: 17m 6s\n",
      "6642:\tlearn: 3.9797106\ttotal: 33m 49s\tremaining: 17m 5s\n",
      "6643:\tlearn: 3.9793200\ttotal: 33m 50s\tremaining: 17m 5s\n",
      "6644:\tlearn: 3.9789850\ttotal: 33m 50s\tremaining: 17m 5s\n",
      "6645:\tlearn: 3.9785194\ttotal: 33m 50s\tremaining: 17m 4s\n",
      "6646:\tlearn: 3.9782944\ttotal: 33m 50s\tremaining: 17m 4s\n",
      "6647:\tlearn: 3.9780019\ttotal: 33m 51s\tremaining: 17m 4s\n",
      "6648:\tlearn: 3.9774751\ttotal: 33m 51s\tremaining: 17m 3s\n",
      "6649:\tlearn: 3.9770793\ttotal: 33m 51s\tremaining: 17m 3s\n",
      "6650:\tlearn: 3.9767836\ttotal: 33m 51s\tremaining: 17m 3s\n",
      "6651:\tlearn: 3.9764396\ttotal: 33m 52s\tremaining: 17m 2s\n",
      "6652:\tlearn: 3.9761652\ttotal: 33m 52s\tremaining: 17m 2s\n",
      "6653:\tlearn: 3.9759142\ttotal: 33m 52s\tremaining: 17m 2s\n",
      "6654:\tlearn: 3.9756729\ttotal: 33m 52s\tremaining: 17m 1s\n",
      "6655:\tlearn: 3.9753689\ttotal: 33m 53s\tremaining: 17m 1s\n",
      "6656:\tlearn: 3.9749097\ttotal: 33m 53s\tremaining: 17m 1s\n",
      "6657:\tlearn: 3.9744591\ttotal: 33m 53s\tremaining: 17m\n",
      "6658:\tlearn: 3.9742175\ttotal: 33m 53s\tremaining: 17m\n",
      "6659:\tlearn: 3.9737444\ttotal: 33m 54s\tremaining: 17m\n",
      "6660:\tlearn: 3.9733627\ttotal: 33m 54s\tremaining: 16m 59s\n",
      "6661:\tlearn: 3.9730565\ttotal: 33m 54s\tremaining: 16m 59s\n",
      "6662:\tlearn: 3.9727420\ttotal: 33m 55s\tremaining: 16m 59s\n",
      "6663:\tlearn: 3.9725217\ttotal: 33m 55s\tremaining: 16m 58s\n",
      "6664:\tlearn: 3.9723167\ttotal: 33m 55s\tremaining: 16m 58s\n",
      "6665:\tlearn: 3.9719762\ttotal: 33m 55s\tremaining: 16m 58s\n",
      "6666:\tlearn: 3.9716345\ttotal: 33m 56s\tremaining: 16m 57s\n",
      "6667:\tlearn: 3.9713924\ttotal: 33m 56s\tremaining: 16m 57s\n",
      "6668:\tlearn: 3.9709331\ttotal: 33m 56s\tremaining: 16m 57s\n",
      "6669:\tlearn: 3.9705853\ttotal: 33m 56s\tremaining: 16m 56s\n",
      "6670:\tlearn: 3.9702459\ttotal: 33m 57s\tremaining: 16m 56s\n",
      "6671:\tlearn: 3.9698167\ttotal: 33m 57s\tremaining: 16m 56s\n",
      "6672:\tlearn: 3.9696564\ttotal: 33m 57s\tremaining: 16m 55s\n",
      "6673:\tlearn: 3.9693521\ttotal: 33m 57s\tremaining: 16m 55s\n",
      "6674:\tlearn: 3.9690437\ttotal: 33m 58s\tremaining: 16m 55s\n",
      "6675:\tlearn: 3.9687607\ttotal: 33m 58s\tremaining: 16m 54s\n",
      "6676:\tlearn: 3.9685018\ttotal: 33m 58s\tremaining: 16m 54s\n",
      "6677:\tlearn: 3.9681587\ttotal: 33m 58s\tremaining: 16m 54s\n",
      "6678:\tlearn: 3.9679426\ttotal: 33m 59s\tremaining: 16m 53s\n",
      "6679:\tlearn: 3.9675433\ttotal: 33m 59s\tremaining: 16m 53s\n",
      "6680:\tlearn: 3.9673590\ttotal: 33m 59s\tremaining: 16m 53s\n",
      "6681:\tlearn: 3.9671426\ttotal: 33m 59s\tremaining: 16m 52s\n",
      "6682:\tlearn: 3.9666973\ttotal: 34m\tremaining: 16m 52s\n",
      "6683:\tlearn: 3.9663526\ttotal: 34m\tremaining: 16m 52s\n",
      "6684:\tlearn: 3.9661483\ttotal: 34m\tremaining: 16m 51s\n",
      "6685:\tlearn: 3.9658542\ttotal: 34m\tremaining: 16m 51s\n",
      "6686:\tlearn: 3.9656295\ttotal: 34m 1s\tremaining: 16m 51s\n",
      "6687:\tlearn: 3.9653063\ttotal: 34m 1s\tremaining: 16m 50s\n",
      "6688:\tlearn: 3.9647730\ttotal: 34m 1s\tremaining: 16m 50s\n",
      "6689:\tlearn: 3.9643596\ttotal: 34m 2s\tremaining: 16m 50s\n",
      "6690:\tlearn: 3.9641225\ttotal: 34m 2s\tremaining: 16m 49s\n",
      "6691:\tlearn: 3.9637555\ttotal: 34m 2s\tremaining: 16m 49s\n",
      "6692:\tlearn: 3.9634933\ttotal: 34m 2s\tremaining: 16m 49s\n",
      "6693:\tlearn: 3.9631741\ttotal: 34m 2s\tremaining: 16m 48s\n",
      "6694:\tlearn: 3.9628235\ttotal: 34m 3s\tremaining: 16m 48s\n",
      "6695:\tlearn: 3.9625731\ttotal: 34m 3s\tremaining: 16m 48s\n",
      "6696:\tlearn: 3.9623218\ttotal: 34m 3s\tremaining: 16m 47s\n",
      "6697:\tlearn: 3.9620329\ttotal: 34m 3s\tremaining: 16m 47s\n",
      "6698:\tlearn: 3.9616206\ttotal: 34m 4s\tremaining: 16m 47s\n",
      "6699:\tlearn: 3.9610870\ttotal: 34m 4s\tremaining: 16m 46s\n",
      "6700:\tlearn: 3.9608324\ttotal: 34m 4s\tremaining: 16m 46s\n",
      "6701:\tlearn: 3.9605017\ttotal: 34m 4s\tremaining: 16m 46s\n",
      "6702:\tlearn: 3.9601911\ttotal: 34m 5s\tremaining: 16m 45s\n",
      "6703:\tlearn: 3.9598968\ttotal: 34m 5s\tremaining: 16m 45s\n",
      "6704:\tlearn: 3.9594543\ttotal: 34m 5s\tremaining: 16m 45s\n",
      "6705:\tlearn: 3.9589540\ttotal: 34m 6s\tremaining: 16m 45s\n",
      "6706:\tlearn: 3.9586359\ttotal: 34m 6s\tremaining: 16m 44s\n",
      "6707:\tlearn: 3.9583314\ttotal: 34m 6s\tremaining: 16m 44s\n",
      "6708:\tlearn: 3.9580879\ttotal: 34m 6s\tremaining: 16m 44s\n",
      "6709:\tlearn: 3.9578078\ttotal: 34m 7s\tremaining: 16m 43s\n",
      "6710:\tlearn: 3.9574915\ttotal: 34m 7s\tremaining: 16m 43s\n",
      "6711:\tlearn: 3.9572234\ttotal: 34m 7s\tremaining: 16m 43s\n",
      "6712:\tlearn: 3.9568135\ttotal: 34m 7s\tremaining: 16m 42s\n",
      "6713:\tlearn: 3.9565179\ttotal: 34m 8s\tremaining: 16m 42s\n",
      "6714:\tlearn: 3.9560768\ttotal: 34m 8s\tremaining: 16m 42s\n",
      "6715:\tlearn: 3.9557991\ttotal: 34m 8s\tremaining: 16m 41s\n",
      "6716:\tlearn: 3.9555217\ttotal: 34m 9s\tremaining: 16m 41s\n",
      "6717:\tlearn: 3.9552344\ttotal: 34m 9s\tremaining: 16m 41s\n",
      "6718:\tlearn: 3.9549717\ttotal: 34m 9s\tremaining: 16m 40s\n",
      "6719:\tlearn: 3.9546362\ttotal: 34m 9s\tremaining: 16m 40s\n",
      "6720:\tlearn: 3.9543511\ttotal: 34m 10s\tremaining: 16m 40s\n",
      "6721:\tlearn: 3.9541246\ttotal: 34m 10s\tremaining: 16m 39s\n",
      "6722:\tlearn: 3.9538767\ttotal: 34m 10s\tremaining: 16m 39s\n",
      "6723:\tlearn: 3.9535188\ttotal: 34m 10s\tremaining: 16m 39s\n",
      "6724:\tlearn: 3.9532621\ttotal: 34m 11s\tremaining: 16m 38s\n",
      "6725:\tlearn: 3.9529844\ttotal: 34m 11s\tremaining: 16m 38s\n",
      "6726:\tlearn: 3.9526561\ttotal: 34m 11s\tremaining: 16m 38s\n",
      "6727:\tlearn: 3.9524327\ttotal: 34m 11s\tremaining: 16m 37s\n",
      "6728:\tlearn: 3.9522207\ttotal: 34m 11s\tremaining: 16m 37s\n",
      "6729:\tlearn: 3.9516967\ttotal: 34m 12s\tremaining: 16m 37s\n",
      "6730:\tlearn: 3.9513090\ttotal: 34m 12s\tremaining: 16m 36s\n",
      "6731:\tlearn: 3.9508808\ttotal: 34m 12s\tremaining: 16m 36s\n",
      "6732:\tlearn: 3.9507303\ttotal: 34m 13s\tremaining: 16m 36s\n",
      "6733:\tlearn: 3.9504555\ttotal: 34m 13s\tremaining: 16m 35s\n",
      "6734:\tlearn: 3.9500411\ttotal: 34m 13s\tremaining: 16m 35s\n",
      "6735:\tlearn: 3.9495429\ttotal: 34m 13s\tremaining: 16m 35s\n",
      "6736:\tlearn: 3.9490951\ttotal: 34m 14s\tremaining: 16m 34s\n",
      "6737:\tlearn: 3.9486043\ttotal: 34m 14s\tremaining: 16m 34s\n",
      "6738:\tlearn: 3.9482474\ttotal: 34m 14s\tremaining: 16m 34s\n",
      "6739:\tlearn: 3.9479324\ttotal: 34m 15s\tremaining: 16m 33s\n",
      "6740:\tlearn: 3.9476458\ttotal: 34m 15s\tremaining: 16m 33s\n",
      "6741:\tlearn: 3.9472372\ttotal: 34m 15s\tremaining: 16m 33s\n",
      "6742:\tlearn: 3.9469728\ttotal: 34m 15s\tremaining: 16m 33s\n",
      "6743:\tlearn: 3.9465869\ttotal: 34m 16s\tremaining: 16m 32s\n",
      "6744:\tlearn: 3.9463219\ttotal: 34m 16s\tremaining: 16m 32s\n",
      "6745:\tlearn: 3.9460905\ttotal: 34m 16s\tremaining: 16m 32s\n",
      "6746:\tlearn: 3.9458027\ttotal: 34m 17s\tremaining: 16m 31s\n",
      "6747:\tlearn: 3.9454051\ttotal: 34m 17s\tremaining: 16m 31s\n",
      "6748:\tlearn: 3.9451322\ttotal: 34m 17s\tremaining: 16m 31s\n",
      "6749:\tlearn: 3.9449155\ttotal: 34m 17s\tremaining: 16m 30s\n",
      "6750:\tlearn: 3.9443864\ttotal: 34m 18s\tremaining: 16m 30s\n",
      "6751:\tlearn: 3.9440479\ttotal: 34m 18s\tremaining: 16m 30s\n",
      "6752:\tlearn: 3.9437021\ttotal: 34m 18s\tremaining: 16m 29s\n",
      "6753:\tlearn: 3.9434852\ttotal: 34m 19s\tremaining: 16m 29s\n",
      "6754:\tlearn: 3.9430821\ttotal: 34m 19s\tremaining: 16m 29s\n",
      "6755:\tlearn: 3.9426478\ttotal: 34m 19s\tremaining: 16m 29s\n",
      "6756:\tlearn: 3.9422527\ttotal: 34m 20s\tremaining: 16m 28s\n",
      "6757:\tlearn: 3.9419512\ttotal: 34m 20s\tremaining: 16m 28s\n",
      "6758:\tlearn: 3.9414988\ttotal: 34m 20s\tremaining: 16m 28s\n",
      "6759:\tlearn: 3.9410963\ttotal: 34m 20s\tremaining: 16m 27s\n",
      "6760:\tlearn: 3.9407545\ttotal: 34m 21s\tremaining: 16m 27s\n",
      "6761:\tlearn: 3.9403930\ttotal: 34m 21s\tremaining: 16m 27s\n",
      "6762:\tlearn: 3.9398285\ttotal: 34m 22s\tremaining: 16m 26s\n",
      "6763:\tlearn: 3.9394788\ttotal: 34m 22s\tremaining: 16m 26s\n",
      "6764:\tlearn: 3.9391986\ttotal: 34m 22s\tremaining: 16m 26s\n",
      "6765:\tlearn: 3.9387658\ttotal: 34m 22s\tremaining: 16m 26s\n",
      "6766:\tlearn: 3.9384387\ttotal: 34m 23s\tremaining: 16m 25s\n",
      "6767:\tlearn: 3.9380054\ttotal: 34m 23s\tremaining: 16m 25s\n",
      "6768:\tlearn: 3.9378041\ttotal: 34m 23s\tremaining: 16m 25s\n",
      "6769:\tlearn: 3.9376364\ttotal: 34m 24s\tremaining: 16m 24s\n",
      "6770:\tlearn: 3.9373054\ttotal: 34m 24s\tremaining: 16m 24s\n",
      "6771:\tlearn: 3.9369501\ttotal: 34m 24s\tremaining: 16m 24s\n",
      "6772:\tlearn: 3.9365543\ttotal: 34m 25s\tremaining: 16m 24s\n",
      "6773:\tlearn: 3.9364099\ttotal: 34m 25s\tremaining: 16m 23s\n",
      "6774:\tlearn: 3.9359588\ttotal: 34m 25s\tremaining: 16m 23s\n",
      "6775:\tlearn: 3.9354809\ttotal: 34m 26s\tremaining: 16m 23s\n",
      "6776:\tlearn: 3.9351576\ttotal: 34m 26s\tremaining: 16m 22s\n",
      "6777:\tlearn: 3.9349123\ttotal: 34m 26s\tremaining: 16m 22s\n",
      "6778:\tlearn: 3.9346795\ttotal: 34m 27s\tremaining: 16m 22s\n",
      "6779:\tlearn: 3.9344314\ttotal: 34m 27s\tremaining: 16m 21s\n",
      "6780:\tlearn: 3.9340467\ttotal: 34m 27s\tremaining: 16m 21s\n",
      "6781:\tlearn: 3.9338107\ttotal: 34m 28s\tremaining: 16m 21s\n",
      "6782:\tlearn: 3.9334086\ttotal: 34m 28s\tremaining: 16m 20s\n",
      "6783:\tlearn: 3.9329271\ttotal: 34m 28s\tremaining: 16m 20s\n",
      "6784:\tlearn: 3.9325713\ttotal: 34m 29s\tremaining: 16m 20s\n",
      "6785:\tlearn: 3.9321889\ttotal: 34m 29s\tremaining: 16m 20s\n",
      "6786:\tlearn: 3.9318613\ttotal: 34m 29s\tremaining: 16m 19s\n",
      "6787:\tlearn: 3.9314939\ttotal: 34m 29s\tremaining: 16m 19s\n",
      "6788:\tlearn: 3.9311941\ttotal: 34m 30s\tremaining: 16m 19s\n",
      "6789:\tlearn: 3.9309075\ttotal: 34m 30s\tremaining: 16m 18s\n",
      "6790:\tlearn: 3.9304712\ttotal: 34m 30s\tremaining: 16m 18s\n",
      "6791:\tlearn: 3.9300256\ttotal: 34m 31s\tremaining: 16m 18s\n",
      "6792:\tlearn: 3.9297195\ttotal: 34m 31s\tremaining: 16m 17s\n",
      "6793:\tlearn: 3.9295513\ttotal: 34m 31s\tremaining: 16m 17s\n",
      "6794:\tlearn: 3.9292486\ttotal: 34m 32s\tremaining: 16m 17s\n",
      "6795:\tlearn: 3.9289891\ttotal: 34m 32s\tremaining: 16m 17s\n",
      "6796:\tlearn: 3.9286569\ttotal: 34m 32s\tremaining: 16m 16s\n",
      "6797:\tlearn: 3.9284225\ttotal: 34m 32s\tremaining: 16m 16s\n",
      "6798:\tlearn: 3.9281230\ttotal: 34m 33s\tremaining: 16m 16s\n",
      "6799:\tlearn: 3.9277912\ttotal: 34m 33s\tremaining: 16m 15s\n",
      "6800:\tlearn: 3.9274477\ttotal: 34m 33s\tremaining: 16m 15s\n",
      "6801:\tlearn: 3.9272216\ttotal: 34m 34s\tremaining: 16m 15s\n",
      "6802:\tlearn: 3.9269120\ttotal: 34m 34s\tremaining: 16m 14s\n",
      "6803:\tlearn: 3.9266037\ttotal: 34m 34s\tremaining: 16m 14s\n",
      "6804:\tlearn: 3.9263361\ttotal: 34m 35s\tremaining: 16m 14s\n",
      "6805:\tlearn: 3.9260362\ttotal: 34m 35s\tremaining: 16m 13s\n",
      "6806:\tlearn: 3.9257952\ttotal: 34m 35s\tremaining: 16m 13s\n",
      "6807:\tlearn: 3.9254903\ttotal: 34m 35s\tremaining: 16m 13s\n",
      "6808:\tlearn: 3.9251419\ttotal: 34m 36s\tremaining: 16m 12s\n",
      "6809:\tlearn: 3.9246916\ttotal: 34m 36s\tremaining: 16m 12s\n",
      "6810:\tlearn: 3.9242925\ttotal: 34m 36s\tremaining: 16m 12s\n",
      "6811:\tlearn: 3.9241650\ttotal: 34m 37s\tremaining: 16m 12s\n",
      "6812:\tlearn: 3.9236973\ttotal: 34m 37s\tremaining: 16m 11s\n",
      "6813:\tlearn: 3.9233212\ttotal: 34m 37s\tremaining: 16m 11s\n",
      "6814:\tlearn: 3.9229806\ttotal: 34m 37s\tremaining: 16m 11s\n",
      "6815:\tlearn: 3.9225806\ttotal: 34m 38s\tremaining: 16m 10s\n",
      "6816:\tlearn: 3.9222752\ttotal: 34m 38s\tremaining: 16m 10s\n",
      "6817:\tlearn: 3.9219832\ttotal: 34m 38s\tremaining: 16m 10s\n",
      "6818:\tlearn: 3.9216539\ttotal: 34m 39s\tremaining: 16m 9s\n",
      "6819:\tlearn: 3.9213643\ttotal: 34m 39s\tremaining: 16m 9s\n",
      "6820:\tlearn: 3.9210268\ttotal: 34m 39s\tremaining: 16m 9s\n",
      "6821:\tlearn: 3.9205897\ttotal: 34m 40s\tremaining: 16m 9s\n",
      "6822:\tlearn: 3.9203065\ttotal: 34m 40s\tremaining: 16m 8s\n",
      "6823:\tlearn: 3.9200240\ttotal: 34m 40s\tremaining: 16m 8s\n",
      "6824:\tlearn: 3.9197250\ttotal: 34m 41s\tremaining: 16m 8s\n",
      "6825:\tlearn: 3.9193965\ttotal: 34m 41s\tremaining: 16m 7s\n",
      "6826:\tlearn: 3.9190913\ttotal: 34m 41s\tremaining: 16m 7s\n",
      "6827:\tlearn: 3.9187827\ttotal: 34m 41s\tremaining: 16m 7s\n",
      "6828:\tlearn: 3.9184936\ttotal: 34m 42s\tremaining: 16m 6s\n",
      "6829:\tlearn: 3.9181589\ttotal: 34m 42s\tremaining: 16m 6s\n",
      "6830:\tlearn: 3.9178665\ttotal: 34m 42s\tremaining: 16m 6s\n",
      "6831:\tlearn: 3.9176539\ttotal: 34m 43s\tremaining: 16m 5s\n",
      "6832:\tlearn: 3.9173973\ttotal: 34m 43s\tremaining: 16m 5s\n",
      "6833:\tlearn: 3.9172103\ttotal: 34m 43s\tremaining: 16m 5s\n",
      "6834:\tlearn: 3.9167942\ttotal: 34m 43s\tremaining: 16m 4s\n",
      "6835:\tlearn: 3.9164683\ttotal: 34m 44s\tremaining: 16m 4s\n",
      "6836:\tlearn: 3.9162169\ttotal: 34m 44s\tremaining: 16m 4s\n",
      "6837:\tlearn: 3.9158102\ttotal: 34m 44s\tremaining: 16m 3s\n",
      "6838:\tlearn: 3.9155414\ttotal: 34m 44s\tremaining: 16m 3s\n",
      "6839:\tlearn: 3.9151335\ttotal: 34m 45s\tremaining: 16m 3s\n",
      "6840:\tlearn: 3.9150215\ttotal: 34m 45s\tremaining: 16m 3s\n",
      "6841:\tlearn: 3.9147548\ttotal: 34m 45s\tremaining: 16m 2s\n",
      "6842:\tlearn: 3.9143924\ttotal: 34m 46s\tremaining: 16m 2s\n",
      "6843:\tlearn: 3.9140888\ttotal: 34m 46s\tremaining: 16m 2s\n",
      "6844:\tlearn: 3.9138890\ttotal: 34m 46s\tremaining: 16m 1s\n",
      "6845:\tlearn: 3.9136157\ttotal: 34m 46s\tremaining: 16m 1s\n",
      "6846:\tlearn: 3.9133034\ttotal: 34m 47s\tremaining: 16m 1s\n",
      "6847:\tlearn: 3.9130146\ttotal: 34m 47s\tremaining: 16m\n",
      "6848:\tlearn: 3.9126442\ttotal: 34m 47s\tremaining: 16m\n",
      "6849:\tlearn: 3.9121917\ttotal: 34m 48s\tremaining: 16m\n",
      "6850:\tlearn: 3.9119222\ttotal: 34m 48s\tremaining: 15m 59s\n",
      "6851:\tlearn: 3.9116058\ttotal: 34m 48s\tremaining: 15m 59s\n",
      "6852:\tlearn: 3.9110674\ttotal: 34m 49s\tremaining: 15m 59s\n",
      "6853:\tlearn: 3.9108154\ttotal: 34m 49s\tremaining: 15m 59s\n",
      "6854:\tlearn: 3.9106495\ttotal: 34m 49s\tremaining: 15m 58s\n",
      "6855:\tlearn: 3.9103825\ttotal: 34m 49s\tremaining: 15m 58s\n",
      "6856:\tlearn: 3.9100423\ttotal: 34m 50s\tremaining: 15m 58s\n",
      "6857:\tlearn: 3.9097618\ttotal: 34m 50s\tremaining: 15m 57s\n",
      "6858:\tlearn: 3.9093117\ttotal: 34m 50s\tremaining: 15m 57s\n",
      "6859:\tlearn: 3.9089913\ttotal: 34m 50s\tremaining: 15m 57s\n",
      "6860:\tlearn: 3.9086898\ttotal: 34m 51s\tremaining: 15m 56s\n",
      "6861:\tlearn: 3.9084863\ttotal: 34m 51s\tremaining: 15m 56s\n",
      "6862:\tlearn: 3.9082858\ttotal: 34m 51s\tremaining: 15m 56s\n",
      "6863:\tlearn: 3.9080675\ttotal: 34m 52s\tremaining: 15m 55s\n",
      "6864:\tlearn: 3.9076970\ttotal: 34m 52s\tremaining: 15m 55s\n",
      "6865:\tlearn: 3.9073005\ttotal: 34m 52s\tremaining: 15m 55s\n",
      "6866:\tlearn: 3.9069688\ttotal: 34m 52s\tremaining: 15m 54s\n",
      "6867:\tlearn: 3.9066347\ttotal: 34m 53s\tremaining: 15m 54s\n",
      "6868:\tlearn: 3.9063423\ttotal: 34m 53s\tremaining: 15m 54s\n",
      "6869:\tlearn: 3.9060646\ttotal: 34m 53s\tremaining: 15m 53s\n",
      "6870:\tlearn: 3.9058962\ttotal: 34m 54s\tremaining: 15m 53s\n",
      "6871:\tlearn: 3.9054652\ttotal: 34m 54s\tremaining: 15m 53s\n",
      "6872:\tlearn: 3.9052031\ttotal: 34m 54s\tremaining: 15m 53s\n",
      "6873:\tlearn: 3.9049553\ttotal: 34m 55s\tremaining: 15m 52s\n",
      "6874:\tlearn: 3.9046684\ttotal: 34m 55s\tremaining: 15m 52s\n",
      "6875:\tlearn: 3.9044057\ttotal: 34m 55s\tremaining: 15m 52s\n",
      "6876:\tlearn: 3.9040853\ttotal: 34m 55s\tremaining: 15m 51s\n",
      "6877:\tlearn: 3.9038276\ttotal: 34m 56s\tremaining: 15m 51s\n",
      "6878:\tlearn: 3.9035679\ttotal: 34m 56s\tremaining: 15m 51s\n",
      "6879:\tlearn: 3.9033214\ttotal: 34m 56s\tremaining: 15m 50s\n",
      "6880:\tlearn: 3.9030008\ttotal: 34m 57s\tremaining: 15m 50s\n",
      "6881:\tlearn: 3.9026549\ttotal: 34m 57s\tremaining: 15m 50s\n",
      "6882:\tlearn: 3.9023235\ttotal: 34m 57s\tremaining: 15m 49s\n",
      "6883:\tlearn: 3.9020331\ttotal: 34m 57s\tremaining: 15m 49s\n",
      "6884:\tlearn: 3.9018425\ttotal: 34m 58s\tremaining: 15m 49s\n",
      "6885:\tlearn: 3.9014415\ttotal: 34m 58s\tremaining: 15m 49s\n",
      "6886:\tlearn: 3.9011250\ttotal: 34m 58s\tremaining: 15m 48s\n",
      "6887:\tlearn: 3.9006364\ttotal: 34m 59s\tremaining: 15m 48s\n",
      "6888:\tlearn: 3.9003753\ttotal: 34m 59s\tremaining: 15m 48s\n",
      "6889:\tlearn: 3.9000111\ttotal: 34m 59s\tremaining: 15m 47s\n",
      "6890:\tlearn: 3.8997039\ttotal: 35m\tremaining: 15m 47s\n",
      "6891:\tlearn: 3.8992911\ttotal: 35m\tremaining: 15m 47s\n",
      "6892:\tlearn: 3.8990028\ttotal: 35m\tremaining: 15m 46s\n",
      "6893:\tlearn: 3.8986807\ttotal: 35m 1s\tremaining: 15m 46s\n",
      "6894:\tlearn: 3.8984948\ttotal: 35m 1s\tremaining: 15m 46s\n",
      "6895:\tlearn: 3.8982105\ttotal: 35m 1s\tremaining: 15m 45s\n",
      "6896:\tlearn: 3.8979581\ttotal: 35m 1s\tremaining: 15m 45s\n",
      "6897:\tlearn: 3.8976710\ttotal: 35m 2s\tremaining: 15m 45s\n",
      "6898:\tlearn: 3.8974534\ttotal: 35m 2s\tremaining: 15m 45s\n",
      "6899:\tlearn: 3.8971950\ttotal: 35m 2s\tremaining: 15m 44s\n",
      "6900:\tlearn: 3.8968436\ttotal: 35m 3s\tremaining: 15m 44s\n",
      "6901:\tlearn: 3.8966010\ttotal: 35m 3s\tremaining: 15m 44s\n",
      "6902:\tlearn: 3.8963195\ttotal: 35m 3s\tremaining: 15m 43s\n",
      "6903:\tlearn: 3.8958415\ttotal: 35m 4s\tremaining: 15m 43s\n",
      "6904:\tlearn: 3.8956289\ttotal: 35m 4s\tremaining: 15m 43s\n",
      "6905:\tlearn: 3.8952815\ttotal: 35m 4s\tremaining: 15m 42s\n",
      "6906:\tlearn: 3.8950112\ttotal: 35m 4s\tremaining: 15m 42s\n",
      "6907:\tlearn: 3.8944438\ttotal: 35m 5s\tremaining: 15m 42s\n",
      "6908:\tlearn: 3.8941878\ttotal: 35m 5s\tremaining: 15m 42s\n",
      "6909:\tlearn: 3.8938644\ttotal: 35m 6s\tremaining: 15m 41s\n",
      "6910:\tlearn: 3.8936119\ttotal: 35m 6s\tremaining: 15m 41s\n",
      "6911:\tlearn: 3.8934147\ttotal: 35m 6s\tremaining: 15m 41s\n",
      "6912:\tlearn: 3.8930989\ttotal: 35m 6s\tremaining: 15m 40s\n",
      "6913:\tlearn: 3.8927991\ttotal: 35m 7s\tremaining: 15m 40s\n",
      "6914:\tlearn: 3.8920652\ttotal: 35m 7s\tremaining: 15m 40s\n",
      "6915:\tlearn: 3.8917366\ttotal: 35m 8s\tremaining: 15m 40s\n",
      "6916:\tlearn: 3.8912605\ttotal: 35m 8s\tremaining: 15m 39s\n",
      "6917:\tlearn: 3.8910754\ttotal: 35m 8s\tremaining: 15m 39s\n",
      "6918:\tlearn: 3.8907455\ttotal: 35m 8s\tremaining: 15m 39s\n",
      "6919:\tlearn: 3.8903582\ttotal: 35m 9s\tremaining: 15m 38s\n",
      "6920:\tlearn: 3.8902316\ttotal: 35m 9s\tremaining: 15m 38s\n",
      "6921:\tlearn: 3.8900091\ttotal: 35m 9s\tremaining: 15m 38s\n",
      "6922:\tlearn: 3.8896865\ttotal: 35m 10s\tremaining: 15m 37s\n",
      "6923:\tlearn: 3.8892630\ttotal: 35m 10s\tremaining: 15m 37s\n",
      "6924:\tlearn: 3.8889356\ttotal: 35m 10s\tremaining: 15m 37s\n",
      "6925:\tlearn: 3.8886578\ttotal: 35m 10s\tremaining: 15m 36s\n",
      "6926:\tlearn: 3.8884611\ttotal: 35m 11s\tremaining: 15m 36s\n",
      "6927:\tlearn: 3.8881547\ttotal: 35m 11s\tremaining: 15m 36s\n",
      "6928:\tlearn: 3.8877402\ttotal: 35m 11s\tremaining: 15m 35s\n",
      "6929:\tlearn: 3.8875341\ttotal: 35m 12s\tremaining: 15m 35s\n",
      "6930:\tlearn: 3.8870835\ttotal: 35m 12s\tremaining: 15m 35s\n",
      "6931:\tlearn: 3.8868826\ttotal: 35m 12s\tremaining: 15m 34s\n",
      "6932:\tlearn: 3.8865310\ttotal: 35m 12s\tremaining: 15m 34s\n",
      "6933:\tlearn: 3.8861079\ttotal: 35m 13s\tremaining: 15m 34s\n",
      "6934:\tlearn: 3.8859587\ttotal: 35m 13s\tremaining: 15m 34s\n",
      "6935:\tlearn: 3.8856997\ttotal: 35m 13s\tremaining: 15m 33s\n",
      "6936:\tlearn: 3.8854242\ttotal: 35m 13s\tremaining: 15m 33s\n",
      "6937:\tlearn: 3.8850828\ttotal: 35m 14s\tremaining: 15m 33s\n",
      "6938:\tlearn: 3.8848977\ttotal: 35m 14s\tremaining: 15m 32s\n",
      "6939:\tlearn: 3.8846910\ttotal: 35m 14s\tremaining: 15m 32s\n",
      "6940:\tlearn: 3.8843420\ttotal: 35m 15s\tremaining: 15m 32s\n",
      "6941:\tlearn: 3.8836072\ttotal: 35m 15s\tremaining: 15m 31s\n",
      "6942:\tlearn: 3.8832119\ttotal: 35m 15s\tremaining: 15m 31s\n",
      "6943:\tlearn: 3.8830142\ttotal: 35m 16s\tremaining: 15m 31s\n",
      "6944:\tlearn: 3.8828053\ttotal: 35m 16s\tremaining: 15m 30s\n",
      "6945:\tlearn: 3.8825818\ttotal: 35m 16s\tremaining: 15m 30s\n",
      "6946:\tlearn: 3.8822944\ttotal: 35m 16s\tremaining: 15m 30s\n",
      "6947:\tlearn: 3.8821837\ttotal: 35m 17s\tremaining: 15m 29s\n",
      "6948:\tlearn: 3.8819260\ttotal: 35m 17s\tremaining: 15m 29s\n",
      "6949:\tlearn: 3.8817365\ttotal: 35m 17s\tremaining: 15m 29s\n",
      "6950:\tlearn: 3.8814883\ttotal: 35m 17s\tremaining: 15m 29s\n",
      "6951:\tlearn: 3.8813448\ttotal: 35m 18s\tremaining: 15m 28s\n",
      "6952:\tlearn: 3.8811366\ttotal: 35m 18s\tremaining: 15m 28s\n",
      "6953:\tlearn: 3.8808224\ttotal: 35m 18s\tremaining: 15m 28s\n",
      "6954:\tlearn: 3.8803351\ttotal: 35m 19s\tremaining: 15m 27s\n",
      "6955:\tlearn: 3.8800037\ttotal: 35m 19s\tremaining: 15m 27s\n",
      "6956:\tlearn: 3.8796797\ttotal: 35m 19s\tremaining: 15m 27s\n",
      "6957:\tlearn: 3.8794855\ttotal: 35m 19s\tremaining: 15m 26s\n",
      "6958:\tlearn: 3.8792881\ttotal: 35m 20s\tremaining: 15m 26s\n",
      "6959:\tlearn: 3.8791218\ttotal: 35m 20s\tremaining: 15m 26s\n",
      "6960:\tlearn: 3.8786546\ttotal: 35m 20s\tremaining: 15m 25s\n",
      "6961:\tlearn: 3.8783402\ttotal: 35m 21s\tremaining: 15m 25s\n",
      "6962:\tlearn: 3.8780309\ttotal: 35m 21s\tremaining: 15m 25s\n",
      "6963:\tlearn: 3.8778014\ttotal: 35m 21s\tremaining: 15m 24s\n",
      "6964:\tlearn: 3.8776044\ttotal: 35m 21s\tremaining: 15m 24s\n",
      "6965:\tlearn: 3.8773631\ttotal: 35m 22s\tremaining: 15m 24s\n",
      "6966:\tlearn: 3.8771964\ttotal: 35m 22s\tremaining: 15m 23s\n",
      "6967:\tlearn: 3.8769182\ttotal: 35m 22s\tremaining: 15m 23s\n",
      "6968:\tlearn: 3.8765245\ttotal: 35m 23s\tremaining: 15m 23s\n",
      "6969:\tlearn: 3.8759874\ttotal: 35m 23s\tremaining: 15m 23s\n",
      "6970:\tlearn: 3.8756289\ttotal: 35m 23s\tremaining: 15m 22s\n",
      "6971:\tlearn: 3.8752577\ttotal: 35m 23s\tremaining: 15m 22s\n",
      "6972:\tlearn: 3.8746729\ttotal: 35m 24s\tremaining: 15m 22s\n",
      "6973:\tlearn: 3.8745301\ttotal: 35m 24s\tremaining: 15m 21s\n",
      "6974:\tlearn: 3.8742157\ttotal: 35m 24s\tremaining: 15m 21s\n",
      "6975:\tlearn: 3.8737505\ttotal: 35m 25s\tremaining: 15m 21s\n",
      "6976:\tlearn: 3.8734009\ttotal: 35m 25s\tremaining: 15m 20s\n",
      "6977:\tlearn: 3.8731121\ttotal: 35m 25s\tremaining: 15m 20s\n",
      "6978:\tlearn: 3.8728233\ttotal: 35m 26s\tremaining: 15m 20s\n",
      "6979:\tlearn: 3.8726149\ttotal: 35m 26s\tremaining: 15m 20s\n",
      "6980:\tlearn: 3.8723485\ttotal: 35m 26s\tremaining: 15m 19s\n",
      "6981:\tlearn: 3.8719649\ttotal: 35m 27s\tremaining: 15m 19s\n",
      "6982:\tlearn: 3.8716905\ttotal: 35m 27s\tremaining: 15m 19s\n",
      "6983:\tlearn: 3.8715047\ttotal: 35m 27s\tremaining: 15m 18s\n",
      "6984:\tlearn: 3.8713153\ttotal: 35m 27s\tremaining: 15m 18s\n",
      "6985:\tlearn: 3.8710258\ttotal: 35m 28s\tremaining: 15m 18s\n",
      "6986:\tlearn: 3.8707189\ttotal: 35m 28s\tremaining: 15m 17s\n",
      "6987:\tlearn: 3.8704402\ttotal: 35m 28s\tremaining: 15m 17s\n",
      "6988:\tlearn: 3.8700880\ttotal: 35m 28s\tremaining: 15m 17s\n",
      "6989:\tlearn: 3.8698171\ttotal: 35m 29s\tremaining: 15m 16s\n",
      "6990:\tlearn: 3.8692780\ttotal: 35m 29s\tremaining: 15m 16s\n",
      "6991:\tlearn: 3.8690274\ttotal: 35m 29s\tremaining: 15m 16s\n",
      "6992:\tlearn: 3.8684807\ttotal: 35m 30s\tremaining: 15m 15s\n",
      "6993:\tlearn: 3.8681096\ttotal: 35m 30s\tremaining: 15m 15s\n",
      "6994:\tlearn: 3.8679033\ttotal: 35m 30s\tremaining: 15m 15s\n",
      "6995:\tlearn: 3.8676443\ttotal: 35m 30s\tremaining: 15m 15s\n",
      "6996:\tlearn: 3.8673963\ttotal: 35m 31s\tremaining: 15m 14s\n",
      "6997:\tlearn: 3.8669528\ttotal: 35m 31s\tremaining: 15m 14s\n",
      "6998:\tlearn: 3.8666637\ttotal: 35m 31s\tremaining: 15m 14s\n",
      "6999:\tlearn: 3.8664091\ttotal: 35m 32s\tremaining: 15m 13s\n",
      "7000:\tlearn: 3.8660614\ttotal: 35m 32s\tremaining: 15m 13s\n",
      "7001:\tlearn: 3.8658348\ttotal: 35m 32s\tremaining: 15m 13s\n",
      "7002:\tlearn: 3.8655526\ttotal: 35m 33s\tremaining: 15m 12s\n",
      "7003:\tlearn: 3.8653020\ttotal: 35m 33s\tremaining: 15m 12s\n",
      "7004:\tlearn: 3.8650187\ttotal: 35m 33s\tremaining: 15m 12s\n",
      "7005:\tlearn: 3.8645916\ttotal: 35m 33s\tremaining: 15m 11s\n",
      "7006:\tlearn: 3.8643248\ttotal: 35m 34s\tremaining: 15m 11s\n",
      "7007:\tlearn: 3.8640451\ttotal: 35m 34s\tremaining: 15m 11s\n",
      "7008:\tlearn: 3.8635829\ttotal: 35m 34s\tremaining: 15m 10s\n",
      "7009:\tlearn: 3.8629932\ttotal: 35m 35s\tremaining: 15m 10s\n",
      "7010:\tlearn: 3.8626514\ttotal: 35m 35s\tremaining: 15m 10s\n",
      "7011:\tlearn: 3.8624068\ttotal: 35m 35s\tremaining: 15m 10s\n",
      "7012:\tlearn: 3.8620069\ttotal: 35m 35s\tremaining: 15m 9s\n",
      "7013:\tlearn: 3.8615920\ttotal: 35m 36s\tremaining: 15m 9s\n",
      "7014:\tlearn: 3.8611317\ttotal: 35m 36s\tremaining: 15m 9s\n",
      "7015:\tlearn: 3.8609162\ttotal: 35m 36s\tremaining: 15m 8s\n",
      "7016:\tlearn: 3.8608292\ttotal: 35m 37s\tremaining: 15m 8s\n",
      "7017:\tlearn: 3.8604410\ttotal: 35m 37s\tremaining: 15m 8s\n",
      "7018:\tlearn: 3.8601211\ttotal: 35m 37s\tremaining: 15m 7s\n",
      "7019:\tlearn: 3.8597262\ttotal: 35m 38s\tremaining: 15m 7s\n",
      "7020:\tlearn: 3.8592757\ttotal: 35m 38s\tremaining: 15m 7s\n",
      "7021:\tlearn: 3.8589208\ttotal: 35m 38s\tremaining: 15m 6s\n",
      "7022:\tlearn: 3.8587583\ttotal: 35m 38s\tremaining: 15m 6s\n",
      "7023:\tlearn: 3.8583979\ttotal: 35m 39s\tremaining: 15m 6s\n",
      "7024:\tlearn: 3.8581289\ttotal: 35m 39s\tremaining: 15m 6s\n",
      "7025:\tlearn: 3.8579112\ttotal: 35m 39s\tremaining: 15m 5s\n",
      "7026:\tlearn: 3.8576760\ttotal: 35m 39s\tremaining: 15m 5s\n",
      "7027:\tlearn: 3.8573130\ttotal: 35m 40s\tremaining: 15m 5s\n",
      "7028:\tlearn: 3.8570198\ttotal: 35m 40s\tremaining: 15m 4s\n",
      "7029:\tlearn: 3.8566515\ttotal: 35m 40s\tremaining: 15m 4s\n",
      "7030:\tlearn: 3.8563745\ttotal: 35m 41s\tremaining: 15m 4s\n",
      "7031:\tlearn: 3.8559192\ttotal: 35m 41s\tremaining: 15m 3s\n",
      "7032:\tlearn: 3.8555793\ttotal: 35m 41s\tremaining: 15m 3s\n",
      "7033:\tlearn: 3.8551787\ttotal: 35m 42s\tremaining: 15m 3s\n",
      "7034:\tlearn: 3.8548068\ttotal: 35m 42s\tremaining: 15m 2s\n",
      "7035:\tlearn: 3.8544570\ttotal: 35m 42s\tremaining: 15m 2s\n",
      "7036:\tlearn: 3.8539502\ttotal: 35m 42s\tremaining: 15m 2s\n",
      "7037:\tlearn: 3.8536970\ttotal: 35m 43s\tremaining: 15m 2s\n",
      "7038:\tlearn: 3.8534290\ttotal: 35m 43s\tremaining: 15m 1s\n",
      "7039:\tlearn: 3.8532095\ttotal: 35m 43s\tremaining: 15m 1s\n",
      "7040:\tlearn: 3.8528439\ttotal: 35m 44s\tremaining: 15m 1s\n",
      "7041:\tlearn: 3.8527191\ttotal: 35m 44s\tremaining: 15m\n",
      "7042:\tlearn: 3.8525295\ttotal: 35m 44s\tremaining: 15m\n",
      "7043:\tlearn: 3.8522782\ttotal: 35m 44s\tremaining: 15m\n",
      "7044:\tlearn: 3.8520026\ttotal: 35m 45s\tremaining: 14m 59s\n",
      "7045:\tlearn: 3.8517053\ttotal: 35m 45s\tremaining: 14m 59s\n",
      "7046:\tlearn: 3.8514706\ttotal: 35m 45s\tremaining: 14m 59s\n",
      "7047:\tlearn: 3.8510984\ttotal: 35m 46s\tremaining: 14m 58s\n",
      "7048:\tlearn: 3.8507888\ttotal: 35m 46s\tremaining: 14m 58s\n",
      "7049:\tlearn: 3.8506126\ttotal: 35m 46s\tremaining: 14m 58s\n",
      "7050:\tlearn: 3.8503599\ttotal: 35m 46s\tremaining: 14m 57s\n",
      "7051:\tlearn: 3.8500448\ttotal: 35m 47s\tremaining: 14m 57s\n",
      "7052:\tlearn: 3.8497958\ttotal: 35m 47s\tremaining: 14m 57s\n",
      "7053:\tlearn: 3.8495713\ttotal: 35m 47s\tremaining: 14m 56s\n",
      "7054:\tlearn: 3.8492332\ttotal: 35m 47s\tremaining: 14m 56s\n",
      "7055:\tlearn: 3.8487988\ttotal: 35m 48s\tremaining: 14m 56s\n",
      "7056:\tlearn: 3.8484593\ttotal: 35m 48s\tremaining: 14m 56s\n",
      "7057:\tlearn: 3.8482007\ttotal: 35m 48s\tremaining: 14m 55s\n",
      "7058:\tlearn: 3.8480049\ttotal: 35m 49s\tremaining: 14m 55s\n",
      "7059:\tlearn: 3.8476037\ttotal: 35m 49s\tremaining: 14m 55s\n",
      "7060:\tlearn: 3.8473831\ttotal: 35m 49s\tremaining: 14m 54s\n",
      "7061:\tlearn: 3.8471065\ttotal: 35m 49s\tremaining: 14m 54s\n",
      "7062:\tlearn: 3.8467755\ttotal: 35m 50s\tremaining: 14m 54s\n",
      "7063:\tlearn: 3.8464595\ttotal: 35m 50s\tremaining: 14m 53s\n",
      "7064:\tlearn: 3.8459639\ttotal: 35m 51s\tremaining: 14m 53s\n",
      "7065:\tlearn: 3.8455150\ttotal: 35m 51s\tremaining: 14m 53s\n",
      "7066:\tlearn: 3.8452813\ttotal: 35m 51s\tremaining: 14m 52s\n",
      "7067:\tlearn: 3.8450178\ttotal: 35m 51s\tremaining: 14m 52s\n",
      "7068:\tlearn: 3.8446285\ttotal: 35m 52s\tremaining: 14m 52s\n",
      "7069:\tlearn: 3.8442584\ttotal: 35m 52s\tremaining: 14m 52s\n",
      "7070:\tlearn: 3.8440241\ttotal: 35m 52s\tremaining: 14m 51s\n",
      "7071:\tlearn: 3.8436989\ttotal: 35m 53s\tremaining: 14m 51s\n",
      "7072:\tlearn: 3.8434328\ttotal: 35m 53s\tremaining: 14m 51s\n",
      "7073:\tlearn: 3.8431363\ttotal: 35m 53s\tremaining: 14m 50s\n",
      "7074:\tlearn: 3.8428361\ttotal: 35m 54s\tremaining: 14m 50s\n",
      "7075:\tlearn: 3.8424885\ttotal: 35m 54s\tremaining: 14m 50s\n",
      "7076:\tlearn: 3.8421231\ttotal: 35m 54s\tremaining: 14m 49s\n",
      "7077:\tlearn: 3.8418269\ttotal: 35m 55s\tremaining: 14m 49s\n",
      "7078:\tlearn: 3.8417085\ttotal: 35m 55s\tremaining: 14m 49s\n",
      "7079:\tlearn: 3.8414441\ttotal: 35m 55s\tremaining: 14m 49s\n",
      "7080:\tlearn: 3.8410265\ttotal: 35m 55s\tremaining: 14m 48s\n",
      "7081:\tlearn: 3.8407491\ttotal: 35m 55s\tremaining: 14m 48s\n",
      "7082:\tlearn: 3.8405609\ttotal: 35m 56s\tremaining: 14m 48s\n",
      "7083:\tlearn: 3.8401361\ttotal: 35m 56s\tremaining: 14m 47s\n",
      "7084:\tlearn: 3.8398186\ttotal: 35m 56s\tremaining: 14m 47s\n",
      "7085:\tlearn: 3.8395771\ttotal: 35m 57s\tremaining: 14m 47s\n",
      "7086:\tlearn: 3.8394331\ttotal: 35m 57s\tremaining: 14m 46s\n",
      "7087:\tlearn: 3.8391476\ttotal: 35m 57s\tremaining: 14m 46s\n",
      "7088:\tlearn: 3.8388838\ttotal: 35m 57s\tremaining: 14m 46s\n",
      "7089:\tlearn: 3.8386089\ttotal: 35m 58s\tremaining: 14m 45s\n",
      "7090:\tlearn: 3.8384198\ttotal: 35m 58s\tremaining: 14m 45s\n",
      "7091:\tlearn: 3.8382048\ttotal: 35m 58s\tremaining: 14m 45s\n",
      "7092:\tlearn: 3.8377989\ttotal: 35m 58s\tremaining: 14m 44s\n",
      "7093:\tlearn: 3.8373654\ttotal: 35m 59s\tremaining: 14m 44s\n",
      "7094:\tlearn: 3.8371368\ttotal: 35m 59s\tremaining: 14m 44s\n",
      "7095:\tlearn: 3.8367520\ttotal: 35m 59s\tremaining: 14m 43s\n",
      "7096:\tlearn: 3.8365554\ttotal: 36m\tremaining: 14m 43s\n",
      "7097:\tlearn: 3.8363661\ttotal: 36m\tremaining: 14m 43s\n",
      "7098:\tlearn: 3.8360170\ttotal: 36m\tremaining: 14m 42s\n",
      "7099:\tlearn: 3.8357108\ttotal: 36m\tremaining: 14m 42s\n",
      "7100:\tlearn: 3.8354922\ttotal: 36m 1s\tremaining: 14m 42s\n",
      "7101:\tlearn: 3.8352107\ttotal: 36m 1s\tremaining: 14m 41s\n",
      "7102:\tlearn: 3.8349570\ttotal: 36m 1s\tremaining: 14m 41s\n",
      "7103:\tlearn: 3.8348285\ttotal: 36m 1s\tremaining: 14m 41s\n",
      "7104:\tlearn: 3.8344410\ttotal: 36m 2s\tremaining: 14m 41s\n",
      "7105:\tlearn: 3.8340276\ttotal: 36m 2s\tremaining: 14m 40s\n",
      "7106:\tlearn: 3.8338684\ttotal: 36m 2s\tremaining: 14m 40s\n",
      "7107:\tlearn: 3.8336968\ttotal: 36m 3s\tremaining: 14m 40s\n",
      "7108:\tlearn: 3.8333144\ttotal: 36m 3s\tremaining: 14m 39s\n",
      "7109:\tlearn: 3.8328578\ttotal: 36m 3s\tremaining: 14m 39s\n",
      "7110:\tlearn: 3.8327530\ttotal: 36m 3s\tremaining: 14m 39s\n",
      "7111:\tlearn: 3.8325406\ttotal: 36m 4s\tremaining: 14m 38s\n",
      "7112:\tlearn: 3.8321960\ttotal: 36m 4s\tremaining: 14m 38s\n",
      "7113:\tlearn: 3.8318584\ttotal: 36m 4s\tremaining: 14m 38s\n",
      "7114:\tlearn: 3.8317102\ttotal: 36m 4s\tremaining: 14m 37s\n",
      "7115:\tlearn: 3.8314974\ttotal: 36m 5s\tremaining: 14m 37s\n",
      "7116:\tlearn: 3.8312011\ttotal: 36m 5s\tremaining: 14m 37s\n",
      "7117:\tlearn: 3.8304482\ttotal: 36m 5s\tremaining: 14m 36s\n",
      "7118:\tlearn: 3.8303145\ttotal: 36m 6s\tremaining: 14m 36s\n",
      "7119:\tlearn: 3.8300277\ttotal: 36m 6s\tremaining: 14m 36s\n",
      "7120:\tlearn: 3.8297529\ttotal: 36m 6s\tremaining: 14m 36s\n",
      "7121:\tlearn: 3.8294611\ttotal: 36m 7s\tremaining: 14m 35s\n",
      "7122:\tlearn: 3.8290119\ttotal: 36m 7s\tremaining: 14m 35s\n",
      "7123:\tlearn: 3.8286758\ttotal: 36m 7s\tremaining: 14m 35s\n",
      "7124:\tlearn: 3.8284291\ttotal: 36m 7s\tremaining: 14m 34s\n",
      "7125:\tlearn: 3.8281458\ttotal: 36m 8s\tremaining: 14m 34s\n",
      "7126:\tlearn: 3.8279660\ttotal: 36m 8s\tremaining: 14m 34s\n",
      "7127:\tlearn: 3.8276310\ttotal: 36m 8s\tremaining: 14m 33s\n",
      "7128:\tlearn: 3.8273848\ttotal: 36m 8s\tremaining: 14m 33s\n",
      "7129:\tlearn: 3.8270430\ttotal: 36m 9s\tremaining: 14m 33s\n",
      "7130:\tlearn: 3.8266053\ttotal: 36m 9s\tremaining: 14m 32s\n",
      "7131:\tlearn: 3.8263762\ttotal: 36m 9s\tremaining: 14m 32s\n",
      "7132:\tlearn: 3.8260650\ttotal: 36m 10s\tremaining: 14m 32s\n",
      "7133:\tlearn: 3.8257427\ttotal: 36m 10s\tremaining: 14m 31s\n",
      "7134:\tlearn: 3.8252648\ttotal: 36m 10s\tremaining: 14m 31s\n",
      "7135:\tlearn: 3.8248461\ttotal: 36m 11s\tremaining: 14m 31s\n",
      "7136:\tlearn: 3.8246315\ttotal: 36m 11s\tremaining: 14m 31s\n",
      "7137:\tlearn: 3.8242001\ttotal: 36m 11s\tremaining: 14m 30s\n",
      "7138:\tlearn: 3.8239422\ttotal: 36m 11s\tremaining: 14m 30s\n",
      "7139:\tlearn: 3.8236529\ttotal: 36m 12s\tremaining: 14m 30s\n",
      "7140:\tlearn: 3.8232483\ttotal: 36m 12s\tremaining: 14m 29s\n",
      "7141:\tlearn: 3.8229220\ttotal: 36m 12s\tremaining: 14m 29s\n",
      "7142:\tlearn: 3.8224862\ttotal: 36m 13s\tremaining: 14m 29s\n",
      "7143:\tlearn: 3.8219611\ttotal: 36m 13s\tremaining: 14m 28s\n",
      "7144:\tlearn: 3.8215706\ttotal: 36m 13s\tremaining: 14m 28s\n",
      "7145:\tlearn: 3.8209003\ttotal: 36m 14s\tremaining: 14m 28s\n",
      "7146:\tlearn: 3.8205062\ttotal: 36m 14s\tremaining: 14m 27s\n",
      "7147:\tlearn: 3.8202634\ttotal: 36m 14s\tremaining: 14m 27s\n",
      "7148:\tlearn: 3.8200008\ttotal: 36m 14s\tremaining: 14m 27s\n",
      "7149:\tlearn: 3.8198097\ttotal: 36m 15s\tremaining: 14m 27s\n",
      "7150:\tlearn: 3.8193090\ttotal: 36m 15s\tremaining: 14m 26s\n",
      "7151:\tlearn: 3.8189743\ttotal: 36m 15s\tremaining: 14m 26s\n",
      "7152:\tlearn: 3.8186900\ttotal: 36m 16s\tremaining: 14m 26s\n",
      "7153:\tlearn: 3.8184083\ttotal: 36m 16s\tremaining: 14m 25s\n",
      "7154:\tlearn: 3.8181204\ttotal: 36m 16s\tremaining: 14m 25s\n",
      "7155:\tlearn: 3.8178535\ttotal: 36m 16s\tremaining: 14m 25s\n",
      "7156:\tlearn: 3.8175408\ttotal: 36m 17s\tremaining: 14m 24s\n",
      "7157:\tlearn: 3.8171930\ttotal: 36m 17s\tremaining: 14m 24s\n",
      "7158:\tlearn: 3.8169798\ttotal: 36m 17s\tremaining: 14m 24s\n",
      "7159:\tlearn: 3.8166561\ttotal: 36m 17s\tremaining: 14m 23s\n",
      "7160:\tlearn: 3.8163518\ttotal: 36m 18s\tremaining: 14m 23s\n",
      "7161:\tlearn: 3.8160698\ttotal: 36m 18s\tremaining: 14m 23s\n",
      "7162:\tlearn: 3.8154944\ttotal: 36m 18s\tremaining: 14m 22s\n",
      "7163:\tlearn: 3.8152775\ttotal: 36m 18s\tremaining: 14m 22s\n",
      "7164:\tlearn: 3.8150351\ttotal: 36m 19s\tremaining: 14m 22s\n",
      "7165:\tlearn: 3.8147622\ttotal: 36m 19s\tremaining: 14m 21s\n",
      "7166:\tlearn: 3.8144959\ttotal: 36m 19s\tremaining: 14m 21s\n",
      "7167:\tlearn: 3.8142269\ttotal: 36m 20s\tremaining: 14m 21s\n",
      "7168:\tlearn: 3.8139913\ttotal: 36m 20s\tremaining: 14m 20s\n",
      "7169:\tlearn: 3.8136696\ttotal: 36m 20s\tremaining: 14m 20s\n",
      "7170:\tlearn: 3.8132648\ttotal: 36m 20s\tremaining: 14m 20s\n",
      "7171:\tlearn: 3.8129728\ttotal: 36m 21s\tremaining: 14m 20s\n",
      "7172:\tlearn: 3.8127823\ttotal: 36m 21s\tremaining: 14m 19s\n",
      "7173:\tlearn: 3.8125589\ttotal: 36m 21s\tremaining: 14m 19s\n",
      "7174:\tlearn: 3.8122712\ttotal: 36m 21s\tremaining: 14m 19s\n",
      "7175:\tlearn: 3.8120626\ttotal: 36m 22s\tremaining: 14m 18s\n",
      "7176:\tlearn: 3.8117820\ttotal: 36m 22s\tremaining: 14m 18s\n",
      "7177:\tlearn: 3.8112204\ttotal: 36m 22s\tremaining: 14m 18s\n",
      "7178:\tlearn: 3.8108930\ttotal: 36m 23s\tremaining: 14m 17s\n",
      "7179:\tlearn: 3.8105543\ttotal: 36m 23s\tremaining: 14m 17s\n",
      "7180:\tlearn: 3.8102327\ttotal: 36m 23s\tremaining: 14m 17s\n",
      "7181:\tlearn: 3.8098603\ttotal: 36m 24s\tremaining: 14m 16s\n",
      "7182:\tlearn: 3.8096257\ttotal: 36m 24s\tremaining: 14m 16s\n",
      "7183:\tlearn: 3.8093627\ttotal: 36m 24s\tremaining: 14m 16s\n",
      "7184:\tlearn: 3.8090607\ttotal: 36m 24s\tremaining: 14m 16s\n",
      "7185:\tlearn: 3.8085345\ttotal: 36m 25s\tremaining: 14m 15s\n",
      "7186:\tlearn: 3.8083311\ttotal: 36m 25s\tremaining: 14m 15s\n",
      "7187:\tlearn: 3.8080296\ttotal: 36m 25s\tremaining: 14m 15s\n",
      "7188:\tlearn: 3.8077692\ttotal: 36m 25s\tremaining: 14m 14s\n",
      "7189:\tlearn: 3.8074169\ttotal: 36m 26s\tremaining: 14m 14s\n",
      "7190:\tlearn: 3.8070311\ttotal: 36m 26s\tremaining: 14m 14s\n",
      "7191:\tlearn: 3.8065984\ttotal: 36m 26s\tremaining: 14m 13s\n",
      "7192:\tlearn: 3.8063608\ttotal: 36m 27s\tremaining: 14m 13s\n",
      "7193:\tlearn: 3.8061502\ttotal: 36m 27s\tremaining: 14m 13s\n",
      "7194:\tlearn: 3.8059127\ttotal: 36m 27s\tremaining: 14m 12s\n",
      "7195:\tlearn: 3.8055974\ttotal: 36m 27s\tremaining: 14m 12s\n",
      "7196:\tlearn: 3.8054199\ttotal: 36m 28s\tremaining: 14m 12s\n",
      "7197:\tlearn: 3.8050998\ttotal: 36m 28s\tremaining: 14m 11s\n",
      "7198:\tlearn: 3.8048866\ttotal: 36m 28s\tremaining: 14m 11s\n",
      "7199:\tlearn: 3.8047396\ttotal: 36m 28s\tremaining: 14m 11s\n",
      "7200:\tlearn: 3.8043877\ttotal: 36m 29s\tremaining: 14m 10s\n",
      "7201:\tlearn: 3.8042076\ttotal: 36m 29s\tremaining: 14m 10s\n",
      "7202:\tlearn: 3.8038706\ttotal: 36m 29s\tremaining: 14m 10s\n",
      "7203:\tlearn: 3.8037894\ttotal: 36m 30s\tremaining: 14m 9s\n",
      "7204:\tlearn: 3.8034331\ttotal: 36m 30s\tremaining: 14m 9s\n",
      "7205:\tlearn: 3.8030992\ttotal: 36m 30s\tremaining: 14m 9s\n",
      "7206:\tlearn: 3.8026961\ttotal: 36m 30s\tremaining: 14m 9s\n",
      "7207:\tlearn: 3.8025371\ttotal: 36m 31s\tremaining: 14m 8s\n",
      "7208:\tlearn: 3.8023798\ttotal: 36m 31s\tremaining: 14m 8s\n",
      "7209:\tlearn: 3.8021171\ttotal: 36m 31s\tremaining: 14m 8s\n",
      "7210:\tlearn: 3.8018841\ttotal: 36m 31s\tremaining: 14m 7s\n",
      "7211:\tlearn: 3.8016134\ttotal: 36m 32s\tremaining: 14m 7s\n",
      "7212:\tlearn: 3.8014311\ttotal: 36m 32s\tremaining: 14m 7s\n",
      "7213:\tlearn: 3.8011959\ttotal: 36m 32s\tremaining: 14m 6s\n",
      "7214:\tlearn: 3.8008457\ttotal: 36m 33s\tremaining: 14m 6s\n",
      "7215:\tlearn: 3.8006254\ttotal: 36m 33s\tremaining: 14m 6s\n",
      "7216:\tlearn: 3.8002930\ttotal: 36m 33s\tremaining: 14m 5s\n",
      "7217:\tlearn: 3.7998686\ttotal: 36m 33s\tremaining: 14m 5s\n",
      "7218:\tlearn: 3.7996082\ttotal: 36m 34s\tremaining: 14m 5s\n",
      "7219:\tlearn: 3.7990820\ttotal: 36m 34s\tremaining: 14m 4s\n",
      "7220:\tlearn: 3.7986968\ttotal: 36m 34s\tremaining: 14m 4s\n",
      "7221:\tlearn: 3.7984229\ttotal: 36m 35s\tremaining: 14m 4s\n",
      "7222:\tlearn: 3.7981756\ttotal: 36m 35s\tremaining: 14m 4s\n",
      "7223:\tlearn: 3.7977762\ttotal: 36m 35s\tremaining: 14m 3s\n",
      "7224:\tlearn: 3.7974975\ttotal: 36m 35s\tremaining: 14m 3s\n",
      "7225:\tlearn: 3.7971757\ttotal: 36m 36s\tremaining: 14m 3s\n",
      "7226:\tlearn: 3.7970992\ttotal: 36m 36s\tremaining: 14m 2s\n",
      "7227:\tlearn: 3.7967924\ttotal: 36m 36s\tremaining: 14m 2s\n",
      "7228:\tlearn: 3.7966231\ttotal: 36m 37s\tremaining: 14m 2s\n",
      "7229:\tlearn: 3.7963578\ttotal: 36m 37s\tremaining: 14m 1s\n",
      "7230:\tlearn: 3.7961574\ttotal: 36m 37s\tremaining: 14m 1s\n",
      "7231:\tlearn: 3.7958754\ttotal: 36m 37s\tremaining: 14m 1s\n",
      "7232:\tlearn: 3.7955646\ttotal: 36m 38s\tremaining: 14m\n",
      "7233:\tlearn: 3.7953490\ttotal: 36m 38s\tremaining: 14m\n",
      "7234:\tlearn: 3.7948803\ttotal: 36m 38s\tremaining: 14m\n",
      "7235:\tlearn: 3.7945976\ttotal: 36m 39s\tremaining: 14m\n",
      "7236:\tlearn: 3.7942253\ttotal: 36m 39s\tremaining: 13m 59s\n",
      "7237:\tlearn: 3.7939653\ttotal: 36m 39s\tremaining: 13m 59s\n",
      "7238:\tlearn: 3.7936567\ttotal: 36m 40s\tremaining: 13m 59s\n",
      "7239:\tlearn: 3.7933275\ttotal: 36m 40s\tremaining: 13m 58s\n",
      "7240:\tlearn: 3.7927665\ttotal: 36m 40s\tremaining: 13m 58s\n",
      "7241:\tlearn: 3.7924589\ttotal: 36m 40s\tremaining: 13m 58s\n",
      "7242:\tlearn: 3.7922504\ttotal: 36m 41s\tremaining: 13m 57s\n",
      "7243:\tlearn: 3.7920510\ttotal: 36m 41s\tremaining: 13m 57s\n",
      "7244:\tlearn: 3.7917059\ttotal: 36m 41s\tremaining: 13m 57s\n",
      "7245:\tlearn: 3.7914226\ttotal: 36m 42s\tremaining: 13m 56s\n",
      "7246:\tlearn: 3.7911557\ttotal: 36m 42s\tremaining: 13m 56s\n",
      "7247:\tlearn: 3.7908643\ttotal: 36m 42s\tremaining: 13m 56s\n",
      "7248:\tlearn: 3.7906031\ttotal: 36m 42s\tremaining: 13m 55s\n",
      "7249:\tlearn: 3.7902918\ttotal: 36m 43s\tremaining: 13m 55s\n",
      "7250:\tlearn: 3.7901026\ttotal: 36m 43s\tremaining: 13m 55s\n",
      "7251:\tlearn: 3.7898626\ttotal: 36m 43s\tremaining: 13m 55s\n",
      "7252:\tlearn: 3.7894297\ttotal: 36m 43s\tremaining: 13m 54s\n",
      "7253:\tlearn: 3.7892874\ttotal: 36m 44s\tremaining: 13m 54s\n",
      "7254:\tlearn: 3.7890326\ttotal: 36m 44s\tremaining: 13m 54s\n",
      "7255:\tlearn: 3.7887497\ttotal: 36m 44s\tremaining: 13m 53s\n",
      "7256:\tlearn: 3.7884825\ttotal: 36m 44s\tremaining: 13m 53s\n",
      "7257:\tlearn: 3.7882605\ttotal: 36m 45s\tremaining: 13m 53s\n",
      "7258:\tlearn: 3.7879334\ttotal: 36m 45s\tremaining: 13m 52s\n",
      "7259:\tlearn: 3.7877302\ttotal: 36m 45s\tremaining: 13m 52s\n",
      "7260:\tlearn: 3.7875601\ttotal: 36m 46s\tremaining: 13m 52s\n",
      "7261:\tlearn: 3.7873431\ttotal: 36m 46s\tremaining: 13m 51s\n",
      "7262:\tlearn: 3.7870087\ttotal: 36m 46s\tremaining: 13m 51s\n",
      "7263:\tlearn: 3.7867973\ttotal: 36m 46s\tremaining: 13m 51s\n",
      "7264:\tlearn: 3.7865569\ttotal: 36m 47s\tremaining: 13m 50s\n",
      "7265:\tlearn: 3.7862560\ttotal: 36m 47s\tremaining: 13m 50s\n",
      "7266:\tlearn: 3.7858539\ttotal: 36m 47s\tremaining: 13m 50s\n",
      "7267:\tlearn: 3.7855290\ttotal: 36m 48s\tremaining: 13m 50s\n",
      "7268:\tlearn: 3.7853745\ttotal: 36m 48s\tremaining: 13m 49s\n",
      "7269:\tlearn: 3.7849395\ttotal: 36m 48s\tremaining: 13m 49s\n",
      "7270:\tlearn: 3.7847112\ttotal: 36m 48s\tremaining: 13m 49s\n",
      "7271:\tlearn: 3.7845295\ttotal: 36m 49s\tremaining: 13m 48s\n",
      "7272:\tlearn: 3.7840419\ttotal: 36m 49s\tremaining: 13m 48s\n",
      "7273:\tlearn: 3.7837127\ttotal: 36m 49s\tremaining: 13m 48s\n",
      "7274:\tlearn: 3.7833340\ttotal: 36m 49s\tremaining: 13m 47s\n",
      "7275:\tlearn: 3.7829481\ttotal: 36m 50s\tremaining: 13m 47s\n",
      "7276:\tlearn: 3.7827212\ttotal: 36m 50s\tremaining: 13m 47s\n",
      "7277:\tlearn: 3.7825364\ttotal: 36m 50s\tremaining: 13m 46s\n",
      "7278:\tlearn: 3.7821628\ttotal: 36m 51s\tremaining: 13m 46s\n",
      "7279:\tlearn: 3.7817380\ttotal: 36m 51s\tremaining: 13m 46s\n",
      "7280:\tlearn: 3.7815293\ttotal: 36m 51s\tremaining: 13m 45s\n",
      "7281:\tlearn: 3.7811755\ttotal: 36m 52s\tremaining: 13m 45s\n",
      "7282:\tlearn: 3.7806946\ttotal: 36m 52s\tremaining: 13m 45s\n",
      "7283:\tlearn: 3.7804287\ttotal: 36m 52s\tremaining: 13m 45s\n",
      "7284:\tlearn: 3.7802118\ttotal: 36m 52s\tremaining: 13m 44s\n",
      "7285:\tlearn: 3.7799018\ttotal: 36m 53s\tremaining: 13m 44s\n",
      "7286:\tlearn: 3.7796057\ttotal: 36m 53s\tremaining: 13m 44s\n",
      "7287:\tlearn: 3.7794104\ttotal: 36m 53s\tremaining: 13m 43s\n",
      "7288:\tlearn: 3.7792251\ttotal: 36m 54s\tremaining: 13m 43s\n",
      "7289:\tlearn: 3.7788419\ttotal: 36m 54s\tremaining: 13m 43s\n",
      "7290:\tlearn: 3.7786180\ttotal: 36m 54s\tremaining: 13m 42s\n",
      "7291:\tlearn: 3.7783383\ttotal: 36m 55s\tremaining: 13m 42s\n",
      "7292:\tlearn: 3.7781076\ttotal: 36m 55s\tremaining: 13m 42s\n",
      "7293:\tlearn: 3.7777668\ttotal: 36m 55s\tremaining: 13m 42s\n",
      "7294:\tlearn: 3.7773325\ttotal: 36m 56s\tremaining: 13m 41s\n",
      "7295:\tlearn: 3.7771315\ttotal: 36m 56s\tremaining: 13m 41s\n",
      "7296:\tlearn: 3.7767688\ttotal: 36m 56s\tremaining: 13m 41s\n",
      "7297:\tlearn: 3.7764136\ttotal: 36m 56s\tremaining: 13m 40s\n",
      "7298:\tlearn: 3.7761776\ttotal: 36m 57s\tremaining: 13m 40s\n",
      "7299:\tlearn: 3.7757613\ttotal: 36m 57s\tremaining: 13m 40s\n",
      "7300:\tlearn: 3.7754899\ttotal: 36m 57s\tremaining: 13m 39s\n",
      "7301:\tlearn: 3.7752240\ttotal: 36m 58s\tremaining: 13m 39s\n",
      "7302:\tlearn: 3.7748850\ttotal: 36m 58s\tremaining: 13m 39s\n",
      "7303:\tlearn: 3.7746348\ttotal: 36m 58s\tremaining: 13m 38s\n",
      "7304:\tlearn: 3.7743236\ttotal: 36m 59s\tremaining: 13m 38s\n",
      "7305:\tlearn: 3.7738444\ttotal: 36m 59s\tremaining: 13m 38s\n",
      "7306:\tlearn: 3.7735382\ttotal: 36m 59s\tremaining: 13m 38s\n",
      "7307:\tlearn: 3.7731018\ttotal: 36m 59s\tremaining: 13m 37s\n",
      "7308:\tlearn: 3.7727878\ttotal: 37m\tremaining: 13m 37s\n",
      "7309:\tlearn: 3.7724672\ttotal: 37m\tremaining: 13m 37s\n",
      "7310:\tlearn: 3.7719639\ttotal: 37m\tremaining: 13m 36s\n",
      "7311:\tlearn: 3.7716715\ttotal: 37m 1s\tremaining: 13m 36s\n",
      "7312:\tlearn: 3.7714389\ttotal: 37m 1s\tremaining: 13m 36s\n",
      "7313:\tlearn: 3.7711514\ttotal: 37m 1s\tremaining: 13m 35s\n",
      "7314:\tlearn: 3.7706799\ttotal: 37m 1s\tremaining: 13m 35s\n",
      "7315:\tlearn: 3.7703654\ttotal: 37m 2s\tremaining: 13m 35s\n",
      "7316:\tlearn: 3.7699037\ttotal: 37m 2s\tremaining: 13m 35s\n",
      "7317:\tlearn: 3.7693649\ttotal: 37m 3s\tremaining: 13m 34s\n",
      "7318:\tlearn: 3.7691612\ttotal: 37m 3s\tremaining: 13m 34s\n",
      "7319:\tlearn: 3.7689486\ttotal: 37m 3s\tremaining: 13m 34s\n",
      "7320:\tlearn: 3.7685875\ttotal: 37m 4s\tremaining: 13m 33s\n",
      "7321:\tlearn: 3.7682459\ttotal: 37m 4s\tremaining: 13m 33s\n",
      "7322:\tlearn: 3.7680710\ttotal: 37m 4s\tremaining: 13m 33s\n",
      "7323:\tlearn: 3.7677091\ttotal: 37m 4s\tremaining: 13m 32s\n",
      "7324:\tlearn: 3.7674475\ttotal: 37m 5s\tremaining: 13m 32s\n",
      "7325:\tlearn: 3.7670885\ttotal: 37m 5s\tremaining: 13m 32s\n",
      "7326:\tlearn: 3.7666602\ttotal: 37m 5s\tremaining: 13m 32s\n",
      "7327:\tlearn: 3.7663731\ttotal: 37m 6s\tremaining: 13m 31s\n",
      "7328:\tlearn: 3.7660557\ttotal: 37m 6s\tremaining: 13m 31s\n",
      "7329:\tlearn: 3.7659343\ttotal: 37m 6s\tremaining: 13m 31s\n",
      "7330:\tlearn: 3.7655751\ttotal: 37m 7s\tremaining: 13m 30s\n",
      "7331:\tlearn: 3.7652382\ttotal: 37m 7s\tremaining: 13m 30s\n",
      "7332:\tlearn: 3.7648876\ttotal: 37m 7s\tremaining: 13m 30s\n",
      "7333:\tlearn: 3.7644789\ttotal: 37m 7s\tremaining: 13m 29s\n",
      "7334:\tlearn: 3.7641305\ttotal: 37m 8s\tremaining: 13m 29s\n",
      "7335:\tlearn: 3.7639088\ttotal: 37m 8s\tremaining: 13m 29s\n",
      "7336:\tlearn: 3.7636505\ttotal: 37m 8s\tremaining: 13m 28s\n",
      "7337:\tlearn: 3.7633170\ttotal: 37m 9s\tremaining: 13m 28s\n",
      "7338:\tlearn: 3.7630879\ttotal: 37m 9s\tremaining: 13m 28s\n",
      "7339:\tlearn: 3.7626762\ttotal: 37m 9s\tremaining: 13m 28s\n",
      "7340:\tlearn: 3.7623442\ttotal: 37m 10s\tremaining: 13m 27s\n",
      "7341:\tlearn: 3.7619084\ttotal: 37m 10s\tremaining: 13m 27s\n",
      "7342:\tlearn: 3.7615918\ttotal: 37m 10s\tremaining: 13m 27s\n",
      "7343:\tlearn: 3.7614078\ttotal: 37m 10s\tremaining: 13m 26s\n",
      "7344:\tlearn: 3.7611149\ttotal: 37m 11s\tremaining: 13m 26s\n",
      "7345:\tlearn: 3.7607278\ttotal: 37m 11s\tremaining: 13m 26s\n",
      "7346:\tlearn: 3.7603759\ttotal: 37m 11s\tremaining: 13m 25s\n",
      "7347:\tlearn: 3.7600258\ttotal: 37m 12s\tremaining: 13m 25s\n",
      "7348:\tlearn: 3.7597391\ttotal: 37m 12s\tremaining: 13m 25s\n",
      "7349:\tlearn: 3.7595682\ttotal: 37m 12s\tremaining: 13m 24s\n",
      "7350:\tlearn: 3.7591418\ttotal: 37m 12s\tremaining: 13m 24s\n",
      "7351:\tlearn: 3.7588619\ttotal: 37m 13s\tremaining: 13m 24s\n",
      "7352:\tlearn: 3.7586698\ttotal: 37m 13s\tremaining: 13m 24s\n",
      "7353:\tlearn: 3.7583907\ttotal: 37m 13s\tremaining: 13m 23s\n",
      "7354:\tlearn: 3.7580772\ttotal: 37m 14s\tremaining: 13m 23s\n",
      "7355:\tlearn: 3.7577753\ttotal: 37m 14s\tremaining: 13m 23s\n",
      "7356:\tlearn: 3.7575119\ttotal: 37m 14s\tremaining: 13m 22s\n",
      "7357:\tlearn: 3.7573412\ttotal: 37m 14s\tremaining: 13m 22s\n",
      "7358:\tlearn: 3.7571636\ttotal: 37m 15s\tremaining: 13m 22s\n",
      "7359:\tlearn: 3.7567999\ttotal: 37m 15s\tremaining: 13m 21s\n",
      "7360:\tlearn: 3.7565584\ttotal: 37m 15s\tremaining: 13m 21s\n",
      "7361:\tlearn: 3.7561931\ttotal: 37m 15s\tremaining: 13m 21s\n",
      "7362:\tlearn: 3.7558868\ttotal: 37m 16s\tremaining: 13m 20s\n",
      "7363:\tlearn: 3.7555219\ttotal: 37m 16s\tremaining: 13m 20s\n",
      "7364:\tlearn: 3.7552156\ttotal: 37m 16s\tremaining: 13m 20s\n",
      "7365:\tlearn: 3.7547759\ttotal: 37m 17s\tremaining: 13m 19s\n",
      "7366:\tlearn: 3.7543949\ttotal: 37m 17s\tremaining: 13m 19s\n",
      "7367:\tlearn: 3.7540476\ttotal: 37m 17s\tremaining: 13m 19s\n",
      "7368:\tlearn: 3.7536785\ttotal: 37m 17s\tremaining: 13m 19s\n",
      "7369:\tlearn: 3.7534260\ttotal: 37m 18s\tremaining: 13m 18s\n",
      "7370:\tlearn: 3.7531409\ttotal: 37m 18s\tremaining: 13m 18s\n",
      "7371:\tlearn: 3.7529859\ttotal: 37m 18s\tremaining: 13m 18s\n",
      "7372:\tlearn: 3.7525708\ttotal: 37m 19s\tremaining: 13m 17s\n",
      "7373:\tlearn: 3.7521571\ttotal: 37m 19s\tremaining: 13m 17s\n",
      "7374:\tlearn: 3.7518183\ttotal: 37m 19s\tremaining: 13m 17s\n",
      "7375:\tlearn: 3.7516620\ttotal: 37m 19s\tremaining: 13m 16s\n",
      "7376:\tlearn: 3.7512912\ttotal: 37m 20s\tremaining: 13m 16s\n",
      "7377:\tlearn: 3.7509618\ttotal: 37m 20s\tremaining: 13m 16s\n",
      "7378:\tlearn: 3.7507326\ttotal: 37m 20s\tremaining: 13m 15s\n",
      "7379:\tlearn: 3.7502498\ttotal: 37m 21s\tremaining: 13m 15s\n",
      "7380:\tlearn: 3.7500359\ttotal: 37m 21s\tremaining: 13m 15s\n",
      "7381:\tlearn: 3.7496765\ttotal: 37m 21s\tremaining: 13m 15s\n",
      "7382:\tlearn: 3.7493714\ttotal: 37m 21s\tremaining: 13m 14s\n",
      "7383:\tlearn: 3.7491391\ttotal: 37m 22s\tremaining: 13m 14s\n",
      "7384:\tlearn: 3.7488388\ttotal: 37m 22s\tremaining: 13m 14s\n",
      "7385:\tlearn: 3.7485585\ttotal: 37m 22s\tremaining: 13m 13s\n",
      "7386:\tlearn: 3.7481901\ttotal: 37m 23s\tremaining: 13m 13s\n",
      "7387:\tlearn: 3.7478351\ttotal: 37m 23s\tremaining: 13m 13s\n",
      "7388:\tlearn: 3.7475291\ttotal: 37m 23s\tremaining: 13m 12s\n",
      "7389:\tlearn: 3.7472425\ttotal: 37m 23s\tremaining: 13m 12s\n",
      "7390:\tlearn: 3.7471015\ttotal: 37m 24s\tremaining: 13m 12s\n",
      "7391:\tlearn: 3.7468079\ttotal: 37m 24s\tremaining: 13m 11s\n",
      "7392:\tlearn: 3.7465238\ttotal: 37m 24s\tremaining: 13m 11s\n",
      "7393:\tlearn: 3.7461648\ttotal: 37m 25s\tremaining: 13m 11s\n",
      "7394:\tlearn: 3.7458986\ttotal: 37m 25s\tremaining: 13m 10s\n",
      "7395:\tlearn: 3.7456785\ttotal: 37m 25s\tremaining: 13m 10s\n",
      "7396:\tlearn: 3.7452577\ttotal: 37m 25s\tremaining: 13m 10s\n",
      "7397:\tlearn: 3.7448816\ttotal: 37m 26s\tremaining: 13m 10s\n",
      "7398:\tlearn: 3.7445747\ttotal: 37m 26s\tremaining: 13m 9s\n",
      "7399:\tlearn: 3.7442211\ttotal: 37m 26s\tremaining: 13m 9s\n",
      "7400:\tlearn: 3.7439056\ttotal: 37m 27s\tremaining: 13m 9s\n",
      "7401:\tlearn: 3.7435758\ttotal: 37m 27s\tremaining: 13m 8s\n",
      "7402:\tlearn: 3.7431694\ttotal: 37m 27s\tremaining: 13m 8s\n",
      "7403:\tlearn: 3.7429866\ttotal: 37m 28s\tremaining: 13m 8s\n",
      "7404:\tlearn: 3.7427678\ttotal: 37m 28s\tremaining: 13m 7s\n",
      "7405:\tlearn: 3.7424854\ttotal: 37m 28s\tremaining: 13m 7s\n",
      "7406:\tlearn: 3.7421824\ttotal: 37m 29s\tremaining: 13m 7s\n",
      "7407:\tlearn: 3.7418975\ttotal: 37m 29s\tremaining: 13m 7s\n",
      "7408:\tlearn: 3.7417197\ttotal: 37m 29s\tremaining: 13m 6s\n",
      "7409:\tlearn: 3.7414990\ttotal: 37m 29s\tremaining: 13m 6s\n",
      "7410:\tlearn: 3.7411733\ttotal: 37m 29s\tremaining: 13m 6s\n",
      "7411:\tlearn: 3.7409251\ttotal: 37m 30s\tremaining: 13m 5s\n",
      "7412:\tlearn: 3.7405983\ttotal: 37m 30s\tremaining: 13m 5s\n",
      "7413:\tlearn: 3.7401144\ttotal: 37m 30s\tremaining: 13m 5s\n",
      "7414:\tlearn: 3.7397202\ttotal: 37m 31s\tremaining: 13m 4s\n",
      "7415:\tlearn: 3.7392152\ttotal: 37m 31s\tremaining: 13m 4s\n",
      "7416:\tlearn: 3.7387266\ttotal: 37m 31s\tremaining: 13m 4s\n",
      "7417:\tlearn: 3.7384129\ttotal: 37m 32s\tremaining: 13m 3s\n",
      "7418:\tlearn: 3.7380943\ttotal: 37m 32s\tremaining: 13m 3s\n",
      "7419:\tlearn: 3.7378892\ttotal: 37m 32s\tremaining: 13m 3s\n",
      "7420:\tlearn: 3.7375131\ttotal: 37m 32s\tremaining: 13m 2s\n",
      "7421:\tlearn: 3.7370391\ttotal: 37m 33s\tremaining: 13m 2s\n",
      "7422:\tlearn: 3.7365735\ttotal: 37m 33s\tremaining: 13m 2s\n",
      "7423:\tlearn: 3.7363422\ttotal: 37m 33s\tremaining: 13m 2s\n",
      "7424:\tlearn: 3.7360158\ttotal: 37m 34s\tremaining: 13m 1s\n",
      "7425:\tlearn: 3.7356223\ttotal: 37m 34s\tremaining: 13m 1s\n",
      "7426:\tlearn: 3.7353539\ttotal: 37m 34s\tremaining: 13m 1s\n",
      "7427:\tlearn: 3.7351121\ttotal: 37m 35s\tremaining: 13m\n",
      "7428:\tlearn: 3.7347595\ttotal: 37m 35s\tremaining: 13m\n",
      "7429:\tlearn: 3.7344174\ttotal: 37m 35s\tremaining: 13m\n",
      "7430:\tlearn: 3.7341596\ttotal: 37m 35s\tremaining: 12m 59s\n",
      "7431:\tlearn: 3.7338289\ttotal: 37m 36s\tremaining: 12m 59s\n",
      "7432:\tlearn: 3.7335652\ttotal: 37m 36s\tremaining: 12m 59s\n",
      "7433:\tlearn: 3.7332958\ttotal: 37m 36s\tremaining: 12m 58s\n",
      "7434:\tlearn: 3.7328210\ttotal: 37m 37s\tremaining: 12m 58s\n",
      "7435:\tlearn: 3.7325446\ttotal: 37m 37s\tremaining: 12m 58s\n",
      "7436:\tlearn: 3.7321861\ttotal: 37m 37s\tremaining: 12m 58s\n",
      "7437:\tlearn: 3.7319140\ttotal: 37m 38s\tremaining: 12m 57s\n",
      "7438:\tlearn: 3.7316177\ttotal: 37m 38s\tremaining: 12m 57s\n",
      "7439:\tlearn: 3.7312827\ttotal: 37m 38s\tremaining: 12m 57s\n",
      "7440:\tlearn: 3.7309027\ttotal: 37m 39s\tremaining: 12m 56s\n",
      "7441:\tlearn: 3.7305804\ttotal: 37m 39s\tremaining: 12m 56s\n",
      "7442:\tlearn: 3.7301798\ttotal: 37m 39s\tremaining: 12m 56s\n",
      "7443:\tlearn: 3.7299839\ttotal: 37m 39s\tremaining: 12m 55s\n",
      "7444:\tlearn: 3.7296324\ttotal: 37m 40s\tremaining: 12m 55s\n",
      "7445:\tlearn: 3.7292128\ttotal: 37m 40s\tremaining: 12m 55s\n",
      "7446:\tlearn: 3.7290516\ttotal: 37m 40s\tremaining: 12m 55s\n",
      "7447:\tlearn: 3.7288062\ttotal: 37m 41s\tremaining: 12m 54s\n",
      "7448:\tlearn: 3.7285316\ttotal: 37m 41s\tremaining: 12m 54s\n",
      "7449:\tlearn: 3.7282062\ttotal: 37m 41s\tremaining: 12m 54s\n",
      "7450:\tlearn: 3.7280039\ttotal: 37m 41s\tremaining: 12m 53s\n",
      "7451:\tlearn: 3.7277265\ttotal: 37m 42s\tremaining: 12m 53s\n",
      "7452:\tlearn: 3.7274633\ttotal: 37m 42s\tremaining: 12m 53s\n",
      "7453:\tlearn: 3.7271458\ttotal: 37m 42s\tremaining: 12m 52s\n",
      "7454:\tlearn: 3.7268072\ttotal: 37m 43s\tremaining: 12m 52s\n",
      "7455:\tlearn: 3.7264203\ttotal: 37m 43s\tremaining: 12m 52s\n",
      "7456:\tlearn: 3.7260683\ttotal: 37m 43s\tremaining: 12m 52s\n",
      "7457:\tlearn: 3.7256991\ttotal: 37m 44s\tremaining: 12m 51s\n",
      "7458:\tlearn: 3.7254086\ttotal: 37m 44s\tremaining: 12m 51s\n",
      "7459:\tlearn: 3.7251636\ttotal: 37m 44s\tremaining: 12m 51s\n",
      "7460:\tlearn: 3.7249226\ttotal: 37m 45s\tremaining: 12m 50s\n",
      "7461:\tlearn: 3.7245607\ttotal: 37m 45s\tremaining: 12m 50s\n",
      "7462:\tlearn: 3.7242149\ttotal: 37m 45s\tremaining: 12m 50s\n",
      "7463:\tlearn: 3.7239520\ttotal: 37m 45s\tremaining: 12m 49s\n",
      "7464:\tlearn: 3.7235823\ttotal: 37m 46s\tremaining: 12m 49s\n",
      "7465:\tlearn: 3.7233052\ttotal: 37m 46s\tremaining: 12m 49s\n",
      "7466:\tlearn: 3.7230869\ttotal: 37m 46s\tremaining: 12m 48s\n",
      "7467:\tlearn: 3.7228708\ttotal: 37m 47s\tremaining: 12m 48s\n",
      "7468:\tlearn: 3.7226724\ttotal: 37m 47s\tremaining: 12m 48s\n",
      "7469:\tlearn: 3.7224330\ttotal: 37m 47s\tremaining: 12m 48s\n",
      "7470:\tlearn: 3.7223032\ttotal: 37m 47s\tremaining: 12m 47s\n",
      "7471:\tlearn: 3.7221469\ttotal: 37m 48s\tremaining: 12m 47s\n",
      "7472:\tlearn: 3.7217161\ttotal: 37m 48s\tremaining: 12m 47s\n",
      "7473:\tlearn: 3.7212901\ttotal: 37m 48s\tremaining: 12m 46s\n",
      "7474:\tlearn: 3.7210702\ttotal: 37m 49s\tremaining: 12m 46s\n",
      "7475:\tlearn: 3.7209070\ttotal: 37m 49s\tremaining: 12m 46s\n",
      "7476:\tlearn: 3.7205599\ttotal: 37m 49s\tremaining: 12m 45s\n",
      "7477:\tlearn: 3.7203625\ttotal: 37m 49s\tremaining: 12m 45s\n",
      "7478:\tlearn: 3.7200916\ttotal: 37m 50s\tremaining: 12m 45s\n",
      "7479:\tlearn: 3.7198341\ttotal: 37m 50s\tremaining: 12m 44s\n",
      "7480:\tlearn: 3.7194338\ttotal: 37m 50s\tremaining: 12m 44s\n",
      "7481:\tlearn: 3.7191642\ttotal: 37m 50s\tremaining: 12m 44s\n",
      "7482:\tlearn: 3.7187303\ttotal: 37m 51s\tremaining: 12m 43s\n",
      "7483:\tlearn: 3.7185128\ttotal: 37m 51s\tremaining: 12m 43s\n",
      "7484:\tlearn: 3.7182692\ttotal: 37m 51s\tremaining: 12m 43s\n",
      "7485:\tlearn: 3.7180828\ttotal: 37m 52s\tremaining: 12m 43s\n",
      "7486:\tlearn: 3.7177447\ttotal: 37m 52s\tremaining: 12m 42s\n",
      "7487:\tlearn: 3.7174726\ttotal: 37m 52s\tremaining: 12m 42s\n",
      "7488:\tlearn: 3.7170937\ttotal: 37m 52s\tremaining: 12m 42s\n",
      "7489:\tlearn: 3.7168194\ttotal: 37m 53s\tremaining: 12m 41s\n",
      "7490:\tlearn: 3.7165240\ttotal: 37m 53s\tremaining: 12m 41s\n",
      "7491:\tlearn: 3.7162141\ttotal: 37m 53s\tremaining: 12m 41s\n",
      "7492:\tlearn: 3.7158645\ttotal: 37m 54s\tremaining: 12m 40s\n",
      "7493:\tlearn: 3.7157151\ttotal: 37m 54s\tremaining: 12m 40s\n",
      "7494:\tlearn: 3.7153395\ttotal: 37m 54s\tremaining: 12m 40s\n",
      "7495:\tlearn: 3.7151753\ttotal: 37m 55s\tremaining: 12m 39s\n",
      "7496:\tlearn: 3.7149030\ttotal: 37m 55s\tremaining: 12m 39s\n",
      "7497:\tlearn: 3.7146244\ttotal: 37m 55s\tremaining: 12m 39s\n",
      "7498:\tlearn: 3.7143517\ttotal: 37m 55s\tremaining: 12m 39s\n",
      "7499:\tlearn: 3.7140693\ttotal: 37m 56s\tremaining: 12m 38s\n",
      "7500:\tlearn: 3.7135725\ttotal: 37m 56s\tremaining: 12m 38s\n",
      "7501:\tlearn: 3.7132672\ttotal: 37m 56s\tremaining: 12m 38s\n",
      "7502:\tlearn: 3.7130030\ttotal: 37m 57s\tremaining: 12m 37s\n",
      "7503:\tlearn: 3.7125174\ttotal: 37m 57s\tremaining: 12m 37s\n",
      "7504:\tlearn: 3.7123729\ttotal: 37m 57s\tremaining: 12m 37s\n",
      "7505:\tlearn: 3.7120794\ttotal: 37m 57s\tremaining: 12m 36s\n",
      "7506:\tlearn: 3.7117326\ttotal: 37m 58s\tremaining: 12m 36s\n",
      "7507:\tlearn: 3.7113444\ttotal: 37m 58s\tremaining: 12m 36s\n",
      "7508:\tlearn: 3.7111191\ttotal: 37m 58s\tremaining: 12m 35s\n",
      "7509:\tlearn: 3.7107538\ttotal: 37m 59s\tremaining: 12m 35s\n",
      "7510:\tlearn: 3.7105031\ttotal: 37m 59s\tremaining: 12m 35s\n",
      "7511:\tlearn: 3.7102041\ttotal: 37m 59s\tremaining: 12m 35s\n",
      "7512:\tlearn: 3.7099618\ttotal: 37m 59s\tremaining: 12m 34s\n",
      "7513:\tlearn: 3.7097325\ttotal: 38m\tremaining: 12m 34s\n",
      "7514:\tlearn: 3.7093411\ttotal: 38m\tremaining: 12m 34s\n",
      "7515:\tlearn: 3.7088699\ttotal: 38m\tremaining: 12m 33s\n",
      "7516:\tlearn: 3.7085123\ttotal: 38m 1s\tremaining: 12m 33s\n",
      "7517:\tlearn: 3.7082457\ttotal: 38m 1s\tremaining: 12m 33s\n",
      "7518:\tlearn: 3.7080010\ttotal: 38m 1s\tremaining: 12m 32s\n",
      "7519:\tlearn: 3.7075553\ttotal: 38m 2s\tremaining: 12m 32s\n",
      "7520:\tlearn: 3.7073666\ttotal: 38m 2s\tremaining: 12m 32s\n",
      "7521:\tlearn: 3.7072439\ttotal: 38m 2s\tremaining: 12m 31s\n",
      "7522:\tlearn: 3.7070118\ttotal: 38m 2s\tremaining: 12m 31s\n",
      "7523:\tlearn: 3.7066416\ttotal: 38m 3s\tremaining: 12m 31s\n",
      "7524:\tlearn: 3.7064773\ttotal: 38m 3s\tremaining: 12m 31s\n",
      "7525:\tlearn: 3.7062529\ttotal: 38m 3s\tremaining: 12m 30s\n",
      "7526:\tlearn: 3.7059863\ttotal: 38m 3s\tremaining: 12m 30s\n",
      "7527:\tlearn: 3.7056634\ttotal: 38m 4s\tremaining: 12m 30s\n",
      "7528:\tlearn: 3.7053226\ttotal: 38m 4s\tremaining: 12m 29s\n",
      "7529:\tlearn: 3.7049051\ttotal: 38m 4s\tremaining: 12m 29s\n",
      "7530:\tlearn: 3.7045493\ttotal: 38m 5s\tremaining: 12m 29s\n",
      "7531:\tlearn: 3.7044151\ttotal: 38m 5s\tremaining: 12m 28s\n",
      "7532:\tlearn: 3.7041771\ttotal: 38m 5s\tremaining: 12m 28s\n",
      "7533:\tlearn: 3.7039409\ttotal: 38m 5s\tremaining: 12m 28s\n",
      "7534:\tlearn: 3.7036374\ttotal: 38m 6s\tremaining: 12m 27s\n",
      "7535:\tlearn: 3.7034413\ttotal: 38m 6s\tremaining: 12m 27s\n",
      "7536:\tlearn: 3.7031772\ttotal: 38m 6s\tremaining: 12m 27s\n",
      "7537:\tlearn: 3.7029028\ttotal: 38m 7s\tremaining: 12m 26s\n",
      "7538:\tlearn: 3.7027087\ttotal: 38m 7s\tremaining: 12m 26s\n",
      "7539:\tlearn: 3.7025657\ttotal: 38m 7s\tremaining: 12m 26s\n",
      "7540:\tlearn: 3.7022369\ttotal: 38m 7s\tremaining: 12m 26s\n",
      "7541:\tlearn: 3.7020138\ttotal: 38m 7s\tremaining: 12m 25s\n",
      "7542:\tlearn: 3.7016516\ttotal: 38m 8s\tremaining: 12m 25s\n",
      "7543:\tlearn: 3.7013842\ttotal: 38m 8s\tremaining: 12m 25s\n",
      "7544:\tlearn: 3.7012449\ttotal: 38m 8s\tremaining: 12m 24s\n",
      "7545:\tlearn: 3.7010071\ttotal: 38m 9s\tremaining: 12m 24s\n",
      "7546:\tlearn: 3.7007898\ttotal: 38m 9s\tremaining: 12m 24s\n",
      "7547:\tlearn: 3.7004756\ttotal: 38m 9s\tremaining: 12m 23s\n",
      "7548:\tlearn: 3.7002117\ttotal: 38m 10s\tremaining: 12m 23s\n",
      "7549:\tlearn: 3.6999012\ttotal: 38m 10s\tremaining: 12m 23s\n",
      "7550:\tlearn: 3.6997065\ttotal: 38m 10s\tremaining: 12m 22s\n",
      "7551:\tlearn: 3.6994967\ttotal: 38m 10s\tremaining: 12m 22s\n",
      "7552:\tlearn: 3.6990756\ttotal: 38m 11s\tremaining: 12m 22s\n",
      "7553:\tlearn: 3.6987876\ttotal: 38m 11s\tremaining: 12m 21s\n",
      "7554:\tlearn: 3.6985654\ttotal: 38m 11s\tremaining: 12m 21s\n",
      "7555:\tlearn: 3.6983319\ttotal: 38m 12s\tremaining: 12m 21s\n",
      "7556:\tlearn: 3.6980690\ttotal: 38m 12s\tremaining: 12m 21s\n",
      "7557:\tlearn: 3.6978075\ttotal: 38m 12s\tremaining: 12m 20s\n",
      "7558:\tlearn: 3.6976130\ttotal: 38m 12s\tremaining: 12m 20s\n",
      "7559:\tlearn: 3.6973345\ttotal: 38m 13s\tremaining: 12m 20s\n",
      "7560:\tlearn: 3.6970745\ttotal: 38m 13s\tremaining: 12m 19s\n",
      "7561:\tlearn: 3.6968174\ttotal: 38m 13s\tremaining: 12m 19s\n",
      "7562:\tlearn: 3.6966656\ttotal: 38m 13s\tremaining: 12m 19s\n",
      "7563:\tlearn: 3.6961388\ttotal: 38m 14s\tremaining: 12m 18s\n",
      "7564:\tlearn: 3.6958642\ttotal: 38m 14s\tremaining: 12m 18s\n",
      "7565:\tlearn: 3.6956115\ttotal: 38m 14s\tremaining: 12m 18s\n",
      "7566:\tlearn: 3.6952853\ttotal: 38m 14s\tremaining: 12m 17s\n",
      "7567:\tlearn: 3.6949519\ttotal: 38m 15s\tremaining: 12m 17s\n",
      "7568:\tlearn: 3.6947982\ttotal: 38m 15s\tremaining: 12m 17s\n",
      "7569:\tlearn: 3.6945580\ttotal: 38m 15s\tremaining: 12m 16s\n",
      "7570:\tlearn: 3.6942215\ttotal: 38m 16s\tremaining: 12m 16s\n",
      "7571:\tlearn: 3.6938767\ttotal: 38m 16s\tremaining: 12m 16s\n",
      "7572:\tlearn: 3.6935001\ttotal: 38m 16s\tremaining: 12m 16s\n",
      "7573:\tlearn: 3.6931149\ttotal: 38m 17s\tremaining: 12m 15s\n",
      "7574:\tlearn: 3.6928648\ttotal: 38m 17s\tremaining: 12m 15s\n",
      "7575:\tlearn: 3.6926321\ttotal: 38m 17s\tremaining: 12m 15s\n",
      "7576:\tlearn: 3.6921040\ttotal: 38m 17s\tremaining: 12m 14s\n",
      "7577:\tlearn: 3.6917143\ttotal: 38m 18s\tremaining: 12m 14s\n",
      "7578:\tlearn: 3.6915416\ttotal: 38m 18s\tremaining: 12m 14s\n",
      "7579:\tlearn: 3.6911536\ttotal: 38m 18s\tremaining: 12m 13s\n",
      "7580:\tlearn: 3.6908916\ttotal: 38m 18s\tremaining: 12m 13s\n",
      "7581:\tlearn: 3.6905038\ttotal: 38m 19s\tremaining: 12m 13s\n",
      "7582:\tlearn: 3.6902746\ttotal: 38m 19s\tremaining: 12m 12s\n",
      "7583:\tlearn: 3.6900089\ttotal: 38m 19s\tremaining: 12m 12s\n",
      "7584:\tlearn: 3.6894836\ttotal: 38m 20s\tremaining: 12m 12s\n",
      "7585:\tlearn: 3.6891531\ttotal: 38m 20s\tremaining: 12m 12s\n",
      "7586:\tlearn: 3.6887323\ttotal: 38m 20s\tremaining: 12m 11s\n",
      "7587:\tlearn: 3.6884008\ttotal: 38m 21s\tremaining: 12m 11s\n",
      "7588:\tlearn: 3.6882656\ttotal: 38m 21s\tremaining: 12m 11s\n",
      "7589:\tlearn: 3.6880684\ttotal: 38m 21s\tremaining: 12m 10s\n",
      "7590:\tlearn: 3.6879170\ttotal: 38m 21s\tremaining: 12m 10s\n",
      "7591:\tlearn: 3.6876174\ttotal: 38m 22s\tremaining: 12m 10s\n",
      "7592:\tlearn: 3.6873383\ttotal: 38m 22s\tremaining: 12m 9s\n",
      "7593:\tlearn: 3.6870927\ttotal: 38m 22s\tremaining: 12m 9s\n",
      "7594:\tlearn: 3.6867923\ttotal: 38m 22s\tremaining: 12m 9s\n",
      "7595:\tlearn: 3.6865432\ttotal: 38m 22s\tremaining: 12m 8s\n",
      "7596:\tlearn: 3.6861864\ttotal: 38m 23s\tremaining: 12m 8s\n",
      "7597:\tlearn: 3.6858846\ttotal: 38m 23s\tremaining: 12m 8s\n",
      "7598:\tlearn: 3.6855958\ttotal: 38m 23s\tremaining: 12m 7s\n",
      "7599:\tlearn: 3.6851615\ttotal: 38m 24s\tremaining: 12m 7s\n",
      "7600:\tlearn: 3.6847635\ttotal: 38m 24s\tremaining: 12m 7s\n",
      "7601:\tlearn: 3.6843417\ttotal: 38m 24s\tremaining: 12m 7s\n",
      "7602:\tlearn: 3.6841594\ttotal: 38m 24s\tremaining: 12m 6s\n",
      "7603:\tlearn: 3.6838830\ttotal: 38m 25s\tremaining: 12m 6s\n",
      "7604:\tlearn: 3.6837441\ttotal: 38m 25s\tremaining: 12m 6s\n",
      "7605:\tlearn: 3.6834393\ttotal: 38m 25s\tremaining: 12m 5s\n",
      "7606:\tlearn: 3.6832061\ttotal: 38m 26s\tremaining: 12m 5s\n",
      "7607:\tlearn: 3.6827165\ttotal: 38m 26s\tremaining: 12m 5s\n",
      "7608:\tlearn: 3.6824745\ttotal: 38m 26s\tremaining: 12m 4s\n",
      "7609:\tlearn: 3.6820380\ttotal: 38m 26s\tremaining: 12m 4s\n",
      "7610:\tlearn: 3.6816706\ttotal: 38m 27s\tremaining: 12m 4s\n",
      "7611:\tlearn: 3.6812632\ttotal: 38m 27s\tremaining: 12m 3s\n",
      "7612:\tlearn: 3.6810659\ttotal: 38m 27s\tremaining: 12m 3s\n",
      "7613:\tlearn: 3.6808213\ttotal: 38m 27s\tremaining: 12m 3s\n",
      "7614:\tlearn: 3.6806951\ttotal: 38m 28s\tremaining: 12m 2s\n",
      "7615:\tlearn: 3.6803006\ttotal: 38m 28s\tremaining: 12m 2s\n",
      "7616:\tlearn: 3.6799119\ttotal: 38m 28s\tremaining: 12m 2s\n",
      "7617:\tlearn: 3.6795642\ttotal: 38m 29s\tremaining: 12m 1s\n",
      "7618:\tlearn: 3.6792209\ttotal: 38m 29s\tremaining: 12m 1s\n",
      "7619:\tlearn: 3.6789641\ttotal: 38m 29s\tremaining: 12m 1s\n",
      "7620:\tlearn: 3.6786473\ttotal: 38m 29s\tremaining: 12m 1s\n",
      "7621:\tlearn: 3.6783890\ttotal: 38m 30s\tremaining: 12m\n",
      "7622:\tlearn: 3.6781975\ttotal: 38m 30s\tremaining: 12m\n",
      "7623:\tlearn: 3.6779997\ttotal: 38m 30s\tremaining: 12m\n",
      "7624:\tlearn: 3.6778135\ttotal: 38m 30s\tremaining: 11m 59s\n",
      "7625:\tlearn: 3.6775767\ttotal: 38m 31s\tremaining: 11m 59s\n",
      "7626:\tlearn: 3.6773192\ttotal: 38m 31s\tremaining: 11m 59s\n",
      "7627:\tlearn: 3.6771281\ttotal: 38m 31s\tremaining: 11m 58s\n",
      "7628:\tlearn: 3.6769016\ttotal: 38m 31s\tremaining: 11m 58s\n",
      "7629:\tlearn: 3.6764356\ttotal: 38m 32s\tremaining: 11m 58s\n",
      "7630:\tlearn: 3.6761441\ttotal: 38m 32s\tremaining: 11m 57s\n",
      "7631:\tlearn: 3.6758318\ttotal: 38m 32s\tremaining: 11m 57s\n",
      "7632:\tlearn: 3.6754869\ttotal: 38m 33s\tremaining: 11m 57s\n",
      "7633:\tlearn: 3.6751615\ttotal: 38m 33s\tremaining: 11m 56s\n",
      "7634:\tlearn: 3.6749959\ttotal: 38m 33s\tremaining: 11m 56s\n",
      "7635:\tlearn: 3.6746650\ttotal: 38m 33s\tremaining: 11m 56s\n",
      "7636:\tlearn: 3.6745424\ttotal: 38m 34s\tremaining: 11m 56s\n",
      "7637:\tlearn: 3.6741096\ttotal: 38m 34s\tremaining: 11m 55s\n",
      "7638:\tlearn: 3.6736486\ttotal: 38m 34s\tremaining: 11m 55s\n",
      "7639:\tlearn: 3.6733750\ttotal: 38m 35s\tremaining: 11m 55s\n",
      "7640:\tlearn: 3.6730549\ttotal: 38m 35s\tremaining: 11m 54s\n",
      "7641:\tlearn: 3.6727561\ttotal: 38m 35s\tremaining: 11m 54s\n",
      "7642:\tlearn: 3.6725595\ttotal: 38m 35s\tremaining: 11m 54s\n",
      "7643:\tlearn: 3.6723170\ttotal: 38m 36s\tremaining: 11m 53s\n",
      "7644:\tlearn: 3.6721868\ttotal: 38m 36s\tremaining: 11m 53s\n",
      "7645:\tlearn: 3.6715747\ttotal: 38m 36s\tremaining: 11m 53s\n",
      "7646:\tlearn: 3.6712533\ttotal: 38m 36s\tremaining: 11m 52s\n",
      "7647:\tlearn: 3.6710206\ttotal: 38m 37s\tremaining: 11m 52s\n",
      "7648:\tlearn: 3.6706493\ttotal: 38m 37s\tremaining: 11m 52s\n",
      "7649:\tlearn: 3.6704183\ttotal: 38m 37s\tremaining: 11m 51s\n",
      "7650:\tlearn: 3.6702013\ttotal: 38m 38s\tremaining: 11m 51s\n",
      "7651:\tlearn: 3.6699946\ttotal: 38m 38s\tremaining: 11m 51s\n",
      "7652:\tlearn: 3.6696920\ttotal: 38m 38s\tremaining: 11m 51s\n",
      "7653:\tlearn: 3.6694620\ttotal: 38m 38s\tremaining: 11m 50s\n",
      "7654:\tlearn: 3.6691796\ttotal: 38m 39s\tremaining: 11m 50s\n",
      "7655:\tlearn: 3.6688730\ttotal: 38m 39s\tremaining: 11m 50s\n",
      "7656:\tlearn: 3.6685399\ttotal: 38m 39s\tremaining: 11m 49s\n",
      "7657:\tlearn: 3.6683553\ttotal: 38m 39s\tremaining: 11m 49s\n",
      "7658:\tlearn: 3.6680725\ttotal: 38m 40s\tremaining: 11m 49s\n",
      "7659:\tlearn: 3.6678417\ttotal: 38m 40s\tremaining: 11m 48s\n",
      "7660:\tlearn: 3.6676195\ttotal: 38m 40s\tremaining: 11m 48s\n",
      "7661:\tlearn: 3.6673723\ttotal: 38m 40s\tremaining: 11m 48s\n",
      "7662:\tlearn: 3.6668545\ttotal: 38m 41s\tremaining: 11m 47s\n",
      "7663:\tlearn: 3.6666192\ttotal: 38m 41s\tremaining: 11m 47s\n",
      "7664:\tlearn: 3.6664087\ttotal: 38m 41s\tremaining: 11m 47s\n",
      "7665:\tlearn: 3.6662553\ttotal: 38m 41s\tremaining: 11m 46s\n",
      "7666:\tlearn: 3.6659316\ttotal: 38m 42s\tremaining: 11m 46s\n",
      "7667:\tlearn: 3.6657949\ttotal: 38m 42s\tremaining: 11m 46s\n",
      "7668:\tlearn: 3.6652879\ttotal: 38m 42s\tremaining: 11m 46s\n",
      "7669:\tlearn: 3.6649192\ttotal: 38m 43s\tremaining: 11m 45s\n",
      "7670:\tlearn: 3.6646173\ttotal: 38m 43s\tremaining: 11m 45s\n",
      "7671:\tlearn: 3.6643201\ttotal: 38m 43s\tremaining: 11m 45s\n",
      "7672:\tlearn: 3.6640709\ttotal: 38m 43s\tremaining: 11m 44s\n",
      "7673:\tlearn: 3.6638556\ttotal: 38m 44s\tremaining: 11m 44s\n",
      "7674:\tlearn: 3.6635453\ttotal: 38m 44s\tremaining: 11m 44s\n",
      "7675:\tlearn: 3.6633056\ttotal: 38m 44s\tremaining: 11m 43s\n",
      "7676:\tlearn: 3.6629013\ttotal: 38m 44s\tremaining: 11m 43s\n",
      "7677:\tlearn: 3.6626362\ttotal: 38m 45s\tremaining: 11m 43s\n",
      "7678:\tlearn: 3.6623348\ttotal: 38m 45s\tremaining: 11m 42s\n",
      "7679:\tlearn: 3.6619926\ttotal: 38m 45s\tremaining: 11m 42s\n",
      "7680:\tlearn: 3.6617859\ttotal: 38m 45s\tremaining: 11m 42s\n",
      "7681:\tlearn: 3.6615487\ttotal: 38m 46s\tremaining: 11m 41s\n",
      "7682:\tlearn: 3.6612579\ttotal: 38m 46s\tremaining: 11m 41s\n",
      "7683:\tlearn: 3.6608795\ttotal: 38m 47s\tremaining: 11m 41s\n",
      "7684:\tlearn: 3.6606055\ttotal: 38m 47s\tremaining: 11m 41s\n",
      "7685:\tlearn: 3.6601716\ttotal: 38m 47s\tremaining: 11m 40s\n",
      "7686:\tlearn: 3.6599249\ttotal: 38m 48s\tremaining: 11m 40s\n",
      "7687:\tlearn: 3.6594244\ttotal: 38m 48s\tremaining: 11m 40s\n",
      "7688:\tlearn: 3.6592178\ttotal: 38m 48s\tremaining: 11m 39s\n",
      "7689:\tlearn: 3.6590292\ttotal: 38m 48s\tremaining: 11m 39s\n",
      "7690:\tlearn: 3.6586475\ttotal: 38m 49s\tremaining: 11m 39s\n",
      "7691:\tlearn: 3.6584417\ttotal: 38m 49s\tremaining: 11m 38s\n",
      "7692:\tlearn: 3.6579586\ttotal: 38m 49s\tremaining: 11m 38s\n",
      "7693:\tlearn: 3.6577076\ttotal: 38m 50s\tremaining: 11m 38s\n",
      "7694:\tlearn: 3.6574467\ttotal: 38m 50s\tremaining: 11m 38s\n",
      "7695:\tlearn: 3.6573391\ttotal: 38m 50s\tremaining: 11m 37s\n",
      "7696:\tlearn: 3.6571172\ttotal: 38m 50s\tremaining: 11m 37s\n",
      "7697:\tlearn: 3.6566611\ttotal: 38m 51s\tremaining: 11m 37s\n",
      "7698:\tlearn: 3.6563429\ttotal: 38m 51s\tremaining: 11m 36s\n",
      "7699:\tlearn: 3.6560130\ttotal: 38m 51s\tremaining: 11m 36s\n",
      "7700:\tlearn: 3.6557326\ttotal: 38m 52s\tremaining: 11m 36s\n",
      "7701:\tlearn: 3.6554103\ttotal: 38m 52s\tremaining: 11m 35s\n",
      "7702:\tlearn: 3.6550131\ttotal: 38m 52s\tremaining: 11m 35s\n",
      "7703:\tlearn: 3.6546955\ttotal: 38m 53s\tremaining: 11m 35s\n",
      "7704:\tlearn: 3.6544736\ttotal: 38m 53s\tremaining: 11m 35s\n",
      "7705:\tlearn: 3.6541906\ttotal: 38m 53s\tremaining: 11m 34s\n",
      "7706:\tlearn: 3.6537868\ttotal: 38m 54s\tremaining: 11m 34s\n",
      "7707:\tlearn: 3.6535847\ttotal: 38m 54s\tremaining: 11m 34s\n",
      "7708:\tlearn: 3.6532152\ttotal: 38m 54s\tremaining: 11m 33s\n",
      "7709:\tlearn: 3.6529867\ttotal: 38m 55s\tremaining: 11m 33s\n",
      "7710:\tlearn: 3.6527271\ttotal: 38m 55s\tremaining: 11m 33s\n",
      "7711:\tlearn: 3.6524039\ttotal: 38m 55s\tremaining: 11m 32s\n",
      "7712:\tlearn: 3.6520872\ttotal: 38m 55s\tremaining: 11m 32s\n",
      "7713:\tlearn: 3.6516196\ttotal: 38m 56s\tremaining: 11m 32s\n",
      "7714:\tlearn: 3.6512283\ttotal: 38m 56s\tremaining: 11m 32s\n",
      "7715:\tlearn: 3.6509313\ttotal: 38m 56s\tremaining: 11m 31s\n",
      "7716:\tlearn: 3.6504579\ttotal: 38m 57s\tremaining: 11m 31s\n",
      "7717:\tlearn: 3.6500882\ttotal: 38m 57s\tremaining: 11m 31s\n",
      "7718:\tlearn: 3.6497959\ttotal: 38m 57s\tremaining: 11m 30s\n",
      "7719:\tlearn: 3.6496248\ttotal: 38m 58s\tremaining: 11m 30s\n",
      "7720:\tlearn: 3.6493281\ttotal: 38m 58s\tremaining: 11m 30s\n",
      "7721:\tlearn: 3.6491556\ttotal: 38m 58s\tremaining: 11m 29s\n",
      "7722:\tlearn: 3.6489224\ttotal: 38m 59s\tremaining: 11m 29s\n",
      "7723:\tlearn: 3.6487336\ttotal: 38m 59s\tremaining: 11m 29s\n",
      "7724:\tlearn: 3.6483597\ttotal: 38m 59s\tremaining: 11m 29s\n",
      "7725:\tlearn: 3.6479606\ttotal: 38m 59s\tremaining: 11m 28s\n",
      "7726:\tlearn: 3.6475395\ttotal: 39m\tremaining: 11m 28s\n",
      "7727:\tlearn: 3.6472115\ttotal: 39m\tremaining: 11m 28s\n",
      "7728:\tlearn: 3.6469765\ttotal: 39m\tremaining: 11m 27s\n",
      "7729:\tlearn: 3.6467377\ttotal: 39m\tremaining: 11m 27s\n",
      "7730:\tlearn: 3.6462317\ttotal: 39m 1s\tremaining: 11m 27s\n",
      "7731:\tlearn: 3.6459573\ttotal: 39m 1s\tremaining: 11m 26s\n",
      "7732:\tlearn: 3.6457337\ttotal: 39m 1s\tremaining: 11m 26s\n",
      "7733:\tlearn: 3.6455698\ttotal: 39m 2s\tremaining: 11m 26s\n",
      "7734:\tlearn: 3.6452529\ttotal: 39m 2s\tremaining: 11m 25s\n",
      "7735:\tlearn: 3.6448088\ttotal: 39m 2s\tremaining: 11m 25s\n",
      "7736:\tlearn: 3.6445207\ttotal: 39m 3s\tremaining: 11m 25s\n",
      "7737:\tlearn: 3.6440510\ttotal: 39m 3s\tremaining: 11m 25s\n",
      "7738:\tlearn: 3.6437811\ttotal: 39m 3s\tremaining: 11m 24s\n",
      "7739:\tlearn: 3.6434786\ttotal: 39m 4s\tremaining: 11m 24s\n",
      "7740:\tlearn: 3.6432761\ttotal: 39m 4s\tremaining: 11m 24s\n",
      "7741:\tlearn: 3.6429860\ttotal: 39m 4s\tremaining: 11m 23s\n",
      "7742:\tlearn: 3.6427181\ttotal: 39m 5s\tremaining: 11m 23s\n",
      "7743:\tlearn: 3.6424017\ttotal: 39m 5s\tremaining: 11m 23s\n",
      "7744:\tlearn: 3.6419655\ttotal: 39m 5s\tremaining: 11m 22s\n",
      "7745:\tlearn: 3.6418076\ttotal: 39m 5s\tremaining: 11m 22s\n",
      "7746:\tlearn: 3.6415028\ttotal: 39m 6s\tremaining: 11m 22s\n",
      "7747:\tlearn: 3.6410819\ttotal: 39m 6s\tremaining: 11m 22s\n",
      "7748:\tlearn: 3.6408988\ttotal: 39m 6s\tremaining: 11m 21s\n",
      "7749:\tlearn: 3.6405632\ttotal: 39m 7s\tremaining: 11m 21s\n",
      "7750:\tlearn: 3.6402143\ttotal: 39m 7s\tremaining: 11m 21s\n",
      "7751:\tlearn: 3.6399602\ttotal: 39m 7s\tremaining: 11m 20s\n",
      "7752:\tlearn: 3.6397050\ttotal: 39m 7s\tremaining: 11m 20s\n",
      "7753:\tlearn: 3.6394169\ttotal: 39m 8s\tremaining: 11m 20s\n",
      "7754:\tlearn: 3.6390712\ttotal: 39m 8s\tremaining: 11m 19s\n",
      "7755:\tlearn: 3.6387940\ttotal: 39m 8s\tremaining: 11m 19s\n",
      "7756:\tlearn: 3.6384811\ttotal: 39m 9s\tremaining: 11m 19s\n",
      "7757:\tlearn: 3.6382567\ttotal: 39m 9s\tremaining: 11m 18s\n",
      "7758:\tlearn: 3.6380518\ttotal: 39m 9s\tremaining: 11m 18s\n",
      "7759:\tlearn: 3.6377871\ttotal: 39m 9s\tremaining: 11m 18s\n",
      "7760:\tlearn: 3.6375382\ttotal: 39m 10s\tremaining: 11m 18s\n",
      "7761:\tlearn: 3.6372812\ttotal: 39m 10s\tremaining: 11m 17s\n",
      "7762:\tlearn: 3.6370154\ttotal: 39m 10s\tremaining: 11m 17s\n",
      "7763:\tlearn: 3.6368484\ttotal: 39m 11s\tremaining: 11m 17s\n",
      "7764:\tlearn: 3.6366411\ttotal: 39m 11s\tremaining: 11m 16s\n",
      "7765:\tlearn: 3.6363323\ttotal: 39m 11s\tremaining: 11m 16s\n",
      "7766:\tlearn: 3.6361357\ttotal: 39m 11s\tremaining: 11m 16s\n",
      "7767:\tlearn: 3.6358816\ttotal: 39m 12s\tremaining: 11m 15s\n",
      "7768:\tlearn: 3.6356074\ttotal: 39m 12s\tremaining: 11m 15s\n",
      "7769:\tlearn: 3.6354254\ttotal: 39m 12s\tremaining: 11m 15s\n",
      "7770:\tlearn: 3.6351196\ttotal: 39m 13s\tremaining: 11m 14s\n",
      "7771:\tlearn: 3.6349486\ttotal: 39m 13s\tremaining: 11m 14s\n",
      "7772:\tlearn: 3.6347065\ttotal: 39m 13s\tremaining: 11m 14s\n",
      "7773:\tlearn: 3.6345124\ttotal: 39m 13s\tremaining: 11m 13s\n",
      "7774:\tlearn: 3.6342338\ttotal: 39m 14s\tremaining: 11m 13s\n",
      "7775:\tlearn: 3.6339032\ttotal: 39m 14s\tremaining: 11m 13s\n",
      "7776:\tlearn: 3.6334765\ttotal: 39m 14s\tremaining: 11m 13s\n",
      "7777:\tlearn: 3.6331760\ttotal: 39m 14s\tremaining: 11m 12s\n",
      "7778:\tlearn: 3.6330167\ttotal: 39m 15s\tremaining: 11m 12s\n",
      "7779:\tlearn: 3.6325144\ttotal: 39m 15s\tremaining: 11m 12s\n",
      "7780:\tlearn: 3.6322061\ttotal: 39m 15s\tremaining: 11m 11s\n",
      "7781:\tlearn: 3.6320156\ttotal: 39m 16s\tremaining: 11m 11s\n",
      "7782:\tlearn: 3.6318587\ttotal: 39m 16s\tremaining: 11m 11s\n",
      "7783:\tlearn: 3.6315886\ttotal: 39m 16s\tremaining: 11m 10s\n",
      "7784:\tlearn: 3.6312299\ttotal: 39m 16s\tremaining: 11m 10s\n",
      "7785:\tlearn: 3.6308084\ttotal: 39m 17s\tremaining: 11m 10s\n",
      "7786:\tlearn: 3.6305570\ttotal: 39m 17s\tremaining: 11m 10s\n",
      "7787:\tlearn: 3.6302883\ttotal: 39m 17s\tremaining: 11m 9s\n",
      "7788:\tlearn: 3.6300097\ttotal: 39m 18s\tremaining: 11m 9s\n",
      "7789:\tlearn: 3.6296914\ttotal: 39m 18s\tremaining: 11m 9s\n",
      "7790:\tlearn: 3.6294379\ttotal: 39m 18s\tremaining: 11m 8s\n",
      "7791:\tlearn: 3.6290516\ttotal: 39m 18s\tremaining: 11m 8s\n",
      "7792:\tlearn: 3.6287916\ttotal: 39m 19s\tremaining: 11m 8s\n",
      "7793:\tlearn: 3.6284326\ttotal: 39m 19s\tremaining: 11m 7s\n",
      "7794:\tlearn: 3.6280744\ttotal: 39m 19s\tremaining: 11m 7s\n",
      "7795:\tlearn: 3.6277685\ttotal: 39m 20s\tremaining: 11m 7s\n",
      "7796:\tlearn: 3.6275834\ttotal: 39m 20s\tremaining: 11m 6s\n",
      "7797:\tlearn: 3.6273240\ttotal: 39m 20s\tremaining: 11m 6s\n",
      "7798:\tlearn: 3.6269194\ttotal: 39m 21s\tremaining: 11m 6s\n",
      "7799:\tlearn: 3.6267453\ttotal: 39m 21s\tremaining: 11m 6s\n",
      "7800:\tlearn: 3.6264324\ttotal: 39m 21s\tremaining: 11m 5s\n",
      "7801:\tlearn: 3.6260902\ttotal: 39m 21s\tremaining: 11m 5s\n",
      "7802:\tlearn: 3.6259312\ttotal: 39m 22s\tremaining: 11m 5s\n",
      "7803:\tlearn: 3.6256282\ttotal: 39m 22s\tremaining: 11m 4s\n",
      "7804:\tlearn: 3.6253230\ttotal: 39m 22s\tremaining: 11m 4s\n",
      "7805:\tlearn: 3.6251325\ttotal: 39m 22s\tremaining: 11m 4s\n",
      "7806:\tlearn: 3.6247985\ttotal: 39m 23s\tremaining: 11m 3s\n",
      "7807:\tlearn: 3.6244590\ttotal: 39m 23s\tremaining: 11m 3s\n",
      "7808:\tlearn: 3.6241307\ttotal: 39m 23s\tremaining: 11m 3s\n",
      "7809:\tlearn: 3.6237366\ttotal: 39m 24s\tremaining: 11m 2s\n",
      "7810:\tlearn: 3.6232597\ttotal: 39m 24s\tremaining: 11m 2s\n",
      "7811:\tlearn: 3.6230143\ttotal: 39m 24s\tremaining: 11m 2s\n",
      "7812:\tlearn: 3.6226584\ttotal: 39m 24s\tremaining: 11m 2s\n",
      "7813:\tlearn: 3.6224201\ttotal: 39m 25s\tremaining: 11m 1s\n",
      "7814:\tlearn: 3.6220408\ttotal: 39m 25s\tremaining: 11m 1s\n",
      "7815:\tlearn: 3.6218136\ttotal: 39m 25s\tremaining: 11m 1s\n",
      "7816:\tlearn: 3.6214859\ttotal: 39m 26s\tremaining: 11m\n",
      "7817:\tlearn: 3.6212583\ttotal: 39m 26s\tremaining: 11m\n",
      "7818:\tlearn: 3.6210572\ttotal: 39m 26s\tremaining: 11m\n",
      "7819:\tlearn: 3.6207347\ttotal: 39m 26s\tremaining: 10m 59s\n",
      "7820:\tlearn: 3.6204859\ttotal: 39m 27s\tremaining: 10m 59s\n",
      "7821:\tlearn: 3.6201951\ttotal: 39m 27s\tremaining: 10m 59s\n",
      "7822:\tlearn: 3.6199401\ttotal: 39m 27s\tremaining: 10m 58s\n",
      "7823:\tlearn: 3.6197553\ttotal: 39m 28s\tremaining: 10m 58s\n",
      "7824:\tlearn: 3.6195014\ttotal: 39m 28s\tremaining: 10m 58s\n",
      "7825:\tlearn: 3.6192915\ttotal: 39m 28s\tremaining: 10m 57s\n",
      "7826:\tlearn: 3.6189856\ttotal: 39m 28s\tremaining: 10m 57s\n",
      "7827:\tlearn: 3.6186903\ttotal: 39m 29s\tremaining: 10m 57s\n",
      "7828:\tlearn: 3.6183983\ttotal: 39m 29s\tremaining: 10m 57s\n",
      "7829:\tlearn: 3.6180260\ttotal: 39m 29s\tremaining: 10m 56s\n",
      "7830:\tlearn: 3.6178696\ttotal: 39m 29s\tremaining: 10m 56s\n",
      "7831:\tlearn: 3.6175889\ttotal: 39m 30s\tremaining: 10m 56s\n",
      "7832:\tlearn: 3.6173592\ttotal: 39m 30s\tremaining: 10m 55s\n",
      "7833:\tlearn: 3.6171593\ttotal: 39m 30s\tremaining: 10m 55s\n",
      "7834:\tlearn: 3.6170115\ttotal: 39m 30s\tremaining: 10m 55s\n",
      "7835:\tlearn: 3.6166758\ttotal: 39m 31s\tremaining: 10m 54s\n",
      "7836:\tlearn: 3.6162832\ttotal: 39m 31s\tremaining: 10m 54s\n",
      "7837:\tlearn: 3.6159850\ttotal: 39m 31s\tremaining: 10m 54s\n",
      "7838:\tlearn: 3.6156532\ttotal: 39m 32s\tremaining: 10m 53s\n",
      "7839:\tlearn: 3.6153582\ttotal: 39m 32s\tremaining: 10m 53s\n",
      "7840:\tlearn: 3.6150489\ttotal: 39m 32s\tremaining: 10m 53s\n",
      "7841:\tlearn: 3.6145414\ttotal: 39m 33s\tremaining: 10m 53s\n",
      "7842:\tlearn: 3.6143366\ttotal: 39m 33s\tremaining: 10m 52s\n",
      "7843:\tlearn: 3.6141088\ttotal: 39m 33s\tremaining: 10m 52s\n",
      "7844:\tlearn: 3.6137333\ttotal: 39m 34s\tremaining: 10m 52s\n",
      "7845:\tlearn: 3.6135516\ttotal: 39m 34s\tremaining: 10m 51s\n",
      "7846:\tlearn: 3.6133559\ttotal: 39m 34s\tremaining: 10m 51s\n",
      "7847:\tlearn: 3.6131580\ttotal: 39m 34s\tremaining: 10m 51s\n",
      "7848:\tlearn: 3.6127619\ttotal: 39m 35s\tremaining: 10m 50s\n",
      "7849:\tlearn: 3.6126343\ttotal: 39m 35s\tremaining: 10m 50s\n",
      "7850:\tlearn: 3.6123733\ttotal: 39m 35s\tremaining: 10m 50s\n",
      "7851:\tlearn: 3.6121313\ttotal: 39m 35s\tremaining: 10m 49s\n",
      "7852:\tlearn: 3.6117053\ttotal: 39m 36s\tremaining: 10m 49s\n",
      "7853:\tlearn: 3.6113968\ttotal: 39m 36s\tremaining: 10m 49s\n",
      "7854:\tlearn: 3.6109709\ttotal: 39m 36s\tremaining: 10m 49s\n",
      "7855:\tlearn: 3.6107478\ttotal: 39m 37s\tremaining: 10m 48s\n",
      "7856:\tlearn: 3.6104685\ttotal: 39m 37s\tremaining: 10m 48s\n",
      "7857:\tlearn: 3.6101488\ttotal: 39m 37s\tremaining: 10m 48s\n",
      "7858:\tlearn: 3.6099920\ttotal: 39m 37s\tremaining: 10m 47s\n",
      "7859:\tlearn: 3.6098517\ttotal: 39m 38s\tremaining: 10m 47s\n",
      "7860:\tlearn: 3.6096256\ttotal: 39m 38s\tremaining: 10m 47s\n",
      "7861:\tlearn: 3.6094195\ttotal: 39m 38s\tremaining: 10m 46s\n",
      "7862:\tlearn: 3.6090277\ttotal: 39m 38s\tremaining: 10m 46s\n",
      "7863:\tlearn: 3.6085456\ttotal: 39m 39s\tremaining: 10m 46s\n",
      "7864:\tlearn: 3.6082760\ttotal: 39m 39s\tremaining: 10m 45s\n",
      "7865:\tlearn: 3.6079248\ttotal: 39m 39s\tremaining: 10m 45s\n",
      "7866:\tlearn: 3.6076559\ttotal: 39m 40s\tremaining: 10m 45s\n",
      "7867:\tlearn: 3.6073180\ttotal: 39m 40s\tremaining: 10m 45s\n",
      "7868:\tlearn: 3.6071237\ttotal: 39m 40s\tremaining: 10m 44s\n",
      "7869:\tlearn: 3.6068100\ttotal: 39m 41s\tremaining: 10m 44s\n",
      "7870:\tlearn: 3.6065413\ttotal: 39m 41s\tremaining: 10m 44s\n",
      "7871:\tlearn: 3.6062608\ttotal: 39m 41s\tremaining: 10m 43s\n",
      "7872:\tlearn: 3.6060929\ttotal: 39m 41s\tremaining: 10m 43s\n",
      "7873:\tlearn: 3.6057814\ttotal: 39m 42s\tremaining: 10m 43s\n",
      "7874:\tlearn: 3.6055941\ttotal: 39m 42s\tremaining: 10m 42s\n",
      "7875:\tlearn: 3.6053097\ttotal: 39m 42s\tremaining: 10m 42s\n",
      "7876:\tlearn: 3.6050024\ttotal: 39m 42s\tremaining: 10m 42s\n",
      "7877:\tlearn: 3.6048151\ttotal: 39m 43s\tremaining: 10m 41s\n",
      "7878:\tlearn: 3.6046225\ttotal: 39m 43s\tremaining: 10m 41s\n",
      "7879:\tlearn: 3.6044661\ttotal: 39m 43s\tremaining: 10m 41s\n",
      "7880:\tlearn: 3.6040033\ttotal: 39m 44s\tremaining: 10m 41s\n",
      "7881:\tlearn: 3.6036679\ttotal: 39m 44s\tremaining: 10m 40s\n",
      "7882:\tlearn: 3.6033168\ttotal: 39m 44s\tremaining: 10m 40s\n",
      "7883:\tlearn: 3.6030473\ttotal: 39m 44s\tremaining: 10m 40s\n",
      "7884:\tlearn: 3.6027774\ttotal: 39m 45s\tremaining: 10m 39s\n",
      "7885:\tlearn: 3.6024781\ttotal: 39m 45s\tremaining: 10m 39s\n",
      "7886:\tlearn: 3.6021915\ttotal: 39m 45s\tremaining: 10m 39s\n",
      "7887:\tlearn: 3.6019333\ttotal: 39m 45s\tremaining: 10m 38s\n",
      "7888:\tlearn: 3.6015397\ttotal: 39m 46s\tremaining: 10m 38s\n",
      "7889:\tlearn: 3.6013041\ttotal: 39m 46s\tremaining: 10m 38s\n",
      "7890:\tlearn: 3.6010841\ttotal: 39m 46s\tremaining: 10m 37s\n",
      "7891:\tlearn: 3.6008695\ttotal: 39m 47s\tremaining: 10m 37s\n",
      "7892:\tlearn: 3.6006757\ttotal: 39m 47s\tremaining: 10m 37s\n",
      "7893:\tlearn: 3.6004237\ttotal: 39m 47s\tremaining: 10m 36s\n",
      "7894:\tlearn: 3.6002161\ttotal: 39m 47s\tremaining: 10m 36s\n",
      "7895:\tlearn: 3.5999318\ttotal: 39m 48s\tremaining: 10m 36s\n",
      "7896:\tlearn: 3.5997732\ttotal: 39m 48s\tremaining: 10m 36s\n",
      "7897:\tlearn: 3.5995415\ttotal: 39m 48s\tremaining: 10m 35s\n",
      "7898:\tlearn: 3.5993365\ttotal: 39m 48s\tremaining: 10m 35s\n",
      "7899:\tlearn: 3.5990818\ttotal: 39m 49s\tremaining: 10m 35s\n",
      "7900:\tlearn: 3.5986932\ttotal: 39m 49s\tremaining: 10m 34s\n",
      "7901:\tlearn: 3.5982781\ttotal: 39m 49s\tremaining: 10m 34s\n",
      "7902:\tlearn: 3.5981014\ttotal: 39m 50s\tremaining: 10m 34s\n",
      "7903:\tlearn: 3.5977555\ttotal: 39m 50s\tremaining: 10m 33s\n",
      "7904:\tlearn: 3.5975898\ttotal: 39m 50s\tremaining: 10m 33s\n",
      "7905:\tlearn: 3.5973934\ttotal: 39m 50s\tremaining: 10m 33s\n",
      "7906:\tlearn: 3.5971285\ttotal: 39m 51s\tremaining: 10m 32s\n",
      "7907:\tlearn: 3.5969327\ttotal: 39m 51s\tremaining: 10m 32s\n",
      "7908:\tlearn: 3.5966036\ttotal: 39m 51s\tremaining: 10m 32s\n",
      "7909:\tlearn: 3.5963700\ttotal: 39m 51s\tremaining: 10m 31s\n",
      "7910:\tlearn: 3.5959228\ttotal: 39m 52s\tremaining: 10m 31s\n",
      "7911:\tlearn: 3.5957125\ttotal: 39m 52s\tremaining: 10m 31s\n",
      "7912:\tlearn: 3.5953022\ttotal: 39m 52s\tremaining: 10m 31s\n",
      "7913:\tlearn: 3.5949674\ttotal: 39m 53s\tremaining: 10m 30s\n",
      "7914:\tlearn: 3.5947073\ttotal: 39m 53s\tremaining: 10m 30s\n",
      "7915:\tlearn: 3.5944754\ttotal: 39m 53s\tremaining: 10m 30s\n",
      "7916:\tlearn: 3.5941975\ttotal: 39m 53s\tremaining: 10m 29s\n",
      "7917:\tlearn: 3.5938759\ttotal: 39m 54s\tremaining: 10m 29s\n",
      "7918:\tlearn: 3.5935552\ttotal: 39m 54s\tremaining: 10m 29s\n",
      "7919:\tlearn: 3.5933804\ttotal: 39m 54s\tremaining: 10m 28s\n",
      "7920:\tlearn: 3.5929401\ttotal: 39m 54s\tremaining: 10m 28s\n",
      "7921:\tlearn: 3.5925832\ttotal: 39m 55s\tremaining: 10m 28s\n",
      "7922:\tlearn: 3.5922942\ttotal: 39m 55s\tremaining: 10m 27s\n",
      "7923:\tlearn: 3.5919547\ttotal: 39m 55s\tremaining: 10m 27s\n",
      "7924:\tlearn: 3.5917769\ttotal: 39m 55s\tremaining: 10m 27s\n",
      "7925:\tlearn: 3.5915233\ttotal: 39m 56s\tremaining: 10m 27s\n",
      "7926:\tlearn: 3.5912804\ttotal: 39m 56s\tremaining: 10m 26s\n",
      "7927:\tlearn: 3.5910626\ttotal: 39m 56s\tremaining: 10m 26s\n",
      "7928:\tlearn: 3.5908630\ttotal: 39m 57s\tremaining: 10m 26s\n",
      "7929:\tlearn: 3.5906076\ttotal: 39m 57s\tremaining: 10m 25s\n",
      "7930:\tlearn: 3.5904097\ttotal: 39m 57s\tremaining: 10m 25s\n",
      "7931:\tlearn: 3.5900448\ttotal: 39m 58s\tremaining: 10m 25s\n",
      "7932:\tlearn: 3.5898302\ttotal: 39m 58s\tremaining: 10m 24s\n",
      "7933:\tlearn: 3.5895059\ttotal: 39m 58s\tremaining: 10m 24s\n",
      "7934:\tlearn: 3.5891438\ttotal: 39m 58s\tremaining: 10m 24s\n",
      "7935:\tlearn: 3.5888197\ttotal: 39m 59s\tremaining: 10m 24s\n",
      "7936:\tlearn: 3.5885970\ttotal: 39m 59s\tremaining: 10m 23s\n",
      "7937:\tlearn: 3.5883584\ttotal: 39m 59s\tremaining: 10m 23s\n",
      "7938:\tlearn: 3.5880944\ttotal: 40m\tremaining: 10m 23s\n",
      "7939:\tlearn: 3.5879351\ttotal: 40m\tremaining: 10m 22s\n",
      "7940:\tlearn: 3.5876731\ttotal: 40m\tremaining: 10m 22s\n",
      "7941:\tlearn: 3.5872541\ttotal: 40m\tremaining: 10m 22s\n",
      "7942:\tlearn: 3.5869719\ttotal: 40m 1s\tremaining: 10m 21s\n",
      "7943:\tlearn: 3.5866415\ttotal: 40m 1s\tremaining: 10m 21s\n",
      "7944:\tlearn: 3.5863902\ttotal: 40m 1s\tremaining: 10m 21s\n",
      "7945:\tlearn: 3.5861554\ttotal: 40m 1s\tremaining: 10m 20s\n",
      "7946:\tlearn: 3.5857432\ttotal: 40m 2s\tremaining: 10m 20s\n",
      "7947:\tlearn: 3.5854823\ttotal: 40m 2s\tremaining: 10m 20s\n",
      "7948:\tlearn: 3.5852440\ttotal: 40m 2s\tremaining: 10m 19s\n",
      "7949:\tlearn: 3.5849973\ttotal: 40m 2s\tremaining: 10m 19s\n",
      "7950:\tlearn: 3.5846244\ttotal: 40m 3s\tremaining: 10m 19s\n",
      "7951:\tlearn: 3.5844288\ttotal: 40m 3s\tremaining: 10m 19s\n",
      "7952:\tlearn: 3.5841220\ttotal: 40m 3s\tremaining: 10m 18s\n",
      "7953:\tlearn: 3.5839318\ttotal: 40m 4s\tremaining: 10m 18s\n",
      "7954:\tlearn: 3.5837286\ttotal: 40m 4s\tremaining: 10m 18s\n",
      "7955:\tlearn: 3.5834723\ttotal: 40m 4s\tremaining: 10m 17s\n",
      "7956:\tlearn: 3.5831930\ttotal: 40m 4s\tremaining: 10m 17s\n",
      "7957:\tlearn: 3.5829233\ttotal: 40m 5s\tremaining: 10m 17s\n",
      "7958:\tlearn: 3.5827160\ttotal: 40m 5s\tremaining: 10m 16s\n",
      "7959:\tlearn: 3.5823085\ttotal: 40m 5s\tremaining: 10m 16s\n",
      "7960:\tlearn: 3.5820096\ttotal: 40m 5s\tremaining: 10m 16s\n",
      "7961:\tlearn: 3.5817648\ttotal: 40m 6s\tremaining: 10m 15s\n",
      "7962:\tlearn: 3.5814735\ttotal: 40m 6s\tremaining: 10m 15s\n",
      "7963:\tlearn: 3.5811208\ttotal: 40m 6s\tremaining: 10m 15s\n",
      "7964:\tlearn: 3.5807003\ttotal: 40m 7s\tremaining: 10m 15s\n",
      "7965:\tlearn: 3.5804008\ttotal: 40m 7s\tremaining: 10m 14s\n",
      "7966:\tlearn: 3.5799090\ttotal: 40m 7s\tremaining: 10m 14s\n",
      "7967:\tlearn: 3.5796180\ttotal: 40m 7s\tremaining: 10m 14s\n",
      "7968:\tlearn: 3.5792752\ttotal: 40m 8s\tremaining: 10m 13s\n",
      "7969:\tlearn: 3.5789566\ttotal: 40m 8s\tremaining: 10m 13s\n",
      "7970:\tlearn: 3.5786672\ttotal: 40m 8s\tremaining: 10m 13s\n",
      "7971:\tlearn: 3.5782854\ttotal: 40m 9s\tremaining: 10m 12s\n",
      "7972:\tlearn: 3.5780881\ttotal: 40m 9s\tremaining: 10m 12s\n",
      "7973:\tlearn: 3.5778845\ttotal: 40m 9s\tremaining: 10m 12s\n",
      "7974:\tlearn: 3.5775887\ttotal: 40m 10s\tremaining: 10m 11s\n",
      "7975:\tlearn: 3.5772992\ttotal: 40m 10s\tremaining: 10m 11s\n",
      "7976:\tlearn: 3.5770833\ttotal: 40m 10s\tremaining: 10m 11s\n",
      "7977:\tlearn: 3.5768250\ttotal: 40m 10s\tremaining: 10m 11s\n",
      "7978:\tlearn: 3.5765111\ttotal: 40m 11s\tremaining: 10m 10s\n",
      "7979:\tlearn: 3.5761814\ttotal: 40m 11s\tremaining: 10m 10s\n",
      "7980:\tlearn: 3.5758110\ttotal: 40m 11s\tremaining: 10m 10s\n",
      "7981:\tlearn: 3.5755173\ttotal: 40m 12s\tremaining: 10m 9s\n",
      "7982:\tlearn: 3.5751400\ttotal: 40m 12s\tremaining: 10m 9s\n",
      "7983:\tlearn: 3.5747795\ttotal: 40m 12s\tremaining: 10m 9s\n",
      "7984:\tlearn: 3.5745425\ttotal: 40m 12s\tremaining: 10m 8s\n",
      "7985:\tlearn: 3.5743143\ttotal: 40m 13s\tremaining: 10m 8s\n",
      "7986:\tlearn: 3.5739705\ttotal: 40m 13s\tremaining: 10m 8s\n",
      "7987:\tlearn: 3.5736441\ttotal: 40m 13s\tremaining: 10m 7s\n",
      "7988:\tlearn: 3.5733568\ttotal: 40m 13s\tremaining: 10m 7s\n",
      "7989:\tlearn: 3.5732310\ttotal: 40m 14s\tremaining: 10m 7s\n",
      "7990:\tlearn: 3.5729112\ttotal: 40m 14s\tremaining: 10m 7s\n",
      "7991:\tlearn: 3.5726876\ttotal: 40m 14s\tremaining: 10m 6s\n",
      "7992:\tlearn: 3.5723746\ttotal: 40m 14s\tremaining: 10m 6s\n",
      "7993:\tlearn: 3.5719309\ttotal: 40m 15s\tremaining: 10m 6s\n",
      "7994:\tlearn: 3.5714976\ttotal: 40m 15s\tremaining: 10m 5s\n",
      "7995:\tlearn: 3.5711685\ttotal: 40m 15s\tremaining: 10m 5s\n",
      "7996:\tlearn: 3.5708957\ttotal: 40m 16s\tremaining: 10m 5s\n",
      "7997:\tlearn: 3.5706982\ttotal: 40m 16s\tremaining: 10m 4s\n",
      "7998:\tlearn: 3.5705468\ttotal: 40m 16s\tremaining: 10m 4s\n",
      "7999:\tlearn: 3.5702962\ttotal: 40m 16s\tremaining: 10m 4s\n",
      "8000:\tlearn: 3.5700971\ttotal: 40m 17s\tremaining: 10m 3s\n",
      "8001:\tlearn: 3.5698275\ttotal: 40m 17s\tremaining: 10m 3s\n",
      "8002:\tlearn: 3.5693838\ttotal: 40m 17s\tremaining: 10m 3s\n",
      "8003:\tlearn: 3.5691454\ttotal: 40m 18s\tremaining: 10m 2s\n",
      "8004:\tlearn: 3.5689382\ttotal: 40m 18s\tremaining: 10m 2s\n",
      "8005:\tlearn: 3.5687124\ttotal: 40m 18s\tremaining: 10m 2s\n",
      "8006:\tlearn: 3.5683777\ttotal: 40m 18s\tremaining: 10m 2s\n",
      "8007:\tlearn: 3.5682027\ttotal: 40m 19s\tremaining: 10m 1s\n",
      "8008:\tlearn: 3.5677591\ttotal: 40m 19s\tremaining: 10m 1s\n",
      "8009:\tlearn: 3.5676711\ttotal: 40m 19s\tremaining: 10m 1s\n",
      "8010:\tlearn: 3.5672951\ttotal: 40m 19s\tremaining: 10m\n",
      "8011:\tlearn: 3.5670124\ttotal: 40m 20s\tremaining: 10m\n",
      "8012:\tlearn: 3.5668004\ttotal: 40m 20s\tremaining: 10m\n",
      "8013:\tlearn: 3.5663588\ttotal: 40m 20s\tremaining: 9m 59s\n",
      "8014:\tlearn: 3.5660255\ttotal: 40m 21s\tremaining: 9m 59s\n",
      "8015:\tlearn: 3.5656474\ttotal: 40m 21s\tremaining: 9m 59s\n",
      "8016:\tlearn: 3.5653501\ttotal: 40m 21s\tremaining: 9m 58s\n",
      "8017:\tlearn: 3.5649856\ttotal: 40m 21s\tremaining: 9m 58s\n",
      "8018:\tlearn: 3.5647112\ttotal: 40m 22s\tremaining: 9m 58s\n",
      "8019:\tlearn: 3.5645732\ttotal: 40m 22s\tremaining: 9m 58s\n",
      "8020:\tlearn: 3.5642829\ttotal: 40m 22s\tremaining: 9m 57s\n",
      "8021:\tlearn: 3.5640316\ttotal: 40m 22s\tremaining: 9m 57s\n",
      "8022:\tlearn: 3.5637562\ttotal: 40m 23s\tremaining: 9m 57s\n",
      "8023:\tlearn: 3.5635524\ttotal: 40m 23s\tremaining: 9m 56s\n",
      "8024:\tlearn: 3.5633345\ttotal: 40m 23s\tremaining: 9m 56s\n",
      "8025:\tlearn: 3.5629825\ttotal: 40m 23s\tremaining: 9m 56s\n",
      "8026:\tlearn: 3.5627480\ttotal: 40m 24s\tremaining: 9m 55s\n",
      "8027:\tlearn: 3.5623658\ttotal: 40m 24s\tremaining: 9m 55s\n",
      "8028:\tlearn: 3.5621939\ttotal: 40m 24s\tremaining: 9m 55s\n",
      "8029:\tlearn: 3.5617798\ttotal: 40m 25s\tremaining: 9m 54s\n",
      "8030:\tlearn: 3.5615875\ttotal: 40m 25s\tremaining: 9m 54s\n",
      "8031:\tlearn: 3.5612047\ttotal: 40m 25s\tremaining: 9m 54s\n",
      "8032:\tlearn: 3.5609636\ttotal: 40m 25s\tremaining: 9m 53s\n",
      "8033:\tlearn: 3.5606669\ttotal: 40m 25s\tremaining: 9m 53s\n",
      "8034:\tlearn: 3.5603315\ttotal: 40m 26s\tremaining: 9m 53s\n",
      "8035:\tlearn: 3.5599397\ttotal: 40m 26s\tremaining: 9m 53s\n",
      "8036:\tlearn: 3.5596923\ttotal: 40m 26s\tremaining: 9m 52s\n",
      "8037:\tlearn: 3.5594390\ttotal: 40m 27s\tremaining: 9m 52s\n",
      "8038:\tlearn: 3.5591418\ttotal: 40m 27s\tremaining: 9m 52s\n",
      "8039:\tlearn: 3.5589576\ttotal: 40m 27s\tremaining: 9m 51s\n",
      "8040:\tlearn: 3.5587384\ttotal: 40m 27s\tremaining: 9m 51s\n",
      "8041:\tlearn: 3.5585583\ttotal: 40m 28s\tremaining: 9m 51s\n",
      "8042:\tlearn: 3.5583635\ttotal: 40m 28s\tremaining: 9m 50s\n",
      "8043:\tlearn: 3.5579983\ttotal: 40m 28s\tremaining: 9m 50s\n",
      "8044:\tlearn: 3.5578323\ttotal: 40m 28s\tremaining: 9m 50s\n",
      "8045:\tlearn: 3.5575681\ttotal: 40m 29s\tremaining: 9m 49s\n",
      "8046:\tlearn: 3.5571630\ttotal: 40m 29s\tremaining: 9m 49s\n",
      "8047:\tlearn: 3.5568784\ttotal: 40m 29s\tremaining: 9m 49s\n",
      "8048:\tlearn: 3.5565896\ttotal: 40m 30s\tremaining: 9m 49s\n",
      "8049:\tlearn: 3.5563472\ttotal: 40m 30s\tremaining: 9m 48s\n",
      "8050:\tlearn: 3.5559859\ttotal: 40m 30s\tremaining: 9m 48s\n",
      "8051:\tlearn: 3.5558385\ttotal: 40m 30s\tremaining: 9m 48s\n",
      "8052:\tlearn: 3.5555565\ttotal: 40m 31s\tremaining: 9m 47s\n",
      "8053:\tlearn: 3.5553504\ttotal: 40m 31s\tremaining: 9m 47s\n",
      "8054:\tlearn: 3.5549831\ttotal: 40m 31s\tremaining: 9m 47s\n",
      "8055:\tlearn: 3.5546100\ttotal: 40m 31s\tremaining: 9m 46s\n",
      "8056:\tlearn: 3.5541390\ttotal: 40m 32s\tremaining: 9m 46s\n",
      "8057:\tlearn: 3.5538445\ttotal: 40m 32s\tremaining: 9m 46s\n",
      "8058:\tlearn: 3.5535115\ttotal: 40m 32s\tremaining: 9m 45s\n",
      "8059:\tlearn: 3.5533641\ttotal: 40m 33s\tremaining: 9m 45s\n",
      "8060:\tlearn: 3.5531158\ttotal: 40m 33s\tremaining: 9m 45s\n",
      "8061:\tlearn: 3.5528375\ttotal: 40m 33s\tremaining: 9m 45s\n",
      "8062:\tlearn: 3.5525419\ttotal: 40m 33s\tremaining: 9m 44s\n",
      "8063:\tlearn: 3.5521807\ttotal: 40m 34s\tremaining: 9m 44s\n",
      "8064:\tlearn: 3.5519551\ttotal: 40m 34s\tremaining: 9m 44s\n",
      "8065:\tlearn: 3.5517650\ttotal: 40m 34s\tremaining: 9m 43s\n",
      "8066:\tlearn: 3.5514583\ttotal: 40m 35s\tremaining: 9m 43s\n",
      "8067:\tlearn: 3.5512475\ttotal: 40m 35s\tremaining: 9m 43s\n",
      "8068:\tlearn: 3.5510011\ttotal: 40m 35s\tremaining: 9m 42s\n",
      "8069:\tlearn: 3.5507768\ttotal: 40m 35s\tremaining: 9m 42s\n",
      "8070:\tlearn: 3.5504252\ttotal: 40m 36s\tremaining: 9m 42s\n",
      "8071:\tlearn: 3.5502671\ttotal: 40m 36s\tremaining: 9m 41s\n",
      "8072:\tlearn: 3.5498901\ttotal: 40m 36s\tremaining: 9m 41s\n",
      "8073:\tlearn: 3.5495942\ttotal: 40m 36s\tremaining: 9m 41s\n",
      "8074:\tlearn: 3.5493389\ttotal: 40m 37s\tremaining: 9m 41s\n",
      "8075:\tlearn: 3.5488527\ttotal: 40m 37s\tremaining: 9m 40s\n",
      "8076:\tlearn: 3.5486529\ttotal: 40m 37s\tremaining: 9m 40s\n",
      "8077:\tlearn: 3.5485269\ttotal: 40m 38s\tremaining: 9m 40s\n",
      "8078:\tlearn: 3.5482850\ttotal: 40m 38s\tremaining: 9m 39s\n",
      "8079:\tlearn: 3.5479101\ttotal: 40m 38s\tremaining: 9m 39s\n",
      "8080:\tlearn: 3.5477007\ttotal: 40m 38s\tremaining: 9m 39s\n",
      "8081:\tlearn: 3.5475466\ttotal: 40m 39s\tremaining: 9m 38s\n",
      "8082:\tlearn: 3.5474000\ttotal: 40m 39s\tremaining: 9m 38s\n",
      "8083:\tlearn: 3.5471330\ttotal: 40m 39s\tremaining: 9m 38s\n",
      "8084:\tlearn: 3.5468136\ttotal: 40m 39s\tremaining: 9m 37s\n",
      "8085:\tlearn: 3.5465786\ttotal: 40m 40s\tremaining: 9m 37s\n",
      "8086:\tlearn: 3.5463639\ttotal: 40m 40s\tremaining: 9m 37s\n",
      "8087:\tlearn: 3.5460736\ttotal: 40m 40s\tremaining: 9m 36s\n",
      "8088:\tlearn: 3.5459971\ttotal: 40m 40s\tremaining: 9m 36s\n",
      "8089:\tlearn: 3.5456735\ttotal: 40m 41s\tremaining: 9m 36s\n",
      "8090:\tlearn: 3.5454009\ttotal: 40m 41s\tremaining: 9m 36s\n",
      "8091:\tlearn: 3.5450128\ttotal: 40m 41s\tremaining: 9m 35s\n",
      "8092:\tlearn: 3.5448315\ttotal: 40m 42s\tremaining: 9m 35s\n",
      "8093:\tlearn: 3.5446058\ttotal: 40m 42s\tremaining: 9m 35s\n",
      "8094:\tlearn: 3.5444650\ttotal: 40m 42s\tremaining: 9m 34s\n",
      "8095:\tlearn: 3.5441570\ttotal: 40m 42s\tremaining: 9m 34s\n",
      "8096:\tlearn: 3.5439330\ttotal: 40m 43s\tremaining: 9m 34s\n",
      "8097:\tlearn: 3.5436250\ttotal: 40m 43s\tremaining: 9m 33s\n",
      "8098:\tlearn: 3.5433999\ttotal: 40m 43s\tremaining: 9m 33s\n",
      "8099:\tlearn: 3.5430202\ttotal: 40m 43s\tremaining: 9m 33s\n",
      "8100:\tlearn: 3.5427418\ttotal: 40m 44s\tremaining: 9m 32s\n",
      "8101:\tlearn: 3.5424356\ttotal: 40m 44s\tremaining: 9m 32s\n",
      "8102:\tlearn: 3.5422113\ttotal: 40m 44s\tremaining: 9m 32s\n",
      "8103:\tlearn: 3.5417922\ttotal: 40m 44s\tremaining: 9m 32s\n",
      "8104:\tlearn: 3.5414822\ttotal: 40m 45s\tremaining: 9m 31s\n",
      "8105:\tlearn: 3.5412421\ttotal: 40m 45s\tremaining: 9m 31s\n",
      "8106:\tlearn: 3.5409558\ttotal: 40m 45s\tremaining: 9m 31s\n",
      "8107:\tlearn: 3.5404750\ttotal: 40m 46s\tremaining: 9m 30s\n",
      "8108:\tlearn: 3.5403090\ttotal: 40m 46s\tremaining: 9m 30s\n",
      "8109:\tlearn: 3.5400918\ttotal: 40m 46s\tremaining: 9m 30s\n",
      "8110:\tlearn: 3.5398559\ttotal: 40m 46s\tremaining: 9m 29s\n",
      "8111:\tlearn: 3.5395569\ttotal: 40m 47s\tremaining: 9m 29s\n",
      "8112:\tlearn: 3.5392979\ttotal: 40m 47s\tremaining: 9m 29s\n",
      "8113:\tlearn: 3.5389501\ttotal: 40m 47s\tremaining: 9m 28s\n",
      "8114:\tlearn: 3.5387880\ttotal: 40m 47s\tremaining: 9m 28s\n",
      "8115:\tlearn: 3.5385399\ttotal: 40m 48s\tremaining: 9m 28s\n",
      "8116:\tlearn: 3.5382983\ttotal: 40m 48s\tremaining: 9m 28s\n",
      "8117:\tlearn: 3.5379725\ttotal: 40m 48s\tremaining: 9m 27s\n",
      "8118:\tlearn: 3.5378308\ttotal: 40m 49s\tremaining: 9m 27s\n",
      "8119:\tlearn: 3.5376061\ttotal: 40m 49s\tremaining: 9m 27s\n",
      "8120:\tlearn: 3.5373432\ttotal: 40m 49s\tremaining: 9m 26s\n",
      "8121:\tlearn: 3.5368842\ttotal: 40m 49s\tremaining: 9m 26s\n",
      "8122:\tlearn: 3.5366747\ttotal: 40m 50s\tremaining: 9m 26s\n",
      "8123:\tlearn: 3.5364509\ttotal: 40m 50s\tremaining: 9m 25s\n",
      "8124:\tlearn: 3.5361926\ttotal: 40m 50s\tremaining: 9m 25s\n",
      "8125:\tlearn: 3.5359114\ttotal: 40m 50s\tremaining: 9m 25s\n",
      "8126:\tlearn: 3.5357150\ttotal: 40m 51s\tremaining: 9m 24s\n",
      "8127:\tlearn: 3.5355437\ttotal: 40m 51s\tremaining: 9m 24s\n",
      "8128:\tlearn: 3.5352517\ttotal: 40m 51s\tremaining: 9m 24s\n",
      "8129:\tlearn: 3.5350865\ttotal: 40m 51s\tremaining: 9m 23s\n",
      "8130:\tlearn: 3.5348469\ttotal: 40m 52s\tremaining: 9m 23s\n",
      "8131:\tlearn: 3.5347045\ttotal: 40m 52s\tremaining: 9m 23s\n",
      "8132:\tlearn: 3.5345816\ttotal: 40m 52s\tremaining: 9m 23s\n",
      "8133:\tlearn: 3.5342801\ttotal: 40m 52s\tremaining: 9m 22s\n",
      "8134:\tlearn: 3.5340182\ttotal: 40m 53s\tremaining: 9m 22s\n",
      "8135:\tlearn: 3.5337830\ttotal: 40m 53s\tremaining: 9m 22s\n",
      "8136:\tlearn: 3.5335657\ttotal: 40m 53s\tremaining: 9m 21s\n",
      "8137:\tlearn: 3.5333047\ttotal: 40m 53s\tremaining: 9m 21s\n",
      "8138:\tlearn: 3.5330443\ttotal: 40m 54s\tremaining: 9m 21s\n",
      "8139:\tlearn: 3.5328011\ttotal: 40m 54s\tremaining: 9m 20s\n",
      "8140:\tlearn: 3.5323950\ttotal: 40m 54s\tremaining: 9m 20s\n",
      "8141:\tlearn: 3.5320062\ttotal: 40m 54s\tremaining: 9m 20s\n",
      "8142:\tlearn: 3.5319449\ttotal: 40m 55s\tremaining: 9m 19s\n",
      "8143:\tlearn: 3.5316504\ttotal: 40m 55s\tremaining: 9m 19s\n",
      "8144:\tlearn: 3.5313745\ttotal: 40m 55s\tremaining: 9m 19s\n",
      "8145:\tlearn: 3.5311871\ttotal: 40m 56s\tremaining: 9m 18s\n",
      "8146:\tlearn: 3.5308253\ttotal: 40m 56s\tremaining: 9m 18s\n",
      "8147:\tlearn: 3.5306294\ttotal: 40m 56s\tremaining: 9m 18s\n",
      "8148:\tlearn: 3.5303777\ttotal: 40m 56s\tremaining: 9m 18s\n",
      "8149:\tlearn: 3.5301242\ttotal: 40m 57s\tremaining: 9m 17s\n",
      "8150:\tlearn: 3.5297585\ttotal: 40m 57s\tremaining: 9m 17s\n",
      "8151:\tlearn: 3.5294520\ttotal: 40m 57s\tremaining: 9m 17s\n",
      "8152:\tlearn: 3.5293160\ttotal: 40m 58s\tremaining: 9m 16s\n",
      "8153:\tlearn: 3.5290872\ttotal: 40m 58s\tremaining: 9m 16s\n",
      "8154:\tlearn: 3.5287664\ttotal: 40m 58s\tremaining: 9m 16s\n",
      "8155:\tlearn: 3.5284416\ttotal: 40m 58s\tremaining: 9m 15s\n",
      "8156:\tlearn: 3.5282473\ttotal: 40m 59s\tremaining: 9m 15s\n",
      "8157:\tlearn: 3.5278095\ttotal: 40m 59s\tremaining: 9m 15s\n",
      "8158:\tlearn: 3.5276643\ttotal: 40m 59s\tremaining: 9m 15s\n",
      "8159:\tlearn: 3.5274822\ttotal: 41m\tremaining: 9m 14s\n",
      "8160:\tlearn: 3.5272816\ttotal: 41m\tremaining: 9m 14s\n",
      "8161:\tlearn: 3.5271457\ttotal: 41m\tremaining: 9m 14s\n",
      "8162:\tlearn: 3.5268322\ttotal: 41m\tremaining: 9m 13s\n",
      "8163:\tlearn: 3.5266101\ttotal: 41m 1s\tremaining: 9m 13s\n",
      "8164:\tlearn: 3.5263617\ttotal: 41m 1s\tremaining: 9m 13s\n",
      "8165:\tlearn: 3.5262263\ttotal: 41m 1s\tremaining: 9m 12s\n",
      "8166:\tlearn: 3.5259772\ttotal: 41m 1s\tremaining: 9m 12s\n",
      "8167:\tlearn: 3.5257451\ttotal: 41m 2s\tremaining: 9m 12s\n",
      "8168:\tlearn: 3.5254681\ttotal: 41m 2s\tremaining: 9m 11s\n",
      "8169:\tlearn: 3.5251591\ttotal: 41m 2s\tremaining: 9m 11s\n",
      "8170:\tlearn: 3.5250194\ttotal: 41m 2s\tremaining: 9m 11s\n",
      "8171:\tlearn: 3.5247511\ttotal: 41m 3s\tremaining: 9m 10s\n",
      "8172:\tlearn: 3.5245561\ttotal: 41m 3s\tremaining: 9m 10s\n",
      "8173:\tlearn: 3.5244178\ttotal: 41m 3s\tremaining: 9m 10s\n",
      "8174:\tlearn: 3.5240136\ttotal: 41m 3s\tremaining: 9m 10s\n",
      "8175:\tlearn: 3.5237569\ttotal: 41m 4s\tremaining: 9m 9s\n",
      "8176:\tlearn: 3.5233645\ttotal: 41m 4s\tremaining: 9m 9s\n",
      "8177:\tlearn: 3.5231103\ttotal: 41m 4s\tremaining: 9m 9s\n",
      "8178:\tlearn: 3.5228891\ttotal: 41m 4s\tremaining: 9m 8s\n",
      "8179:\tlearn: 3.5224698\ttotal: 41m 5s\tremaining: 9m 8s\n",
      "8180:\tlearn: 3.5221560\ttotal: 41m 5s\tremaining: 9m 8s\n",
      "8181:\tlearn: 3.5218604\ttotal: 41m 5s\tremaining: 9m 7s\n",
      "8182:\tlearn: 3.5216129\ttotal: 41m 6s\tremaining: 9m 7s\n",
      "8183:\tlearn: 3.5213926\ttotal: 41m 6s\tremaining: 9m 7s\n",
      "8184:\tlearn: 3.5211275\ttotal: 41m 6s\tremaining: 9m 6s\n",
      "8185:\tlearn: 3.5207217\ttotal: 41m 6s\tremaining: 9m 6s\n",
      "8186:\tlearn: 3.5205401\ttotal: 41m 7s\tremaining: 9m 6s\n",
      "8187:\tlearn: 3.5202132\ttotal: 41m 7s\tremaining: 9m 6s\n",
      "8188:\tlearn: 3.5198538\ttotal: 41m 7s\tremaining: 9m 5s\n",
      "8189:\tlearn: 3.5195116\ttotal: 41m 8s\tremaining: 9m 5s\n",
      "8190:\tlearn: 3.5192557\ttotal: 41m 8s\tremaining: 9m 5s\n",
      "8191:\tlearn: 3.5190265\ttotal: 41m 8s\tremaining: 9m 4s\n",
      "8192:\tlearn: 3.5187916\ttotal: 41m 8s\tremaining: 9m 4s\n",
      "8193:\tlearn: 3.5184611\ttotal: 41m 9s\tremaining: 9m 4s\n",
      "8194:\tlearn: 3.5181953\ttotal: 41m 9s\tremaining: 9m 3s\n",
      "8195:\tlearn: 3.5179816\ttotal: 41m 9s\tremaining: 9m 3s\n",
      "8196:\tlearn: 3.5177100\ttotal: 41m 10s\tremaining: 9m 3s\n",
      "8197:\tlearn: 3.5174471\ttotal: 41m 10s\tremaining: 9m 3s\n",
      "8198:\tlearn: 3.5172126\ttotal: 41m 10s\tremaining: 9m 2s\n",
      "8199:\tlearn: 3.5169306\ttotal: 41m 10s\tremaining: 9m 2s\n",
      "8200:\tlearn: 3.5166210\ttotal: 41m 11s\tremaining: 9m 2s\n",
      "8201:\tlearn: 3.5163769\ttotal: 41m 11s\tremaining: 9m 1s\n",
      "8202:\tlearn: 3.5161672\ttotal: 41m 11s\tremaining: 9m 1s\n",
      "8203:\tlearn: 3.5159286\ttotal: 41m 11s\tremaining: 9m 1s\n",
      "8204:\tlearn: 3.5156518\ttotal: 41m 12s\tremaining: 9m\n",
      "8205:\tlearn: 3.5153809\ttotal: 41m 12s\tremaining: 9m\n",
      "8206:\tlearn: 3.5151244\ttotal: 41m 12s\tremaining: 9m\n",
      "8207:\tlearn: 3.5149290\ttotal: 41m 12s\tremaining: 8m 59s\n",
      "8208:\tlearn: 3.5146407\ttotal: 41m 13s\tremaining: 8m 59s\n",
      "8209:\tlearn: 3.5145519\ttotal: 41m 13s\tremaining: 8m 59s\n",
      "8210:\tlearn: 3.5142589\ttotal: 41m 13s\tremaining: 8m 58s\n",
      "8211:\tlearn: 3.5141246\ttotal: 41m 14s\tremaining: 8m 58s\n",
      "8212:\tlearn: 3.5138879\ttotal: 41m 14s\tremaining: 8m 58s\n",
      "8213:\tlearn: 3.5137231\ttotal: 41m 14s\tremaining: 8m 58s\n",
      "8214:\tlearn: 3.5133624\ttotal: 41m 14s\tremaining: 8m 57s\n",
      "8215:\tlearn: 3.5130555\ttotal: 41m 15s\tremaining: 8m 57s\n",
      "8216:\tlearn: 3.5127452\ttotal: 41m 15s\tremaining: 8m 57s\n",
      "8217:\tlearn: 3.5124653\ttotal: 41m 15s\tremaining: 8m 56s\n",
      "8218:\tlearn: 3.5122480\ttotal: 41m 16s\tremaining: 8m 56s\n",
      "8219:\tlearn: 3.5119193\ttotal: 41m 16s\tremaining: 8m 56s\n",
      "8220:\tlearn: 3.5116949\ttotal: 41m 16s\tremaining: 8m 55s\n",
      "8221:\tlearn: 3.5113746\ttotal: 41m 16s\tremaining: 8m 55s\n",
      "8222:\tlearn: 3.5110540\ttotal: 41m 17s\tremaining: 8m 55s\n",
      "8223:\tlearn: 3.5107977\ttotal: 41m 17s\tremaining: 8m 55s\n",
      "8224:\tlearn: 3.5105352\ttotal: 41m 17s\tremaining: 8m 54s\n",
      "8225:\tlearn: 3.5102399\ttotal: 41m 18s\tremaining: 8m 54s\n",
      "8226:\tlearn: 3.5098786\ttotal: 41m 18s\tremaining: 8m 54s\n",
      "8227:\tlearn: 3.5096369\ttotal: 41m 18s\tremaining: 8m 53s\n",
      "8228:\tlearn: 3.5094988\ttotal: 41m 18s\tremaining: 8m 53s\n",
      "8229:\tlearn: 3.5091956\ttotal: 41m 19s\tremaining: 8m 53s\n",
      "8230:\tlearn: 3.5089895\ttotal: 41m 19s\tremaining: 8m 52s\n",
      "8231:\tlearn: 3.5087619\ttotal: 41m 19s\tremaining: 8m 52s\n",
      "8232:\tlearn: 3.5085276\ttotal: 41m 19s\tremaining: 8m 52s\n",
      "8233:\tlearn: 3.5082377\ttotal: 41m 20s\tremaining: 8m 51s\n",
      "8234:\tlearn: 3.5078207\ttotal: 41m 20s\tremaining: 8m 51s\n",
      "8235:\tlearn: 3.5075299\ttotal: 41m 20s\tremaining: 8m 51s\n",
      "8236:\tlearn: 3.5073005\ttotal: 41m 21s\tremaining: 8m 51s\n",
      "8237:\tlearn: 3.5069526\ttotal: 41m 21s\tremaining: 8m 50s\n",
      "8238:\tlearn: 3.5067512\ttotal: 41m 21s\tremaining: 8m 50s\n",
      "8239:\tlearn: 3.5065954\ttotal: 41m 21s\tremaining: 8m 50s\n",
      "8240:\tlearn: 3.5063103\ttotal: 41m 22s\tremaining: 8m 49s\n",
      "8241:\tlearn: 3.5061169\ttotal: 41m 22s\tremaining: 8m 49s\n",
      "8242:\tlearn: 3.5059061\ttotal: 41m 22s\tremaining: 8m 49s\n",
      "8243:\tlearn: 3.5055348\ttotal: 41m 23s\tremaining: 8m 48s\n",
      "8244:\tlearn: 3.5052486\ttotal: 41m 23s\tremaining: 8m 48s\n",
      "8245:\tlearn: 3.5050234\ttotal: 41m 23s\tremaining: 8m 48s\n",
      "8246:\tlearn: 3.5046577\ttotal: 41m 23s\tremaining: 8m 47s\n",
      "8247:\tlearn: 3.5042842\ttotal: 41m 24s\tremaining: 8m 47s\n",
      "8248:\tlearn: 3.5040916\ttotal: 41m 24s\tremaining: 8m 47s\n",
      "8249:\tlearn: 3.5039064\ttotal: 41m 24s\tremaining: 8m 47s\n",
      "8250:\tlearn: 3.5036585\ttotal: 41m 25s\tremaining: 8m 46s\n",
      "8251:\tlearn: 3.5033965\ttotal: 41m 25s\tremaining: 8m 46s\n",
      "8252:\tlearn: 3.5031959\ttotal: 41m 25s\tremaining: 8m 46s\n",
      "8253:\tlearn: 3.5029621\ttotal: 41m 25s\tremaining: 8m 45s\n",
      "8254:\tlearn: 3.5026451\ttotal: 41m 26s\tremaining: 8m 45s\n",
      "8255:\tlearn: 3.5023419\ttotal: 41m 26s\tremaining: 8m 45s\n",
      "8256:\tlearn: 3.5021718\ttotal: 41m 26s\tremaining: 8m 44s\n",
      "8257:\tlearn: 3.5019856\ttotal: 41m 26s\tremaining: 8m 44s\n",
      "8258:\tlearn: 3.5016644\ttotal: 41m 27s\tremaining: 8m 44s\n",
      "8259:\tlearn: 3.5013829\ttotal: 41m 27s\tremaining: 8m 44s\n",
      "8260:\tlearn: 3.5011546\ttotal: 41m 27s\tremaining: 8m 43s\n",
      "8261:\tlearn: 3.5009115\ttotal: 41m 28s\tremaining: 8m 43s\n",
      "8262:\tlearn: 3.5007093\ttotal: 41m 28s\tremaining: 8m 43s\n",
      "8263:\tlearn: 3.5004494\ttotal: 41m 28s\tremaining: 8m 42s\n",
      "8264:\tlearn: 3.5002721\ttotal: 41m 28s\tremaining: 8m 42s\n",
      "8265:\tlearn: 3.5000694\ttotal: 41m 29s\tremaining: 8m 42s\n",
      "8266:\tlearn: 3.4998052\ttotal: 41m 29s\tremaining: 8m 41s\n",
      "8267:\tlearn: 3.4995272\ttotal: 41m 29s\tremaining: 8m 41s\n",
      "8268:\tlearn: 3.4992763\ttotal: 41m 30s\tremaining: 8m 41s\n",
      "8269:\tlearn: 3.4990306\ttotal: 41m 30s\tremaining: 8m 40s\n",
      "8270:\tlearn: 3.4988004\ttotal: 41m 30s\tremaining: 8m 40s\n",
      "8271:\tlearn: 3.4984382\ttotal: 41m 30s\tremaining: 8m 40s\n",
      "8272:\tlearn: 3.4981656\ttotal: 41m 31s\tremaining: 8m 40s\n",
      "8273:\tlearn: 3.4977810\ttotal: 41m 31s\tremaining: 8m 39s\n",
      "8274:\tlearn: 3.4976447\ttotal: 41m 31s\tremaining: 8m 39s\n",
      "8275:\tlearn: 3.4973586\ttotal: 41m 31s\tremaining: 8m 39s\n",
      "8276:\tlearn: 3.4970741\ttotal: 41m 32s\tremaining: 8m 38s\n",
      "8277:\tlearn: 3.4968230\ttotal: 41m 32s\tremaining: 8m 38s\n",
      "8278:\tlearn: 3.4966314\ttotal: 41m 32s\tremaining: 8m 38s\n",
      "8279:\tlearn: 3.4963598\ttotal: 41m 33s\tremaining: 8m 37s\n",
      "8280:\tlearn: 3.4961775\ttotal: 41m 33s\tremaining: 8m 37s\n",
      "8281:\tlearn: 3.4959381\ttotal: 41m 33s\tremaining: 8m 37s\n",
      "8282:\tlearn: 3.4957653\ttotal: 41m 33s\tremaining: 8m 36s\n",
      "8283:\tlearn: 3.4955655\ttotal: 41m 34s\tremaining: 8m 36s\n",
      "8284:\tlearn: 3.4953489\ttotal: 41m 34s\tremaining: 8m 36s\n",
      "8285:\tlearn: 3.4952038\ttotal: 41m 34s\tremaining: 8m 36s\n",
      "8286:\tlearn: 3.4949052\ttotal: 41m 34s\tremaining: 8m 35s\n",
      "8287:\tlearn: 3.4946321\ttotal: 41m 35s\tremaining: 8m 35s\n",
      "8288:\tlearn: 3.4944198\ttotal: 41m 35s\tremaining: 8m 35s\n",
      "8289:\tlearn: 3.4941943\ttotal: 41m 35s\tremaining: 8m 34s\n",
      "8290:\tlearn: 3.4938261\ttotal: 41m 35s\tremaining: 8m 34s\n",
      "8291:\tlearn: 3.4937318\ttotal: 41m 36s\tremaining: 8m 34s\n",
      "8292:\tlearn: 3.4934441\ttotal: 41m 36s\tremaining: 8m 33s\n",
      "8293:\tlearn: 3.4932002\ttotal: 41m 36s\tremaining: 8m 33s\n",
      "8294:\tlearn: 3.4929506\ttotal: 41m 36s\tremaining: 8m 33s\n",
      "8295:\tlearn: 3.4927050\ttotal: 41m 37s\tremaining: 8m 32s\n",
      "8296:\tlearn: 3.4923239\ttotal: 41m 37s\tremaining: 8m 32s\n",
      "8297:\tlearn: 3.4920638\ttotal: 41m 37s\tremaining: 8m 32s\n",
      "8298:\tlearn: 3.4918647\ttotal: 41m 38s\tremaining: 8m 32s\n",
      "8299:\tlearn: 3.4915858\ttotal: 41m 38s\tremaining: 8m 31s\n",
      "8300:\tlearn: 3.4912554\ttotal: 41m 38s\tremaining: 8m 31s\n",
      "8301:\tlearn: 3.4911017\ttotal: 41m 38s\tremaining: 8m 31s\n",
      "8302:\tlearn: 3.4909119\ttotal: 41m 39s\tremaining: 8m 30s\n",
      "8303:\tlearn: 3.4905356\ttotal: 41m 39s\tremaining: 8m 30s\n",
      "8304:\tlearn: 3.4903887\ttotal: 41m 39s\tremaining: 8m 30s\n",
      "8305:\tlearn: 3.4900968\ttotal: 41m 39s\tremaining: 8m 29s\n",
      "8306:\tlearn: 3.4899215\ttotal: 41m 40s\tremaining: 8m 29s\n",
      "8307:\tlearn: 3.4895640\ttotal: 41m 40s\tremaining: 8m 29s\n",
      "8308:\tlearn: 3.4893744\ttotal: 41m 40s\tremaining: 8m 28s\n",
      "8309:\tlearn: 3.4891454\ttotal: 41m 41s\tremaining: 8m 28s\n",
      "8310:\tlearn: 3.4889644\ttotal: 41m 41s\tremaining: 8m 28s\n",
      "8311:\tlearn: 3.4887135\ttotal: 41m 41s\tremaining: 8m 28s\n",
      "8312:\tlearn: 3.4884741\ttotal: 41m 41s\tremaining: 8m 27s\n",
      "8313:\tlearn: 3.4882378\ttotal: 41m 41s\tremaining: 8m 27s\n",
      "8314:\tlearn: 3.4879876\ttotal: 41m 42s\tremaining: 8m 27s\n",
      "8315:\tlearn: 3.4876308\ttotal: 41m 42s\tremaining: 8m 26s\n",
      "8316:\tlearn: 3.4873672\ttotal: 41m 42s\tremaining: 8m 26s\n",
      "8317:\tlearn: 3.4871265\ttotal: 41m 43s\tremaining: 8m 26s\n",
      "8318:\tlearn: 3.4869667\ttotal: 41m 43s\tremaining: 8m 25s\n",
      "8319:\tlearn: 3.4867478\ttotal: 41m 43s\tremaining: 8m 25s\n",
      "8320:\tlearn: 3.4865376\ttotal: 41m 43s\tremaining: 8m 25s\n",
      "8321:\tlearn: 3.4862121\ttotal: 41m 44s\tremaining: 8m 24s\n",
      "8322:\tlearn: 3.4859272\ttotal: 41m 44s\tremaining: 8m 24s\n",
      "8323:\tlearn: 3.4855694\ttotal: 41m 44s\tremaining: 8m 24s\n",
      "8324:\tlearn: 3.4852978\ttotal: 41m 45s\tremaining: 8m 24s\n",
      "8325:\tlearn: 3.4850930\ttotal: 41m 45s\tremaining: 8m 23s\n",
      "8326:\tlearn: 3.4848126\ttotal: 41m 45s\tremaining: 8m 23s\n",
      "8327:\tlearn: 3.4845125\ttotal: 41m 45s\tremaining: 8m 23s\n",
      "8328:\tlearn: 3.4843014\ttotal: 41m 46s\tremaining: 8m 22s\n",
      "8329:\tlearn: 3.4840819\ttotal: 41m 46s\tremaining: 8m 22s\n",
      "8330:\tlearn: 3.4838290\ttotal: 41m 46s\tremaining: 8m 22s\n",
      "8331:\tlearn: 3.4835743\ttotal: 41m 47s\tremaining: 8m 21s\n",
      "8332:\tlearn: 3.4833654\ttotal: 41m 47s\tremaining: 8m 21s\n",
      "8333:\tlearn: 3.4831458\ttotal: 41m 47s\tremaining: 8m 21s\n",
      "8334:\tlearn: 3.4829411\ttotal: 41m 47s\tremaining: 8m 20s\n",
      "8335:\tlearn: 3.4827012\ttotal: 41m 48s\tremaining: 8m 20s\n",
      "8336:\tlearn: 3.4823813\ttotal: 41m 48s\tremaining: 8m 20s\n",
      "8337:\tlearn: 3.4821664\ttotal: 41m 48s\tremaining: 8m 20s\n",
      "8338:\tlearn: 3.4818952\ttotal: 41m 48s\tremaining: 8m 19s\n",
      "8339:\tlearn: 3.4816460\ttotal: 41m 49s\tremaining: 8m 19s\n",
      "8340:\tlearn: 3.4814742\ttotal: 41m 49s\tremaining: 8m 19s\n",
      "8341:\tlearn: 3.4812137\ttotal: 41m 49s\tremaining: 8m 18s\n",
      "8342:\tlearn: 3.4811137\ttotal: 41m 49s\tremaining: 8m 18s\n",
      "8343:\tlearn: 3.4808727\ttotal: 41m 50s\tremaining: 8m 18s\n",
      "8344:\tlearn: 3.4805999\ttotal: 41m 50s\tremaining: 8m 17s\n",
      "8345:\tlearn: 3.4804742\ttotal: 41m 50s\tremaining: 8m 17s\n",
      "8346:\tlearn: 3.4801999\ttotal: 41m 50s\tremaining: 8m 17s\n",
      "8347:\tlearn: 3.4799408\ttotal: 41m 51s\tremaining: 8m 16s\n",
      "8348:\tlearn: 3.4797946\ttotal: 41m 51s\tremaining: 8m 16s\n",
      "8349:\tlearn: 3.4795628\ttotal: 41m 51s\tremaining: 8m 16s\n",
      "8350:\tlearn: 3.4793151\ttotal: 41m 52s\tremaining: 8m 16s\n",
      "8351:\tlearn: 3.4791753\ttotal: 41m 52s\tremaining: 8m 15s\n",
      "8352:\tlearn: 3.4790340\ttotal: 41m 52s\tremaining: 8m 15s\n",
      "8353:\tlearn: 3.4788572\ttotal: 41m 52s\tremaining: 8m 15s\n",
      "8354:\tlearn: 3.4785579\ttotal: 41m 53s\tremaining: 8m 14s\n",
      "8355:\tlearn: 3.4784075\ttotal: 41m 53s\tremaining: 8m 14s\n",
      "8356:\tlearn: 3.4781110\ttotal: 41m 53s\tremaining: 8m 14s\n",
      "8357:\tlearn: 3.4779645\ttotal: 41m 53s\tremaining: 8m 13s\n",
      "8358:\tlearn: 3.4777609\ttotal: 41m 54s\tremaining: 8m 13s\n",
      "8359:\tlearn: 3.4771994\ttotal: 41m 54s\tremaining: 8m 13s\n",
      "8360:\tlearn: 3.4770299\ttotal: 41m 54s\tremaining: 8m 12s\n",
      "8361:\tlearn: 3.4766972\ttotal: 41m 54s\tremaining: 8m 12s\n",
      "8362:\tlearn: 3.4765412\ttotal: 41m 55s\tremaining: 8m 12s\n",
      "8363:\tlearn: 3.4763814\ttotal: 41m 55s\tremaining: 8m 12s\n",
      "8364:\tlearn: 3.4761392\ttotal: 41m 55s\tremaining: 8m 11s\n",
      "8365:\tlearn: 3.4760010\ttotal: 41m 55s\tremaining: 8m 11s\n",
      "8366:\tlearn: 3.4757495\ttotal: 41m 56s\tremaining: 8m 11s\n",
      "8367:\tlearn: 3.4754663\ttotal: 41m 56s\tremaining: 8m 10s\n",
      "8368:\tlearn: 3.4751749\ttotal: 41m 56s\tremaining: 8m 10s\n",
      "8369:\tlearn: 3.4748168\ttotal: 41m 57s\tremaining: 8m 10s\n",
      "8370:\tlearn: 3.4743540\ttotal: 41m 57s\tremaining: 8m 9s\n",
      "8371:\tlearn: 3.4742000\ttotal: 41m 57s\tremaining: 8m 9s\n",
      "8372:\tlearn: 3.4739137\ttotal: 41m 57s\tremaining: 8m 9s\n",
      "8373:\tlearn: 3.4737625\ttotal: 41m 58s\tremaining: 8m 8s\n",
      "8374:\tlearn: 3.4736238\ttotal: 41m 58s\tremaining: 8m 8s\n",
      "8375:\tlearn: 3.4734048\ttotal: 41m 58s\tremaining: 8m 8s\n",
      "8376:\tlearn: 3.4732201\ttotal: 41m 58s\tremaining: 8m 8s\n",
      "8377:\tlearn: 3.4728846\ttotal: 41m 59s\tremaining: 8m 7s\n",
      "8378:\tlearn: 3.4727638\ttotal: 41m 59s\tremaining: 8m 7s\n",
      "8379:\tlearn: 3.4725286\ttotal: 41m 59s\tremaining: 8m 7s\n",
      "8380:\tlearn: 3.4723728\ttotal: 41m 59s\tremaining: 8m 6s\n",
      "8381:\tlearn: 3.4720361\ttotal: 42m\tremaining: 8m 6s\n",
      "8382:\tlearn: 3.4718791\ttotal: 42m\tremaining: 8m 6s\n",
      "8383:\tlearn: 3.4715438\ttotal: 42m\tremaining: 8m 5s\n",
      "8384:\tlearn: 3.4713404\ttotal: 42m\tremaining: 8m 5s\n",
      "8385:\tlearn: 3.4708982\ttotal: 42m 1s\tremaining: 8m 5s\n",
      "8386:\tlearn: 3.4707479\ttotal: 42m 1s\tremaining: 8m 4s\n",
      "8387:\tlearn: 3.4705630\ttotal: 42m 1s\tremaining: 8m 4s\n",
      "8388:\tlearn: 3.4701594\ttotal: 42m 2s\tremaining: 8m 4s\n",
      "8389:\tlearn: 3.4699563\ttotal: 42m 2s\tremaining: 8m 4s\n",
      "8390:\tlearn: 3.4697190\ttotal: 42m 2s\tremaining: 8m 3s\n",
      "8391:\tlearn: 3.4695343\ttotal: 42m 2s\tremaining: 8m 3s\n",
      "8392:\tlearn: 3.4693860\ttotal: 42m 3s\tremaining: 8m 3s\n",
      "8393:\tlearn: 3.4691889\ttotal: 42m 3s\tremaining: 8m 2s\n",
      "8394:\tlearn: 3.4689826\ttotal: 42m 3s\tremaining: 8m 2s\n",
      "8395:\tlearn: 3.4686572\ttotal: 42m 3s\tremaining: 8m 2s\n",
      "8396:\tlearn: 3.4683959\ttotal: 42m 4s\tremaining: 8m 1s\n",
      "8397:\tlearn: 3.4682026\ttotal: 42m 4s\tremaining: 8m 1s\n",
      "8398:\tlearn: 3.4679345\ttotal: 42m 4s\tremaining: 8m 1s\n",
      "8399:\tlearn: 3.4676314\ttotal: 42m 5s\tremaining: 8m\n",
      "8400:\tlearn: 3.4674555\ttotal: 42m 5s\tremaining: 8m\n",
      "8401:\tlearn: 3.4672394\ttotal: 42m 5s\tremaining: 8m\n",
      "8402:\tlearn: 3.4671234\ttotal: 42m 5s\tremaining: 8m\n",
      "8403:\tlearn: 3.4669348\ttotal: 42m 6s\tremaining: 7m 59s\n",
      "8404:\tlearn: 3.4666056\ttotal: 42m 6s\tremaining: 7m 59s\n",
      "8405:\tlearn: 3.4662800\ttotal: 42m 6s\tremaining: 7m 59s\n",
      "8406:\tlearn: 3.4659018\ttotal: 42m 7s\tremaining: 7m 58s\n",
      "8407:\tlearn: 3.4657089\ttotal: 42m 7s\tremaining: 7m 58s\n",
      "8408:\tlearn: 3.4653891\ttotal: 42m 7s\tremaining: 7m 58s\n",
      "8409:\tlearn: 3.4652558\ttotal: 42m 7s\tremaining: 7m 57s\n",
      "8410:\tlearn: 3.4650490\ttotal: 42m 8s\tremaining: 7m 57s\n",
      "8411:\tlearn: 3.4647922\ttotal: 42m 8s\tremaining: 7m 57s\n",
      "8412:\tlearn: 3.4645679\ttotal: 42m 8s\tremaining: 7m 56s\n",
      "8413:\tlearn: 3.4644686\ttotal: 42m 8s\tremaining: 7m 56s\n",
      "8414:\tlearn: 3.4642383\ttotal: 42m 9s\tremaining: 7m 56s\n",
      "8415:\tlearn: 3.4639798\ttotal: 42m 9s\tremaining: 7m 56s\n",
      "8416:\tlearn: 3.4636747\ttotal: 42m 9s\tremaining: 7m 55s\n",
      "8417:\tlearn: 3.4632525\ttotal: 42m 10s\tremaining: 7m 55s\n",
      "8418:\tlearn: 3.4630997\ttotal: 42m 10s\tremaining: 7m 55s\n",
      "8419:\tlearn: 3.4626822\ttotal: 42m 10s\tremaining: 7m 54s\n",
      "8420:\tlearn: 3.4623562\ttotal: 42m 10s\tremaining: 7m 54s\n",
      "8421:\tlearn: 3.4621847\ttotal: 42m 11s\tremaining: 7m 54s\n",
      "8422:\tlearn: 3.4617584\ttotal: 42m 11s\tremaining: 7m 53s\n",
      "8423:\tlearn: 3.4615490\ttotal: 42m 11s\tremaining: 7m 53s\n",
      "8424:\tlearn: 3.4612438\ttotal: 42m 12s\tremaining: 7m 53s\n",
      "8425:\tlearn: 3.4609859\ttotal: 42m 12s\tremaining: 7m 53s\n",
      "8426:\tlearn: 3.4607862\ttotal: 42m 12s\tremaining: 7m 52s\n",
      "8427:\tlearn: 3.4605487\ttotal: 42m 12s\tremaining: 7m 52s\n",
      "8428:\tlearn: 3.4603254\ttotal: 42m 13s\tremaining: 7m 52s\n",
      "8429:\tlearn: 3.4601679\ttotal: 42m 13s\tremaining: 7m 51s\n",
      "8430:\tlearn: 3.4599408\ttotal: 42m 13s\tremaining: 7m 51s\n",
      "8431:\tlearn: 3.4597692\ttotal: 42m 13s\tremaining: 7m 51s\n",
      "8432:\tlearn: 3.4596275\ttotal: 42m 14s\tremaining: 7m 50s\n",
      "8433:\tlearn: 3.4593257\ttotal: 42m 14s\tremaining: 7m 50s\n",
      "8434:\tlearn: 3.4590748\ttotal: 42m 14s\tremaining: 7m 50s\n",
      "8435:\tlearn: 3.4588815\ttotal: 42m 14s\tremaining: 7m 49s\n",
      "8436:\tlearn: 3.4586595\ttotal: 42m 15s\tremaining: 7m 49s\n",
      "8437:\tlearn: 3.4584373\ttotal: 42m 15s\tremaining: 7m 49s\n",
      "8438:\tlearn: 3.4581280\ttotal: 42m 15s\tremaining: 7m 49s\n",
      "8439:\tlearn: 3.4579012\ttotal: 42m 16s\tremaining: 7m 48s\n",
      "8440:\tlearn: 3.4575920\ttotal: 42m 16s\tremaining: 7m 48s\n",
      "8441:\tlearn: 3.4574293\ttotal: 42m 16s\tremaining: 7m 48s\n",
      "8442:\tlearn: 3.4572332\ttotal: 42m 16s\tremaining: 7m 47s\n",
      "8443:\tlearn: 3.4569221\ttotal: 42m 17s\tremaining: 7m 47s\n",
      "8444:\tlearn: 3.4566846\ttotal: 42m 17s\tremaining: 7m 47s\n",
      "8445:\tlearn: 3.4565260\ttotal: 42m 17s\tremaining: 7m 46s\n",
      "8446:\tlearn: 3.4562817\ttotal: 42m 18s\tremaining: 7m 46s\n",
      "8447:\tlearn: 3.4559578\ttotal: 42m 18s\tremaining: 7m 46s\n",
      "8448:\tlearn: 3.4556958\ttotal: 42m 18s\tremaining: 7m 46s\n",
      "8449:\tlearn: 3.4554730\ttotal: 42m 18s\tremaining: 7m 45s\n",
      "8450:\tlearn: 3.4552042\ttotal: 42m 19s\tremaining: 7m 45s\n",
      "8451:\tlearn: 3.4550242\ttotal: 42m 19s\tremaining: 7m 45s\n",
      "8452:\tlearn: 3.4548121\ttotal: 42m 19s\tremaining: 7m 44s\n",
      "8453:\tlearn: 3.4545079\ttotal: 42m 20s\tremaining: 7m 44s\n",
      "8454:\tlearn: 3.4542987\ttotal: 42m 20s\tremaining: 7m 44s\n",
      "8455:\tlearn: 3.4540173\ttotal: 42m 20s\tremaining: 7m 43s\n",
      "8456:\tlearn: 3.4537122\ttotal: 42m 20s\tremaining: 7m 43s\n",
      "8457:\tlearn: 3.4534021\ttotal: 42m 21s\tremaining: 7m 43s\n",
      "8458:\tlearn: 3.4531474\ttotal: 42m 21s\tremaining: 7m 43s\n",
      "8459:\tlearn: 3.4529154\ttotal: 42m 21s\tremaining: 7m 42s\n",
      "8460:\tlearn: 3.4527429\ttotal: 42m 22s\tremaining: 7m 42s\n",
      "8461:\tlearn: 3.4524457\ttotal: 42m 22s\tremaining: 7m 42s\n",
      "8462:\tlearn: 3.4520805\ttotal: 42m 22s\tremaining: 7m 41s\n",
      "8463:\tlearn: 3.4518625\ttotal: 42m 23s\tremaining: 7m 41s\n",
      "8464:\tlearn: 3.4515001\ttotal: 42m 23s\tremaining: 7m 41s\n",
      "8465:\tlearn: 3.4512387\ttotal: 42m 23s\tremaining: 7m 40s\n",
      "8466:\tlearn: 3.4509842\ttotal: 42m 23s\tremaining: 7m 40s\n",
      "8467:\tlearn: 3.4508004\ttotal: 42m 24s\tremaining: 7m 40s\n",
      "8468:\tlearn: 3.4504786\ttotal: 42m 24s\tremaining: 7m 39s\n",
      "8469:\tlearn: 3.4503003\ttotal: 42m 24s\tremaining: 7m 39s\n",
      "8470:\tlearn: 3.4499072\ttotal: 42m 25s\tremaining: 7m 39s\n",
      "8471:\tlearn: 3.4496790\ttotal: 42m 25s\tremaining: 7m 39s\n",
      "8472:\tlearn: 3.4494020\ttotal: 42m 25s\tremaining: 7m 38s\n",
      "8473:\tlearn: 3.4491481\ttotal: 42m 25s\tremaining: 7m 38s\n",
      "8474:\tlearn: 3.4489073\ttotal: 42m 26s\tremaining: 7m 38s\n",
      "8475:\tlearn: 3.4485830\ttotal: 42m 26s\tremaining: 7m 37s\n",
      "8476:\tlearn: 3.4482690\ttotal: 42m 26s\tremaining: 7m 37s\n",
      "8477:\tlearn: 3.4480197\ttotal: 42m 27s\tremaining: 7m 37s\n",
      "8478:\tlearn: 3.4477492\ttotal: 42m 27s\tremaining: 7m 36s\n",
      "8479:\tlearn: 3.4474969\ttotal: 42m 27s\tremaining: 7m 36s\n",
      "8480:\tlearn: 3.4472119\ttotal: 42m 27s\tremaining: 7m 36s\n",
      "8481:\tlearn: 3.4469634\ttotal: 42m 28s\tremaining: 7m 36s\n",
      "8482:\tlearn: 3.4468523\ttotal: 42m 28s\tremaining: 7m 35s\n",
      "8483:\tlearn: 3.4466535\ttotal: 42m 28s\tremaining: 7m 35s\n",
      "8484:\tlearn: 3.4464010\ttotal: 42m 28s\tremaining: 7m 35s\n",
      "8485:\tlearn: 3.4460917\ttotal: 42m 29s\tremaining: 7m 34s\n",
      "8486:\tlearn: 3.4458699\ttotal: 42m 29s\tremaining: 7m 34s\n",
      "8487:\tlearn: 3.4456225\ttotal: 42m 29s\tremaining: 7m 34s\n",
      "8488:\tlearn: 3.4453347\ttotal: 42m 29s\tremaining: 7m 33s\n",
      "8489:\tlearn: 3.4450942\ttotal: 42m 30s\tremaining: 7m 33s\n",
      "8490:\tlearn: 3.4449081\ttotal: 42m 30s\tremaining: 7m 33s\n",
      "8491:\tlearn: 3.4446511\ttotal: 42m 30s\tremaining: 7m 32s\n",
      "8492:\tlearn: 3.4445029\ttotal: 42m 31s\tremaining: 7m 32s\n",
      "8493:\tlearn: 3.4441435\ttotal: 42m 31s\tremaining: 7m 32s\n",
      "8494:\tlearn: 3.4439776\ttotal: 42m 31s\tremaining: 7m 32s\n",
      "8495:\tlearn: 3.4436601\ttotal: 42m 32s\tremaining: 7m 31s\n",
      "8496:\tlearn: 3.4433099\ttotal: 42m 32s\tremaining: 7m 31s\n",
      "8497:\tlearn: 3.4428116\ttotal: 42m 32s\tremaining: 7m 31s\n",
      "8498:\tlearn: 3.4425136\ttotal: 42m 33s\tremaining: 7m 30s\n",
      "8499:\tlearn: 3.4423189\ttotal: 42m 33s\tremaining: 7m 30s\n",
      "8500:\tlearn: 3.4420917\ttotal: 42m 33s\tremaining: 7m 30s\n",
      "8501:\tlearn: 3.4417775\ttotal: 42m 34s\tremaining: 7m 30s\n",
      "8502:\tlearn: 3.4415014\ttotal: 42m 34s\tremaining: 7m 29s\n",
      "8503:\tlearn: 3.4413704\ttotal: 42m 34s\tremaining: 7m 29s\n",
      "8504:\tlearn: 3.4410384\ttotal: 42m 34s\tremaining: 7m 29s\n",
      "8505:\tlearn: 3.4407948\ttotal: 42m 35s\tremaining: 7m 28s\n",
      "8506:\tlearn: 3.4405554\ttotal: 42m 35s\tremaining: 7m 28s\n",
      "8507:\tlearn: 3.4403688\ttotal: 42m 35s\tremaining: 7m 28s\n",
      "8508:\tlearn: 3.4401700\ttotal: 42m 36s\tremaining: 7m 27s\n",
      "8509:\tlearn: 3.4399468\ttotal: 42m 36s\tremaining: 7m 27s\n",
      "8510:\tlearn: 3.4397132\ttotal: 42m 36s\tremaining: 7m 27s\n",
      "8511:\tlearn: 3.4395091\ttotal: 42m 36s\tremaining: 7m 26s\n",
      "8512:\tlearn: 3.4393531\ttotal: 42m 37s\tremaining: 7m 26s\n",
      "8513:\tlearn: 3.4389052\ttotal: 42m 37s\tremaining: 7m 26s\n",
      "8514:\tlearn: 3.4386606\ttotal: 42m 37s\tremaining: 7m 26s\n",
      "8515:\tlearn: 3.4384370\ttotal: 42m 38s\tremaining: 7m 25s\n",
      "8516:\tlearn: 3.4381596\ttotal: 42m 38s\tremaining: 7m 25s\n",
      "8517:\tlearn: 3.4378482\ttotal: 42m 38s\tremaining: 7m 25s\n",
      "8518:\tlearn: 3.4375721\ttotal: 42m 38s\tremaining: 7m 24s\n",
      "8519:\tlearn: 3.4372112\ttotal: 42m 39s\tremaining: 7m 24s\n",
      "8520:\tlearn: 3.4367961\ttotal: 42m 39s\tremaining: 7m 24s\n",
      "8521:\tlearn: 3.4365045\ttotal: 42m 39s\tremaining: 7m 23s\n",
      "8522:\tlearn: 3.4361678\ttotal: 42m 40s\tremaining: 7m 23s\n",
      "8523:\tlearn: 3.4358404\ttotal: 42m 40s\tremaining: 7m 23s\n",
      "8524:\tlearn: 3.4355942\ttotal: 42m 40s\tremaining: 7m 23s\n",
      "8525:\tlearn: 3.4353935\ttotal: 42m 40s\tremaining: 7m 22s\n",
      "8526:\tlearn: 3.4350633\ttotal: 42m 41s\tremaining: 7m 22s\n",
      "8527:\tlearn: 3.4348127\ttotal: 42m 41s\tremaining: 7m 22s\n",
      "8528:\tlearn: 3.4346041\ttotal: 42m 41s\tremaining: 7m 21s\n",
      "8529:\tlearn: 3.4344351\ttotal: 42m 42s\tremaining: 7m 21s\n",
      "8530:\tlearn: 3.4341194\ttotal: 42m 42s\tremaining: 7m 21s\n",
      "8531:\tlearn: 3.4339843\ttotal: 42m 42s\tremaining: 7m 20s\n",
      "8532:\tlearn: 3.4337169\ttotal: 42m 42s\tremaining: 7m 20s\n",
      "8533:\tlearn: 3.4334422\ttotal: 42m 43s\tremaining: 7m 20s\n",
      "8534:\tlearn: 3.4331958\ttotal: 42m 43s\tremaining: 7m 20s\n",
      "8535:\tlearn: 3.4329232\ttotal: 42m 43s\tremaining: 7m 19s\n",
      "8536:\tlearn: 3.4326828\ttotal: 42m 43s\tremaining: 7m 19s\n",
      "8537:\tlearn: 3.4324910\ttotal: 42m 44s\tremaining: 7m 19s\n",
      "8538:\tlearn: 3.4321742\ttotal: 42m 44s\tremaining: 7m 18s\n",
      "8539:\tlearn: 3.4319698\ttotal: 42m 44s\tremaining: 7m 18s\n",
      "8540:\tlearn: 3.4317388\ttotal: 42m 44s\tremaining: 7m 18s\n",
      "8541:\tlearn: 3.4315292\ttotal: 42m 45s\tremaining: 7m 17s\n",
      "8542:\tlearn: 3.4312993\ttotal: 42m 45s\tremaining: 7m 17s\n",
      "8543:\tlearn: 3.4310414\ttotal: 42m 45s\tremaining: 7m 17s\n",
      "8544:\tlearn: 3.4307516\ttotal: 42m 45s\tremaining: 7m 16s\n",
      "8545:\tlearn: 3.4304964\ttotal: 42m 46s\tremaining: 7m 16s\n",
      "8546:\tlearn: 3.4302562\ttotal: 42m 46s\tremaining: 7m 16s\n",
      "8547:\tlearn: 3.4301024\ttotal: 42m 46s\tremaining: 7m 16s\n",
      "8548:\tlearn: 3.4298829\ttotal: 42m 47s\tremaining: 7m 15s\n",
      "8549:\tlearn: 3.4295829\ttotal: 42m 47s\tremaining: 7m 15s\n",
      "8550:\tlearn: 3.4292363\ttotal: 42m 47s\tremaining: 7m 15s\n",
      "8551:\tlearn: 3.4287746\ttotal: 42m 48s\tremaining: 7m 14s\n",
      "8552:\tlearn: 3.4285316\ttotal: 42m 48s\tremaining: 7m 14s\n",
      "8553:\tlearn: 3.4282986\ttotal: 42m 48s\tremaining: 7m 14s\n",
      "8554:\tlearn: 3.4281306\ttotal: 42m 48s\tremaining: 7m 13s\n",
      "8555:\tlearn: 3.4278889\ttotal: 42m 49s\tremaining: 7m 13s\n",
      "8556:\tlearn: 3.4276852\ttotal: 42m 49s\tremaining: 7m 13s\n",
      "8557:\tlearn: 3.4274977\ttotal: 42m 49s\tremaining: 7m 12s\n",
      "8558:\tlearn: 3.4272594\ttotal: 42m 49s\tremaining: 7m 12s\n",
      "8559:\tlearn: 3.4270604\ttotal: 42m 50s\tremaining: 7m 12s\n",
      "8560:\tlearn: 3.4267669\ttotal: 42m 50s\tremaining: 7m 12s\n",
      "8561:\tlearn: 3.4263836\ttotal: 42m 50s\tremaining: 7m 11s\n",
      "8562:\tlearn: 3.4262220\ttotal: 42m 51s\tremaining: 7m 11s\n",
      "8563:\tlearn: 3.4260103\ttotal: 42m 51s\tremaining: 7m 11s\n",
      "8564:\tlearn: 3.4258195\ttotal: 42m 51s\tremaining: 7m 10s\n",
      "8565:\tlearn: 3.4255726\ttotal: 42m 51s\tremaining: 7m 10s\n",
      "8566:\tlearn: 3.4253330\ttotal: 42m 52s\tremaining: 7m 10s\n",
      "8567:\tlearn: 3.4250740\ttotal: 42m 52s\tremaining: 7m 9s\n",
      "8568:\tlearn: 3.4246437\ttotal: 42m 52s\tremaining: 7m 9s\n",
      "8569:\tlearn: 3.4243776\ttotal: 42m 52s\tremaining: 7m 9s\n",
      "8570:\tlearn: 3.4240751\ttotal: 42m 53s\tremaining: 7m 9s\n",
      "8571:\tlearn: 3.4238964\ttotal: 42m 53s\tremaining: 7m 8s\n",
      "8572:\tlearn: 3.4236586\ttotal: 42m 53s\tremaining: 7m 8s\n",
      "8573:\tlearn: 3.4234809\ttotal: 42m 54s\tremaining: 7m 8s\n",
      "8574:\tlearn: 3.4232616\ttotal: 42m 54s\tremaining: 7m 7s\n",
      "8575:\tlearn: 3.4229789\ttotal: 42m 54s\tremaining: 7m 7s\n",
      "8576:\tlearn: 3.4227582\ttotal: 42m 55s\tremaining: 7m 7s\n",
      "8577:\tlearn: 3.4223116\ttotal: 42m 55s\tremaining: 7m 6s\n",
      "8578:\tlearn: 3.4221145\ttotal: 42m 55s\tremaining: 7m 6s\n",
      "8579:\tlearn: 3.4217617\ttotal: 42m 55s\tremaining: 7m 6s\n",
      "8580:\tlearn: 3.4214842\ttotal: 42m 56s\tremaining: 7m 6s\n",
      "8581:\tlearn: 3.4211512\ttotal: 42m 56s\tremaining: 7m 5s\n",
      "8582:\tlearn: 3.4209521\ttotal: 42m 56s\tremaining: 7m 5s\n",
      "8583:\tlearn: 3.4207825\ttotal: 42m 56s\tremaining: 7m 5s\n",
      "8584:\tlearn: 3.4205221\ttotal: 42m 57s\tremaining: 7m 4s\n",
      "8585:\tlearn: 3.4202683\ttotal: 42m 57s\tremaining: 7m 4s\n",
      "8586:\tlearn: 3.4200553\ttotal: 42m 57s\tremaining: 7m 4s\n",
      "8587:\tlearn: 3.4197838\ttotal: 42m 58s\tremaining: 7m 3s\n",
      "8588:\tlearn: 3.4195695\ttotal: 42m 58s\tremaining: 7m 3s\n",
      "8589:\tlearn: 3.4194256\ttotal: 42m 58s\tremaining: 7m 3s\n",
      "8590:\tlearn: 3.4191361\ttotal: 42m 58s\tremaining: 7m 2s\n",
      "8591:\tlearn: 3.4189239\ttotal: 42m 59s\tremaining: 7m 2s\n",
      "8592:\tlearn: 3.4186324\ttotal: 42m 59s\tremaining: 7m 2s\n",
      "8593:\tlearn: 3.4182338\ttotal: 42m 59s\tremaining: 7m 2s\n",
      "8594:\tlearn: 3.4179640\ttotal: 43m\tremaining: 7m 1s\n",
      "8595:\tlearn: 3.4176892\ttotal: 43m\tremaining: 7m 1s\n",
      "8596:\tlearn: 3.4174676\ttotal: 43m\tremaining: 7m 1s\n",
      "8597:\tlearn: 3.4170892\ttotal: 43m\tremaining: 7m\n",
      "8598:\tlearn: 3.4168853\ttotal: 43m 1s\tremaining: 7m\n",
      "8599:\tlearn: 3.4165393\ttotal: 43m 1s\tremaining: 7m\n",
      "8600:\tlearn: 3.4162974\ttotal: 43m 1s\tremaining: 6m 59s\n",
      "8601:\tlearn: 3.4159612\ttotal: 43m 1s\tremaining: 6m 59s\n",
      "8602:\tlearn: 3.4157744\ttotal: 43m 2s\tremaining: 6m 59s\n",
      "8603:\tlearn: 3.4153896\ttotal: 43m 2s\tremaining: 6m 59s\n",
      "8604:\tlearn: 3.4150177\ttotal: 43m 2s\tremaining: 6m 58s\n",
      "8605:\tlearn: 3.4147810\ttotal: 43m 3s\tremaining: 6m 58s\n",
      "8606:\tlearn: 3.4145467\ttotal: 43m 3s\tremaining: 6m 58s\n",
      "8607:\tlearn: 3.4143584\ttotal: 43m 3s\tremaining: 6m 57s\n",
      "8608:\tlearn: 3.4141539\ttotal: 43m 3s\tremaining: 6m 57s\n",
      "8609:\tlearn: 3.4139355\ttotal: 43m 4s\tremaining: 6m 57s\n",
      "8610:\tlearn: 3.4135991\ttotal: 43m 4s\tremaining: 6m 56s\n",
      "8611:\tlearn: 3.4133102\ttotal: 43m 4s\tremaining: 6m 56s\n",
      "8612:\tlearn: 3.4131453\ttotal: 43m 4s\tremaining: 6m 56s\n",
      "8613:\tlearn: 3.4128675\ttotal: 43m 5s\tremaining: 6m 55s\n",
      "8614:\tlearn: 3.4126164\ttotal: 43m 5s\tremaining: 6m 55s\n",
      "8615:\tlearn: 3.4123167\ttotal: 43m 5s\tremaining: 6m 55s\n",
      "8616:\tlearn: 3.4120706\ttotal: 43m 5s\tremaining: 6m 55s\n",
      "8617:\tlearn: 3.4117551\ttotal: 43m 6s\tremaining: 6m 54s\n",
      "8618:\tlearn: 3.4116232\ttotal: 43m 6s\tremaining: 6m 54s\n",
      "8619:\tlearn: 3.4114097\ttotal: 43m 6s\tremaining: 6m 54s\n",
      "8620:\tlearn: 3.4112530\ttotal: 43m 7s\tremaining: 6m 53s\n",
      "8621:\tlearn: 3.4110403\ttotal: 43m 7s\tremaining: 6m 53s\n",
      "8622:\tlearn: 3.4107828\ttotal: 43m 7s\tremaining: 6m 53s\n",
      "8623:\tlearn: 3.4103914\ttotal: 43m 8s\tremaining: 6m 52s\n",
      "8624:\tlearn: 3.4101108\ttotal: 43m 8s\tremaining: 6m 52s\n",
      "8625:\tlearn: 3.4099433\ttotal: 43m 8s\tremaining: 6m 52s\n",
      "8626:\tlearn: 3.4097286\ttotal: 43m 8s\tremaining: 6m 52s\n",
      "8627:\tlearn: 3.4094415\ttotal: 43m 9s\tremaining: 6m 51s\n",
      "8628:\tlearn: 3.4092307\ttotal: 43m 9s\tremaining: 6m 51s\n",
      "8629:\tlearn: 3.4087992\ttotal: 43m 9s\tremaining: 6m 51s\n",
      "8630:\tlearn: 3.4085222\ttotal: 43m 10s\tremaining: 6m 50s\n",
      "8631:\tlearn: 3.4081184\ttotal: 43m 10s\tremaining: 6m 50s\n",
      "8632:\tlearn: 3.4078256\ttotal: 43m 10s\tremaining: 6m 50s\n",
      "8633:\tlearn: 3.4075507\ttotal: 43m 10s\tremaining: 6m 49s\n",
      "8634:\tlearn: 3.4073186\ttotal: 43m 11s\tremaining: 6m 49s\n",
      "8635:\tlearn: 3.4069302\ttotal: 43m 11s\tremaining: 6m 49s\n",
      "8636:\tlearn: 3.4067145\ttotal: 43m 11s\tremaining: 6m 49s\n",
      "8637:\tlearn: 3.4064576\ttotal: 43m 12s\tremaining: 6m 48s\n",
      "8638:\tlearn: 3.4062515\ttotal: 43m 12s\tremaining: 6m 48s\n",
      "8639:\tlearn: 3.4060723\ttotal: 43m 12s\tremaining: 6m 48s\n",
      "8640:\tlearn: 3.4059325\ttotal: 43m 12s\tremaining: 6m 47s\n",
      "8641:\tlearn: 3.4055730\ttotal: 43m 13s\tremaining: 6m 47s\n",
      "8642:\tlearn: 3.4053533\ttotal: 43m 13s\tremaining: 6m 47s\n",
      "8643:\tlearn: 3.4051090\ttotal: 43m 13s\tremaining: 6m 46s\n",
      "8644:\tlearn: 3.4048345\ttotal: 43m 14s\tremaining: 6m 46s\n",
      "8645:\tlearn: 3.4046049\ttotal: 43m 14s\tremaining: 6m 46s\n",
      "8646:\tlearn: 3.4043573\ttotal: 43m 14s\tremaining: 6m 45s\n",
      "8647:\tlearn: 3.4040035\ttotal: 43m 14s\tremaining: 6m 45s\n",
      "8648:\tlearn: 3.4038633\ttotal: 43m 15s\tremaining: 6m 45s\n",
      "8649:\tlearn: 3.4036847\ttotal: 43m 15s\tremaining: 6m 45s\n",
      "8650:\tlearn: 3.4034100\ttotal: 43m 15s\tremaining: 6m 44s\n",
      "8651:\tlearn: 3.4030237\ttotal: 43m 15s\tremaining: 6m 44s\n",
      "8652:\tlearn: 3.4025758\ttotal: 43m 16s\tremaining: 6m 44s\n",
      "8653:\tlearn: 3.4023194\ttotal: 43m 16s\tremaining: 6m 43s\n",
      "8654:\tlearn: 3.4020317\ttotal: 43m 16s\tremaining: 6m 43s\n",
      "8655:\tlearn: 3.4018445\ttotal: 43m 17s\tremaining: 6m 43s\n",
      "8656:\tlearn: 3.4015720\ttotal: 43m 17s\tremaining: 6m 42s\n",
      "8657:\tlearn: 3.4012685\ttotal: 43m 17s\tremaining: 6m 42s\n",
      "8658:\tlearn: 3.4010700\ttotal: 43m 18s\tremaining: 6m 42s\n",
      "8659:\tlearn: 3.4009412\ttotal: 43m 18s\tremaining: 6m 42s\n",
      "8660:\tlearn: 3.4006972\ttotal: 43m 18s\tremaining: 6m 41s\n",
      "8661:\tlearn: 3.4003937\ttotal: 43m 18s\tremaining: 6m 41s\n",
      "8662:\tlearn: 3.4001297\ttotal: 43m 19s\tremaining: 6m 41s\n",
      "8663:\tlearn: 3.3999309\ttotal: 43m 19s\tremaining: 6m 40s\n",
      "8664:\tlearn: 3.3997343\ttotal: 43m 19s\tremaining: 6m 40s\n",
      "8665:\tlearn: 3.3995208\ttotal: 43m 19s\tremaining: 6m 40s\n",
      "8666:\tlearn: 3.3992688\ttotal: 43m 20s\tremaining: 6m 39s\n",
      "8667:\tlearn: 3.3988994\ttotal: 43m 20s\tremaining: 6m 39s\n",
      "8668:\tlearn: 3.3987203\ttotal: 43m 20s\tremaining: 6m 39s\n",
      "8669:\tlearn: 3.3985773\ttotal: 43m 21s\tremaining: 6m 39s\n",
      "8670:\tlearn: 3.3981867\ttotal: 43m 21s\tremaining: 6m 38s\n",
      "8671:\tlearn: 3.3978524\ttotal: 43m 21s\tremaining: 6m 38s\n",
      "8672:\tlearn: 3.3975463\ttotal: 43m 21s\tremaining: 6m 38s\n",
      "8673:\tlearn: 3.3972970\ttotal: 43m 22s\tremaining: 6m 37s\n",
      "8674:\tlearn: 3.3971248\ttotal: 43m 22s\tremaining: 6m 37s\n",
      "8675:\tlearn: 3.3969736\ttotal: 43m 22s\tremaining: 6m 37s\n",
      "8676:\tlearn: 3.3967304\ttotal: 43m 23s\tremaining: 6m 36s\n",
      "8677:\tlearn: 3.3965174\ttotal: 43m 23s\tremaining: 6m 36s\n",
      "8678:\tlearn: 3.3962448\ttotal: 43m 23s\tremaining: 6m 36s\n",
      "8679:\tlearn: 3.3960418\ttotal: 43m 23s\tremaining: 6m 35s\n",
      "8680:\tlearn: 3.3958003\ttotal: 43m 24s\tremaining: 6m 35s\n",
      "8681:\tlearn: 3.3956737\ttotal: 43m 24s\tremaining: 6m 35s\n",
      "8682:\tlearn: 3.3955088\ttotal: 43m 24s\tremaining: 6m 35s\n",
      "8683:\tlearn: 3.3952618\ttotal: 43m 24s\tremaining: 6m 34s\n",
      "8684:\tlearn: 3.3950034\ttotal: 43m 25s\tremaining: 6m 34s\n",
      "8685:\tlearn: 3.3947475\ttotal: 43m 25s\tremaining: 6m 34s\n",
      "8686:\tlearn: 3.3944503\ttotal: 43m 25s\tremaining: 6m 33s\n",
      "8687:\tlearn: 3.3941717\ttotal: 43m 25s\tremaining: 6m 33s\n",
      "8688:\tlearn: 3.3939199\ttotal: 43m 26s\tremaining: 6m 33s\n",
      "8689:\tlearn: 3.3936196\ttotal: 43m 26s\tremaining: 6m 32s\n",
      "8690:\tlearn: 3.3932630\ttotal: 43m 26s\tremaining: 6m 32s\n",
      "8691:\tlearn: 3.3930920\ttotal: 43m 27s\tremaining: 6m 32s\n",
      "8692:\tlearn: 3.3927706\ttotal: 43m 27s\tremaining: 6m 32s\n",
      "8693:\tlearn: 3.3924224\ttotal: 43m 27s\tremaining: 6m 31s\n",
      "8694:\tlearn: 3.3921136\ttotal: 43m 27s\tremaining: 6m 31s\n",
      "8695:\tlearn: 3.3918988\ttotal: 43m 28s\tremaining: 6m 31s\n",
      "8696:\tlearn: 3.3916997\ttotal: 43m 28s\tremaining: 6m 30s\n",
      "8697:\tlearn: 3.3913818\ttotal: 43m 28s\tremaining: 6m 30s\n",
      "8698:\tlearn: 3.3911702\ttotal: 43m 28s\tremaining: 6m 30s\n",
      "8699:\tlearn: 3.3909133\ttotal: 43m 29s\tremaining: 6m 29s\n",
      "8700:\tlearn: 3.3907437\ttotal: 43m 29s\tremaining: 6m 29s\n",
      "8701:\tlearn: 3.3905367\ttotal: 43m 29s\tremaining: 6m 29s\n",
      "8702:\tlearn: 3.3901462\ttotal: 43m 29s\tremaining: 6m 28s\n",
      "8703:\tlearn: 3.3898306\ttotal: 43m 30s\tremaining: 6m 28s\n",
      "8704:\tlearn: 3.3897396\ttotal: 43m 30s\tremaining: 6m 28s\n",
      "8705:\tlearn: 3.3895434\ttotal: 43m 30s\tremaining: 6m 28s\n",
      "8706:\tlearn: 3.3893809\ttotal: 43m 30s\tremaining: 6m 27s\n",
      "8707:\tlearn: 3.3892483\ttotal: 43m 31s\tremaining: 6m 27s\n",
      "8708:\tlearn: 3.3889383\ttotal: 43m 31s\tremaining: 6m 27s\n",
      "8709:\tlearn: 3.3885791\ttotal: 43m 31s\tremaining: 6m 26s\n",
      "8710:\tlearn: 3.3884211\ttotal: 43m 32s\tremaining: 6m 26s\n",
      "8711:\tlearn: 3.3881571\ttotal: 43m 32s\tremaining: 6m 26s\n",
      "8712:\tlearn: 3.3879541\ttotal: 43m 32s\tremaining: 6m 25s\n",
      "8713:\tlearn: 3.3876794\ttotal: 43m 32s\tremaining: 6m 25s\n",
      "8714:\tlearn: 3.3874638\ttotal: 43m 33s\tremaining: 6m 25s\n",
      "8715:\tlearn: 3.3872700\ttotal: 43m 33s\tremaining: 6m 24s\n",
      "8716:\tlearn: 3.3870377\ttotal: 43m 33s\tremaining: 6m 24s\n",
      "8717:\tlearn: 3.3867296\ttotal: 43m 33s\tremaining: 6m 24s\n",
      "8718:\tlearn: 3.3866159\ttotal: 43m 34s\tremaining: 6m 24s\n",
      "8719:\tlearn: 3.3864119\ttotal: 43m 34s\tremaining: 6m 23s\n",
      "8720:\tlearn: 3.3861540\ttotal: 43m 34s\tremaining: 6m 23s\n",
      "8721:\tlearn: 3.3859564\ttotal: 43m 35s\tremaining: 6m 23s\n",
      "8722:\tlearn: 3.3856748\ttotal: 43m 35s\tremaining: 6m 22s\n",
      "8723:\tlearn: 3.3855075\ttotal: 43m 35s\tremaining: 6m 22s\n",
      "8724:\tlearn: 3.3852373\ttotal: 43m 35s\tremaining: 6m 22s\n",
      "8725:\tlearn: 3.3849654\ttotal: 43m 36s\tremaining: 6m 21s\n",
      "8726:\tlearn: 3.3847376\ttotal: 43m 36s\tremaining: 6m 21s\n",
      "8727:\tlearn: 3.3843979\ttotal: 43m 36s\tremaining: 6m 21s\n",
      "8728:\tlearn: 3.3841431\ttotal: 43m 37s\tremaining: 6m 21s\n",
      "8729:\tlearn: 3.3838315\ttotal: 43m 37s\tremaining: 6m 20s\n",
      "8730:\tlearn: 3.3834911\ttotal: 43m 37s\tremaining: 6m 20s\n",
      "8731:\tlearn: 3.3832966\ttotal: 43m 37s\tremaining: 6m 20s\n",
      "8732:\tlearn: 3.3829810\ttotal: 43m 38s\tremaining: 6m 19s\n",
      "8733:\tlearn: 3.3828222\ttotal: 43m 38s\tremaining: 6m 19s\n",
      "8734:\tlearn: 3.3825373\ttotal: 43m 38s\tremaining: 6m 19s\n",
      "8735:\tlearn: 3.3823109\ttotal: 43m 39s\tremaining: 6m 18s\n",
      "8736:\tlearn: 3.3820288\ttotal: 43m 39s\tremaining: 6m 18s\n",
      "8737:\tlearn: 3.3818294\ttotal: 43m 39s\tremaining: 6m 18s\n",
      "8738:\tlearn: 3.3815543\ttotal: 43m 39s\tremaining: 6m 18s\n",
      "8739:\tlearn: 3.3812391\ttotal: 43m 40s\tremaining: 6m 17s\n",
      "8740:\tlearn: 3.3809394\ttotal: 43m 40s\tremaining: 6m 17s\n",
      "8741:\tlearn: 3.3806799\ttotal: 43m 40s\tremaining: 6m 17s\n",
      "8742:\tlearn: 3.3804837\ttotal: 43m 40s\tremaining: 6m 16s\n",
      "8743:\tlearn: 3.3802225\ttotal: 43m 41s\tremaining: 6m 16s\n",
      "8744:\tlearn: 3.3798681\ttotal: 43m 41s\tremaining: 6m 16s\n",
      "8745:\tlearn: 3.3796212\ttotal: 43m 41s\tremaining: 6m 15s\n",
      "8746:\tlearn: 3.3794530\ttotal: 43m 42s\tremaining: 6m 15s\n",
      "8747:\tlearn: 3.3792772\ttotal: 43m 42s\tremaining: 6m 15s\n",
      "8748:\tlearn: 3.3790082\ttotal: 43m 42s\tremaining: 6m 14s\n",
      "8749:\tlearn: 3.3788939\ttotal: 43m 42s\tremaining: 6m 14s\n",
      "8750:\tlearn: 3.3787454\ttotal: 43m 43s\tremaining: 6m 14s\n",
      "8751:\tlearn: 3.3785441\ttotal: 43m 43s\tremaining: 6m 14s\n",
      "8752:\tlearn: 3.3783396\ttotal: 43m 43s\tremaining: 6m 13s\n",
      "8753:\tlearn: 3.3780795\ttotal: 43m 43s\tremaining: 6m 13s\n",
      "8754:\tlearn: 3.3778634\ttotal: 43m 44s\tremaining: 6m 13s\n",
      "8755:\tlearn: 3.3775363\ttotal: 43m 44s\tremaining: 6m 12s\n",
      "8756:\tlearn: 3.3772650\ttotal: 43m 44s\tremaining: 6m 12s\n",
      "8757:\tlearn: 3.3770163\ttotal: 43m 45s\tremaining: 6m 12s\n",
      "8758:\tlearn: 3.3767060\ttotal: 43m 45s\tremaining: 6m 11s\n",
      "8759:\tlearn: 3.3763509\ttotal: 43m 45s\tremaining: 6m 11s\n",
      "8760:\tlearn: 3.3760778\ttotal: 43m 45s\tremaining: 6m 11s\n",
      "8761:\tlearn: 3.3759188\ttotal: 43m 46s\tremaining: 6m 11s\n",
      "8762:\tlearn: 3.3756700\ttotal: 43m 46s\tremaining: 6m 10s\n",
      "8763:\tlearn: 3.3754085\ttotal: 43m 46s\tremaining: 6m 10s\n",
      "8764:\tlearn: 3.3751828\ttotal: 43m 47s\tremaining: 6m 10s\n",
      "8765:\tlearn: 3.3749391\ttotal: 43m 47s\tremaining: 6m 9s\n",
      "8766:\tlearn: 3.3747447\ttotal: 43m 47s\tremaining: 6m 9s\n",
      "8767:\tlearn: 3.3744728\ttotal: 43m 47s\tremaining: 6m 9s\n",
      "8768:\tlearn: 3.3742879\ttotal: 43m 48s\tremaining: 6m 8s\n",
      "8769:\tlearn: 3.3740608\ttotal: 43m 48s\tremaining: 6m 8s\n",
      "8770:\tlearn: 3.3739159\ttotal: 43m 48s\tremaining: 6m 8s\n",
      "8771:\tlearn: 3.3736780\ttotal: 43m 48s\tremaining: 6m 8s\n",
      "8772:\tlearn: 3.3734805\ttotal: 43m 49s\tremaining: 6m 7s\n",
      "8773:\tlearn: 3.3731119\ttotal: 43m 49s\tremaining: 6m 7s\n",
      "8774:\tlearn: 3.3728590\ttotal: 43m 49s\tremaining: 6m 7s\n",
      "8775:\tlearn: 3.3727252\ttotal: 43m 49s\tremaining: 6m 6s\n",
      "8776:\tlearn: 3.3724783\ttotal: 43m 50s\tremaining: 6m 6s\n",
      "8777:\tlearn: 3.3722863\ttotal: 43m 50s\tremaining: 6m 6s\n",
      "8778:\tlearn: 3.3720054\ttotal: 43m 50s\tremaining: 6m 5s\n",
      "8779:\tlearn: 3.3717695\ttotal: 43m 50s\tremaining: 6m 5s\n",
      "8780:\tlearn: 3.3715049\ttotal: 43m 51s\tremaining: 6m 5s\n",
      "8781:\tlearn: 3.3713274\ttotal: 43m 51s\tremaining: 6m 4s\n",
      "8782:\tlearn: 3.3710425\ttotal: 43m 51s\tremaining: 6m 4s\n",
      "8783:\tlearn: 3.3707886\ttotal: 43m 52s\tremaining: 6m 4s\n",
      "8784:\tlearn: 3.3706073\ttotal: 43m 52s\tremaining: 6m 4s\n",
      "8785:\tlearn: 3.3705155\ttotal: 43m 52s\tremaining: 6m 3s\n",
      "8786:\tlearn: 3.3702777\ttotal: 43m 52s\tremaining: 6m 3s\n",
      "8787:\tlearn: 3.3700778\ttotal: 43m 53s\tremaining: 6m 3s\n",
      "8788:\tlearn: 3.3697582\ttotal: 43m 53s\tremaining: 6m 2s\n",
      "8789:\tlearn: 3.3696205\ttotal: 43m 53s\tremaining: 6m 2s\n",
      "8790:\tlearn: 3.3694244\ttotal: 43m 53s\tremaining: 6m 2s\n",
      "8791:\tlearn: 3.3692021\ttotal: 43m 54s\tremaining: 6m 1s\n",
      "8792:\tlearn: 3.3690343\ttotal: 43m 54s\tremaining: 6m 1s\n",
      "8793:\tlearn: 3.3688061\ttotal: 43m 54s\tremaining: 6m 1s\n",
      "8794:\tlearn: 3.3685652\ttotal: 43m 54s\tremaining: 6m 1s\n",
      "8795:\tlearn: 3.3683641\ttotal: 43m 55s\tremaining: 6m\n",
      "8796:\tlearn: 3.3681579\ttotal: 43m 55s\tremaining: 6m\n",
      "8797:\tlearn: 3.3677843\ttotal: 43m 55s\tremaining: 6m\n",
      "8798:\tlearn: 3.3674177\ttotal: 43m 56s\tremaining: 5m 59s\n",
      "8799:\tlearn: 3.3672358\ttotal: 43m 56s\tremaining: 5m 59s\n",
      "8800:\tlearn: 3.3669629\ttotal: 43m 56s\tremaining: 5m 59s\n",
      "8801:\tlearn: 3.3666367\ttotal: 43m 57s\tremaining: 5m 58s\n",
      "8802:\tlearn: 3.3662277\ttotal: 43m 57s\tremaining: 5m 58s\n",
      "8803:\tlearn: 3.3659309\ttotal: 43m 57s\tremaining: 5m 58s\n",
      "8804:\tlearn: 3.3658222\ttotal: 43m 57s\tremaining: 5m 57s\n",
      "8805:\tlearn: 3.3656154\ttotal: 43m 57s\tremaining: 5m 57s\n",
      "8806:\tlearn: 3.3654227\ttotal: 43m 58s\tremaining: 5m 57s\n",
      "8807:\tlearn: 3.3651676\ttotal: 43m 58s\tremaining: 5m 57s\n",
      "8808:\tlearn: 3.3648489\ttotal: 43m 58s\tremaining: 5m 56s\n",
      "8809:\tlearn: 3.3647126\ttotal: 43m 59s\tremaining: 5m 56s\n",
      "8810:\tlearn: 3.3644467\ttotal: 43m 59s\tremaining: 5m 56s\n",
      "8811:\tlearn: 3.3641719\ttotal: 43m 59s\tremaining: 5m 55s\n",
      "8812:\tlearn: 3.3638698\ttotal: 43m 59s\tremaining: 5m 55s\n",
      "8813:\tlearn: 3.3636108\ttotal: 44m\tremaining: 5m 55s\n",
      "8814:\tlearn: 3.3633876\ttotal: 44m\tremaining: 5m 54s\n",
      "8815:\tlearn: 3.3631892\ttotal: 44m\tremaining: 5m 54s\n",
      "8816:\tlearn: 3.3630298\ttotal: 44m\tremaining: 5m 54s\n",
      "8817:\tlearn: 3.3628006\ttotal: 44m 1s\tremaining: 5m 54s\n",
      "8818:\tlearn: 3.3625720\ttotal: 44m 1s\tremaining: 5m 53s\n",
      "8819:\tlearn: 3.3623052\ttotal: 44m 1s\tremaining: 5m 53s\n",
      "8820:\tlearn: 3.3621909\ttotal: 44m 1s\tremaining: 5m 53s\n",
      "8821:\tlearn: 3.3618884\ttotal: 44m 2s\tremaining: 5m 52s\n",
      "8822:\tlearn: 3.3615473\ttotal: 44m 2s\tremaining: 5m 52s\n",
      "8823:\tlearn: 3.3612965\ttotal: 44m 2s\tremaining: 5m 52s\n",
      "8824:\tlearn: 3.3610255\ttotal: 44m 3s\tremaining: 5m 51s\n",
      "8825:\tlearn: 3.3608518\ttotal: 44m 3s\tremaining: 5m 51s\n",
      "8826:\tlearn: 3.3606135\ttotal: 44m 3s\tremaining: 5m 51s\n",
      "8827:\tlearn: 3.3604275\ttotal: 44m 3s\tremaining: 5m 51s\n",
      "8828:\tlearn: 3.3601869\ttotal: 44m 4s\tremaining: 5m 50s\n",
      "8829:\tlearn: 3.3600592\ttotal: 44m 4s\tremaining: 5m 50s\n",
      "8830:\tlearn: 3.3598958\ttotal: 44m 4s\tremaining: 5m 50s\n",
      "8831:\tlearn: 3.3596309\ttotal: 44m 4s\tremaining: 5m 49s\n",
      "8832:\tlearn: 3.3593221\ttotal: 44m 5s\tremaining: 5m 49s\n",
      "8833:\tlearn: 3.3590363\ttotal: 44m 5s\tremaining: 5m 49s\n",
      "8834:\tlearn: 3.3588817\ttotal: 44m 5s\tremaining: 5m 48s\n",
      "8835:\tlearn: 3.3587402\ttotal: 44m 6s\tremaining: 5m 48s\n",
      "8836:\tlearn: 3.3585052\ttotal: 44m 6s\tremaining: 5m 48s\n",
      "8837:\tlearn: 3.3582410\ttotal: 44m 6s\tremaining: 5m 47s\n",
      "8838:\tlearn: 3.3580075\ttotal: 44m 6s\tremaining: 5m 47s\n",
      "8839:\tlearn: 3.3578382\ttotal: 44m 7s\tremaining: 5m 47s\n",
      "8840:\tlearn: 3.3576046\ttotal: 44m 7s\tremaining: 5m 47s\n",
      "8841:\tlearn: 3.3573254\ttotal: 44m 7s\tremaining: 5m 46s\n",
      "8842:\tlearn: 3.3571077\ttotal: 44m 8s\tremaining: 5m 46s\n",
      "8843:\tlearn: 3.3569239\ttotal: 44m 8s\tremaining: 5m 46s\n",
      "8844:\tlearn: 3.3567163\ttotal: 44m 8s\tremaining: 5m 45s\n",
      "8845:\tlearn: 3.3564120\ttotal: 44m 8s\tremaining: 5m 45s\n",
      "8846:\tlearn: 3.3560965\ttotal: 44m 9s\tremaining: 5m 45s\n",
      "8847:\tlearn: 3.3558789\ttotal: 44m 9s\tremaining: 5m 44s\n",
      "8848:\tlearn: 3.3556897\ttotal: 44m 9s\tremaining: 5m 44s\n",
      "8849:\tlearn: 3.3554338\ttotal: 44m 9s\tremaining: 5m 44s\n",
      "8850:\tlearn: 3.3552945\ttotal: 44m 10s\tremaining: 5m 44s\n",
      "8851:\tlearn: 3.3550820\ttotal: 44m 10s\tremaining: 5m 43s\n",
      "8852:\tlearn: 3.3547497\ttotal: 44m 10s\tremaining: 5m 43s\n",
      "8853:\tlearn: 3.3545481\ttotal: 44m 10s\tremaining: 5m 43s\n",
      "8854:\tlearn: 3.3543067\ttotal: 44m 11s\tremaining: 5m 42s\n",
      "8855:\tlearn: 3.3540887\ttotal: 44m 11s\tremaining: 5m 42s\n",
      "8856:\tlearn: 3.3538356\ttotal: 44m 11s\tremaining: 5m 42s\n",
      "8857:\tlearn: 3.3535859\ttotal: 44m 12s\tremaining: 5m 41s\n",
      "8858:\tlearn: 3.3533044\ttotal: 44m 12s\tremaining: 5m 41s\n",
      "8859:\tlearn: 3.3528950\ttotal: 44m 12s\tremaining: 5m 41s\n",
      "8860:\tlearn: 3.3527076\ttotal: 44m 12s\tremaining: 5m 41s\n",
      "8861:\tlearn: 3.3525285\ttotal: 44m 13s\tremaining: 5m 40s\n",
      "8862:\tlearn: 3.3523216\ttotal: 44m 13s\tremaining: 5m 40s\n",
      "8863:\tlearn: 3.3520089\ttotal: 44m 13s\tremaining: 5m 40s\n",
      "8864:\tlearn: 3.3518983\ttotal: 44m 13s\tremaining: 5m 39s\n",
      "8865:\tlearn: 3.3516886\ttotal: 44m 14s\tremaining: 5m 39s\n",
      "8866:\tlearn: 3.3515470\ttotal: 44m 14s\tremaining: 5m 39s\n",
      "8867:\tlearn: 3.3512393\ttotal: 44m 14s\tremaining: 5m 38s\n",
      "8868:\tlearn: 3.3509705\ttotal: 44m 15s\tremaining: 5m 38s\n",
      "8869:\tlearn: 3.3506875\ttotal: 44m 15s\tremaining: 5m 38s\n",
      "8870:\tlearn: 3.3505424\ttotal: 44m 15s\tremaining: 5m 37s\n",
      "8871:\tlearn: 3.3501857\ttotal: 44m 15s\tremaining: 5m 37s\n",
      "8872:\tlearn: 3.3499344\ttotal: 44m 16s\tremaining: 5m 37s\n",
      "8873:\tlearn: 3.3497503\ttotal: 44m 16s\tremaining: 5m 37s\n",
      "8874:\tlearn: 3.3494612\ttotal: 44m 16s\tremaining: 5m 36s\n",
      "8875:\tlearn: 3.3491978\ttotal: 44m 17s\tremaining: 5m 36s\n",
      "8876:\tlearn: 3.3489643\ttotal: 44m 17s\tremaining: 5m 36s\n",
      "8877:\tlearn: 3.3486804\ttotal: 44m 17s\tremaining: 5m 35s\n",
      "8878:\tlearn: 3.3485156\ttotal: 44m 17s\tremaining: 5m 35s\n",
      "8879:\tlearn: 3.3483220\ttotal: 44m 18s\tremaining: 5m 35s\n",
      "8880:\tlearn: 3.3480925\ttotal: 44m 18s\tremaining: 5m 34s\n",
      "8881:\tlearn: 3.3477541\ttotal: 44m 18s\tremaining: 5m 34s\n",
      "8882:\tlearn: 3.3474947\ttotal: 44m 19s\tremaining: 5m 34s\n",
      "8883:\tlearn: 3.3471196\ttotal: 44m 19s\tremaining: 5m 34s\n",
      "8884:\tlearn: 3.3468790\ttotal: 44m 19s\tremaining: 5m 33s\n",
      "8885:\tlearn: 3.3466279\ttotal: 44m 19s\tremaining: 5m 33s\n",
      "8886:\tlearn: 3.3464257\ttotal: 44m 20s\tremaining: 5m 33s\n",
      "8887:\tlearn: 3.3462005\ttotal: 44m 20s\tremaining: 5m 32s\n",
      "8888:\tlearn: 3.3460046\ttotal: 44m 20s\tremaining: 5m 32s\n",
      "8889:\tlearn: 3.3457775\ttotal: 44m 20s\tremaining: 5m 32s\n",
      "8890:\tlearn: 3.3454725\ttotal: 44m 21s\tremaining: 5m 31s\n",
      "8891:\tlearn: 3.3452566\ttotal: 44m 21s\tremaining: 5m 31s\n",
      "8892:\tlearn: 3.3449735\ttotal: 44m 21s\tremaining: 5m 31s\n",
      "8893:\tlearn: 3.3448279\ttotal: 44m 21s\tremaining: 5m 31s\n",
      "8894:\tlearn: 3.3445569\ttotal: 44m 22s\tremaining: 5m 30s\n",
      "8895:\tlearn: 3.3442910\ttotal: 44m 22s\tremaining: 5m 30s\n",
      "8896:\tlearn: 3.3441318\ttotal: 44m 22s\tremaining: 5m 30s\n",
      "8897:\tlearn: 3.3438846\ttotal: 44m 22s\tremaining: 5m 29s\n",
      "8898:\tlearn: 3.3436228\ttotal: 44m 23s\tremaining: 5m 29s\n",
      "8899:\tlearn: 3.3433336\ttotal: 44m 23s\tremaining: 5m 29s\n",
      "8900:\tlearn: 3.3431297\ttotal: 44m 23s\tremaining: 5m 28s\n",
      "8901:\tlearn: 3.3429976\ttotal: 44m 24s\tremaining: 5m 28s\n",
      "8902:\tlearn: 3.3427239\ttotal: 44m 24s\tremaining: 5m 28s\n",
      "8903:\tlearn: 3.3424272\ttotal: 44m 24s\tremaining: 5m 27s\n",
      "8904:\tlearn: 3.3422160\ttotal: 44m 24s\tremaining: 5m 27s\n",
      "8905:\tlearn: 3.3420833\ttotal: 44m 25s\tremaining: 5m 27s\n",
      "8906:\tlearn: 3.3419190\ttotal: 44m 25s\tremaining: 5m 27s\n",
      "8907:\tlearn: 3.3416723\ttotal: 44m 25s\tremaining: 5m 26s\n",
      "8908:\tlearn: 3.3414478\ttotal: 44m 25s\tremaining: 5m 26s\n",
      "8909:\tlearn: 3.3411676\ttotal: 44m 26s\tremaining: 5m 26s\n",
      "8910:\tlearn: 3.3408807\ttotal: 44m 26s\tremaining: 5m 25s\n",
      "8911:\tlearn: 3.3406374\ttotal: 44m 26s\tremaining: 5m 25s\n",
      "8912:\tlearn: 3.3403391\ttotal: 44m 27s\tremaining: 5m 25s\n",
      "8913:\tlearn: 3.3399772\ttotal: 44m 27s\tremaining: 5m 24s\n",
      "8914:\tlearn: 3.3397438\ttotal: 44m 27s\tremaining: 5m 24s\n",
      "8915:\tlearn: 3.3395252\ttotal: 44m 28s\tremaining: 5m 24s\n",
      "8916:\tlearn: 3.3393185\ttotal: 44m 28s\tremaining: 5m 24s\n",
      "8917:\tlearn: 3.3389648\ttotal: 44m 28s\tremaining: 5m 23s\n",
      "8918:\tlearn: 3.3387232\ttotal: 44m 28s\tremaining: 5m 23s\n",
      "8919:\tlearn: 3.3385579\ttotal: 44m 29s\tremaining: 5m 23s\n",
      "8920:\tlearn: 3.3383923\ttotal: 44m 29s\tremaining: 5m 22s\n",
      "8921:\tlearn: 3.3381450\ttotal: 44m 29s\tremaining: 5m 22s\n",
      "8922:\tlearn: 3.3378714\ttotal: 44m 30s\tremaining: 5m 22s\n",
      "8923:\tlearn: 3.3376496\ttotal: 44m 30s\tremaining: 5m 21s\n",
      "8924:\tlearn: 3.3374350\ttotal: 44m 30s\tremaining: 5m 21s\n",
      "8925:\tlearn: 3.3372311\ttotal: 44m 30s\tremaining: 5m 21s\n",
      "8926:\tlearn: 3.3369621\ttotal: 44m 31s\tremaining: 5m 21s\n",
      "8927:\tlearn: 3.3366446\ttotal: 44m 31s\tremaining: 5m 20s\n",
      "8928:\tlearn: 3.3364010\ttotal: 44m 31s\tremaining: 5m 20s\n",
      "8929:\tlearn: 3.3361333\ttotal: 44m 32s\tremaining: 5m 20s\n",
      "8930:\tlearn: 3.3358610\ttotal: 44m 32s\tremaining: 5m 19s\n",
      "8931:\tlearn: 3.3357604\ttotal: 44m 32s\tremaining: 5m 19s\n",
      "8932:\tlearn: 3.3353386\ttotal: 44m 32s\tremaining: 5m 19s\n",
      "8933:\tlearn: 3.3350752\ttotal: 44m 33s\tremaining: 5m 18s\n",
      "8934:\tlearn: 3.3346982\ttotal: 44m 33s\tremaining: 5m 18s\n",
      "8935:\tlearn: 3.3344156\ttotal: 44m 33s\tremaining: 5m 18s\n",
      "8936:\tlearn: 3.3340909\ttotal: 44m 34s\tremaining: 5m 18s\n",
      "8937:\tlearn: 3.3339021\ttotal: 44m 34s\tremaining: 5m 17s\n",
      "8938:\tlearn: 3.3336549\ttotal: 44m 34s\tremaining: 5m 17s\n",
      "8939:\tlearn: 3.3333935\ttotal: 44m 34s\tremaining: 5m 17s\n",
      "8940:\tlearn: 3.3331349\ttotal: 44m 35s\tremaining: 5m 16s\n",
      "8941:\tlearn: 3.3329627\ttotal: 44m 35s\tremaining: 5m 16s\n",
      "8942:\tlearn: 3.3327078\ttotal: 44m 35s\tremaining: 5m 16s\n",
      "8943:\tlearn: 3.3325162\ttotal: 44m 36s\tremaining: 5m 15s\n",
      "8944:\tlearn: 3.3323114\ttotal: 44m 36s\tremaining: 5m 15s\n",
      "8945:\tlearn: 3.3320392\ttotal: 44m 36s\tremaining: 5m 15s\n",
      "8946:\tlearn: 3.3318582\ttotal: 44m 36s\tremaining: 5m 15s\n",
      "8947:\tlearn: 3.3316080\ttotal: 44m 37s\tremaining: 5m 14s\n",
      "8948:\tlearn: 3.3313724\ttotal: 44m 37s\tremaining: 5m 14s\n",
      "8949:\tlearn: 3.3311698\ttotal: 44m 37s\tremaining: 5m 14s\n",
      "8950:\tlearn: 3.3308894\ttotal: 44m 37s\tremaining: 5m 13s\n",
      "8951:\tlearn: 3.3307063\ttotal: 44m 38s\tremaining: 5m 13s\n",
      "8952:\tlearn: 3.3303751\ttotal: 44m 38s\tremaining: 5m 13s\n",
      "8953:\tlearn: 3.3301124\ttotal: 44m 38s\tremaining: 5m 12s\n",
      "8954:\tlearn: 3.3299800\ttotal: 44m 38s\tremaining: 5m 12s\n",
      "8955:\tlearn: 3.3296674\ttotal: 44m 39s\tremaining: 5m 12s\n",
      "8956:\tlearn: 3.3295516\ttotal: 44m 39s\tremaining: 5m 12s\n",
      "8957:\tlearn: 3.3293189\ttotal: 44m 39s\tremaining: 5m 11s\n",
      "8958:\tlearn: 3.3290625\ttotal: 44m 40s\tremaining: 5m 11s\n",
      "8959:\tlearn: 3.3287843\ttotal: 44m 40s\tremaining: 5m 11s\n",
      "8960:\tlearn: 3.3285221\ttotal: 44m 40s\tremaining: 5m 10s\n",
      "8961:\tlearn: 3.3283171\ttotal: 44m 40s\tremaining: 5m 10s\n",
      "8962:\tlearn: 3.3280289\ttotal: 44m 41s\tremaining: 5m 10s\n",
      "8963:\tlearn: 3.3278421\ttotal: 44m 41s\tremaining: 5m 9s\n",
      "8964:\tlearn: 3.3275739\ttotal: 44m 41s\tremaining: 5m 9s\n",
      "8965:\tlearn: 3.3273993\ttotal: 44m 42s\tremaining: 5m 9s\n",
      "8966:\tlearn: 3.3271494\ttotal: 44m 42s\tremaining: 5m 9s\n",
      "8967:\tlearn: 3.3268213\ttotal: 44m 42s\tremaining: 5m 8s\n",
      "8968:\tlearn: 3.3265474\ttotal: 44m 42s\tremaining: 5m 8s\n",
      "8969:\tlearn: 3.3262351\ttotal: 44m 43s\tremaining: 5m 8s\n",
      "8970:\tlearn: 3.3259709\ttotal: 44m 43s\tremaining: 5m 7s\n",
      "8971:\tlearn: 3.3257138\ttotal: 44m 43s\tremaining: 5m 7s\n",
      "8972:\tlearn: 3.3253448\ttotal: 44m 44s\tremaining: 5m 7s\n",
      "8973:\tlearn: 3.3251791\ttotal: 44m 44s\tremaining: 5m 6s\n",
      "8974:\tlearn: 3.3249936\ttotal: 44m 44s\tremaining: 5m 6s\n",
      "8975:\tlearn: 3.3247908\ttotal: 44m 44s\tremaining: 5m 6s\n",
      "8976:\tlearn: 3.3245836\ttotal: 44m 45s\tremaining: 5m 6s\n",
      "8977:\tlearn: 3.3243741\ttotal: 44m 45s\tremaining: 5m 5s\n",
      "8978:\tlearn: 3.3241706\ttotal: 44m 45s\tremaining: 5m 5s\n",
      "8979:\tlearn: 3.3239070\ttotal: 44m 46s\tremaining: 5m 5s\n",
      "8980:\tlearn: 3.3236626\ttotal: 44m 46s\tremaining: 5m 4s\n",
      "8981:\tlearn: 3.3234272\ttotal: 44m 46s\tremaining: 5m 4s\n",
      "8982:\tlearn: 3.3231861\ttotal: 44m 46s\tremaining: 5m 4s\n",
      "8983:\tlearn: 3.3229540\ttotal: 44m 47s\tremaining: 5m 3s\n",
      "8984:\tlearn: 3.3226229\ttotal: 44m 47s\tremaining: 5m 3s\n",
      "8985:\tlearn: 3.3224103\ttotal: 44m 47s\tremaining: 5m 3s\n",
      "8986:\tlearn: 3.3221919\ttotal: 44m 47s\tremaining: 5m 2s\n",
      "8987:\tlearn: 3.3220112\ttotal: 44m 48s\tremaining: 5m 2s\n",
      "8988:\tlearn: 3.3218381\ttotal: 44m 48s\tremaining: 5m 2s\n",
      "8989:\tlearn: 3.3215585\ttotal: 44m 48s\tremaining: 5m 2s\n",
      "8990:\tlearn: 3.3212936\ttotal: 44m 49s\tremaining: 5m 1s\n",
      "8991:\tlearn: 3.3211403\ttotal: 44m 49s\tremaining: 5m 1s\n",
      "8992:\tlearn: 3.3208906\ttotal: 44m 49s\tremaining: 5m 1s\n",
      "8993:\tlearn: 3.3205975\ttotal: 44m 49s\tremaining: 5m\n",
      "8994:\tlearn: 3.3203342\ttotal: 44m 50s\tremaining: 5m\n",
      "8995:\tlearn: 3.3200542\ttotal: 44m 50s\tremaining: 5m\n",
      "8996:\tlearn: 3.3198090\ttotal: 44m 50s\tremaining: 4m 59s\n",
      "8997:\tlearn: 3.3195251\ttotal: 44m 50s\tremaining: 4m 59s\n",
      "8998:\tlearn: 3.3192504\ttotal: 44m 51s\tremaining: 4m 59s\n",
      "8999:\tlearn: 3.3189903\ttotal: 44m 51s\tremaining: 4m 59s\n",
      "9000:\tlearn: 3.3187786\ttotal: 44m 51s\tremaining: 4m 58s\n",
      "9001:\tlearn: 3.3184741\ttotal: 44m 52s\tremaining: 4m 58s\n",
      "9002:\tlearn: 3.3182182\ttotal: 44m 52s\tremaining: 4m 58s\n",
      "9003:\tlearn: 3.3180029\ttotal: 44m 52s\tremaining: 4m 57s\n",
      "9004:\tlearn: 3.3177212\ttotal: 44m 52s\tremaining: 4m 57s\n",
      "9005:\tlearn: 3.3174956\ttotal: 44m 53s\tremaining: 4m 57s\n",
      "9006:\tlearn: 3.3172628\ttotal: 44m 53s\tremaining: 4m 56s\n",
      "9007:\tlearn: 3.3170278\ttotal: 44m 53s\tremaining: 4m 56s\n",
      "9008:\tlearn: 3.3168548\ttotal: 44m 53s\tremaining: 4m 56s\n",
      "9009:\tlearn: 3.3166920\ttotal: 44m 54s\tremaining: 4m 56s\n",
      "9010:\tlearn: 3.3164011\ttotal: 44m 54s\tremaining: 4m 55s\n",
      "9011:\tlearn: 3.3160489\ttotal: 44m 54s\tremaining: 4m 55s\n",
      "9012:\tlearn: 3.3158292\ttotal: 44m 55s\tremaining: 4m 55s\n",
      "9013:\tlearn: 3.3156215\ttotal: 44m 55s\tremaining: 4m 54s\n",
      "9014:\tlearn: 3.3154704\ttotal: 44m 55s\tremaining: 4m 54s\n",
      "9015:\tlearn: 3.3152605\ttotal: 44m 56s\tremaining: 4m 54s\n",
      "9016:\tlearn: 3.3150752\ttotal: 44m 56s\tremaining: 4m 53s\n",
      "9017:\tlearn: 3.3148344\ttotal: 44m 56s\tremaining: 4m 53s\n",
      "9018:\tlearn: 3.3146509\ttotal: 44m 56s\tremaining: 4m 53s\n",
      "9019:\tlearn: 3.3144328\ttotal: 44m 57s\tremaining: 4m 53s\n",
      "9020:\tlearn: 3.3142077\ttotal: 44m 57s\tremaining: 4m 52s\n",
      "9021:\tlearn: 3.3138869\ttotal: 44m 57s\tremaining: 4m 52s\n",
      "9022:\tlearn: 3.3135661\ttotal: 44m 57s\tremaining: 4m 52s\n",
      "9023:\tlearn: 3.3132984\ttotal: 44m 58s\tremaining: 4m 51s\n",
      "9024:\tlearn: 3.3131434\ttotal: 44m 58s\tremaining: 4m 51s\n",
      "9025:\tlearn: 3.3129500\ttotal: 44m 58s\tremaining: 4m 51s\n",
      "9026:\tlearn: 3.3126979\ttotal: 44m 59s\tremaining: 4m 50s\n",
      "9027:\tlearn: 3.3125245\ttotal: 44m 59s\tremaining: 4m 50s\n",
      "9028:\tlearn: 3.3122930\ttotal: 44m 59s\tremaining: 4m 50s\n",
      "9029:\tlearn: 3.3119890\ttotal: 44m 59s\tremaining: 4m 50s\n",
      "9030:\tlearn: 3.3119257\ttotal: 45m\tremaining: 4m 49s\n",
      "9031:\tlearn: 3.3117488\ttotal: 45m\tremaining: 4m 49s\n",
      "9032:\tlearn: 3.3114815\ttotal: 45m\tremaining: 4m 49s\n",
      "9033:\tlearn: 3.3113297\ttotal: 45m\tremaining: 4m 48s\n",
      "9034:\tlearn: 3.3111297\ttotal: 45m 1s\tremaining: 4m 48s\n",
      "9035:\tlearn: 3.3110538\ttotal: 45m 1s\tremaining: 4m 48s\n",
      "9036:\tlearn: 3.3108058\ttotal: 45m 1s\tremaining: 4m 47s\n",
      "9037:\tlearn: 3.3106027\ttotal: 45m 1s\tremaining: 4m 47s\n",
      "9038:\tlearn: 3.3104407\ttotal: 45m 2s\tremaining: 4m 47s\n",
      "9039:\tlearn: 3.3101563\ttotal: 45m 2s\tremaining: 4m 46s\n",
      "9040:\tlearn: 3.3099584\ttotal: 45m 2s\tremaining: 4m 46s\n",
      "9041:\tlearn: 3.3097622\ttotal: 45m 2s\tremaining: 4m 46s\n",
      "9042:\tlearn: 3.3095399\ttotal: 45m 3s\tremaining: 4m 46s\n",
      "9043:\tlearn: 3.3092247\ttotal: 45m 3s\tremaining: 4m 45s\n",
      "9044:\tlearn: 3.3090246\ttotal: 45m 3s\tremaining: 4m 45s\n",
      "9045:\tlearn: 3.3087412\ttotal: 45m 3s\tremaining: 4m 45s\n",
      "9046:\tlearn: 3.3085207\ttotal: 45m 4s\tremaining: 4m 44s\n",
      "9047:\tlearn: 3.3083500\ttotal: 45m 4s\tremaining: 4m 44s\n",
      "9048:\tlearn: 3.3080697\ttotal: 45m 4s\tremaining: 4m 44s\n",
      "9049:\tlearn: 3.3077694\ttotal: 45m 5s\tremaining: 4m 43s\n",
      "9050:\tlearn: 3.3076346\ttotal: 45m 5s\tremaining: 4m 43s\n",
      "9051:\tlearn: 3.3074076\ttotal: 45m 5s\tremaining: 4m 43s\n",
      "9052:\tlearn: 3.3072021\ttotal: 45m 5s\tremaining: 4m 43s\n",
      "9053:\tlearn: 3.3069604\ttotal: 45m 6s\tremaining: 4m 42s\n",
      "9054:\tlearn: 3.3066985\ttotal: 45m 6s\tremaining: 4m 42s\n",
      "9055:\tlearn: 3.3064604\ttotal: 45m 6s\tremaining: 4m 42s\n",
      "9056:\tlearn: 3.3062139\ttotal: 45m 7s\tremaining: 4m 41s\n",
      "9057:\tlearn: 3.3060466\ttotal: 45m 7s\tremaining: 4m 41s\n",
      "9058:\tlearn: 3.3058671\ttotal: 45m 7s\tremaining: 4m 41s\n",
      "9059:\tlearn: 3.3056431\ttotal: 45m 7s\tremaining: 4m 40s\n",
      "9060:\tlearn: 3.3054765\ttotal: 45m 8s\tremaining: 4m 40s\n",
      "9061:\tlearn: 3.3051902\ttotal: 45m 8s\tremaining: 4m 40s\n",
      "9062:\tlearn: 3.3050136\ttotal: 45m 8s\tremaining: 4m 40s\n",
      "9063:\tlearn: 3.3047591\ttotal: 45m 8s\tremaining: 4m 39s\n",
      "9064:\tlearn: 3.3045104\ttotal: 45m 9s\tremaining: 4m 39s\n",
      "9065:\tlearn: 3.3042800\ttotal: 45m 9s\tremaining: 4m 39s\n",
      "9066:\tlearn: 3.3041007\ttotal: 45m 9s\tremaining: 4m 38s\n",
      "9067:\tlearn: 3.3038688\ttotal: 45m 9s\tremaining: 4m 38s\n",
      "9068:\tlearn: 3.3036273\ttotal: 45m 10s\tremaining: 4m 38s\n",
      "9069:\tlearn: 3.3033857\ttotal: 45m 10s\tremaining: 4m 37s\n",
      "9070:\tlearn: 3.3031665\ttotal: 45m 10s\tremaining: 4m 37s\n",
      "9071:\tlearn: 3.3029754\ttotal: 45m 11s\tremaining: 4m 37s\n",
      "9072:\tlearn: 3.3028939\ttotal: 45m 11s\tremaining: 4m 37s\n",
      "9073:\tlearn: 3.3026415\ttotal: 45m 11s\tremaining: 4m 36s\n",
      "9074:\tlearn: 3.3022825\ttotal: 45m 12s\tremaining: 4m 36s\n",
      "9075:\tlearn: 3.3020579\ttotal: 45m 12s\tremaining: 4m 36s\n",
      "9076:\tlearn: 3.3017885\ttotal: 45m 12s\tremaining: 4m 35s\n",
      "9077:\tlearn: 3.3014996\ttotal: 45m 12s\tremaining: 4m 35s\n",
      "9078:\tlearn: 3.3013207\ttotal: 45m 13s\tremaining: 4m 35s\n",
      "9079:\tlearn: 3.3011881\ttotal: 45m 13s\tremaining: 4m 34s\n",
      "9080:\tlearn: 3.3009334\ttotal: 45m 13s\tremaining: 4m 34s\n",
      "9081:\tlearn: 3.3007463\ttotal: 45m 13s\tremaining: 4m 34s\n",
      "9082:\tlearn: 3.3003848\ttotal: 45m 14s\tremaining: 4m 34s\n",
      "9083:\tlearn: 3.3001839\ttotal: 45m 14s\tremaining: 4m 33s\n",
      "9084:\tlearn: 3.3000600\ttotal: 45m 14s\tremaining: 4m 33s\n",
      "9085:\tlearn: 3.2998467\ttotal: 45m 15s\tremaining: 4m 33s\n",
      "9086:\tlearn: 3.2996166\ttotal: 45m 15s\tremaining: 4m 32s\n",
      "9087:\tlearn: 3.2993879\ttotal: 45m 15s\tremaining: 4m 32s\n",
      "9088:\tlearn: 3.2991054\ttotal: 45m 15s\tremaining: 4m 32s\n",
      "9089:\tlearn: 3.2989513\ttotal: 45m 16s\tremaining: 4m 31s\n",
      "9090:\tlearn: 3.2987735\ttotal: 45m 16s\tremaining: 4m 31s\n",
      "9091:\tlearn: 3.2986007\ttotal: 45m 16s\tremaining: 4m 31s\n",
      "9092:\tlearn: 3.2984103\ttotal: 45m 16s\tremaining: 4m 31s\n",
      "9093:\tlearn: 3.2982335\ttotal: 45m 17s\tremaining: 4m 30s\n",
      "9094:\tlearn: 3.2979997\ttotal: 45m 17s\tremaining: 4m 30s\n",
      "9095:\tlearn: 3.2977898\ttotal: 45m 17s\tremaining: 4m 30s\n",
      "9096:\tlearn: 3.2976060\ttotal: 45m 17s\tremaining: 4m 29s\n",
      "9097:\tlearn: 3.2974579\ttotal: 45m 18s\tremaining: 4m 29s\n",
      "9098:\tlearn: 3.2973245\ttotal: 45m 18s\tremaining: 4m 29s\n",
      "9099:\tlearn: 3.2971992\ttotal: 45m 18s\tremaining: 4m 28s\n",
      "9100:\tlearn: 3.2970079\ttotal: 45m 18s\tremaining: 4m 28s\n",
      "9101:\tlearn: 3.2966363\ttotal: 45m 19s\tremaining: 4m 28s\n",
      "9102:\tlearn: 3.2964947\ttotal: 45m 19s\tremaining: 4m 27s\n",
      "9103:\tlearn: 3.2962408\ttotal: 45m 19s\tremaining: 4m 27s\n",
      "9104:\tlearn: 3.2960715\ttotal: 45m 20s\tremaining: 4m 27s\n",
      "9105:\tlearn: 3.2957999\ttotal: 45m 20s\tremaining: 4m 27s\n",
      "9106:\tlearn: 3.2955926\ttotal: 45m 20s\tremaining: 4m 26s\n",
      "9107:\tlearn: 3.2953806\ttotal: 45m 20s\tremaining: 4m 26s\n",
      "9108:\tlearn: 3.2949988\ttotal: 45m 21s\tremaining: 4m 26s\n",
      "9109:\tlearn: 3.2946684\ttotal: 45m 21s\tremaining: 4m 25s\n",
      "9110:\tlearn: 3.2944443\ttotal: 45m 21s\tremaining: 4m 25s\n",
      "9111:\tlearn: 3.2940623\ttotal: 45m 22s\tremaining: 4m 25s\n",
      "9112:\tlearn: 3.2938477\ttotal: 45m 22s\tremaining: 4m 24s\n",
      "9113:\tlearn: 3.2935716\ttotal: 45m 22s\tremaining: 4m 24s\n",
      "9114:\tlearn: 3.2932572\ttotal: 45m 23s\tremaining: 4m 24s\n",
      "9115:\tlearn: 3.2930822\ttotal: 45m 23s\tremaining: 4m 24s\n",
      "9116:\tlearn: 3.2927699\ttotal: 45m 23s\tremaining: 4m 23s\n",
      "9117:\tlearn: 3.2924908\ttotal: 45m 23s\tremaining: 4m 23s\n",
      "9118:\tlearn: 3.2920955\ttotal: 45m 24s\tremaining: 4m 23s\n",
      "9119:\tlearn: 3.2918337\ttotal: 45m 24s\tremaining: 4m 22s\n",
      "9120:\tlearn: 3.2916597\ttotal: 45m 24s\tremaining: 4m 22s\n",
      "9121:\tlearn: 3.2914277\ttotal: 45m 24s\tremaining: 4m 22s\n",
      "9122:\tlearn: 3.2912721\ttotal: 45m 25s\tremaining: 4m 21s\n",
      "9123:\tlearn: 3.2910217\ttotal: 45m 25s\tremaining: 4m 21s\n",
      "9124:\tlearn: 3.2908382\ttotal: 45m 25s\tremaining: 4m 21s\n",
      "9125:\tlearn: 3.2906798\ttotal: 45m 26s\tremaining: 4m 21s\n",
      "9126:\tlearn: 3.2904344\ttotal: 45m 26s\tremaining: 4m 20s\n",
      "9127:\tlearn: 3.2902615\ttotal: 45m 26s\tremaining: 4m 20s\n",
      "9128:\tlearn: 3.2899851\ttotal: 45m 26s\tremaining: 4m 20s\n",
      "9129:\tlearn: 3.2898120\ttotal: 45m 27s\tremaining: 4m 19s\n",
      "9130:\tlearn: 3.2895966\ttotal: 45m 27s\tremaining: 4m 19s\n",
      "9131:\tlearn: 3.2893882\ttotal: 45m 27s\tremaining: 4m 19s\n",
      "9132:\tlearn: 3.2891765\ttotal: 45m 27s\tremaining: 4m 18s\n",
      "9133:\tlearn: 3.2890125\ttotal: 45m 28s\tremaining: 4m 18s\n",
      "9134:\tlearn: 3.2888281\ttotal: 45m 28s\tremaining: 4m 18s\n",
      "9135:\tlearn: 3.2886871\ttotal: 45m 28s\tremaining: 4m 18s\n",
      "9136:\tlearn: 3.2885038\ttotal: 45m 29s\tremaining: 4m 17s\n",
      "9137:\tlearn: 3.2882543\ttotal: 45m 29s\tremaining: 4m 17s\n",
      "9138:\tlearn: 3.2880733\ttotal: 45m 29s\tremaining: 4m 17s\n",
      "9139:\tlearn: 3.2878000\ttotal: 45m 29s\tremaining: 4m 16s\n",
      "9140:\tlearn: 3.2875383\ttotal: 45m 30s\tremaining: 4m 16s\n",
      "9141:\tlearn: 3.2873532\ttotal: 45m 30s\tremaining: 4m 16s\n",
      "9142:\tlearn: 3.2870740\ttotal: 45m 30s\tremaining: 4m 15s\n",
      "9143:\tlearn: 3.2868932\ttotal: 45m 31s\tremaining: 4m 15s\n",
      "9144:\tlearn: 3.2867288\ttotal: 45m 31s\tremaining: 4m 15s\n",
      "9145:\tlearn: 3.2865535\ttotal: 45m 31s\tremaining: 4m 15s\n",
      "9146:\tlearn: 3.2863640\ttotal: 45m 31s\tremaining: 4m 14s\n",
      "9147:\tlearn: 3.2861998\ttotal: 45m 32s\tremaining: 4m 14s\n",
      "9148:\tlearn: 3.2859539\ttotal: 45m 32s\tremaining: 4m 14s\n",
      "9149:\tlearn: 3.2858060\ttotal: 45m 32s\tremaining: 4m 13s\n",
      "9150:\tlearn: 3.2855583\ttotal: 45m 32s\tremaining: 4m 13s\n",
      "9151:\tlearn: 3.2853970\ttotal: 45m 33s\tremaining: 4m 13s\n",
      "9152:\tlearn: 3.2852497\ttotal: 45m 33s\tremaining: 4m 12s\n",
      "9153:\tlearn: 3.2850438\ttotal: 45m 33s\tremaining: 4m 12s\n",
      "9154:\tlearn: 3.2849104\ttotal: 45m 33s\tremaining: 4m 12s\n",
      "9155:\tlearn: 3.2846621\ttotal: 45m 34s\tremaining: 4m 12s\n",
      "9156:\tlearn: 3.2845616\ttotal: 45m 34s\tremaining: 4m 11s\n",
      "9157:\tlearn: 3.2843270\ttotal: 45m 34s\tremaining: 4m 11s\n",
      "9158:\tlearn: 3.2839740\ttotal: 45m 35s\tremaining: 4m 11s\n",
      "9159:\tlearn: 3.2837178\ttotal: 45m 35s\tremaining: 4m 10s\n",
      "9160:\tlearn: 3.2835233\ttotal: 45m 35s\tremaining: 4m 10s\n",
      "9161:\tlearn: 3.2833066\ttotal: 45m 35s\tremaining: 4m 10s\n",
      "9162:\tlearn: 3.2830168\ttotal: 45m 36s\tremaining: 4m 9s\n",
      "9163:\tlearn: 3.2827656\ttotal: 45m 36s\tremaining: 4m 9s\n",
      "9164:\tlearn: 3.2825952\ttotal: 45m 36s\tremaining: 4m 9s\n",
      "9165:\tlearn: 3.2823961\ttotal: 45m 37s\tremaining: 4m 9s\n",
      "9166:\tlearn: 3.2821598\ttotal: 45m 37s\tremaining: 4m 8s\n",
      "9167:\tlearn: 3.2819178\ttotal: 45m 37s\tremaining: 4m 8s\n",
      "9168:\tlearn: 3.2815242\ttotal: 45m 37s\tremaining: 4m 8s\n",
      "9169:\tlearn: 3.2812780\ttotal: 45m 38s\tremaining: 4m 7s\n",
      "9170:\tlearn: 3.2809867\ttotal: 45m 38s\tremaining: 4m 7s\n",
      "9171:\tlearn: 3.2807516\ttotal: 45m 38s\tremaining: 4m 7s\n",
      "9172:\tlearn: 3.2805332\ttotal: 45m 39s\tremaining: 4m 6s\n",
      "9173:\tlearn: 3.2803444\ttotal: 45m 39s\tremaining: 4m 6s\n",
      "9174:\tlearn: 3.2801036\ttotal: 45m 39s\tremaining: 4m 6s\n",
      "9175:\tlearn: 3.2797982\ttotal: 45m 40s\tremaining: 4m 6s\n",
      "9176:\tlearn: 3.2795690\ttotal: 45m 40s\tremaining: 4m 5s\n",
      "9177:\tlearn: 3.2793616\ttotal: 45m 40s\tremaining: 4m 5s\n",
      "9178:\tlearn: 3.2791764\ttotal: 45m 40s\tremaining: 4m 5s\n",
      "9179:\tlearn: 3.2789714\ttotal: 45m 41s\tremaining: 4m 4s\n",
      "9180:\tlearn: 3.2787198\ttotal: 45m 41s\tremaining: 4m 4s\n",
      "9181:\tlearn: 3.2784646\ttotal: 45m 41s\tremaining: 4m 4s\n",
      "9182:\tlearn: 3.2780127\ttotal: 45m 42s\tremaining: 4m 3s\n",
      "9183:\tlearn: 3.2777370\ttotal: 45m 42s\tremaining: 4m 3s\n",
      "9184:\tlearn: 3.2776148\ttotal: 45m 42s\tremaining: 4m 3s\n",
      "9185:\tlearn: 3.2773747\ttotal: 45m 42s\tremaining: 4m 3s\n",
      "9186:\tlearn: 3.2771733\ttotal: 45m 43s\tremaining: 4m 2s\n",
      "9187:\tlearn: 3.2769653\ttotal: 45m 43s\tremaining: 4m 2s\n",
      "9188:\tlearn: 3.2767077\ttotal: 45m 43s\tremaining: 4m 2s\n",
      "9189:\tlearn: 3.2765038\ttotal: 45m 44s\tremaining: 4m 1s\n",
      "9190:\tlearn: 3.2763415\ttotal: 45m 44s\tremaining: 4m 1s\n",
      "9191:\tlearn: 3.2760878\ttotal: 45m 44s\tremaining: 4m 1s\n",
      "9192:\tlearn: 3.2759499\ttotal: 45m 44s\tremaining: 4m\n",
      "9193:\tlearn: 3.2756975\ttotal: 45m 45s\tremaining: 4m\n",
      "9194:\tlearn: 3.2753791\ttotal: 45m 45s\tremaining: 4m\n",
      "9195:\tlearn: 3.2751820\ttotal: 45m 45s\tremaining: 4m\n",
      "9196:\tlearn: 3.2749508\ttotal: 45m 46s\tremaining: 3m 59s\n",
      "9197:\tlearn: 3.2747502\ttotal: 45m 46s\tremaining: 3m 59s\n",
      "9198:\tlearn: 3.2746306\ttotal: 45m 46s\tremaining: 3m 59s\n",
      "9199:\tlearn: 3.2742686\ttotal: 45m 46s\tremaining: 3m 58s\n",
      "9200:\tlearn: 3.2741019\ttotal: 45m 47s\tremaining: 3m 58s\n",
      "9201:\tlearn: 3.2739646\ttotal: 45m 47s\tremaining: 3m 58s\n",
      "9202:\tlearn: 3.2737788\ttotal: 45m 47s\tremaining: 3m 57s\n",
      "9203:\tlearn: 3.2735655\ttotal: 45m 47s\tremaining: 3m 57s\n",
      "9204:\tlearn: 3.2733682\ttotal: 45m 48s\tremaining: 3m 57s\n",
      "9205:\tlearn: 3.2731974\ttotal: 45m 48s\tremaining: 3m 57s\n",
      "9206:\tlearn: 3.2730267\ttotal: 45m 48s\tremaining: 3m 56s\n",
      "9207:\tlearn: 3.2726658\ttotal: 45m 48s\tremaining: 3m 56s\n",
      "9208:\tlearn: 3.2723366\ttotal: 45m 49s\tremaining: 3m 56s\n",
      "9209:\tlearn: 3.2721384\ttotal: 45m 49s\tremaining: 3m 55s\n",
      "9210:\tlearn: 3.2718076\ttotal: 45m 49s\tremaining: 3m 55s\n",
      "9211:\tlearn: 3.2714870\ttotal: 45m 49s\tremaining: 3m 55s\n",
      "9212:\tlearn: 3.2711868\ttotal: 45m 50s\tremaining: 3m 54s\n",
      "9213:\tlearn: 3.2709343\ttotal: 45m 50s\tremaining: 3m 54s\n",
      "9214:\tlearn: 3.2708386\ttotal: 45m 50s\tremaining: 3m 54s\n",
      "9215:\tlearn: 3.2705340\ttotal: 45m 51s\tremaining: 3m 54s\n",
      "9216:\tlearn: 3.2703693\ttotal: 45m 51s\tremaining: 3m 53s\n",
      "9217:\tlearn: 3.2701857\ttotal: 45m 51s\tremaining: 3m 53s\n",
      "9218:\tlearn: 3.2699701\ttotal: 45m 52s\tremaining: 3m 53s\n",
      "9219:\tlearn: 3.2698400\ttotal: 45m 52s\tremaining: 3m 52s\n",
      "9220:\tlearn: 3.2696526\ttotal: 45m 52s\tremaining: 3m 52s\n",
      "9221:\tlearn: 3.2693878\ttotal: 45m 52s\tremaining: 3m 52s\n",
      "9222:\tlearn: 3.2692310\ttotal: 45m 53s\tremaining: 3m 51s\n",
      "9223:\tlearn: 3.2689379\ttotal: 45m 53s\tremaining: 3m 51s\n",
      "9224:\tlearn: 3.2686344\ttotal: 45m 53s\tremaining: 3m 51s\n",
      "9225:\tlearn: 3.2683274\ttotal: 45m 54s\tremaining: 3m 51s\n",
      "9226:\tlearn: 3.2681242\ttotal: 45m 54s\tremaining: 3m 50s\n",
      "9227:\tlearn: 3.2678209\ttotal: 45m 54s\tremaining: 3m 50s\n",
      "9228:\tlearn: 3.2672846\ttotal: 45m 54s\tremaining: 3m 50s\n",
      "9229:\tlearn: 3.2671184\ttotal: 45m 55s\tremaining: 3m 49s\n",
      "9230:\tlearn: 3.2668422\ttotal: 45m 55s\tremaining: 3m 49s\n",
      "9231:\tlearn: 3.2667060\ttotal: 45m 55s\tremaining: 3m 49s\n",
      "9232:\tlearn: 3.2665489\ttotal: 45m 55s\tremaining: 3m 48s\n",
      "9233:\tlearn: 3.2662554\ttotal: 45m 56s\tremaining: 3m 48s\n",
      "9234:\tlearn: 3.2660459\ttotal: 45m 56s\tremaining: 3m 48s\n",
      "9235:\tlearn: 3.2657560\ttotal: 45m 56s\tremaining: 3m 48s\n",
      "9236:\tlearn: 3.2654226\ttotal: 45m 57s\tremaining: 3m 47s\n",
      "9237:\tlearn: 3.2650980\ttotal: 45m 57s\tremaining: 3m 47s\n",
      "9238:\tlearn: 3.2648716\ttotal: 45m 57s\tremaining: 3m 47s\n",
      "9239:\tlearn: 3.2646477\ttotal: 45m 57s\tremaining: 3m 46s\n",
      "9240:\tlearn: 3.2645052\ttotal: 45m 58s\tremaining: 3m 46s\n",
      "9241:\tlearn: 3.2643083\ttotal: 45m 58s\tremaining: 3m 46s\n",
      "9242:\tlearn: 3.2642351\ttotal: 45m 58s\tremaining: 3m 45s\n",
      "9243:\tlearn: 3.2640855\ttotal: 45m 58s\tremaining: 3m 45s\n",
      "9244:\tlearn: 3.2639691\ttotal: 45m 59s\tremaining: 3m 45s\n",
      "9245:\tlearn: 3.2637822\ttotal: 45m 59s\tremaining: 3m 45s\n",
      "9246:\tlearn: 3.2635905\ttotal: 45m 59s\tremaining: 3m 44s\n",
      "9247:\tlearn: 3.2632707\ttotal: 45m 59s\tremaining: 3m 44s\n",
      "9248:\tlearn: 3.2630197\ttotal: 46m\tremaining: 3m 44s\n",
      "9249:\tlearn: 3.2628378\ttotal: 46m\tremaining: 3m 43s\n",
      "9250:\tlearn: 3.2626042\ttotal: 46m\tremaining: 3m 43s\n",
      "9251:\tlearn: 3.2624532\ttotal: 46m 1s\tremaining: 3m 43s\n",
      "9252:\tlearn: 3.2621951\ttotal: 46m 1s\tremaining: 3m 42s\n",
      "9253:\tlearn: 3.2619080\ttotal: 46m 1s\tremaining: 3m 42s\n",
      "9254:\tlearn: 3.2617026\ttotal: 46m 1s\tremaining: 3m 42s\n",
      "9255:\tlearn: 3.2615187\ttotal: 46m 2s\tremaining: 3m 42s\n",
      "9256:\tlearn: 3.2612429\ttotal: 46m 2s\tremaining: 3m 41s\n",
      "9257:\tlearn: 3.2610943\ttotal: 46m 2s\tremaining: 3m 41s\n",
      "9258:\tlearn: 3.2608673\ttotal: 46m 3s\tremaining: 3m 41s\n",
      "9259:\tlearn: 3.2606791\ttotal: 46m 3s\tremaining: 3m 40s\n",
      "9260:\tlearn: 3.2604907\ttotal: 46m 3s\tremaining: 3m 40s\n",
      "9261:\tlearn: 3.2602779\ttotal: 46m 3s\tremaining: 3m 40s\n",
      "9262:\tlearn: 3.2600199\ttotal: 46m 4s\tremaining: 3m 39s\n",
      "9263:\tlearn: 3.2597941\ttotal: 46m 4s\tremaining: 3m 39s\n",
      "9264:\tlearn: 3.2594806\ttotal: 46m 4s\tremaining: 3m 39s\n",
      "9265:\tlearn: 3.2592278\ttotal: 46m 5s\tremaining: 3m 39s\n",
      "9266:\tlearn: 3.2589116\ttotal: 46m 5s\tremaining: 3m 38s\n",
      "9267:\tlearn: 3.2586302\ttotal: 46m 5s\tremaining: 3m 38s\n",
      "9268:\tlearn: 3.2584145\ttotal: 46m 5s\tremaining: 3m 38s\n",
      "9269:\tlearn: 3.2582545\ttotal: 46m 6s\tremaining: 3m 37s\n",
      "9270:\tlearn: 3.2579366\ttotal: 46m 6s\tremaining: 3m 37s\n",
      "9271:\tlearn: 3.2577296\ttotal: 46m 6s\tremaining: 3m 37s\n",
      "9272:\tlearn: 3.2575096\ttotal: 46m 7s\tremaining: 3m 36s\n",
      "9273:\tlearn: 3.2573156\ttotal: 46m 7s\tremaining: 3m 36s\n",
      "9274:\tlearn: 3.2570668\ttotal: 46m 7s\tremaining: 3m 36s\n",
      "9275:\tlearn: 3.2568913\ttotal: 46m 7s\tremaining: 3m 36s\n",
      "9276:\tlearn: 3.2566660\ttotal: 46m 8s\tremaining: 3m 35s\n",
      "9277:\tlearn: 3.2562946\ttotal: 46m 8s\tremaining: 3m 35s\n",
      "9278:\tlearn: 3.2559619\ttotal: 46m 8s\tremaining: 3m 35s\n",
      "9279:\tlearn: 3.2557020\ttotal: 46m 8s\tremaining: 3m 34s\n",
      "9280:\tlearn: 3.2555385\ttotal: 46m 9s\tremaining: 3m 34s\n",
      "9281:\tlearn: 3.2552955\ttotal: 46m 9s\tremaining: 3m 34s\n",
      "9282:\tlearn: 3.2550628\ttotal: 46m 9s\tremaining: 3m 33s\n",
      "9283:\tlearn: 3.2549540\ttotal: 46m 10s\tremaining: 3m 33s\n",
      "9284:\tlearn: 3.2547761\ttotal: 46m 10s\tremaining: 3m 33s\n",
      "9285:\tlearn: 3.2546086\ttotal: 46m 10s\tremaining: 3m 33s\n",
      "9286:\tlearn: 3.2544476\ttotal: 46m 11s\tremaining: 3m 32s\n",
      "9287:\tlearn: 3.2542463\ttotal: 46m 11s\tremaining: 3m 32s\n",
      "9288:\tlearn: 3.2539289\ttotal: 46m 11s\tremaining: 3m 32s\n",
      "9289:\tlearn: 3.2536322\ttotal: 46m 11s\tremaining: 3m 31s\n",
      "9290:\tlearn: 3.2534327\ttotal: 46m 12s\tremaining: 3m 31s\n",
      "9291:\tlearn: 3.2532036\ttotal: 46m 12s\tremaining: 3m 31s\n",
      "9292:\tlearn: 3.2529616\ttotal: 46m 12s\tremaining: 3m 30s\n",
      "9293:\tlearn: 3.2527275\ttotal: 46m 12s\tremaining: 3m 30s\n",
      "9294:\tlearn: 3.2525159\ttotal: 46m 13s\tremaining: 3m 30s\n",
      "9295:\tlearn: 3.2523458\ttotal: 46m 13s\tremaining: 3m 30s\n",
      "9296:\tlearn: 3.2521253\ttotal: 46m 13s\tremaining: 3m 29s\n",
      "9297:\tlearn: 3.2518955\ttotal: 46m 13s\tremaining: 3m 29s\n",
      "9298:\tlearn: 3.2517396\ttotal: 46m 14s\tremaining: 3m 29s\n",
      "9299:\tlearn: 3.2515984\ttotal: 46m 14s\tremaining: 3m 28s\n",
      "9300:\tlearn: 3.2513823\ttotal: 46m 14s\tremaining: 3m 28s\n",
      "9301:\tlearn: 3.2510606\ttotal: 46m 15s\tremaining: 3m 28s\n",
      "9302:\tlearn: 3.2507123\ttotal: 46m 15s\tremaining: 3m 27s\n",
      "9303:\tlearn: 3.2505302\ttotal: 46m 15s\tremaining: 3m 27s\n",
      "9304:\tlearn: 3.2503277\ttotal: 46m 15s\tremaining: 3m 27s\n",
      "9305:\tlearn: 3.2502127\ttotal: 46m 16s\tremaining: 3m 27s\n",
      "9306:\tlearn: 3.2500892\ttotal: 46m 16s\tremaining: 3m 26s\n",
      "9307:\tlearn: 3.2497774\ttotal: 46m 16s\tremaining: 3m 26s\n",
      "9308:\tlearn: 3.2495898\ttotal: 46m 17s\tremaining: 3m 26s\n",
      "9309:\tlearn: 3.2494079\ttotal: 46m 17s\tremaining: 3m 25s\n",
      "9310:\tlearn: 3.2492002\ttotal: 46m 17s\tremaining: 3m 25s\n",
      "9311:\tlearn: 3.2490231\ttotal: 46m 17s\tremaining: 3m 25s\n",
      "9312:\tlearn: 3.2487629\ttotal: 46m 18s\tremaining: 3m 24s\n",
      "9313:\tlearn: 3.2485194\ttotal: 46m 18s\tremaining: 3m 24s\n",
      "9314:\tlearn: 3.2482362\ttotal: 46m 18s\tremaining: 3m 24s\n",
      "9315:\tlearn: 3.2479005\ttotal: 46m 19s\tremaining: 3m 24s\n",
      "9316:\tlearn: 3.2477117\ttotal: 46m 19s\tremaining: 3m 23s\n",
      "9317:\tlearn: 3.2475317\ttotal: 46m 19s\tremaining: 3m 23s\n",
      "9318:\tlearn: 3.2472305\ttotal: 46m 20s\tremaining: 3m 23s\n",
      "9319:\tlearn: 3.2468990\ttotal: 46m 20s\tremaining: 3m 22s\n",
      "9320:\tlearn: 3.2467006\ttotal: 46m 20s\tremaining: 3m 22s\n",
      "9321:\tlearn: 3.2465362\ttotal: 46m 20s\tremaining: 3m 22s\n",
      "9322:\tlearn: 3.2464134\ttotal: 46m 21s\tremaining: 3m 21s\n",
      "9323:\tlearn: 3.2462063\ttotal: 46m 21s\tremaining: 3m 21s\n",
      "9324:\tlearn: 3.2460488\ttotal: 46m 21s\tremaining: 3m 21s\n",
      "9325:\tlearn: 3.2458242\ttotal: 46m 21s\tremaining: 3m 21s\n",
      "9326:\tlearn: 3.2456187\ttotal: 46m 22s\tremaining: 3m 20s\n",
      "9327:\tlearn: 3.2453412\ttotal: 46m 22s\tremaining: 3m 20s\n",
      "9328:\tlearn: 3.2451521\ttotal: 46m 22s\tremaining: 3m 20s\n",
      "9329:\tlearn: 3.2449540\ttotal: 46m 22s\tremaining: 3m 19s\n",
      "9330:\tlearn: 3.2446391\ttotal: 46m 23s\tremaining: 3m 19s\n",
      "9331:\tlearn: 3.2443823\ttotal: 46m 23s\tremaining: 3m 19s\n",
      "9332:\tlearn: 3.2441928\ttotal: 46m 23s\tremaining: 3m 18s\n",
      "9333:\tlearn: 3.2440185\ttotal: 46m 23s\tremaining: 3m 18s\n",
      "9334:\tlearn: 3.2438408\ttotal: 46m 24s\tremaining: 3m 18s\n",
      "9335:\tlearn: 3.2436188\ttotal: 46m 24s\tremaining: 3m 18s\n",
      "9336:\tlearn: 3.2434699\ttotal: 46m 24s\tremaining: 3m 17s\n",
      "9337:\tlearn: 3.2432641\ttotal: 46m 24s\tremaining: 3m 17s\n",
      "9338:\tlearn: 3.2431345\ttotal: 46m 25s\tremaining: 3m 17s\n",
      "9339:\tlearn: 3.2428966\ttotal: 46m 25s\tremaining: 3m 16s\n",
      "9340:\tlearn: 3.2425752\ttotal: 46m 25s\tremaining: 3m 16s\n",
      "9341:\tlearn: 3.2424471\ttotal: 46m 26s\tremaining: 3m 16s\n",
      "9342:\tlearn: 3.2423204\ttotal: 46m 26s\tremaining: 3m 15s\n",
      "9343:\tlearn: 3.2420457\ttotal: 46m 26s\tremaining: 3m 15s\n",
      "9344:\tlearn: 3.2418811\ttotal: 46m 26s\tremaining: 3m 15s\n",
      "9345:\tlearn: 3.2417272\ttotal: 46m 27s\tremaining: 3m 15s\n",
      "9346:\tlearn: 3.2414334\ttotal: 46m 27s\tremaining: 3m 14s\n",
      "9347:\tlearn: 3.2411882\ttotal: 46m 27s\tremaining: 3m 14s\n",
      "9348:\tlearn: 3.2409617\ttotal: 46m 27s\tremaining: 3m 14s\n",
      "9349:\tlearn: 3.2407056\ttotal: 46m 28s\tremaining: 3m 13s\n",
      "9350:\tlearn: 3.2405043\ttotal: 46m 28s\tremaining: 3m 13s\n",
      "9351:\tlearn: 3.2403143\ttotal: 46m 28s\tremaining: 3m 13s\n",
      "9352:\tlearn: 3.2400983\ttotal: 46m 29s\tremaining: 3m 12s\n",
      "9353:\tlearn: 3.2397731\ttotal: 46m 29s\tremaining: 3m 12s\n",
      "9354:\tlearn: 3.2395429\ttotal: 46m 29s\tremaining: 3m 12s\n",
      "9355:\tlearn: 3.2392423\ttotal: 46m 30s\tremaining: 3m 12s\n",
      "9356:\tlearn: 3.2390573\ttotal: 46m 30s\tremaining: 3m 11s\n",
      "9357:\tlearn: 3.2386940\ttotal: 46m 30s\tremaining: 3m 11s\n",
      "9358:\tlearn: 3.2384293\ttotal: 46m 30s\tremaining: 3m 11s\n",
      "9359:\tlearn: 3.2381290\ttotal: 46m 31s\tremaining: 3m 10s\n",
      "9360:\tlearn: 3.2378963\ttotal: 46m 31s\tremaining: 3m 10s\n",
      "9361:\tlearn: 3.2376871\ttotal: 46m 31s\tremaining: 3m 10s\n",
      "9362:\tlearn: 3.2374766\ttotal: 46m 32s\tremaining: 3m 9s\n",
      "9363:\tlearn: 3.2371733\ttotal: 46m 32s\tremaining: 3m 9s\n",
      "9364:\tlearn: 3.2369268\ttotal: 46m 32s\tremaining: 3m 9s\n",
      "9365:\tlearn: 3.2367637\ttotal: 46m 32s\tremaining: 3m 9s\n",
      "9366:\tlearn: 3.2365665\ttotal: 46m 33s\tremaining: 3m 8s\n",
      "9367:\tlearn: 3.2363064\ttotal: 46m 33s\tremaining: 3m 8s\n",
      "9368:\tlearn: 3.2360942\ttotal: 46m 33s\tremaining: 3m 8s\n",
      "9369:\tlearn: 3.2360068\ttotal: 46m 33s\tremaining: 3m 7s\n",
      "9370:\tlearn: 3.2357171\ttotal: 46m 34s\tremaining: 3m 7s\n",
      "9371:\tlearn: 3.2355759\ttotal: 46m 34s\tremaining: 3m 7s\n",
      "9372:\tlearn: 3.2351640\ttotal: 46m 34s\tremaining: 3m 6s\n",
      "9373:\tlearn: 3.2348159\ttotal: 46m 34s\tremaining: 3m 6s\n",
      "9374:\tlearn: 3.2345446\ttotal: 46m 35s\tremaining: 3m 6s\n",
      "9375:\tlearn: 3.2343859\ttotal: 46m 35s\tremaining: 3m 6s\n",
      "9376:\tlearn: 3.2340966\ttotal: 46m 35s\tremaining: 3m 5s\n",
      "9377:\tlearn: 3.2339372\ttotal: 46m 36s\tremaining: 3m 5s\n",
      "9378:\tlearn: 3.2336814\ttotal: 46m 36s\tremaining: 3m 5s\n",
      "9379:\tlearn: 3.2334346\ttotal: 46m 36s\tremaining: 3m 4s\n",
      "9380:\tlearn: 3.2331923\ttotal: 46m 36s\tremaining: 3m 4s\n",
      "9381:\tlearn: 3.2330418\ttotal: 46m 37s\tremaining: 3m 4s\n",
      "9382:\tlearn: 3.2328726\ttotal: 46m 37s\tremaining: 3m 3s\n",
      "9383:\tlearn: 3.2326275\ttotal: 46m 37s\tremaining: 3m 3s\n",
      "9384:\tlearn: 3.2324277\ttotal: 46m 38s\tremaining: 3m 3s\n",
      "9385:\tlearn: 3.2323219\ttotal: 46m 38s\tremaining: 3m 3s\n",
      "9386:\tlearn: 3.2320706\ttotal: 46m 38s\tremaining: 3m 2s\n",
      "9387:\tlearn: 3.2316832\ttotal: 46m 38s\tremaining: 3m 2s\n",
      "9388:\tlearn: 3.2314434\ttotal: 46m 39s\tremaining: 3m 2s\n",
      "9389:\tlearn: 3.2312520\ttotal: 46m 39s\tremaining: 3m 1s\n",
      "9390:\tlearn: 3.2310331\ttotal: 46m 39s\tremaining: 3m 1s\n",
      "9391:\tlearn: 3.2308463\ttotal: 46m 40s\tremaining: 3m 1s\n",
      "9392:\tlearn: 3.2306778\ttotal: 46m 40s\tremaining: 3m\n",
      "9393:\tlearn: 3.2305046\ttotal: 46m 40s\tremaining: 3m\n",
      "9394:\tlearn: 3.2302555\ttotal: 46m 41s\tremaining: 3m\n",
      "9395:\tlearn: 3.2300164\ttotal: 46m 41s\tremaining: 3m\n",
      "9396:\tlearn: 3.2299439\ttotal: 46m 41s\tremaining: 2m 59s\n",
      "9397:\tlearn: 3.2296862\ttotal: 46m 41s\tremaining: 2m 59s\n",
      "9398:\tlearn: 3.2294515\ttotal: 46m 42s\tremaining: 2m 59s\n",
      "9399:\tlearn: 3.2292128\ttotal: 46m 42s\tremaining: 2m 58s\n",
      "9400:\tlearn: 3.2290351\ttotal: 46m 42s\tremaining: 2m 58s\n",
      "9401:\tlearn: 3.2287852\ttotal: 46m 42s\tremaining: 2m 58s\n",
      "9402:\tlearn: 3.2284728\ttotal: 46m 43s\tremaining: 2m 57s\n",
      "9403:\tlearn: 3.2282638\ttotal: 46m 43s\tremaining: 2m 57s\n",
      "9404:\tlearn: 3.2281303\ttotal: 46m 43s\tremaining: 2m 57s\n",
      "9405:\tlearn: 3.2279199\ttotal: 46m 43s\tremaining: 2m 57s\n",
      "9406:\tlearn: 3.2277334\ttotal: 46m 44s\tremaining: 2m 56s\n",
      "9407:\tlearn: 3.2274243\ttotal: 46m 44s\tremaining: 2m 56s\n",
      "9408:\tlearn: 3.2273148\ttotal: 46m 44s\tremaining: 2m 56s\n",
      "9409:\tlearn: 3.2270640\ttotal: 46m 45s\tremaining: 2m 55s\n",
      "9410:\tlearn: 3.2268121\ttotal: 46m 45s\tremaining: 2m 55s\n",
      "9411:\tlearn: 3.2263711\ttotal: 46m 45s\tremaining: 2m 55s\n",
      "9412:\tlearn: 3.2261339\ttotal: 46m 45s\tremaining: 2m 54s\n",
      "9413:\tlearn: 3.2259250\ttotal: 46m 46s\tremaining: 2m 54s\n",
      "9414:\tlearn: 3.2257222\ttotal: 46m 46s\tremaining: 2m 54s\n",
      "9415:\tlearn: 3.2254815\ttotal: 46m 46s\tremaining: 2m 54s\n",
      "9416:\tlearn: 3.2252893\ttotal: 46m 47s\tremaining: 2m 53s\n",
      "9417:\tlearn: 3.2250722\ttotal: 46m 47s\tremaining: 2m 53s\n",
      "9418:\tlearn: 3.2248966\ttotal: 46m 47s\tremaining: 2m 53s\n",
      "9419:\tlearn: 3.2245977\ttotal: 46m 48s\tremaining: 2m 52s\n",
      "9420:\tlearn: 3.2244473\ttotal: 46m 48s\tremaining: 2m 52s\n",
      "9421:\tlearn: 3.2241558\ttotal: 46m 48s\tremaining: 2m 52s\n",
      "9422:\tlearn: 3.2239461\ttotal: 46m 48s\tremaining: 2m 52s\n",
      "9423:\tlearn: 3.2237536\ttotal: 46m 49s\tremaining: 2m 51s\n",
      "9424:\tlearn: 3.2235696\ttotal: 46m 49s\tremaining: 2m 51s\n",
      "9425:\tlearn: 3.2233732\ttotal: 46m 49s\tremaining: 2m 51s\n",
      "9426:\tlearn: 3.2232146\ttotal: 46m 50s\tremaining: 2m 50s\n",
      "9427:\tlearn: 3.2228961\ttotal: 46m 50s\tremaining: 2m 50s\n",
      "9428:\tlearn: 3.2227196\ttotal: 46m 50s\tremaining: 2m 50s\n",
      "9429:\tlearn: 3.2225346\ttotal: 46m 50s\tremaining: 2m 49s\n",
      "9430:\tlearn: 3.2222795\ttotal: 46m 51s\tremaining: 2m 49s\n",
      "9431:\tlearn: 3.2221193\ttotal: 46m 51s\tremaining: 2m 49s\n",
      "9432:\tlearn: 3.2219028\ttotal: 46m 51s\tremaining: 2m 49s\n",
      "9433:\tlearn: 3.2215396\ttotal: 46m 52s\tremaining: 2m 48s\n",
      "9434:\tlearn: 3.2212443\ttotal: 46m 52s\tremaining: 2m 48s\n",
      "9435:\tlearn: 3.2210599\ttotal: 46m 52s\tremaining: 2m 48s\n",
      "9436:\tlearn: 3.2208850\ttotal: 46m 52s\tremaining: 2m 47s\n",
      "9437:\tlearn: 3.2207145\ttotal: 46m 53s\tremaining: 2m 47s\n",
      "9438:\tlearn: 3.2205556\ttotal: 46m 53s\tremaining: 2m 47s\n",
      "9439:\tlearn: 3.2201781\ttotal: 46m 53s\tremaining: 2m 46s\n",
      "9440:\tlearn: 3.2199857\ttotal: 46m 53s\tremaining: 2m 46s\n",
      "9441:\tlearn: 3.2198657\ttotal: 46m 54s\tremaining: 2m 46s\n",
      "9442:\tlearn: 3.2196937\ttotal: 46m 54s\tremaining: 2m 46s\n",
      "9443:\tlearn: 3.2194815\ttotal: 46m 54s\tremaining: 2m 45s\n",
      "9444:\tlearn: 3.2192413\ttotal: 46m 54s\tremaining: 2m 45s\n",
      "9445:\tlearn: 3.2190170\ttotal: 46m 55s\tremaining: 2m 45s\n",
      "9446:\tlearn: 3.2189119\ttotal: 46m 55s\tremaining: 2m 44s\n",
      "9447:\tlearn: 3.2186591\ttotal: 46m 55s\tremaining: 2m 44s\n",
      "9448:\tlearn: 3.2185092\ttotal: 46m 55s\tremaining: 2m 44s\n",
      "9449:\tlearn: 3.2183531\ttotal: 46m 56s\tremaining: 2m 43s\n",
      "9450:\tlearn: 3.2179893\ttotal: 46m 56s\tremaining: 2m 43s\n",
      "9451:\tlearn: 3.2178542\ttotal: 46m 56s\tremaining: 2m 43s\n",
      "9452:\tlearn: 3.2176138\ttotal: 46m 57s\tremaining: 2m 43s\n",
      "9453:\tlearn: 3.2174660\ttotal: 46m 57s\tremaining: 2m 42s\n",
      "9454:\tlearn: 3.2172675\ttotal: 46m 57s\tremaining: 2m 42s\n",
      "9455:\tlearn: 3.2171303\ttotal: 46m 57s\tremaining: 2m 42s\n",
      "9456:\tlearn: 3.2169238\ttotal: 46m 58s\tremaining: 2m 41s\n",
      "9457:\tlearn: 3.2167473\ttotal: 46m 58s\tremaining: 2m 41s\n",
      "9458:\tlearn: 3.2164813\ttotal: 46m 58s\tremaining: 2m 41s\n",
      "9459:\tlearn: 3.2162334\ttotal: 46m 59s\tremaining: 2m 40s\n",
      "9460:\tlearn: 3.2160648\ttotal: 46m 59s\tremaining: 2m 40s\n",
      "9461:\tlearn: 3.2158405\ttotal: 46m 59s\tremaining: 2m 40s\n",
      "9462:\tlearn: 3.2156931\ttotal: 46m 59s\tremaining: 2m 40s\n",
      "9463:\tlearn: 3.2154859\ttotal: 47m\tremaining: 2m 39s\n",
      "9464:\tlearn: 3.2151912\ttotal: 47m\tremaining: 2m 39s\n",
      "9465:\tlearn: 3.2149977\ttotal: 47m\tremaining: 2m 39s\n",
      "9466:\tlearn: 3.2147375\ttotal: 47m 1s\tremaining: 2m 38s\n",
      "9467:\tlearn: 3.2145533\ttotal: 47m 1s\tremaining: 2m 38s\n",
      "9468:\tlearn: 3.2144381\ttotal: 47m 1s\tremaining: 2m 38s\n",
      "9469:\tlearn: 3.2142253\ttotal: 47m 1s\tremaining: 2m 37s\n",
      "9470:\tlearn: 3.2140849\ttotal: 47m 2s\tremaining: 2m 37s\n",
      "9471:\tlearn: 3.2138974\ttotal: 47m 2s\tremaining: 2m 37s\n",
      "9472:\tlearn: 3.2137588\ttotal: 47m 2s\tremaining: 2m 37s\n",
      "9473:\tlearn: 3.2135450\ttotal: 47m 2s\tremaining: 2m 36s\n",
      "9474:\tlearn: 3.2132968\ttotal: 47m 3s\tremaining: 2m 36s\n",
      "9475:\tlearn: 3.2129843\ttotal: 47m 3s\tremaining: 2m 36s\n",
      "9476:\tlearn: 3.2127712\ttotal: 47m 3s\tremaining: 2m 35s\n",
      "9477:\tlearn: 3.2125419\ttotal: 47m 4s\tremaining: 2m 35s\n",
      "9478:\tlearn: 3.2123678\ttotal: 47m 4s\tremaining: 2m 35s\n",
      "9479:\tlearn: 3.2121742\ttotal: 47m 4s\tremaining: 2m 34s\n",
      "9480:\tlearn: 3.2120365\ttotal: 47m 4s\tremaining: 2m 34s\n",
      "9481:\tlearn: 3.2117798\ttotal: 47m 5s\tremaining: 2m 34s\n",
      "9482:\tlearn: 3.2116174\ttotal: 47m 5s\tremaining: 2m 34s\n",
      "9483:\tlearn: 3.2114024\ttotal: 47m 5s\tremaining: 2m 33s\n",
      "9484:\tlearn: 3.2112239\ttotal: 47m 6s\tremaining: 2m 33s\n",
      "9485:\tlearn: 3.2110123\ttotal: 47m 6s\tremaining: 2m 33s\n",
      "9486:\tlearn: 3.2107869\ttotal: 47m 6s\tremaining: 2m 32s\n",
      "9487:\tlearn: 3.2105772\ttotal: 47m 6s\tremaining: 2m 32s\n",
      "9488:\tlearn: 3.2102417\ttotal: 47m 7s\tremaining: 2m 32s\n",
      "9489:\tlearn: 3.2100250\ttotal: 47m 7s\tremaining: 2m 31s\n",
      "9490:\tlearn: 3.2097950\ttotal: 47m 7s\tremaining: 2m 31s\n",
      "9491:\tlearn: 3.2095553\ttotal: 47m 8s\tremaining: 2m 31s\n",
      "9492:\tlearn: 3.2094269\ttotal: 47m 8s\tremaining: 2m 31s\n",
      "9493:\tlearn: 3.2091685\ttotal: 47m 8s\tremaining: 2m 30s\n",
      "9494:\tlearn: 3.2089997\ttotal: 47m 8s\tremaining: 2m 30s\n",
      "9495:\tlearn: 3.2086492\ttotal: 47m 9s\tremaining: 2m 30s\n",
      "9496:\tlearn: 3.2082567\ttotal: 47m 9s\tremaining: 2m 29s\n",
      "9497:\tlearn: 3.2080403\ttotal: 47m 9s\tremaining: 2m 29s\n",
      "9498:\tlearn: 3.2077014\ttotal: 47m 10s\tremaining: 2m 29s\n",
      "9499:\tlearn: 3.2074908\ttotal: 47m 10s\tremaining: 2m 28s\n",
      "9500:\tlearn: 3.2073246\ttotal: 47m 10s\tremaining: 2m 28s\n",
      "9501:\tlearn: 3.2072178\ttotal: 47m 10s\tremaining: 2m 28s\n",
      "9502:\tlearn: 3.2070483\ttotal: 47m 11s\tremaining: 2m 28s\n",
      "9503:\tlearn: 3.2068996\ttotal: 47m 11s\tremaining: 2m 27s\n",
      "9504:\tlearn: 3.2066353\ttotal: 47m 11s\tremaining: 2m 27s\n",
      "9505:\tlearn: 3.2063523\ttotal: 47m 12s\tremaining: 2m 27s\n",
      "9506:\tlearn: 3.2061430\ttotal: 47m 12s\tremaining: 2m 26s\n",
      "9507:\tlearn: 3.2060375\ttotal: 47m 12s\tremaining: 2m 26s\n",
      "9508:\tlearn: 3.2056959\ttotal: 47m 12s\tremaining: 2m 26s\n",
      "9509:\tlearn: 3.2055020\ttotal: 47m 13s\tremaining: 2m 25s\n",
      "9510:\tlearn: 3.2052442\ttotal: 47m 13s\tremaining: 2m 25s\n",
      "9511:\tlearn: 3.2049405\ttotal: 47m 14s\tremaining: 2m 25s\n",
      "9512:\tlearn: 3.2048650\ttotal: 47m 14s\tremaining: 2m 25s\n",
      "9513:\tlearn: 3.2046881\ttotal: 47m 14s\tremaining: 2m 24s\n",
      "9514:\tlearn: 3.2044609\ttotal: 47m 14s\tremaining: 2m 24s\n",
      "9515:\tlearn: 3.2042263\ttotal: 47m 15s\tremaining: 2m 24s\n",
      "9516:\tlearn: 3.2040008\ttotal: 47m 15s\tremaining: 2m 23s\n",
      "9517:\tlearn: 3.2036295\ttotal: 47m 15s\tremaining: 2m 23s\n",
      "9518:\tlearn: 3.2032991\ttotal: 47m 15s\tremaining: 2m 23s\n",
      "9519:\tlearn: 3.2030787\ttotal: 47m 16s\tremaining: 2m 23s\n",
      "9520:\tlearn: 3.2028588\ttotal: 47m 16s\tremaining: 2m 22s\n",
      "9521:\tlearn: 3.2026919\ttotal: 47m 16s\tremaining: 2m 22s\n",
      "9522:\tlearn: 3.2024489\ttotal: 47m 17s\tremaining: 2m 22s\n",
      "9523:\tlearn: 3.2021682\ttotal: 47m 17s\tremaining: 2m 21s\n",
      "9524:\tlearn: 3.2019859\ttotal: 47m 17s\tremaining: 2m 21s\n",
      "9525:\tlearn: 3.2017165\ttotal: 47m 17s\tremaining: 2m 21s\n",
      "9526:\tlearn: 3.2014742\ttotal: 47m 18s\tremaining: 2m 20s\n",
      "9527:\tlearn: 3.2013421\ttotal: 47m 18s\tremaining: 2m 20s\n",
      "9528:\tlearn: 3.2009101\ttotal: 47m 18s\tremaining: 2m 20s\n",
      "9529:\tlearn: 3.2005967\ttotal: 47m 19s\tremaining: 2m 20s\n",
      "9530:\tlearn: 3.2003847\ttotal: 47m 19s\tremaining: 2m 19s\n",
      "9531:\tlearn: 3.2001234\ttotal: 47m 19s\tremaining: 2m 19s\n",
      "9532:\tlearn: 3.1999933\ttotal: 47m 19s\tremaining: 2m 19s\n",
      "9533:\tlearn: 3.1997804\ttotal: 47m 20s\tremaining: 2m 18s\n",
      "9534:\tlearn: 3.1996453\ttotal: 47m 20s\tremaining: 2m 18s\n",
      "9535:\tlearn: 3.1993447\ttotal: 47m 20s\tremaining: 2m 18s\n",
      "9536:\tlearn: 3.1992410\ttotal: 47m 20s\tremaining: 2m 17s\n",
      "9537:\tlearn: 3.1990393\ttotal: 47m 21s\tremaining: 2m 17s\n",
      "9538:\tlearn: 3.1988781\ttotal: 47m 21s\tremaining: 2m 17s\n",
      "9539:\tlearn: 3.1987092\ttotal: 47m 21s\tremaining: 2m 17s\n",
      "9540:\tlearn: 3.1983755\ttotal: 47m 22s\tremaining: 2m 16s\n",
      "9541:\tlearn: 3.1981704\ttotal: 47m 22s\tremaining: 2m 16s\n",
      "9542:\tlearn: 3.1979639\ttotal: 47m 22s\tremaining: 2m 16s\n",
      "9543:\tlearn: 3.1977828\ttotal: 47m 22s\tremaining: 2m 15s\n",
      "9544:\tlearn: 3.1975599\ttotal: 47m 23s\tremaining: 2m 15s\n",
      "9545:\tlearn: 3.1973805\ttotal: 47m 23s\tremaining: 2m 15s\n",
      "9546:\tlearn: 3.1971694\ttotal: 47m 23s\tremaining: 2m 14s\n",
      "9547:\tlearn: 3.1970168\ttotal: 47m 23s\tremaining: 2m 14s\n",
      "9548:\tlearn: 3.1968494\ttotal: 47m 24s\tremaining: 2m 14s\n",
      "9549:\tlearn: 3.1965878\ttotal: 47m 24s\tremaining: 2m 14s\n",
      "9550:\tlearn: 3.1964041\ttotal: 47m 24s\tremaining: 2m 13s\n",
      "9551:\tlearn: 3.1961403\ttotal: 47m 24s\tremaining: 2m 13s\n",
      "9552:\tlearn: 3.1958882\ttotal: 47m 25s\tremaining: 2m 13s\n",
      "9553:\tlearn: 3.1956072\ttotal: 47m 25s\tremaining: 2m 12s\n",
      "9554:\tlearn: 3.1952954\ttotal: 47m 25s\tremaining: 2m 12s\n",
      "9555:\tlearn: 3.1949168\ttotal: 47m 26s\tremaining: 2m 12s\n",
      "9556:\tlearn: 3.1946948\ttotal: 47m 26s\tremaining: 2m 11s\n",
      "9557:\tlearn: 3.1945230\ttotal: 47m 26s\tremaining: 2m 11s\n",
      "9558:\tlearn: 3.1943360\ttotal: 47m 26s\tremaining: 2m 11s\n",
      "9559:\tlearn: 3.1941391\ttotal: 47m 27s\tremaining: 2m 11s\n",
      "9560:\tlearn: 3.1938017\ttotal: 47m 27s\tremaining: 2m 10s\n",
      "9561:\tlearn: 3.1936885\ttotal: 47m 27s\tremaining: 2m 10s\n",
      "9562:\tlearn: 3.1934057\ttotal: 47m 28s\tremaining: 2m 10s\n",
      "9563:\tlearn: 3.1930803\ttotal: 47m 28s\tremaining: 2m 9s\n",
      "9564:\tlearn: 3.1928669\ttotal: 47m 28s\tremaining: 2m 9s\n",
      "9565:\tlearn: 3.1926791\ttotal: 47m 28s\tremaining: 2m 9s\n",
      "9566:\tlearn: 3.1924726\ttotal: 47m 29s\tremaining: 2m 8s\n",
      "9567:\tlearn: 3.1922512\ttotal: 47m 29s\tremaining: 2m 8s\n",
      "9568:\tlearn: 3.1920562\ttotal: 47m 29s\tremaining: 2m 8s\n",
      "9569:\tlearn: 3.1917844\ttotal: 47m 29s\tremaining: 2m 8s\n",
      "9570:\tlearn: 3.1915877\ttotal: 47m 30s\tremaining: 2m 7s\n",
      "9571:\tlearn: 3.1914221\ttotal: 47m 30s\tremaining: 2m 7s\n",
      "9572:\tlearn: 3.1911808\ttotal: 47m 30s\tremaining: 2m 7s\n",
      "9573:\tlearn: 3.1909760\ttotal: 47m 30s\tremaining: 2m 6s\n",
      "9574:\tlearn: 3.1907412\ttotal: 47m 31s\tremaining: 2m 6s\n",
      "9575:\tlearn: 3.1904589\ttotal: 47m 31s\tremaining: 2m 6s\n",
      "9576:\tlearn: 3.1902776\ttotal: 47m 31s\tremaining: 2m 5s\n",
      "9577:\tlearn: 3.1899403\ttotal: 47m 32s\tremaining: 2m 5s\n",
      "9578:\tlearn: 3.1896776\ttotal: 47m 32s\tremaining: 2m 5s\n",
      "9579:\tlearn: 3.1895556\ttotal: 47m 32s\tremaining: 2m 5s\n",
      "9580:\tlearn: 3.1893336\ttotal: 47m 32s\tremaining: 2m 4s\n",
      "9581:\tlearn: 3.1891504\ttotal: 47m 33s\tremaining: 2m 4s\n",
      "9582:\tlearn: 3.1888997\ttotal: 47m 33s\tremaining: 2m 4s\n",
      "9583:\tlearn: 3.1886937\ttotal: 47m 33s\tremaining: 2m 3s\n",
      "9584:\tlearn: 3.1884942\ttotal: 47m 33s\tremaining: 2m 3s\n",
      "9585:\tlearn: 3.1881638\ttotal: 47m 34s\tremaining: 2m 3s\n",
      "9586:\tlearn: 3.1877797\ttotal: 47m 34s\tremaining: 2m 2s\n",
      "9587:\tlearn: 3.1875269\ttotal: 47m 34s\tremaining: 2m 2s\n",
      "9588:\tlearn: 3.1873118\ttotal: 47m 35s\tremaining: 2m 2s\n",
      "9589:\tlearn: 3.1869551\ttotal: 47m 35s\tremaining: 2m 2s\n",
      "9590:\tlearn: 3.1867327\ttotal: 47m 35s\tremaining: 2m 1s\n",
      "9591:\tlearn: 3.1865182\ttotal: 47m 36s\tremaining: 2m 1s\n",
      "9592:\tlearn: 3.1862426\ttotal: 47m 36s\tremaining: 2m 1s\n",
      "9593:\tlearn: 3.1859478\ttotal: 47m 36s\tremaining: 2m\n",
      "9594:\tlearn: 3.1857618\ttotal: 47m 36s\tremaining: 2m\n",
      "9595:\tlearn: 3.1855671\ttotal: 47m 37s\tremaining: 2m\n",
      "9596:\tlearn: 3.1853118\ttotal: 47m 37s\tremaining: 1m 59s\n",
      "9597:\tlearn: 3.1849692\ttotal: 47m 37s\tremaining: 1m 59s\n",
      "9598:\tlearn: 3.1848190\ttotal: 47m 38s\tremaining: 1m 59s\n",
      "9599:\tlearn: 3.1845855\ttotal: 47m 38s\tremaining: 1m 59s\n",
      "9600:\tlearn: 3.1843653\ttotal: 47m 38s\tremaining: 1m 58s\n",
      "9601:\tlearn: 3.1841606\ttotal: 47m 39s\tremaining: 1m 58s\n",
      "9602:\tlearn: 3.1840406\ttotal: 47m 39s\tremaining: 1m 58s\n",
      "9603:\tlearn: 3.1837795\ttotal: 47m 39s\tremaining: 1m 57s\n",
      "9604:\tlearn: 3.1836532\ttotal: 47m 39s\tremaining: 1m 57s\n",
      "9605:\tlearn: 3.1835183\ttotal: 47m 40s\tremaining: 1m 57s\n",
      "9606:\tlearn: 3.1832162\ttotal: 47m 40s\tremaining: 1m 57s\n",
      "9607:\tlearn: 3.1830069\ttotal: 47m 40s\tremaining: 1m 56s\n",
      "9608:\tlearn: 3.1828480\ttotal: 47m 41s\tremaining: 1m 56s\n",
      "9609:\tlearn: 3.1827187\ttotal: 47m 41s\tremaining: 1m 56s\n",
      "9610:\tlearn: 3.1825828\ttotal: 47m 41s\tremaining: 1m 55s\n",
      "9611:\tlearn: 3.1824348\ttotal: 47m 41s\tremaining: 1m 55s\n",
      "9612:\tlearn: 3.1821878\ttotal: 47m 42s\tremaining: 1m 55s\n",
      "9613:\tlearn: 3.1820154\ttotal: 47m 42s\tremaining: 1m 54s\n",
      "9614:\tlearn: 3.1817853\ttotal: 47m 42s\tremaining: 1m 54s\n",
      "9615:\tlearn: 3.1815591\ttotal: 47m 43s\tremaining: 1m 54s\n",
      "9616:\tlearn: 3.1812596\ttotal: 47m 43s\tremaining: 1m 54s\n",
      "9617:\tlearn: 3.1810931\ttotal: 47m 43s\tremaining: 1m 53s\n",
      "9618:\tlearn: 3.1808786\ttotal: 47m 43s\tremaining: 1m 53s\n",
      "9619:\tlearn: 3.1807486\ttotal: 47m 44s\tremaining: 1m 53s\n",
      "9620:\tlearn: 3.1804156\ttotal: 47m 44s\tremaining: 1m 52s\n",
      "9621:\tlearn: 3.1802719\ttotal: 47m 44s\tremaining: 1m 52s\n",
      "9622:\tlearn: 3.1800337\ttotal: 47m 44s\tremaining: 1m 52s\n",
      "9623:\tlearn: 3.1798344\ttotal: 47m 45s\tremaining: 1m 51s\n",
      "9624:\tlearn: 3.1795719\ttotal: 47m 45s\tremaining: 1m 51s\n",
      "9625:\tlearn: 3.1793131\ttotal: 47m 45s\tremaining: 1m 51s\n",
      "9626:\tlearn: 3.1790550\ttotal: 47m 46s\tremaining: 1m 51s\n",
      "9627:\tlearn: 3.1788788\ttotal: 47m 46s\tremaining: 1m 50s\n",
      "9628:\tlearn: 3.1786219\ttotal: 47m 46s\tremaining: 1m 50s\n",
      "9629:\tlearn: 3.1784801\ttotal: 47m 46s\tremaining: 1m 50s\n",
      "9630:\tlearn: 3.1782839\ttotal: 47m 47s\tremaining: 1m 49s\n",
      "9631:\tlearn: 3.1780764\ttotal: 47m 47s\tremaining: 1m 49s\n",
      "9632:\tlearn: 3.1778313\ttotal: 47m 47s\tremaining: 1m 49s\n",
      "9633:\tlearn: 3.1775744\ttotal: 47m 47s\tremaining: 1m 48s\n",
      "9634:\tlearn: 3.1774463\ttotal: 47m 48s\tremaining: 1m 48s\n",
      "9635:\tlearn: 3.1772881\ttotal: 47m 48s\tremaining: 1m 48s\n",
      "9636:\tlearn: 3.1771323\ttotal: 47m 48s\tremaining: 1m 48s\n",
      "9637:\tlearn: 3.1769703\ttotal: 47m 49s\tremaining: 1m 47s\n",
      "9638:\tlearn: 3.1768082\ttotal: 47m 49s\tremaining: 1m 47s\n",
      "9639:\tlearn: 3.1765989\ttotal: 47m 49s\tremaining: 1m 47s\n",
      "9640:\tlearn: 3.1762752\ttotal: 47m 49s\tremaining: 1m 46s\n",
      "9641:\tlearn: 3.1760605\ttotal: 47m 50s\tremaining: 1m 46s\n",
      "9642:\tlearn: 3.1759390\ttotal: 47m 50s\tremaining: 1m 46s\n",
      "9643:\tlearn: 3.1757183\ttotal: 47m 50s\tremaining: 1m 45s\n",
      "9644:\tlearn: 3.1754218\ttotal: 47m 50s\tremaining: 1m 45s\n",
      "9645:\tlearn: 3.1751623\ttotal: 47m 51s\tremaining: 1m 45s\n",
      "9646:\tlearn: 3.1749974\ttotal: 47m 51s\tremaining: 1m 45s\n",
      "9647:\tlearn: 3.1748923\ttotal: 47m 51s\tremaining: 1m 44s\n",
      "9648:\tlearn: 3.1747061\ttotal: 47m 51s\tremaining: 1m 44s\n",
      "9649:\tlearn: 3.1744871\ttotal: 47m 52s\tremaining: 1m 44s\n",
      "9650:\tlearn: 3.1742869\ttotal: 47m 52s\tremaining: 1m 43s\n",
      "9651:\tlearn: 3.1740641\ttotal: 47m 52s\tremaining: 1m 43s\n",
      "9652:\tlearn: 3.1739132\ttotal: 47m 53s\tremaining: 1m 43s\n",
      "9653:\tlearn: 3.1738041\ttotal: 47m 53s\tremaining: 1m 42s\n",
      "9654:\tlearn: 3.1736305\ttotal: 47m 53s\tremaining: 1m 42s\n",
      "9655:\tlearn: 3.1735275\ttotal: 47m 53s\tremaining: 1m 42s\n",
      "9656:\tlearn: 3.1732961\ttotal: 47m 54s\tremaining: 1m 42s\n",
      "9657:\tlearn: 3.1729699\ttotal: 47m 54s\tremaining: 1m 41s\n",
      "9658:\tlearn: 3.1726428\ttotal: 47m 54s\tremaining: 1m 41s\n",
      "9659:\tlearn: 3.1723657\ttotal: 47m 54s\tremaining: 1m 41s\n",
      "9660:\tlearn: 3.1721333\ttotal: 47m 55s\tremaining: 1m 40s\n",
      "9661:\tlearn: 3.1719724\ttotal: 47m 55s\tremaining: 1m 40s\n",
      "9662:\tlearn: 3.1717389\ttotal: 47m 55s\tremaining: 1m 40s\n",
      "9663:\tlearn: 3.1715550\ttotal: 47m 56s\tremaining: 1m 39s\n",
      "9664:\tlearn: 3.1712949\ttotal: 47m 56s\tremaining: 1m 39s\n",
      "9665:\tlearn: 3.1711417\ttotal: 47m 56s\tremaining: 1m 39s\n",
      "9666:\tlearn: 3.1709388\ttotal: 47m 56s\tremaining: 1m 39s\n",
      "9667:\tlearn: 3.1706764\ttotal: 47m 57s\tremaining: 1m 38s\n",
      "9668:\tlearn: 3.1704317\ttotal: 47m 57s\tremaining: 1m 38s\n",
      "9669:\tlearn: 3.1701396\ttotal: 47m 57s\tremaining: 1m 38s\n",
      "9670:\tlearn: 3.1699407\ttotal: 47m 58s\tremaining: 1m 37s\n",
      "9671:\tlearn: 3.1696518\ttotal: 47m 58s\tremaining: 1m 37s\n",
      "9672:\tlearn: 3.1694410\ttotal: 47m 58s\tremaining: 1m 37s\n",
      "9673:\tlearn: 3.1691815\ttotal: 47m 58s\tremaining: 1m 37s\n",
      "9674:\tlearn: 3.1689444\ttotal: 47m 59s\tremaining: 1m 36s\n",
      "9675:\tlearn: 3.1687306\ttotal: 47m 59s\tremaining: 1m 36s\n",
      "9676:\tlearn: 3.1684523\ttotal: 47m 59s\tremaining: 1m 36s\n",
      "9677:\tlearn: 3.1681786\ttotal: 47m 59s\tremaining: 1m 35s\n",
      "9678:\tlearn: 3.1679400\ttotal: 48m\tremaining: 1m 35s\n",
      "9679:\tlearn: 3.1677561\ttotal: 48m\tremaining: 1m 35s\n",
      "9680:\tlearn: 3.1675853\ttotal: 48m\tremaining: 1m 34s\n",
      "9681:\tlearn: 3.1673158\ttotal: 48m 1s\tremaining: 1m 34s\n",
      "9682:\tlearn: 3.1671073\ttotal: 48m 1s\tremaining: 1m 34s\n",
      "9683:\tlearn: 3.1669061\ttotal: 48m 1s\tremaining: 1m 34s\n",
      "9684:\tlearn: 3.1665658\ttotal: 48m 2s\tremaining: 1m 33s\n",
      "9685:\tlearn: 3.1664220\ttotal: 48m 2s\tremaining: 1m 33s\n",
      "9686:\tlearn: 3.1662069\ttotal: 48m 2s\tremaining: 1m 33s\n",
      "9687:\tlearn: 3.1660420\ttotal: 48m 2s\tremaining: 1m 32s\n",
      "9688:\tlearn: 3.1657510\ttotal: 48m 3s\tremaining: 1m 32s\n",
      "9689:\tlearn: 3.1655450\ttotal: 48m 3s\tremaining: 1m 32s\n",
      "9690:\tlearn: 3.1652917\ttotal: 48m 3s\tremaining: 1m 31s\n",
      "9691:\tlearn: 3.1650546\ttotal: 48m 4s\tremaining: 1m 31s\n",
      "9692:\tlearn: 3.1648722\ttotal: 48m 4s\tremaining: 1m 31s\n",
      "9693:\tlearn: 3.1647208\ttotal: 48m 4s\tremaining: 1m 31s\n",
      "9694:\tlearn: 3.1645489\ttotal: 48m 4s\tremaining: 1m 30s\n",
      "9695:\tlearn: 3.1643340\ttotal: 48m 5s\tremaining: 1m 30s\n",
      "9696:\tlearn: 3.1641371\ttotal: 48m 5s\tremaining: 1m 30s\n",
      "9697:\tlearn: 3.1639588\ttotal: 48m 5s\tremaining: 1m 29s\n",
      "9698:\tlearn: 3.1637510\ttotal: 48m 5s\tremaining: 1m 29s\n",
      "9699:\tlearn: 3.1635310\ttotal: 48m 6s\tremaining: 1m 29s\n",
      "9700:\tlearn: 3.1632974\ttotal: 48m 6s\tremaining: 1m 28s\n",
      "9701:\tlearn: 3.1628872\ttotal: 48m 6s\tremaining: 1m 28s\n",
      "9702:\tlearn: 3.1627150\ttotal: 48m 7s\tremaining: 1m 28s\n",
      "9703:\tlearn: 3.1625414\ttotal: 48m 7s\tremaining: 1m 28s\n",
      "9704:\tlearn: 3.1623694\ttotal: 48m 7s\tremaining: 1m 27s\n",
      "9705:\tlearn: 3.1622154\ttotal: 48m 8s\tremaining: 1m 27s\n",
      "9706:\tlearn: 3.1620318\ttotal: 48m 8s\tremaining: 1m 27s\n",
      "9707:\tlearn: 3.1617938\ttotal: 48m 8s\tremaining: 1m 26s\n",
      "9708:\tlearn: 3.1615844\ttotal: 48m 8s\tremaining: 1m 26s\n",
      "9709:\tlearn: 3.1613035\ttotal: 48m 9s\tremaining: 1m 26s\n",
      "9710:\tlearn: 3.1610635\ttotal: 48m 9s\tremaining: 1m 25s\n",
      "9711:\tlearn: 3.1609090\ttotal: 48m 9s\tremaining: 1m 25s\n",
      "9712:\tlearn: 3.1607610\ttotal: 48m 10s\tremaining: 1m 25s\n",
      "9713:\tlearn: 3.1605485\ttotal: 48m 10s\tremaining: 1m 25s\n",
      "9714:\tlearn: 3.1602884\ttotal: 48m 10s\tremaining: 1m 24s\n",
      "9715:\tlearn: 3.1601695\ttotal: 48m 10s\tremaining: 1m 24s\n",
      "9716:\tlearn: 3.1599259\ttotal: 48m 11s\tremaining: 1m 24s\n",
      "9717:\tlearn: 3.1596557\ttotal: 48m 11s\tremaining: 1m 23s\n",
      "9718:\tlearn: 3.1594773\ttotal: 48m 11s\tremaining: 1m 23s\n",
      "9719:\tlearn: 3.1592243\ttotal: 48m 11s\tremaining: 1m 23s\n",
      "9720:\tlearn: 3.1590419\ttotal: 48m 12s\tremaining: 1m 23s\n",
      "9721:\tlearn: 3.1588268\ttotal: 48m 12s\tremaining: 1m 22s\n",
      "9722:\tlearn: 3.1586486\ttotal: 48m 12s\tremaining: 1m 22s\n",
      "9723:\tlearn: 3.1584458\ttotal: 48m 12s\tremaining: 1m 22s\n",
      "9724:\tlearn: 3.1582318\ttotal: 48m 13s\tremaining: 1m 21s\n",
      "9725:\tlearn: 3.1579946\ttotal: 48m 13s\tremaining: 1m 21s\n",
      "9726:\tlearn: 3.1577467\ttotal: 48m 13s\tremaining: 1m 21s\n",
      "9727:\tlearn: 3.1575198\ttotal: 48m 14s\tremaining: 1m 20s\n",
      "9728:\tlearn: 3.1572013\ttotal: 48m 14s\tremaining: 1m 20s\n",
      "9729:\tlearn: 3.1569446\ttotal: 48m 14s\tremaining: 1m 20s\n",
      "9730:\tlearn: 3.1568351\ttotal: 48m 14s\tremaining: 1m 20s\n",
      "9731:\tlearn: 3.1566354\ttotal: 48m 15s\tremaining: 1m 19s\n",
      "9732:\tlearn: 3.1564359\ttotal: 48m 15s\tremaining: 1m 19s\n",
      "9733:\tlearn: 3.1562650\ttotal: 48m 15s\tremaining: 1m 19s\n",
      "9734:\tlearn: 3.1561103\ttotal: 48m 15s\tremaining: 1m 18s\n",
      "9735:\tlearn: 3.1559508\ttotal: 48m 16s\tremaining: 1m 18s\n",
      "9736:\tlearn: 3.1558832\ttotal: 48m 16s\tremaining: 1m 18s\n",
      "9737:\tlearn: 3.1556710\ttotal: 48m 16s\tremaining: 1m 17s\n",
      "9738:\tlearn: 3.1554587\ttotal: 48m 16s\tremaining: 1m 17s\n",
      "9739:\tlearn: 3.1552891\ttotal: 48m 17s\tremaining: 1m 17s\n",
      "9740:\tlearn: 3.1551115\ttotal: 48m 17s\tremaining: 1m 17s\n",
      "9741:\tlearn: 3.1549729\ttotal: 48m 17s\tremaining: 1m 16s\n",
      "9742:\tlearn: 3.1547874\ttotal: 48m 18s\tremaining: 1m 16s\n",
      "9743:\tlearn: 3.1545540\ttotal: 48m 18s\tremaining: 1m 16s\n",
      "9744:\tlearn: 3.1543207\ttotal: 48m 18s\tremaining: 1m 15s\n",
      "9745:\tlearn: 3.1539614\ttotal: 48m 18s\tremaining: 1m 15s\n",
      "9746:\tlearn: 3.1537892\ttotal: 48m 19s\tremaining: 1m 15s\n",
      "9747:\tlearn: 3.1535706\ttotal: 48m 19s\tremaining: 1m 14s\n",
      "9748:\tlearn: 3.1533799\ttotal: 48m 19s\tremaining: 1m 14s\n",
      "9749:\tlearn: 3.1531437\ttotal: 48m 20s\tremaining: 1m 14s\n",
      "9750:\tlearn: 3.1528710\ttotal: 48m 20s\tremaining: 1m 14s\n",
      "9751:\tlearn: 3.1526317\ttotal: 48m 20s\tremaining: 1m 13s\n",
      "9752:\tlearn: 3.1525271\ttotal: 48m 20s\tremaining: 1m 13s\n",
      "9753:\tlearn: 3.1523560\ttotal: 48m 21s\tremaining: 1m 13s\n",
      "9754:\tlearn: 3.1521937\ttotal: 48m 21s\tremaining: 1m 12s\n",
      "9755:\tlearn: 3.1519670\ttotal: 48m 21s\tremaining: 1m 12s\n",
      "9756:\tlearn: 3.1517928\ttotal: 48m 21s\tremaining: 1m 12s\n",
      "9757:\tlearn: 3.1516797\ttotal: 48m 22s\tremaining: 1m 11s\n",
      "9758:\tlearn: 3.1514876\ttotal: 48m 22s\tremaining: 1m 11s\n",
      "9759:\tlearn: 3.1512467\ttotal: 48m 22s\tremaining: 1m 11s\n",
      "9760:\tlearn: 3.1509843\ttotal: 48m 23s\tremaining: 1m 11s\n",
      "9761:\tlearn: 3.1508186\ttotal: 48m 23s\tremaining: 1m 10s\n",
      "9762:\tlearn: 3.1505167\ttotal: 48m 23s\tremaining: 1m 10s\n",
      "9763:\tlearn: 3.1502458\ttotal: 48m 23s\tremaining: 1m 10s\n",
      "9764:\tlearn: 3.1499180\ttotal: 48m 24s\tremaining: 1m 9s\n",
      "9765:\tlearn: 3.1497096\ttotal: 48m 24s\tremaining: 1m 9s\n",
      "9766:\tlearn: 3.1495186\ttotal: 48m 24s\tremaining: 1m 9s\n",
      "9767:\tlearn: 3.1493173\ttotal: 48m 24s\tremaining: 1m 8s\n",
      "9768:\tlearn: 3.1491593\ttotal: 48m 25s\tremaining: 1m 8s\n",
      "9769:\tlearn: 3.1489394\ttotal: 48m 25s\tremaining: 1m 8s\n",
      "9770:\tlearn: 3.1486848\ttotal: 48m 25s\tremaining: 1m 8s\n",
      "9771:\tlearn: 3.1485101\ttotal: 48m 26s\tremaining: 1m 7s\n",
      "9772:\tlearn: 3.1483196\ttotal: 48m 26s\tremaining: 1m 7s\n",
      "9773:\tlearn: 3.1481176\ttotal: 48m 26s\tremaining: 1m 7s\n",
      "9774:\tlearn: 3.1479328\ttotal: 48m 26s\tremaining: 1m 6s\n",
      "9775:\tlearn: 3.1477184\ttotal: 48m 27s\tremaining: 1m 6s\n",
      "9776:\tlearn: 3.1473826\ttotal: 48m 27s\tremaining: 1m 6s\n",
      "9777:\tlearn: 3.1471993\ttotal: 48m 27s\tremaining: 1m 6s\n",
      "9778:\tlearn: 3.1470285\ttotal: 48m 28s\tremaining: 1m 5s\n",
      "9779:\tlearn: 3.1469229\ttotal: 48m 28s\tremaining: 1m 5s\n",
      "9780:\tlearn: 3.1466798\ttotal: 48m 28s\tremaining: 1m 5s\n",
      "9781:\tlearn: 3.1464657\ttotal: 48m 28s\tremaining: 1m 4s\n",
      "9782:\tlearn: 3.1463247\ttotal: 48m 29s\tremaining: 1m 4s\n",
      "9783:\tlearn: 3.1461891\ttotal: 48m 29s\tremaining: 1m 4s\n",
      "9784:\tlearn: 3.1458783\ttotal: 48m 29s\tremaining: 1m 3s\n",
      "9785:\tlearn: 3.1456793\ttotal: 48m 29s\tremaining: 1m 3s\n",
      "9786:\tlearn: 3.1455722\ttotal: 48m 30s\tremaining: 1m 3s\n",
      "9787:\tlearn: 3.1453610\ttotal: 48m 30s\tremaining: 1m 3s\n",
      "9788:\tlearn: 3.1450775\ttotal: 48m 30s\tremaining: 1m 2s\n",
      "9789:\tlearn: 3.1448017\ttotal: 48m 31s\tremaining: 1m 2s\n",
      "9790:\tlearn: 3.1445906\ttotal: 48m 31s\tremaining: 1m 2s\n",
      "9791:\tlearn: 3.1443892\ttotal: 48m 31s\tremaining: 1m 1s\n",
      "9792:\tlearn: 3.1441816\ttotal: 48m 31s\tremaining: 1m 1s\n",
      "9793:\tlearn: 3.1439635\ttotal: 48m 32s\tremaining: 1m 1s\n",
      "9794:\tlearn: 3.1436740\ttotal: 48m 32s\tremaining: 1m\n",
      "9795:\tlearn: 3.1434461\ttotal: 48m 32s\tremaining: 1m\n",
      "9796:\tlearn: 3.1432401\ttotal: 48m 32s\tremaining: 1m\n",
      "9797:\tlearn: 3.1430521\ttotal: 48m 33s\tremaining: 1m\n",
      "9798:\tlearn: 3.1428774\ttotal: 48m 33s\tremaining: 59.8s\n",
      "9799:\tlearn: 3.1427151\ttotal: 48m 33s\tremaining: 59.5s\n",
      "9800:\tlearn: 3.1424718\ttotal: 48m 33s\tremaining: 59.2s\n",
      "9801:\tlearn: 3.1423383\ttotal: 48m 34s\tremaining: 58.9s\n",
      "9802:\tlearn: 3.1421361\ttotal: 48m 34s\tremaining: 58.6s\n",
      "9803:\tlearn: 3.1418710\ttotal: 48m 34s\tremaining: 58.3s\n",
      "9804:\tlearn: 3.1416739\ttotal: 48m 34s\tremaining: 58s\n",
      "9805:\tlearn: 3.1414364\ttotal: 48m 35s\tremaining: 57.7s\n",
      "9806:\tlearn: 3.1413324\ttotal: 48m 35s\tremaining: 57.4s\n",
      "9807:\tlearn: 3.1411632\ttotal: 48m 35s\tremaining: 57.1s\n",
      "9808:\tlearn: 3.1410334\ttotal: 48m 35s\tremaining: 56.8s\n",
      "9809:\tlearn: 3.1408511\ttotal: 48m 36s\tremaining: 56.5s\n",
      "9810:\tlearn: 3.1406376\ttotal: 48m 36s\tremaining: 56.2s\n",
      "9811:\tlearn: 3.1404819\ttotal: 48m 36s\tremaining: 55.9s\n",
      "9812:\tlearn: 3.1403396\ttotal: 48m 36s\tremaining: 55.6s\n",
      "9813:\tlearn: 3.1401323\ttotal: 48m 37s\tremaining: 55.3s\n",
      "9814:\tlearn: 3.1399501\ttotal: 48m 37s\tremaining: 55s\n",
      "9815:\tlearn: 3.1396667\ttotal: 48m 37s\tremaining: 54.7s\n",
      "9816:\tlearn: 3.1393928\ttotal: 48m 37s\tremaining: 54.4s\n",
      "9817:\tlearn: 3.1392277\ttotal: 48m 38s\tremaining: 54.1s\n",
      "9818:\tlearn: 3.1390122\ttotal: 48m 38s\tremaining: 53.8s\n",
      "9819:\tlearn: 3.1388483\ttotal: 48m 38s\tremaining: 53.5s\n",
      "9820:\tlearn: 3.1386880\ttotal: 48m 38s\tremaining: 53.2s\n",
      "9821:\tlearn: 3.1384153\ttotal: 48m 39s\tremaining: 52.9s\n",
      "9822:\tlearn: 3.1382313\ttotal: 48m 39s\tremaining: 52.6s\n",
      "9823:\tlearn: 3.1380359\ttotal: 48m 39s\tremaining: 52.3s\n",
      "9824:\tlearn: 3.1378006\ttotal: 48m 39s\tremaining: 52s\n",
      "9825:\tlearn: 3.1376356\ttotal: 48m 40s\tremaining: 51.7s\n",
      "9826:\tlearn: 3.1373639\ttotal: 48m 40s\tremaining: 51.4s\n",
      "9827:\tlearn: 3.1372215\ttotal: 48m 40s\tremaining: 51.1s\n",
      "9828:\tlearn: 3.1370419\ttotal: 48m 41s\tremaining: 50.8s\n",
      "9829:\tlearn: 3.1368783\ttotal: 48m 41s\tremaining: 50.5s\n",
      "9830:\tlearn: 3.1365886\ttotal: 48m 41s\tremaining: 50.2s\n",
      "9831:\tlearn: 3.1363568\ttotal: 48m 41s\tremaining: 49.9s\n",
      "9832:\tlearn: 3.1361887\ttotal: 48m 42s\tremaining: 49.6s\n",
      "9833:\tlearn: 3.1360599\ttotal: 48m 42s\tremaining: 49.3s\n",
      "9834:\tlearn: 3.1359111\ttotal: 48m 42s\tremaining: 49s\n",
      "9835:\tlearn: 3.1357434\ttotal: 48m 42s\tremaining: 48.7s\n",
      "9836:\tlearn: 3.1355340\ttotal: 48m 43s\tremaining: 48.4s\n",
      "9837:\tlearn: 3.1353424\ttotal: 48m 43s\tremaining: 48.1s\n",
      "9838:\tlearn: 3.1351082\ttotal: 48m 43s\tremaining: 47.8s\n",
      "9839:\tlearn: 3.1349125\ttotal: 48m 43s\tremaining: 47.5s\n",
      "9840:\tlearn: 3.1346800\ttotal: 48m 44s\tremaining: 47.2s\n",
      "9841:\tlearn: 3.1343870\ttotal: 48m 44s\tremaining: 46.9s\n",
      "9842:\tlearn: 3.1342617\ttotal: 48m 44s\tremaining: 46.6s\n",
      "9843:\tlearn: 3.1339930\ttotal: 48m 44s\tremaining: 46.4s\n",
      "9844:\tlearn: 3.1338040\ttotal: 48m 45s\tremaining: 46.1s\n",
      "9845:\tlearn: 3.1336985\ttotal: 48m 45s\tremaining: 45.8s\n",
      "9846:\tlearn: 3.1334615\ttotal: 48m 45s\tremaining: 45.5s\n",
      "9847:\tlearn: 3.1332679\ttotal: 48m 45s\tremaining: 45.2s\n",
      "9848:\tlearn: 3.1330250\ttotal: 48m 46s\tremaining: 44.9s\n",
      "9849:\tlearn: 3.1328200\ttotal: 48m 46s\tremaining: 44.6s\n",
      "9850:\tlearn: 3.1326811\ttotal: 48m 46s\tremaining: 44.3s\n",
      "9851:\tlearn: 3.1324429\ttotal: 48m 46s\tremaining: 44s\n",
      "9852:\tlearn: 3.1322578\ttotal: 48m 47s\tremaining: 43.7s\n",
      "9853:\tlearn: 3.1319673\ttotal: 48m 47s\tremaining: 43.4s\n",
      "9854:\tlearn: 3.1318318\ttotal: 48m 47s\tremaining: 43.1s\n",
      "9855:\tlearn: 3.1316528\ttotal: 48m 48s\tremaining: 42.8s\n",
      "9856:\tlearn: 3.1313893\ttotal: 48m 48s\tremaining: 42.5s\n",
      "9857:\tlearn: 3.1311720\ttotal: 48m 48s\tremaining: 42.2s\n",
      "9858:\tlearn: 3.1309720\ttotal: 48m 48s\tremaining: 41.9s\n",
      "9859:\tlearn: 3.1307428\ttotal: 48m 49s\tremaining: 41.6s\n",
      "9860:\tlearn: 3.1306284\ttotal: 48m 49s\tremaining: 41.3s\n",
      "9861:\tlearn: 3.1304433\ttotal: 48m 49s\tremaining: 41s\n",
      "9862:\tlearn: 3.1302263\ttotal: 48m 49s\tremaining: 40.7s\n",
      "9863:\tlearn: 3.1300903\ttotal: 48m 50s\tremaining: 40.4s\n",
      "9864:\tlearn: 3.1298847\ttotal: 48m 50s\tremaining: 40.1s\n",
      "9865:\tlearn: 3.1297154\ttotal: 48m 50s\tremaining: 39.8s\n",
      "9866:\tlearn: 3.1295318\ttotal: 48m 50s\tremaining: 39.5s\n",
      "9867:\tlearn: 3.1293944\ttotal: 48m 51s\tremaining: 39.2s\n",
      "9868:\tlearn: 3.1291773\ttotal: 48m 51s\tremaining: 38.9s\n",
      "9869:\tlearn: 3.1289598\ttotal: 48m 51s\tremaining: 38.6s\n",
      "9870:\tlearn: 3.1286370\ttotal: 48m 52s\tremaining: 38.3s\n",
      "9871:\tlearn: 3.1282879\ttotal: 48m 52s\tremaining: 38s\n",
      "9872:\tlearn: 3.1281274\ttotal: 48m 52s\tremaining: 37.7s\n",
      "9873:\tlearn: 3.1279455\ttotal: 48m 52s\tremaining: 37.4s\n",
      "9874:\tlearn: 3.1276444\ttotal: 48m 53s\tremaining: 37.1s\n",
      "9875:\tlearn: 3.1274764\ttotal: 48m 53s\tremaining: 36.8s\n",
      "9876:\tlearn: 3.1271919\ttotal: 48m 53s\tremaining: 36.5s\n",
      "9877:\tlearn: 3.1270622\ttotal: 48m 53s\tremaining: 36.2s\n",
      "9878:\tlearn: 3.1268746\ttotal: 48m 54s\tremaining: 35.9s\n",
      "9879:\tlearn: 3.1266098\ttotal: 48m 54s\tremaining: 35.6s\n",
      "9880:\tlearn: 3.1263977\ttotal: 48m 54s\tremaining: 35.3s\n",
      "9881:\tlearn: 3.1262067\ttotal: 48m 55s\tremaining: 35s\n",
      "9882:\tlearn: 3.1259467\ttotal: 48m 55s\tremaining: 34.8s\n",
      "9883:\tlearn: 3.1256034\ttotal: 48m 55s\tremaining: 34.5s\n",
      "9884:\tlearn: 3.1254616\ttotal: 48m 56s\tremaining: 34.2s\n",
      "9885:\tlearn: 3.1253228\ttotal: 48m 56s\tremaining: 33.9s\n",
      "9886:\tlearn: 3.1251330\ttotal: 48m 56s\tremaining: 33.6s\n",
      "9887:\tlearn: 3.1248715\ttotal: 48m 56s\tremaining: 33.3s\n",
      "9888:\tlearn: 3.1246457\ttotal: 48m 57s\tremaining: 33s\n",
      "9889:\tlearn: 3.1245244\ttotal: 48m 57s\tremaining: 32.7s\n",
      "9890:\tlearn: 3.1243183\ttotal: 48m 57s\tremaining: 32.4s\n",
      "9891:\tlearn: 3.1241160\ttotal: 48m 57s\tremaining: 32.1s\n",
      "9892:\tlearn: 3.1239567\ttotal: 48m 58s\tremaining: 31.8s\n",
      "9893:\tlearn: 3.1237723\ttotal: 48m 58s\tremaining: 31.5s\n",
      "9894:\tlearn: 3.1234628\ttotal: 48m 58s\tremaining: 31.2s\n",
      "9895:\tlearn: 3.1232173\ttotal: 48m 58s\tremaining: 30.9s\n",
      "9896:\tlearn: 3.1230114\ttotal: 48m 59s\tremaining: 30.6s\n",
      "9897:\tlearn: 3.1226593\ttotal: 48m 59s\tremaining: 30.3s\n",
      "9898:\tlearn: 3.1224757\ttotal: 48m 59s\tremaining: 30s\n",
      "9899:\tlearn: 3.1223137\ttotal: 49m\tremaining: 29.7s\n",
      "9900:\tlearn: 3.1220901\ttotal: 49m\tremaining: 29.4s\n",
      "9901:\tlearn: 3.1218711\ttotal: 49m\tremaining: 29.1s\n",
      "9902:\tlearn: 3.1214945\ttotal: 49m\tremaining: 28.8s\n",
      "9903:\tlearn: 3.1212798\ttotal: 49m 1s\tremaining: 28.5s\n",
      "9904:\tlearn: 3.1211579\ttotal: 49m 1s\tremaining: 28.2s\n",
      "9905:\tlearn: 3.1209323\ttotal: 49m 1s\tremaining: 27.9s\n",
      "9906:\tlearn: 3.1207608\ttotal: 49m 1s\tremaining: 27.6s\n",
      "9907:\tlearn: 3.1204892\ttotal: 49m 2s\tremaining: 27.3s\n",
      "9908:\tlearn: 3.1203257\ttotal: 49m 2s\tremaining: 27s\n",
      "9909:\tlearn: 3.1200265\ttotal: 49m 2s\tremaining: 26.7s\n",
      "9910:\tlearn: 3.1198742\ttotal: 49m 2s\tremaining: 26.4s\n",
      "9911:\tlearn: 3.1196964\ttotal: 49m 3s\tremaining: 26.1s\n",
      "9912:\tlearn: 3.1194497\ttotal: 49m 3s\tremaining: 25.8s\n",
      "9913:\tlearn: 3.1193160\ttotal: 49m 3s\tremaining: 25.5s\n",
      "9914:\tlearn: 3.1191283\ttotal: 49m 4s\tremaining: 25.2s\n",
      "9915:\tlearn: 3.1190471\ttotal: 49m 4s\tremaining: 24.9s\n",
      "9916:\tlearn: 3.1186830\ttotal: 49m 4s\tremaining: 24.6s\n",
      "9917:\tlearn: 3.1184675\ttotal: 49m 4s\tremaining: 24.3s\n",
      "9918:\tlearn: 3.1182446\ttotal: 49m 5s\tremaining: 24.1s\n",
      "9919:\tlearn: 3.1180973\ttotal: 49m 5s\tremaining: 23.8s\n",
      "9920:\tlearn: 3.1178947\ttotal: 49m 5s\tremaining: 23.5s\n",
      "9921:\tlearn: 3.1176753\ttotal: 49m 5s\tremaining: 23.2s\n",
      "9922:\tlearn: 3.1175634\ttotal: 49m 6s\tremaining: 22.9s\n",
      "9923:\tlearn: 3.1172929\ttotal: 49m 6s\tremaining: 22.6s\n",
      "9924:\tlearn: 3.1171384\ttotal: 49m 6s\tremaining: 22.3s\n",
      "9925:\tlearn: 3.1168119\ttotal: 49m 6s\tremaining: 22s\n",
      "9926:\tlearn: 3.1166459\ttotal: 49m 7s\tremaining: 21.7s\n",
      "9927:\tlearn: 3.1162869\ttotal: 49m 7s\tremaining: 21.4s\n",
      "9928:\tlearn: 3.1160304\ttotal: 49m 7s\tremaining: 21.1s\n",
      "9929:\tlearn: 3.1159083\ttotal: 49m 8s\tremaining: 20.8s\n",
      "9930:\tlearn: 3.1156782\ttotal: 49m 8s\tremaining: 20.5s\n",
      "9931:\tlearn: 3.1154172\ttotal: 49m 8s\tremaining: 20.2s\n",
      "9932:\tlearn: 3.1152808\ttotal: 49m 8s\tremaining: 19.9s\n",
      "9933:\tlearn: 3.1151034\ttotal: 49m 9s\tremaining: 19.6s\n",
      "9934:\tlearn: 3.1149211\ttotal: 49m 9s\tremaining: 19.3s\n",
      "9935:\tlearn: 3.1147541\ttotal: 49m 9s\tremaining: 19s\n",
      "9936:\tlearn: 3.1144729\ttotal: 49m 9s\tremaining: 18.7s\n",
      "9937:\tlearn: 3.1141411\ttotal: 49m 10s\tremaining: 18.4s\n",
      "9938:\tlearn: 3.1139346\ttotal: 49m 10s\tremaining: 18.1s\n",
      "9939:\tlearn: 3.1137723\ttotal: 49m 10s\tremaining: 17.8s\n",
      "9940:\tlearn: 3.1135846\ttotal: 49m 10s\tremaining: 17.5s\n",
      "9941:\tlearn: 3.1133933\ttotal: 49m 11s\tremaining: 17.2s\n",
      "9942:\tlearn: 3.1132275\ttotal: 49m 11s\tremaining: 16.9s\n",
      "9943:\tlearn: 3.1129359\ttotal: 49m 11s\tremaining: 16.6s\n",
      "9944:\tlearn: 3.1126734\ttotal: 49m 11s\tremaining: 16.3s\n",
      "9945:\tlearn: 3.1125591\ttotal: 49m 12s\tremaining: 16s\n",
      "9946:\tlearn: 3.1124227\ttotal: 49m 12s\tremaining: 15.7s\n",
      "9947:\tlearn: 3.1121372\ttotal: 49m 12s\tremaining: 15.4s\n",
      "9948:\tlearn: 3.1119496\ttotal: 49m 12s\tremaining: 15.1s\n",
      "9949:\tlearn: 3.1116700\ttotal: 49m 13s\tremaining: 14.8s\n",
      "9950:\tlearn: 3.1114894\ttotal: 49m 13s\tremaining: 14.5s\n",
      "9951:\tlearn: 3.1111871\ttotal: 49m 13s\tremaining: 14.2s\n",
      "9952:\tlearn: 3.1109541\ttotal: 49m 14s\tremaining: 13.9s\n",
      "9953:\tlearn: 3.1106923\ttotal: 49m 14s\tremaining: 13.7s\n",
      "9954:\tlearn: 3.1104885\ttotal: 49m 14s\tremaining: 13.4s\n",
      "9955:\tlearn: 3.1103686\ttotal: 49m 14s\tremaining: 13.1s\n",
      "9956:\tlearn: 3.1101993\ttotal: 49m 15s\tremaining: 12.8s\n",
      "9957:\tlearn: 3.1100371\ttotal: 49m 15s\tremaining: 12.5s\n",
      "9958:\tlearn: 3.1098202\ttotal: 49m 15s\tremaining: 12.2s\n",
      "9959:\tlearn: 3.1094914\ttotal: 49m 15s\tremaining: 11.9s\n",
      "9960:\tlearn: 3.1092440\ttotal: 49m 16s\tremaining: 11.6s\n",
      "9961:\tlearn: 3.1090692\ttotal: 49m 16s\tremaining: 11.3s\n",
      "9962:\tlearn: 3.1088138\ttotal: 49m 16s\tremaining: 11s\n",
      "9963:\tlearn: 3.1085916\ttotal: 49m 16s\tremaining: 10.7s\n",
      "9964:\tlearn: 3.1083717\ttotal: 49m 17s\tremaining: 10.4s\n",
      "9965:\tlearn: 3.1082644\ttotal: 49m 17s\tremaining: 10.1s\n",
      "9966:\tlearn: 3.1079893\ttotal: 49m 17s\tremaining: 9.79s\n",
      "9967:\tlearn: 3.1076663\ttotal: 49m 17s\tremaining: 9.49s\n",
      "9968:\tlearn: 3.1074990\ttotal: 49m 18s\tremaining: 9.2s\n",
      "9969:\tlearn: 3.1072441\ttotal: 49m 18s\tremaining: 8.9s\n",
      "9970:\tlearn: 3.1070799\ttotal: 49m 18s\tremaining: 8.6s\n",
      "9971:\tlearn: 3.1069155\ttotal: 49m 18s\tremaining: 8.31s\n",
      "9972:\tlearn: 3.1067584\ttotal: 49m 19s\tremaining: 8.01s\n",
      "9973:\tlearn: 3.1064129\ttotal: 49m 19s\tremaining: 7.71s\n",
      "9974:\tlearn: 3.1062206\ttotal: 49m 19s\tremaining: 7.42s\n",
      "9975:\tlearn: 3.1060252\ttotal: 49m 19s\tremaining: 7.12s\n",
      "9976:\tlearn: 3.1058117\ttotal: 49m 20s\tremaining: 6.82s\n",
      "9977:\tlearn: 3.1056363\ttotal: 49m 20s\tremaining: 6.53s\n",
      "9978:\tlearn: 3.1053081\ttotal: 49m 20s\tremaining: 6.23s\n",
      "9979:\tlearn: 3.1051145\ttotal: 49m 20s\tremaining: 5.93s\n",
      "9980:\tlearn: 3.1049347\ttotal: 49m 21s\tremaining: 5.64s\n",
      "9981:\tlearn: 3.1047653\ttotal: 49m 21s\tremaining: 5.34s\n",
      "9982:\tlearn: 3.1045385\ttotal: 49m 21s\tremaining: 5.04s\n",
      "9983:\tlearn: 3.1043839\ttotal: 49m 21s\tremaining: 4.75s\n",
      "9984:\tlearn: 3.1042555\ttotal: 49m 22s\tremaining: 4.45s\n",
      "9985:\tlearn: 3.1038758\ttotal: 49m 22s\tremaining: 4.15s\n",
      "9986:\tlearn: 3.1036872\ttotal: 49m 22s\tremaining: 3.86s\n",
      "9987:\tlearn: 3.1035396\ttotal: 49m 22s\tremaining: 3.56s\n",
      "9988:\tlearn: 3.1033508\ttotal: 49m 23s\tremaining: 3.26s\n",
      "9989:\tlearn: 3.1031466\ttotal: 49m 23s\tremaining: 2.97s\n",
      "9990:\tlearn: 3.1029153\ttotal: 49m 23s\tremaining: 2.67s\n",
      "9991:\tlearn: 3.1027593\ttotal: 49m 23s\tremaining: 2.37s\n",
      "9992:\tlearn: 3.1025202\ttotal: 49m 24s\tremaining: 2.08s\n",
      "9993:\tlearn: 3.1023781\ttotal: 49m 24s\tremaining: 1.78s\n",
      "9994:\tlearn: 3.1022066\ttotal: 49m 24s\tremaining: 1.48s\n",
      "9995:\tlearn: 3.1020187\ttotal: 49m 24s\tremaining: 1.19s\n",
      "9996:\tlearn: 3.1018443\ttotal: 49m 25s\tremaining: 890ms\n",
      "9997:\tlearn: 3.1016231\ttotal: 49m 25s\tremaining: 593ms\n",
      "9998:\tlearn: 3.1014927\ttotal: 49m 25s\tremaining: 297ms\n",
      "9999:\tlearn: 3.1011959\ttotal: 49m 25s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nparams_catboost = {\\n    'iterations': [100, 200, 300, 500, 1000, 3000, 10000],\\n    'learning_rate': [0.001, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05, 0.1],\\n    'depth': range(1, 10, 1),\\n    'subsample': [x / 100.0 for x in range(5, 100, 10)],\\n    'colsample_bylevel': [x / 100.0 for x in range(5, 100, 10)],\\n    'min_data_in_leaf': range(1, 100, 10),\\n    'l2_leaf_reg': [1, 3, 5, 7, 9],\\n    #'min_child_samples': [1, 4, 8, 16, 32]\\n}\\n\\ngrid_search_catboost = RandomizedSearchCV(\\n    catboost_model,\\n    cv = TimeSeriesSplit(n_splits=4),\\n    param_distributions=params_catboost,\\n    n_jobs=-1,\\n    random_state=random_state,\\n    scoring='neg_mean_absolute_error'\\n)\\n\\ngrid_search_catboost.fit(features_train, target_train)\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catboost_model = CatBoostRegressor(\n",
    "    iterations=10000,\n",
    "    learning_rate=0.015,\n",
    "    depth=6,\n",
    "    subsample=0.65,\n",
    "    colsample_bylevel=0.95,\n",
    "    min_data_in_leaf=51,\n",
    "    l2_leaf_reg=9\n",
    ")\n",
    "\n",
    "catboost_model.fit(feat_xgb_train , target_all_train)\n",
    "\"\"\"\n",
    "params_catboost = {\n",
    "    'iterations': [100, 200, 300, 500, 1000, 3000, 10000],\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05, 0.1],\n",
    "    'depth': range(1, 10, 1),\n",
    "    'subsample': [x / 100.0 for x in range(5, 100, 10)],\n",
    "    'colsample_bylevel': [x / 100.0 for x in range(5, 100, 10)],\n",
    "    'min_data_in_leaf': range(1, 100, 10),\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    #'min_child_samples': [1, 4, 8, 16, 32]\n",
    "}\n",
    "\n",
    "grid_search_catboost = RandomizedSearchCV(\n",
    "    catboost_model,\n",
    "    cv = TimeSeriesSplit(n_splits=4),\n",
    "    param_distributions=params_catboost,\n",
    "    n_jobs=-1,\n",
    "    random_state=random_state,\n",
    "    scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "grid_search_catboost.fit(features_train, target_train)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d94fd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_predict_test = catboost_model.predict(feat_xgb_test)\n",
    "cat_predict_train = catboost_model.predict(feat_xgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6602d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "mae_train, mape_train, r2_train = metrics_hour(target_all_train, cat_predict_train )\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test, cat_predict_test )\n",
    "\n",
    "results = pd.concat([results,\n",
    "pd.DataFrame([[f'тренировочная CAT {FEATURES}', mae_train, mape_train, r2_train], [f'тестовая CAT {FEATURES}', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "205f9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = xgb_model.feature_importances_\n",
    "# предположим, что 'X' - это ваши данные\n",
    "feature_name = feat_xgb_test.columns\n",
    "# создание DataFrame\n",
    "importance_df_xgb = pd.DataFrame({'feature': feature_name, 'importance': importance})\n",
    "# сортировка по важности\n",
    "importance_df_xgb = importance_df_xgb.sort_values(by='importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0fd63ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>target_lag_24</td>\n",
       "      <td>0.469577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>day_of_week_6</td>\n",
       "      <td>0.016503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>day_of_week_4</td>\n",
       "      <td>0.011926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>day_of_week_7</td>\n",
       "      <td>0.011528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>day_of_week_21</td>\n",
       "      <td>0.010968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>has_rain_probability_8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>year_15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>has_rain_probability_14</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>year_9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>year_8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>774 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature  importance\n",
       "8              target_lag_24    0.469577\n",
       "196            day_of_week_6    0.016503\n",
       "132            day_of_week_4    0.011926\n",
       "228            day_of_week_7    0.011528\n",
       "676           day_of_week_21    0.010968\n",
       "..                       ...         ...\n",
       "273   has_rain_probability_8    0.000000\n",
       "482                  year_15    0.000000\n",
       "465  has_rain_probability_14    0.000000\n",
       "290                   year_9    0.000000\n",
       "258                   year_8    0.000000\n",
       "\n",
       "[774 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3365ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(importance_df_lgbm, importance_df_xgb, on='feature', how='outer', suffixes=('_lgbm', '_xgb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09e15c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_lgbm</th>\n",
       "      <th>importance_xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>target_lag_24</td>\n",
       "      <td>5168</td>\n",
       "      <td>0.469577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>target_lag_336</td>\n",
       "      <td>3262</td>\n",
       "      <td>0.006438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>target_lag_72</td>\n",
       "      <td>2998</td>\n",
       "      <td>0.002726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>target_lag_24_1</td>\n",
       "      <td>2148</td>\n",
       "      <td>0.000919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>temp_pred</td>\n",
       "      <td>2112</td>\n",
       "      <td>0.000611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>year_14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>year_10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>preholidays_8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>preholidays_14</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>year_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>798 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  importance_lgbm  importance_xgb\n",
       "0      target_lag_24             5168        0.469577\n",
       "1     target_lag_336             3262        0.006438\n",
       "2      target_lag_72             2998        0.002726\n",
       "3    target_lag_24_1             2148        0.000919\n",
       "4          temp_pred             2112        0.000611\n",
       "..               ...              ...             ...\n",
       "793          year_14                2        0.000217\n",
       "794          year_10                1        0.000538\n",
       "795    preholidays_8                1             NaN\n",
       "796   preholidays_14                1             NaN\n",
       "797           year_4                0        0.000075\n",
       "\n",
       "[798 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe31a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замена NaN на 0 в столбце 'importance_xgb'\n",
    "merged_df['importance_xgb'].fillna(0, inplace=True)\n",
    "\n",
    "# Нормализация важности признаков\n",
    "merged_df['importance_lgbm'] = merged_df['importance_lgbm'] / merged_df['importance_lgbm'].max()\n",
    "merged_df['importance_xgb'] = merged_df['importance_xgb'] / merged_df['importance_xgb'].max()\n",
    "\n",
    "# Создание столбца 'importance_ensemble', который является средним значением 'importance_lgbm' и 'importance_xgb'\n",
    "merged_df['importance_ensemble'] = (merged_df['importance_lgbm'] + merged_df['importance_xgb']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ecf1b32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_lgbm</th>\n",
       "      <th>importance_xgb</th>\n",
       "      <th>importance_ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>target_lag_24</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>target_lag_336</td>\n",
       "      <td>0.631192</td>\n",
       "      <td>0.013711</td>\n",
       "      <td>0.322452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>target_lag_72</td>\n",
       "      <td>0.580108</td>\n",
       "      <td>0.005806</td>\n",
       "      <td>0.292957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>target_lag_24_1</td>\n",
       "      <td>0.415635</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.208795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>temp_pred</td>\n",
       "      <td>0.408669</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.204985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>year_14</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>year_10</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>preholidays_8</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>preholidays_14</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>year_4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>798 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  importance_lgbm  importance_xgb  importance_ensemble\n",
       "0      target_lag_24         1.000000        1.000000             1.000000\n",
       "1     target_lag_336         0.631192        0.013711             0.322452\n",
       "2      target_lag_72         0.580108        0.005806             0.292957\n",
       "3    target_lag_24_1         0.415635        0.001956             0.208795\n",
       "4          temp_pred         0.408669        0.001300             0.204985\n",
       "..               ...              ...             ...                  ...\n",
       "793          year_14         0.000387        0.000462             0.000425\n",
       "794          year_10         0.000193        0.001145             0.000669\n",
       "795    preholidays_8         0.000193        0.000000             0.000097\n",
       "796   preholidays_14         0.000193        0.000000             0.000097\n",
       "797           year_4         0.000000        0.000160             0.000080\n",
       "\n",
       "[798 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c55ddba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /opt/anaconda3/envs/p311/lib/python3.11/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /opt/anaconda3/envs/p311/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ab34de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_excel('feature_importance.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Объединяем результаты ансамбля моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5dd1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_simple_ensemble_train = (xgb_predict_train + l_predict_train)/2\n",
    "predict_simple_ensemble_test = (xgb_predict_test + l_predict_test)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20d789ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "mae_train, mape_train, r2_train = metrics_hour(target_all_train, predict_simple_ensemble_train)\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test, predict_simple_ensemble_test)\n",
    "\n",
    "results_ensemble = results\n",
    "results_ensemble = pd.concat([results_ensemble,\n",
    "pd.DataFrame([[f'тренировочная ансамбля LGBM и XGB  {FEATURES}', mae_train, mape_train, r2_train], [f'тестовая ансамбля LGBM и XGB {FEATURES}', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03b383f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Выборка</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная LGBM _Aug_27_alpha_3</td>\n",
       "      <td>3.403002</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.996599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая LGBM _Aug_27_alpha_3</td>\n",
       "      <td>6.092151</td>\n",
       "      <td>0.014274</td>\n",
       "      <td>0.985890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная XGB _Aug_27_alpha_3</td>\n",
       "      <td>3.828241</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.997552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая XGB _Aug_27_alpha_3</td>\n",
       "      <td>6.141930</td>\n",
       "      <td>0.014345</td>\n",
       "      <td>0.984693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная CAT _Aug_27_alpha_3</td>\n",
       "      <td>2.389001</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.999066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая CAT _Aug_27_alpha_3</td>\n",
       "      <td>6.003542</td>\n",
       "      <td>0.014254</td>\n",
       "      <td>0.986401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная ансамбля LGBM и XGB  _Aug_27_alp...</td>\n",
       "      <td>3.502858</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.997425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая ансамбля LGBM и XGB _Aug_27_alpha_3</td>\n",
       "      <td>5.964305</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.986072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Выборка       MAE      MAPE  \\\n",
       "0                 тренировочная LGBM _Aug_27_alpha_3  3.403002  0.007062   \n",
       "1                      тестовая LGBM _Aug_27_alpha_3  6.092151  0.014274   \n",
       "0                  тренировочная XGB _Aug_27_alpha_3  3.828241  0.008054   \n",
       "1                       тестовая XGB _Aug_27_alpha_3  6.141930  0.014345   \n",
       "0                  тренировочная CAT _Aug_27_alpha_3  2.389001  0.005083   \n",
       "1                       тестовая CAT _Aug_27_alpha_3  6.003542  0.014254   \n",
       "0  тренировочная ансамбля LGBM и XGB  _Aug_27_alp...  3.502858  0.007314   \n",
       "1       тестовая ансамбля LGBM и XGB _Aug_27_alpha_3  5.964305  0.013936   \n",
       "\n",
       "         R2  \n",
       "0  0.996599  \n",
       "1  0.985890  \n",
       "0  0.997552  \n",
       "1  0.984693  \n",
       "0  0.999066  \n",
       "1  0.986401  \n",
       "0  0.997425  \n",
       "1  0.986072  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f154ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58dffb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_simple_ensemble_train = (cat_predict_train + l_predict_train)/2\n",
    "predict_simple_ensemble_test = (cat_predict_test + l_predict_test)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e38922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "mae_train, mape_train, r2_train = metrics_hour(target_all_train, predict_simple_ensemble_train)\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test, predict_simple_ensemble_test)\n",
    "\n",
    "\n",
    "results_ensemble = pd.concat([results_ensemble,\n",
    "pd.DataFrame([[f'тренировочная ансамбля CatBoost и LGBM  {FEATURES}', mae_train, mape_train, r2_train], [f'тестовая ансамбля CatBoost и LGBM {FEATURES} ', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98c7da7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Выборка</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная LGBM _Aug_27_alpha_3</td>\n",
       "      <td>3.403002</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.996599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая LGBM _Aug_27_alpha_3</td>\n",
       "      <td>6.092151</td>\n",
       "      <td>0.014274</td>\n",
       "      <td>0.985890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная XGB _Aug_27_alpha_3</td>\n",
       "      <td>3.828241</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.997552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая XGB _Aug_27_alpha_3</td>\n",
       "      <td>6.141930</td>\n",
       "      <td>0.014345</td>\n",
       "      <td>0.984693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная CAT _Aug_27_alpha_3</td>\n",
       "      <td>2.389001</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.999066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая CAT _Aug_27_alpha_3</td>\n",
       "      <td>6.003542</td>\n",
       "      <td>0.014254</td>\n",
       "      <td>0.986401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная ансамбля LGBM и XGB  _Aug_27_alp...</td>\n",
       "      <td>3.502858</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.997425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая ансамбля LGBM и XGB _Aug_27_alpha_3</td>\n",
       "      <td>5.964305</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.986072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная ансамбля CatBoost и LGBM  _Aug_2...</td>\n",
       "      <td>2.789836</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>0.998304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая ансамбля CatBoost и LGBM _Aug_27_alph...</td>\n",
       "      <td>5.851637</td>\n",
       "      <td>0.013786</td>\n",
       "      <td>0.987096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Выборка       MAE      MAPE  \\\n",
       "0                 тренировочная LGBM _Aug_27_alpha_3  3.403002  0.007062   \n",
       "1                      тестовая LGBM _Aug_27_alpha_3  6.092151  0.014274   \n",
       "0                  тренировочная XGB _Aug_27_alpha_3  3.828241  0.008054   \n",
       "1                       тестовая XGB _Aug_27_alpha_3  6.141930  0.014345   \n",
       "0                  тренировочная CAT _Aug_27_alpha_3  2.389001  0.005083   \n",
       "1                       тестовая CAT _Aug_27_alpha_3  6.003542  0.014254   \n",
       "0  тренировочная ансамбля LGBM и XGB  _Aug_27_alp...  3.502858  0.007314   \n",
       "1       тестовая ансамбля LGBM и XGB _Aug_27_alpha_3  5.964305  0.013936   \n",
       "0  тренировочная ансамбля CatBoost и LGBM  _Aug_2...  2.789836  0.005842   \n",
       "1  тестовая ансамбля CatBoost и LGBM _Aug_27_alph...  5.851637  0.013786   \n",
       "\n",
       "         R2  \n",
       "0  0.996599  \n",
       "1  0.985890  \n",
       "0  0.997552  \n",
       "1  0.984693  \n",
       "0  0.999066  \n",
       "1  0.986401  \n",
       "0  0.997425  \n",
       "1  0.986072  \n",
       "0  0.998304  \n",
       "1  0.987096  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01482aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_simple_ensemble_train = xgb_predict_train*0.3 + l_predict_train*0.3 + cat_predict_train*0.4\n",
    "predict_simple_ensemble_test = xgb_predict_test*0.3 + l_predict_test*0.3 + cat_predict_test*0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e2edce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/p311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "mae_train, mape_train, r2_train = metrics_hour(target_all_train, predict_simple_ensemble_train)\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test, predict_simple_ensemble_test)\n",
    "\n",
    "\n",
    "results_ensemble = pd.concat([results_ensemble,\n",
    "pd.DataFrame([[f'тренировочная ансамбля CatBoost, LGBM и XGBoost {FEATURES}', mae_train, mape_train, r2_train], [f'тестовая ансамбля CatBoost, LGBM и XGBoost {FEATURES}', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a31d31c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Выборка</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная LGBM _Aug_27_alpha_3</td>\n",
       "      <td>3.403002</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.996599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая LGBM _Aug_27_alpha_3</td>\n",
       "      <td>6.092151</td>\n",
       "      <td>0.014274</td>\n",
       "      <td>0.985890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная XGB _Aug_27_alpha_3</td>\n",
       "      <td>3.828241</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.997552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая XGB _Aug_27_alpha_3</td>\n",
       "      <td>6.141930</td>\n",
       "      <td>0.014345</td>\n",
       "      <td>0.984693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная CAT _Aug_27_alpha_3</td>\n",
       "      <td>2.389001</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.999066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая CAT _Aug_27_alpha_3</td>\n",
       "      <td>6.003542</td>\n",
       "      <td>0.014254</td>\n",
       "      <td>0.986401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная ансамбля LGBM и XGB  _Aug_27_alp...</td>\n",
       "      <td>3.502858</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.997425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая ансамбля LGBM и XGB _Aug_27_alpha_3</td>\n",
       "      <td>5.964305</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.986072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная ансамбля CatBoost и LGBM  _Aug_2...</td>\n",
       "      <td>2.789836</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>0.998304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая ансамбля CatBoost и LGBM _Aug_27_alph...</td>\n",
       "      <td>5.851637</td>\n",
       "      <td>0.013786</td>\n",
       "      <td>0.987096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная ансамбля CatBoost, LGBM и XGBoos...</td>\n",
       "      <td>2.936119</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.998349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая ансамбля CatBoost, LGBM и XGBoost _Au...</td>\n",
       "      <td>5.828059</td>\n",
       "      <td>0.013690</td>\n",
       "      <td>0.986940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тренировочная ансамбля CatBoost, LGBM и XGBoos...</td>\n",
       "      <td>3.047164</td>\n",
       "      <td>0.006378</td>\n",
       "      <td>0.998113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тестовая ансамбля CatBoost, LGBM и XGBoost _Au...</td>\n",
       "      <td>5.843311</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.986866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Выборка       MAE      MAPE  \\\n",
       "0                 тренировочная LGBM _Aug_27_alpha_3  3.403002  0.007062   \n",
       "1                      тестовая LGBM _Aug_27_alpha_3  6.092151  0.014274   \n",
       "0                  тренировочная XGB _Aug_27_alpha_3  3.828241  0.008054   \n",
       "1                       тестовая XGB _Aug_27_alpha_3  6.141930  0.014345   \n",
       "0                  тренировочная CAT _Aug_27_alpha_3  2.389001  0.005083   \n",
       "1                       тестовая CAT _Aug_27_alpha_3  6.003542  0.014254   \n",
       "0  тренировочная ансамбля LGBM и XGB  _Aug_27_alp...  3.502858  0.007314   \n",
       "1       тестовая ансамбля LGBM и XGB _Aug_27_alpha_3  5.964305  0.013936   \n",
       "0  тренировочная ансамбля CatBoost и LGBM  _Aug_2...  2.789836  0.005842   \n",
       "1  тестовая ансамбля CatBoost и LGBM _Aug_27_alph...  5.851637  0.013786   \n",
       "0  тренировочная ансамбля CatBoost, LGBM и XGBoos...  2.936119  0.006160   \n",
       "1  тестовая ансамбля CatBoost, LGBM и XGBoost _Au...  5.828059  0.013690   \n",
       "0  тренировочная ансамбля CatBoost, LGBM и XGBoos...  3.047164  0.006378   \n",
       "1  тестовая ансамбля CatBoost, LGBM и XGBoost _Au...  5.843311  0.013707   \n",
       "\n",
       "         R2  \n",
       "0  0.996599  \n",
       "1  0.985890  \n",
       "0  0.997552  \n",
       "1  0.984693  \n",
       "0  0.999066  \n",
       "1  0.986401  \n",
       "0  0.997425  \n",
       "1  0.986072  \n",
       "0  0.998304  \n",
       "1  0.987096  \n",
       "0  0.998349  \n",
       "1  0.986940  \n",
       "0  0.998113  \n",
       "1  0.986866  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d79963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f158b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#num = 3\n",
    "# определите путь к папке, которую вы хотите создать\n",
    "folder_path = \"new_models\"\n",
    "\n",
    "# проверьте, существует ли уже папка\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "lgbm_model_all_train.booster_.save_model(f'{folder_path}/lgb_model{FEATURES}_{num}.txt')\n",
    "xgb_model.save_model(f'{folder_path}/xgb_model{FEATURES}_{num}.json')\n",
    "catboost_model.save_model(f'{folder_path}/catboost_model{FEATURES}_{num}.cbm', format=\"cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e42dafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURES = 'BASE'\n",
    "results.to_csv(f'{folder_path}/results_LGBM_XGBoost_Catboost_ensemble_{FEATURES}_{num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "579d5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(xgb_predict_train).to_csv(f'{folder_path}/xgb_predict_train_{FEATURES}_{num}.csv', index=False)\n",
    "pd.DataFrame(xgb_predict_test).to_csv(f'{folder_path}/xgb_predict_test_{FEATURES}_{num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6adb04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(l_predict_train).to_csv(f'{folder_path}/lgbm_predict_train_{FEATURES}_{num}.csv', index=False)\n",
    "pd.DataFrame(l_predict_test).to_csv(f'{folder_path}/lgbm_predict_test_{FEATURES}_{num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eca9afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cat_predict_train).to_csv(f'{folder_path}/cat_predict_train_{FEATURES}_{num}.csv', index=False)\n",
    "pd.DataFrame(cat_predict_test).to_csv(f'{folder_path}/cat_predict_test_{FEATURES}_{num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74d2b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_all_train.to_csv(f'{folder_path}/target_train_{FEATURES}_{num}.csv', index=False)\n",
    "target_open_test.to_csv(f'{folder_path}/target_test{FEATURES}_{num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd65cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
