{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5606014",
   "metadata": {},
   "source": [
    "## Энергетический оракул\n",
    "Ноутбук команды #12\n",
    "\n",
    "Работа выполнена на основе модели LightGBM\n",
    "\n",
    "\n",
    "### 1. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4351135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True )\n",
    "\n",
    "\n",
    "random_state = 12345\n",
    "NUM_ITERATIONS = 5000\n",
    "\n",
    "FEATURES = 'base feature'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45891200",
   "metadata": {},
   "source": [
    "#### 1.1 Функции для расшифровки прогноза погоды в колонке 'weather_pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece12617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расшифровка прогноза в колонке 'weather_pred'\n",
    "\n",
    "# функция формирует колонки 'cloudy', 'rainy', 'windy', 'clear', 'rain_probability', 'has_rain_probability'\n",
    "# в колонках число, которое 0 при отсутсвии упоминания явления в weather_pred или степень упоминания\n",
    "# функция дает в колонках номер первого списка, элемент которого есть в строке плюс 1\n",
    "# списки cloudy_list, rainy_list, windy_list, clear_list можно модифицировать\n",
    "# соответственно, можно экспериментировать с расположением значений в списках\n",
    "# например, сейчас 'дождь', 'снег', 'д+сн' - первая степень  дождя, а 'гроз', 'ливень' - вторая\n",
    "# а можно сделать снег второй, а грозу с ливнем убрать в третью\n",
    "# также сделал отдельный список для \"ясности\", чтобы выделить 'ясно' и 'солнечно'\n",
    "\n",
    "def in_what_list(weather, big_list):\n",
    "    for list_number, small_list in enumerate(big_list):\n",
    "        if any(word in weather for word in small_list):\n",
    "            return list_number+1\n",
    "    return 0\n",
    "\n",
    "def weather_split2(row):\n",
    "    weather = row['weather_pred']\n",
    "    cloudy_list = [['проясн', 'пер.об.', 'п/об'], ['пасм', 'обл']]\n",
    "    rainy_list = [['дождь', 'снег', 'д+сн'], ['гроз', 'ливень']]\n",
    "    windy_list = [['вет'],['штор']]\n",
    "    clear_list = [['проясн'], ['ясно'], ['солнеч']]\n",
    "    numbers = re.findall(r'\\d+', weather)\n",
    "    cloudy = in_what_list(weather, cloudy_list)\n",
    "    rainy = in_what_list(weather, rainy_list)\n",
    "    windy = in_what_list(weather, windy_list)\n",
    "    clear = in_what_list(weather, clear_list)\n",
    "    rain_probability = 0 if len(numbers)==0 else int(numbers[0])\n",
    "    has_rain_probability = int(len(numbers)==0)\n",
    "    return cloudy, rainy, windy, clear, rain_probability, has_rain_probability\n",
    "\n",
    "def fill_weather_columns(df):\n",
    "    df['weather_pred'] = df['weather_pred'].fillna('')\n",
    "    df['cloudy'], df['rainy'], df['windy'], df['clear'], df['rain_probability'], df['has_rain_probability'] = \\\n",
    "                zip(*df.apply(weather_split2, axis=1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f39d660",
   "metadata": {},
   "source": [
    "#### 1.2 Функции для загрузки данных о ВВП \n",
    "данные загружаются из файла 'data/VVP.csv'\n",
    "\n",
    "Некоторые научные работы указывают на прямую связь величины потребления электричества и показателя ВВП, который отражает ситуацию в экономике. Данные по экономике публикуются различными министерствами с разной периодичностью. Для использования в работе были взяты фактические данные по ВВП с сайта investing, который агрегирует публикации Минэкономразвития. Данные за месяц побликуются с месячной задержкой, поэтому модель использует для прогнозирования данные за прошлые месяцы, которые известны.   \n",
    "  \n",
    "Ссылка на данные: https://ru.investing.com/economic-calendar/russian-monthly-gdp-407\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция добавляет данные о ВВП из файла 'data/VVP.csv' в датасет\n",
    "\n",
    "def add_vvp2(data, file_source = 'data/VVP.csv'):\n",
    "    \"\"\"\n",
    "    сырой датафрем подаем на вход\n",
    "    \"\"\"\n",
    "    # обработаем файл с динамикой ВВП\n",
    "    vvp = pd.read_csv(file_source)\n",
    "    # преобразуем дату файла-источника в формат datetime64 и дропнем один столбик\n",
    "    vvp['date'] = pd.to_datetime(vvp['date'], format ='%Y-%m-%d %H:%M:%S')\n",
    "    vvp.drop('for_month',axis=1,inplace=True) \n",
    "    \n",
    "    # обработаем основной фрейм - создадим столбец для соединения, который потом удалим\n",
    "    data['date_temp'] = pd.to_datetime(data['date'], format = '%Y-%m-%d' )\n",
    "    data['date_temp'] = data['date_temp'] + pd.to_timedelta(data['time'] , 'H')\n",
    "    \n",
    "    # соединяем основной фрейм и ВВП по дате объявления показтеля ВВП\n",
    "    for idx in reversed(vvp.index):\n",
    "        data.loc[data['date_temp']>=vvp.date[idx],'VVP'] = vvp.VVP_perc[idx]\n",
    "        \n",
    "    data.drop('date_temp',axis=1,inplace=True)   \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe769599",
   "metadata": {},
   "source": [
    "#### 1.3 Функции для загрузки архива данных о фактической погоде\n",
    "данные загружаются из файла 'data/preprocessing_loaded_table.csv'\n",
    "\n",
    "Изначально данные для формирования таблицы \"preprocessing_loaded_table\" были взяты из с сайта [https://rp5.ru](https://rp5.ru/Архив_погоды_в_Храброво,_им._императрицы_Елизаветы_Петровны_(аэропорт),_METAR), где хранятся архивы погоды в аэрапорту Калининграда, за период с 31.12.2018 по 30.09.2023\n",
    "\n",
    "Описание данных в таблице:\n",
    "- Местное время в Храброво / им. императрицы Елизаветы Петровны (аэропорт) - Дата / Местное время\n",
    "- T -  Темпиратура воздуха\n",
    "- Po - Давление на уровне станции\n",
    "- P - Давление приведённое к уровню моря\n",
    "- U - Относительная влажность\n",
    "- DD - Направление ветра\n",
    "- Ff - Скорость ветра\n",
    "- ff10 - Максимальное значение порыва ветра\n",
    "- WW - Особое явление текущей погоды (осадки)\n",
    "- W'W' - Явление недавней погоды, имеющее оперативное значение\n",
    "- с - Общая облачность\n",
    "- VV - Горизонтальная дальность видимости\n",
    "- Td - Темпиратура точки росы\n",
    "\n",
    "Данные, которые были взяты из данной таблицы и загружаются из 'data/preprocessing_loaded_table.csv':\n",
    "- P - не подверглось изменению\n",
    "- U - не подверглось изменению\n",
    "- Td - не подверглась изменению\n",
    "\n",
    " WW - разделили на 4 категории:\n",
    "- Нет осадков (где были пропуски)\n",
    "- слабый дождь\n",
    "- сильный дождь\n",
    "- снег\n",
    "\n",
    "DD - создали 4 столбца, соответствующих сторонам горизонта, которые принимали значения 0; 0.5 и 1 в зависимости от силы ветра в конкретном направлении\n",
    "- N - north\n",
    "- S - south\n",
    "- W - west\n",
    "- E - east\n",
    "\n",
    "В дальнейшем эти данные использовались с лагом в сутки: в поля на завтрашний день записывались данные сегодняшнего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для работы с данными о фактической погоде из 'data/preprocessing_loaded_table.csv'\n",
    "\n",
    "# Кодировка информации об осадках из колонки WW\n",
    "def true_weather_WW_replace(ww):\n",
    "    if ww=='нет осадков':\n",
    "        return 0\n",
    "    elif ww=='слабый дождь':\n",
    "        return 1\n",
    "    elif (ww=='сильный дождь') or (ww=='снег'):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "# Вычисление Timestamp из даты и времени\n",
    "def row_plus_hours_to_index(row):\n",
    "    return row['date'] + pd.to_timedelta(row['time'] , 'H')\n",
    "\n",
    "# Функция для сдвига на сутки (в скачанном датасете разбивка по 30 мин, поэтому timeshift=48)\n",
    "def shift_features_fact(df, timeshift=48):\n",
    "    list_fact_columns=list(df.columns)\n",
    "    list_fact_columns.remove('date_tw')\n",
    "    new_df = df.copy()\n",
    "    for column in list_fact_columns:\n",
    "        new_df[column] = new_df[column].shift(timeshift)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для вычисления метрики mae по дням из почасовых массивов данных\n",
    "\n",
    "def mae_day(y_true, y_pred):\n",
    "    y_true_copy = pd.DataFrame(y_true).reset_index(drop=True)\n",
    "    y_true_copy['day'] = y_true_copy.index // 24\n",
    "    y_true_grouped = y_true_copy.groupby(by='day').sum()   \n",
    "    y_pred_copy = pd.DataFrame(y_pred).reset_index(drop=True)\n",
    "    y_pred_copy['day'] = y_pred_copy.index // 24\n",
    "    y_pred_grouped = y_pred_copy.groupby(by='day').sum()\n",
    "    \n",
    "    return mean_absolute_error(y_true_grouped, y_pred_grouped)\n",
    "# Функция для вычисления метрик по дням из почасовых массивов данных\n",
    "\n",
    "def metrics_hour(y_true, y_pred):\n",
    "\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mape, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808c109",
   "metadata": {},
   "source": [
    "#### 1.5 Чтение файлов с данными\n",
    "Данные объединяются в один датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем исходные датасеты и складываем в один\n",
    "train_ds = pd.read_csv('data/train_dataset.csv')\n",
    "test_ds = pd.read_csv('data/test_dataset.csv')\n",
    "train_ds = pd.concat([train_ds, test_ds])\n",
    "\n",
    "# запоминаем дату начала тестовых данных, потом также поступим и с закрытым датасетом\n",
    "open_test_begin = pd.to_datetime(test_ds['date']).min()\n",
    "open_test_end = pd.to_datetime(test_ds['date']).max() + pd.to_timedelta(1,'d')\n",
    "print('начало открытого теста:', open_test_begin, '    конец открытого теста:', open_test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237ce32",
   "metadata": {},
   "source": [
    "#### 1.6 Формирование колонок с производными от даты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16090ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразуем дату и делаем из нее колонки\n",
    "train_ds['date'] = pd.to_datetime(train_ds['date'])\n",
    "train_ds['year'] = train_ds['date'].dt.year\n",
    "train_ds['month'] = train_ds['date'].dt.month\n",
    "train_ds['day_of_week'] = train_ds['date'].dt.dayofweek\n",
    "train_ds['day'] = train_ds['date'].dt.day\n",
    "train_ds['day_of_year'] = train_ds['date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98567815",
   "metadata": {},
   "source": [
    "#### 1.7 Подгрузка Auggumentaci в праздниках"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9963b",
   "metadata": {},
   "source": [
    "\n",
    "df_holidays = pd.read_csv('data/holidays_aug.csv')\n",
    "df_holidays['date'] = pd.to_datetime(df_holidays['date'])\n",
    "# Добавление данных о праздниках из файла 'data/holidays.csv'\n",
    "\n",
    "\n",
    "print('размер DS', train_ds.shape, 'дубликатов - ', train_ds.duplicated().sum())\n",
    "# Assuming df_holidays and train_ds are your dataframes\n",
    "train_ds = pd.merge(train_ds, df_holidays, on='date', how='left')\n",
    "print('размер DS', train_ds.shape, 'дубликатов - ', train_ds.duplicated().sum())\n",
    "# Fill NaN values with 0\n",
    "train_ds['holidays'].fillna(0, inplace=True)\n",
    "train_ds['preholidays'].fillna(0, inplace=True)\n",
    "\n",
    "# Convert to int\n",
    "train_ds['holidays'] = train_ds['holidays'].astype(int)\n",
    "train_ds['preholidays'] = train_ds['preholidays'].astype(int)\n",
    "print('размер DS', train_ds.shape, 'дубликатов - ', train_ds.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5a0f9",
   "metadata": {},
   "source": [
    "#### 1.7.1 Подгрузка данных о праздниках на весь DS праздниках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddfd107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_holidays_true = pd.read_csv('data/holidays_true.csv')\n",
    "df_holidays_true['date'] = pd.to_datetime(df_holidays_true['date'])\n",
    "# Добавление данных о праздниках из файла 'data/holidays.csv'\n",
    "print('размер DS', train_ds.shape, 'дубликатов - ', train_ds.duplicated().sum())\n",
    "# Assuming df_holidays and train_ds are your dataframes\n",
    "train_ds = pd.merge(train_ds, df_holidays_true, on='date', how='left')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "train_ds['holidays_true'].fillna(0, inplace=True)\n",
    "train_ds['preholidays_true'].fillna(0, inplace=True)\n",
    "\n",
    "# Convert to int\n",
    "train_ds['holidays_true'] = train_ds['holidays_true'].astype(int)\n",
    "train_ds['preholidays_true'] = train_ds['preholidays_true'].astype(int)\n",
    "print('размер DS', train_ds.shape, 'дубликатов - ', train_ds.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cdfa6e",
   "metadata": {},
   "source": [
    "#### 1.8 Формирование колонок со значением целевого признака в предыдущие дни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfdcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавление колонок с временными лагами\n",
    "\n",
    "# создаем столбец 'temp_last_day'\n",
    "train_ds['temp_last_day'] = train_ds['temp'].shift(24)\n",
    "print('размер DS', train_ds.shape, 'дубликатов - ', train_ds.duplicated().sum())\n",
    "# заполняем пропущенные значения в 'temp_last_day'\n",
    "train_ds['temp_last_day'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# создаем столбцы с временными лагами для 'target'\n",
    "lags = [24, 48, 72, 7*24, 14*24]\n",
    "for lag in lags:\n",
    "    train_ds[f'target_lag_{lag}'] = train_ds['target'].shift(lag)\n",
    "\n",
    "# заполняем пропущенные значения в столбцах с лагами\n",
    "for lag in lags:\n",
    "    train_ds[f'target_lag_{lag}'].fillna(0, inplace=True)\n",
    "\n",
    "print('размер DS', train_ds.shape, 'дубликатов - ', train_ds.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c4022",
   "metadata": {},
   "source": [
    "#### 1.9 Формирование колонок с ВВП и данными о погоде посредством ранее описанных функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# применяем функцию добавления ВВП\n",
    "train_ds = add_vvp2(train_ds)\n",
    "\n",
    "# Расшифровка прогноза в колонке 'weather_pred'\n",
    "train_ds = fill_weather_columns(train_ds)\n",
    "\n",
    "\n",
    "# Читаем файл с архивом фактической погоды\n",
    "df_true_weather = pd.read_csv('data/preprocessing_loaded_table.csv')\n",
    "display(df_true_weather)\n",
    "\n",
    "# Форматируем колонки\n",
    "df_true_weather['WW'] = df_true_weather['WW'].apply(true_weather_WW_replace)\n",
    "df_true_weather['date'] = pd.to_datetime(df_true_weather['date'])\n",
    "df_true_weather = df_true_weather.rename(columns={'date':'date_tw'})\n",
    "# Применяем сдвиг на сутки, чтобы не заглядывать в будущее\n",
    "df_true_weather = shift_features_fact(df_true_weather)\n",
    "# Добавляем в датасет\n",
    "train_ds['date_hours'] = train_ds.apply(row_plus_hours_to_index, axis=1)\n",
    "train_ds = train_ds.merge(df_true_weather, left_on='date_hours', right_on='date_tw')\n",
    "train_ds = train_ds.drop(['date_hours', 'date_tw'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a8cec",
   "metadata": {},
   "source": [
    "###  Новые фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be07ed7",
   "metadata": {},
   "source": [
    "from add_new_data import (add_euro,\n",
    "                            add_light_work,\n",
    "                            add_stock,\n",
    "                            add_ipp_mm,\n",
    "                            add_ipp_yy,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd7b98",
   "metadata": {},
   "source": [
    "#### 1.9.1 Euro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa7db1",
   "metadata": {},
   "source": [
    "FEATURES = 'add_euro'\n",
    "train_ds = add_euro(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e0bc5",
   "metadata": {},
   "source": [
    "#### 1.9.2 Stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf5256",
   "metadata": {},
   "source": [
    "FEATURES = 'add_stock'\n",
    "train_ds = add_stock(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a8a7b",
   "metadata": {},
   "source": [
    "#### 1.9.3 IPP Month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d597e5",
   "metadata": {},
   "source": [
    "FEATURES = 'add_ipp_mm'\n",
    "train_ds = add_ipp_mm(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2343d2",
   "metadata": {},
   "source": [
    "#### 1.9.4 IPP Year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1441c91",
   "metadata": {},
   "source": [
    "FEATURES = 'add_ipp_yy'\n",
    "train_ds = add_ipp_yy(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55438938",
   "metadata": {},
   "source": [
    "#### 1.9.5 Light work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc485f6",
   "metadata": {},
   "source": [
    "FEATURES = 'add_light_work'\n",
    "train_ds = add_light_work(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89840ec4",
   "metadata": {},
   "source": [
    "#### 1.9.6 All"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b29d26",
   "metadata": {},
   "source": [
    "FEATURES = 'add_all'\n",
    "\n",
    "train_ds = add_euro(train_ds)\n",
    "train_ds = add_stock(train_ds)\n",
    "train_ds = add_ipp_mm(train_ds)\n",
    "train_ds = add_ipp_yy(train_ds)\n",
    "train_ds = add_light_work(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc76ee",
   "metadata": {},
   "source": [
    "#### 1.10 Демонстрация сформированного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a939b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Итоговый набор колонок\n",
    "train_ds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17ec6c",
   "metadata": {},
   "source": [
    "#### 1.11.1 Добавление среднего за час предыдущего дня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eebfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_evening(values, evening=19):\n",
    "    return values[evening:].mean()\n",
    "\n",
    "evening_slices = [0, 19, 22]\n",
    "    \n",
    "for evening_slice in evening_slices:\n",
    "    train_ds[['last_evening_avg_target_'+str(evening_slice), 'last_evening_avg_temp_'+str(evening_slice)]] = \\\n",
    "        train_ds[['date', 'target', 'temp']].groupby(by='date').transform(mean_evening, evening=evening_slice).shift(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76699a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69383cb",
   "metadata": {},
   "source": [
    "#### 1.11 Исключение лишних колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d6619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отбираем признаки. Все лишние колонки здесь отбрасываем, кроме 'date', которую уберем позже \n",
    "\n",
    "feature_cols = list(train_ds.columns)\n",
    "\n",
    "# выбрасываем взгляд в прошлое и расшифрованную погоду\n",
    "drop_list = ['target', 'day_of_year', 'weather_pred', 'weather_fact', 'temp']\n",
    "\n",
    "# выбрасываем признаки, найденные процедурно в процессе оптимизации\n",
    "# КОМАНДЕ: здесь можно добавлять признаки на выброс с целью оптимизации\n",
    "drop_list = drop_list + ['target_lag_48', 'target_lag_168'] #, 'temp_pred'] #, 'target_lag_336'] \n",
    "\n",
    "for name in drop_list:\n",
    "    feature_cols.remove(name)\n",
    "\n",
    "# Итоговый список признаков\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38de5b2",
   "metadata": {},
   "source": [
    "#### 1.11.2 Добавление среднего за час предыдущего дня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b859804",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_WINDOW_SIZE = 24\n",
    "feature_cols_no_date = feature_cols.copy()\n",
    "feature_cols_no_date.remove('date')\n",
    "\n",
    "\n",
    "for lag in range(1,FEATURE_WINDOW_SIZE):\n",
    "    for column in feature_cols_no_date:\n",
    "        train_ds[column+'_'+str(lag)] = train_ds[column].shift(lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cc026",
   "metadata": {},
   "source": [
    "#### 1.11.2 Добавление лагов за час "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc38dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lags=[1, 5, 9]\n",
    "\n",
    "for lag in target_lags:\n",
    "    train_ds['target_'+str(lag)] = train_ds['target'].shift(lag).where(train_ds['time']<lag, np.NaN)\n",
    "    train_ds['temp_'+str(lag)] = train_ds['temp'].shift(lag).where(train_ds['time']<lag, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23393e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(train_ds.columns)\n",
    "for name in drop_list:\n",
    "    feature_cols.remove(name)\n",
    "\n",
    "# Итоговый список признаков\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b4fad",
   "metadata": {},
   "source": [
    "#### 1.11.13 Аугументации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f34c90",
   "metadata": {},
   "source": [
    "#### 1.12 Выделение наборов данных для обучения, валидации и тестирования\n",
    "\n",
    "Выделялось два набора данных для обучения и валидации:\n",
    "1. Обучение на данных с 2019 по 2021 с валидацией на 2022\n",
    "2. Обучение на данных с 2019 по 2022 с валидацией на первом квартале 2023\n",
    "\n",
    "Первый набор позволяет оценить влияние сезонности на обучение и предсказания, второй позволяет обучить модель на большем объеме данных и на более актуальных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c73859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds['month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формируем набор датасетов для обучения и проверки\n",
    "train_ds = train_ds.sort_values(by=['date', 'time']).reset_index(drop=True)\n",
    "features = train_ds[feature_cols]\n",
    "target = train_ds['target']\n",
    "\n",
    "# Функция для выделения временных интервалов из таблиц признаков и целей\n",
    "# на этом этапе отбрасываем колонку 'date'\n",
    "def features_interval(features, target, date1, date2):\n",
    "    features_interval = features[ (features['date']>=date1) & (features['date']<date2) ]\n",
    "    target_interval = target[features_interval.index]\n",
    "    features_interval = features_interval.drop('date', axis=1)\n",
    "    return features_interval, target_interval\n",
    "\n",
    "\n",
    "# для проверки на тестовой выборке будем учиться на всем тренировочном датасете\n",
    "\n",
    "features_open_test, target_open_test = features_interval(features, target, open_test_begin, open_test_end)\n",
    "\n",
    "\n",
    "features_open_test_sum, target_open_test_sum = features_interval(features, target, '2023-06-01', open_test_end)\n",
    "\n",
    "\n",
    "\n",
    "FEATURES = 'test_summer_fulltest'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967384f",
   "metadata": {},
   "source": [
    "### 2. Обучение моделей\n",
    "\n",
    "В настоящей работе обучается модель LightGBM\n",
    "\n",
    "#### 2.1 Гиперпараметры\n",
    "Были подобраны следующие значения гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves':15, 'learning_rate':0.02, 'feature_fraction':1, 'num_iterations':NUM_ITERATIONS, 'random_state':random_state, 'objective':'regression_l1', 'n_jobs':-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d16232",
   "metadata": {},
   "source": [
    "## XGBoost build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_train, target_train\n",
    "#features_valid, target_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3f1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f69ef2",
   "metadata": {},
   "source": [
    "### 4. Проверка метрик на тестовом датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca38f19",
   "metadata": {},
   "source": [
    "#### 4.1 LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [#'preholidays',\n",
    "            #'has_rain_probability', 'W', 'E',\n",
    "            #'holidays' \n",
    "            ]\n",
    "feat_lgbm_train = features_all_train.drop(columns=drop_list)\n",
    "feat_lgbm_test = features_open_test.drop(columns=drop_list)\n",
    "feat_lgbm_train.columns, feat_lgbm_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88929019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка метрики лучшей модели на тестовом датасете\n",
    "# Здесь обучаем на всем тренировочном датасете\n",
    "\n",
    "lgbm_model_all_train = lgb.Booster(model_file='models/lgb_model.txt')\n",
    "\n",
    "l_predict_train = lgbm_model_all_train.predict(feat_lgbm_train)\n",
    "l_predict_test = lgbm_model_all_train.predict(feat_lgbm_test)\n",
    "\n",
    "mae_train, mape_train, r2_train = metrics_hour(target_all_train, l_predict_train)\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test, l_predict_test)\n",
    "\n",
    "results = pd.DataFrame([[f'тренировочная LGBM {FEATURES}', mae_train, mape_train, r2_train], [f'тестовая LGBM {FEATURES}', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка метрики лучшей модели на тестовом датасете\n",
    "# Здесь обучаем на всем тренировочном датасете\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "l_predict_test = lgbm_model_all_train.predict(features_open_test_sum)\n",
    "\n",
    "\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test_sum, l_predict_test)\n",
    "\n",
    "results = pd.concat([results,\n",
    "pd.DataFrame([ [f'тестовая LGBM_only summer {FEATURES}', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47121cb",
   "metadata": {},
   "source": [
    "#### 4.2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f228a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8dba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['preholidays_true',\n",
    "            'has_rain_probability', \n",
    "            #'holidays', 'preholidays',\n",
    "             'W', 'E'\n",
    "            ]\n",
    "feat_xgb_train = features_all_train.drop(columns=drop_list)\n",
    "feat_xgb_test = features_open_test.drop(columns=drop_list)\n",
    "feat_xgb_train.columns, feat_xgb_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bab5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['preholidays_true',\n",
    "            #'has_rain_probability', \n",
    "            # 'W', 'E'\n",
    "            ]\n",
    "n_values = range(1, 24)\n",
    "preholidays = ['preholidays_true_{}'.format(n) for n in n_values]\n",
    "#has_rain = ['has_rain_probability_{}'.format(n) for n in n_values]\n",
    "#W_wind = ['W_{}'.format(n) for n in n_values]\n",
    "#E_wind = ['E_{}'.format(n) for n in n_values]\n",
    "\n",
    "drop_list = drop_list + preholidays #+ has_rain + W_wind + E_wind\n",
    "\n",
    "feat_xgb_train = features_all_train.drop(columns=drop_list)\n",
    "feat_xgb_test = features_open_test.drop(columns=drop_list)\n",
    "feat_xgb_train.columns, feat_xgb_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка метрики лучшей модели на тестовом датасете\n",
    "# Здесь обучаем на всем тренировочном датасете\n",
    "\n",
    "\n",
    "xgb_model_all_train = XGBRegressor()\n",
    "\n",
    "# загружаем модель из файла\n",
    "xgb_model_all_train.load_model('models/xgb_modelbase feature_3.json')\n",
    "\n",
    "xgb_predict_test = xgb_model_all_train.predict(feat_xgb_test)\n",
    "xgb_predict_train = xgb_model_all_train.predict(feat_xgb_train)\n",
    "\n",
    "mae_train, mape_train, r2_train = metrics_hour(target_all_train, xgb_predict_train )\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test, xgb_predict_test )\n",
    "\n",
    "results = pd.concat([results,\n",
    "pd.DataFrame([[f'тренировочная XGB {FEATURES}', mae_train, mape_train, r2_train], [f'тестовая XGB {FEATURES}', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))\n",
    " ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predict_test = xgb_model_all_train.predict(feat_xgb_test)\n",
    "\n",
    "\n",
    "mae_train, mape_train, r2_train = metrics_hour(target_all_train, xgb_predict_train )\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test, xgb_predict_test )\n",
    "\n",
    "results = pd.concat([results,\n",
    "pd.DataFrame([[f'тренировочная XGB {FEATURES}', mae_train, mape_train, r2_train], [f'тестовая XGB {FEATURES}', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))\n",
    " ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9b1d1",
   "metadata": {},
   "source": [
    "results.to_csv(f'results_LGBM_XGBoost_{FEATURES}_no_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e9496",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e232b25",
   "metadata": {},
   "source": [
    "### 5. Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ensemble = pd.DataFrame(columns=('Выборка', 'MAE', 'MAPE', 'R2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c8a35",
   "metadata": {},
   "source": [
    "### Simple Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15100001",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_simple_ensemble_train = (xgb_predict_train + l_predict_train)/2\n",
    "predict_simple_ensemble_test = (xgb_predict_test + l_predict_test)/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train, mape_train, r2_train = metrics_hour(target_all_train, predict_simple_ensemble_train)\n",
    "mae_open_test, mape_open_test, r2_open_test = metrics_hour(target_open_test, predict_simple_ensemble_test)\n",
    "\n",
    "results_ensemble = pd.concat([results_ensemble,\n",
    "pd.DataFrame([[f'тренировочная simple_ensemble  {FEATURES}', mae_train, mape_train, r2_train], [f'тестовая simple_ensemble {FEATURES}', mae_open_test, mape_open_test, r2_open_test]], \n",
    "             columns=('Выборка', 'MAE', 'MAPE', 'R2'))\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_ensemble)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864aa633",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_day(target_open_test, predict_simple_ensemble_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7f458",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# определите путь к папке, которую вы хотите создать\n",
    "folder_path = \"models\"\n",
    "\n",
    "# проверьте, существует ли уже папка\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "lgbm_model_all_train.booster_.save_model('models/lgb_model.txt')\n",
    "xgb_model.save_model('models/xgb_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(f'models/results_LGBM_XGBoost_simple_ensemble_{FEATURES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ace227",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
