{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgbm = {'num_leaves': 34, 'min_child_samples': 16, \n",
    "          'max_depth': 8, 'learning_rate': 0.012, \n",
    "          'min_sum_hessian_in_leaf': 1e-4,\n",
    "          'objective': 'regression_l1', 'feature_fraction': 0.9574152630927155,\n",
    "          'n_jobs':-1, 'num_iterations':5000\n",
    "          }\n",
    "def objective(trial):\n",
    "    \n",
    "\n",
    "    params_enet = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 1.0),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    }\n",
    "\n",
    "    N_TRAIN = 0.85\n",
    "    # Определение моделей\n",
    "    lgbm_model = LGBMRegressor(**params_lgbm)\n",
    "\n",
    "\n",
    "    # Определение мета-модели (можно использовать любую модель; в этом примере используется ElasticNet)\n",
    "    meta_model = ElasticNet(alpha=params_enet['alpha'],l1_ratio=params_enet['l1_ratio'])\n",
    "\n",
    "    # Создание стекинг-регрессора\n",
    "    stacking = StackingRegressor(estimators=[('lgbm', lgbm_model)],\n",
    "                                                final_estimator=meta_model, \n",
    "                                               \n",
    "                                                n_jobs= -1\n",
    "                                                )\n",
    "   \n",
    "    \n",
    "    X_train, X_test = feat_lgbm_train[:int(len(feat_lgbm_train)*N_TRAIN)], feat_lgbm_train[int(len(feat_lgbm_train)*N_TRAIN):]\n",
    "    y_train, y_test = target_all_train[:int(len(feat_lgbm_train)*N_TRAIN)], target_all_train[int(len(feat_lgbm_train)*N_TRAIN):]\n",
    "\n",
    "\n",
    "    stacking.fit(X_train, y_train)\n",
    "    y_pred = stacking.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = r2_score(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    trial.set_user_attr('MAE', mae)\n",
    "    trial.set_user_attr('RMSE', rmse)\n",
    "   \n",
    "\n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100, n_jobs = -1, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'max_depth': [7],  # или другие значения, которые вы хотите проверить\n",
    "    'n_estimators': [195],  # или другие значения\n",
    "    'learning_rate': [0.1],  # или другие значения\n",
    "    'tree_method': ['exact'],  # или другие значения\n",
    "    'objective': ['reg:squarederror'],  # или другие значения\n",
    "    'eval_metric': ['rmse'],  # или другие значения\n",
    "    'gamma': [2],  # или другие значения\n",
    "    'colsample_bytree': [1],  # или другие значения\n",
    "    'random_state': [random_state]  # или другие значения\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    params_enet = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 1.0),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    }\n",
    "\n",
    "    N_TRAIN = 0.85\n",
    "    # Определение моделей\n",
    "    lgbm_model = LGBMRegressor(**params_lgbm)\n",
    "    xgb_model = XGBRegressor(**params_xgb)\n",
    "\n",
    "\n",
    "    # Определение мета-модели (можно использовать любую модель; в этом примере используется ElasticNet)\n",
    "    meta_model = ElasticNet(alpha=params_enet['alpha'],l1_ratio=params_enet['l1_ratio'])\n",
    "\n",
    "    # Создание стекинг-регрессора\n",
    "    stacking = StackingRegressor(estimators=[('lgbm', lgbm_model), ('xgb', xgb_model)],\n",
    "                                                final_estimator=meta_model, \n",
    "                                                n_jobs = -1\n",
    "                                                )\n",
    "   \n",
    "    \n",
    "    X_train, X_test = feat_lgbm_train[:int(len(feat_lgbm_train)*N_TRAIN)], feat_lgbm_train[int(len(feat_lgbm_train)*N_TRAIN):]\n",
    "    y_train, y_test = target_all_train[:int(len(feat_lgbm_train)*N_TRAIN)], target_all_train[int(len(feat_lgbm_train)*N_TRAIN):]\n",
    "\n",
    "\n",
    "    stacking.fit(X_train, y_train)\n",
    "    y_pred = stacking.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = r2_score(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    trial.set_user_attr('MAE', mae)\n",
    "    trial.set_user_attr('RMSE', rmse)\n",
    "  \n",
    "    \n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "   \n",
    "    params_sgd = {\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet']),\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 1.0),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0),\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "        'max_iter': trial.suggest_int('max_iter', 1000, 5000),\n",
    "        'tol': trial.suggest_float('tol', 1e-5, 1e-1)\n",
    "    }\n",
    "\n",
    "    N_TRAIN = 0.85\n",
    "    # Определение моделей\n",
    "    lgbm_model = LGBMRegressor(**params_lgbm)\n",
    "    xgb_model = XGBRegressor(**params_xgb)\n",
    "\n",
    "\n",
    "    # Определение мета-модели (можно использовать любую модель; в этом примере используется ElasticNet)\n",
    "    meta_model = SGDRegressor(loss=params_sgd['loss'],\n",
    "                               penalty=params_sgd['penalty'],\n",
    "                               alpha=params_sgd['alpha'],\n",
    "                               l1_ratio=params_sgd['l1_ratio'],\n",
    "                               fit_intercept=params_sgd['fit_intercept'],\n",
    "                               max_iter=params_sgd['max_iter'],\n",
    "                               tol=params_sgd['tol'])\n",
    "\n",
    "    # Создание стекинг-регрессора\n",
    "    stacking = StackingRegressor(estimators=[('lgbm', lgbm_model), ('xgb', xgb_model)],\n",
    "                                                final_estimator=meta_model, \n",
    "                                                #cv=5,\n",
    "                                                n_jobs= -1\n",
    "                                                )\n",
    "   \n",
    "    \n",
    "    X_train, X_test = feat_lgbm_train[:int(len(feat_lgbm_train)*N_TRAIN)], feat_lgbm_train[int(len(feat_lgbm_train)*N_TRAIN):]\n",
    "    y_train, y_test = target_all_train[:int(len(feat_lgbm_train)*N_TRAIN)], target_all_train[int(len(feat_lgbm_train)*N_TRAIN):]\n",
    "\n",
    "\n",
    "    stacking.fit(X_train, y_train)\n",
    "    y_pred = stacking.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = r2_score(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    trial.set_user_attr('MAE', mae)\n",
    "    trial.set_user_attr('RMSE', rmse)\n",
    "   \n",
    "    \n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=150, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "   \n",
    "    params_xgb = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "        'tree_method': trial.suggest_categorical('tree_method', ['approx', 'auto', 'exact', 'hist']),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 1]),\n",
    "        'gamma': trial.suggest_int('gamma', 0, 10),\n",
    "    }\n",
    "\n",
    "\n",
    "    N_TRAIN = 0.85\n",
    "    # Определение моделей\n",
    "    lgbm_model = LGBMRegressor(**params_lgbm)\n",
    "    xgb_model = XGBRegressor(**params_xgb)\n",
    "\n",
    "\n",
    "    # Определение мета-модели (можно использовать любую модель; в этом примере используется ElasticNet)\n",
    "    meta_model = xgb_model(max_depth=params_xgb['max_depth'],\n",
    "                               n_estimators=params_xgb['n_estimators'],\n",
    "                               learning_rate=params_xgb['learning_rate'],\n",
    "                               tree_method=params_xgb['tree_method'],\n",
    "                               fit_intercept=params_xgb['fit_intercept'],\n",
    "                               colsample_bytree=params_xgb['colsample_bytree'],\n",
    "                               gamma=params_xgb['gamma'])\n",
    "\n",
    "    # Создание стекинг-регрессора\n",
    "    stacking = StackingRegressor(estimators=[('lgbm', lgbm_model), ('xgb', xgb_model)],\n",
    "                                                final_estimator=meta_model, \n",
    "                                                #cv=5,\n",
    "                                                passthrough=True,\n",
    "                                                n_jobs= -1\n",
    "                                                )\n",
    "   \n",
    "    \n",
    "    X_train, X_test = feat_lgbm_train[:int(len(feat_lgbm_train)*N_TRAIN)], feat_lgbm_train[int(len(feat_lgbm_train)*N_TRAIN):]\n",
    "    y_train, y_test = target_all_train[:int(len(feat_lgbm_train)*N_TRAIN)], target_all_train[int(len(feat_lgbm_train)*N_TRAIN):]\n",
    "\n",
    "\n",
    "    stacking.fit(X_train, y_train)\n",
    "    y_pred = stacking.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = r2_score(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    trial.set_user_attr('MAE', mae)\n",
    "    trial.set_user_attr('RMSE', rmse)\n",
    "   \n",
    "    \n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100, n_jobs = -1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
